In other words , when there is sufficient evidence to compute term weights for words , we should do so , but in other cases , we should back off and use bins . These changes may improve our system , which is already faring well against competitors . Additional information about this work can be found at _CITE_ , and we will continue to expand this page as our research continues .__label__Supplement|Document|Produce
System administrators from the Texas Advanced Computing Center ( TACC ) at The University of Texas at Austin made configuration changes on our request . This work made use of the resources provided by the Edinburgh Compute and Data Facility ( http :// www . ecdf . ed . ac . uk /). The ECDF is partially supported by the eDIKT initiative ( _CITE_ ). The research leading to these results has received funding from the European Union Seventh Framework Programme ( FP7 / 2007 - 2013 ) under grant agreement 287658 ( EU BRIDGE ).__label__Supplement|License|Other
We use a complex four - class classification problem , where new tweets can be assigned to the classes “ crash ”, “ fire ”, “ shooting ”, and a neutral class “ not incident related ”. This goes beyond related work with a focus on two - class classification . Our classes were identified as the most common incident types in Seattle using the Fire Calls data set ( _CITE_ ), an official incident information source . As ground truth data , we collected several cityspecific datasets using the Twitter Search API . These datasets were collected in a 15 km radius around the city centres of Boston ( USA ), Brisbane ( AUS ), Chicago ( USA ), Dublin ( IRE ), London ( UK ), Memphis ( USA ), New York City ( USA ), San Francisco ( USA ), Seattle ( USA ), Sydney ( AUS ).__label__Material|Data|Use
Lexique Pro , an excellent free tool by SIL International ( http :// www . lexiquepro . com /) is smaller , but there are differences in that Lexique Pro is a Windows - only closed - source program whose native MDF ( Multi - Dictionary - Formatter ) format is not as flexible as XML and therefore cannot be processed by the many tools that handle XML and TEI in particular . Consequently , the perspectives for re - use of Lexique Pro dictionaries in computational linguistic applications are much smaller . To our knowledge , Lexique Pro does not make it possible for users to query words straight off web pages , which can be done thanks to dict , a Firefox add - on ( _CITE_ ). It admittedly has other advantages that make it a serious alternative . 7 The ideal solution would be to have an editing front - end such as Lexique Pro coupled with the openness and modifiability of the data offered by FreeDict . Indeed , there are plans for creating a converter from the new LIFT interchange standard ( http :// code . google . com / p / lift - standard /) that the beta versions of Lexique Pro can read 7 We do not discuss professional commercial dictionary writing systems such as TshwaneLex ( http :// tshwanedje . com / tshwanelex /) because , despite the academic discounts , they may be out of range for the average developer .__label__Method|Tool|Use
However , the Manipuri news is available only in PDF format from these websites . A web based Manipuri news corpus collection is reported ( Singh and Bandyopadhyay , 2010a ) using the Bengali script in Unicode format . The resource constrained Manipuri language news corpus is collected from _CITE_ At present , there is a Manipuri news monolingual corpus of 4 million wordforms in Bengali script Unicode format . Our experiment makes use of this corpus on news domain .__label__Material|Data|Use
This work is licenced under a Creative Commons Attribution 4 . 0 International License . Page numbers and proceedings footer are added by the organizers . License details : _CITE___label__Supplement|License|Other
Box 5 has a different structural status than the other boxes : it marks the ends of sentences . We included box 5 , placing it in the center of the visual array , and making it smaller than the other boxes , to make the task easier relative to a pilot version in which Box 5 was absent . 2 . 1 Simulation Experiment We employed Michal Cernansky ’ s implementation of Elman ( 1990 )’ s Simple Recurrent Network ( _CITE_ ). The network had five input units , five output units and ten hidden units . Activations changed as specified in ( 1 ) and weights changed according to ( 2 ).__label__Method|Code|Use
LanguageTool was developed by Naber ( 2003 ). It can run as a stand - alone program and as an extension for OpenOffice . Org1 and LibreOffice2 . LanguageTool is distributed through LanguageTool ’ s website : _CITE___label__Method|Tool|Produce
LanguageTool was developed by Naber ( 2003 ). It can run as a stand - alone program and as an extension for OpenOffice . Org1 and LibreOffice2 . LanguageTool is distributed through LanguageTool ’ s website : _CITE___label__Supplement|Website|Produce
The task extends considerably on core entities , adding to PROTEIN four new entity types , including CHEMICAL and ORGANISM . The events extend on the GE definitions in allowing arguments of the new entity types as well as in introducing a new event category for high - level biological processes . The task was implemented in collaboration with domain experts and informed by prior studies on domain information extraction requirements ( Pyysalo et al ., 2010 ; Ananiadou et al ., 2011 ), including the support of systems such as PATRIC ( _CITE_ ).__label__Method|Tool|Extent
A peculiar characteristic of a Twitter data are as follow : emoticons are widely used , the This work is licenced under a Creative Commons Attribution 4 . 0 International License . Page numbers and proceedings footer are added by the organizers . License details : _CITE_ org / licenses / by / 4 . 0 / maximum length of a tweet is 140 character , some words are abbreviated , or some are elongated by repeating letters of a word multiple times . The organizers of the SemEval - 2014 has provided a corpus of tweets and posted a task to automatically detect their respective sentiments .__label__Supplement|License|Other
Also , we believe that simpler , heuristic approaches could be used to identify such sentences . We use “ GUG ” (“ Grammatical ” versus “ UnGrammatical ”) to refer to this dataset . The dataset is available for research at _CITE_ gug - data .__label__Material|Data|Produce
Combining all edges ( SR + LM + SB ) does not influence the results any more , but in any case the hybrid configuration achieves the best overall recall ( 0 . 87 ). In conclusion , our experiments on all four datasets consistently demonstrate that combining Dijkstra - WSA with a similarity - based approach as a back - off yields the strongest performance . The results of these best alignments will be made freely available to the research community on our website ( _CITE_ ).__label__Supplement|Website|Produce
Combining all edges ( SR + LM + SB ) does not influence the results any more , but in any case the hybrid configuration achieves the best overall recall ( 0 . 87 ). In conclusion , our experiments on all four datasets consistently demonstrate that combining Dijkstra - WSA with a similarity - based approach as a back - off yields the strongest performance . The results of these best alignments will be made freely available to the research community on our website ( _CITE_ ).__label__Supplement|Document|Produce
However , in contrast to our implementation and that of Shafran et al ( 2011 ), no expansion into an WFST with aligned input / output is described . Lexicographic semirings , used for PoS tagging disambiguation ( Shafran et al ., 2011 ), have been also shown to be useful in other tasks ( Sproat et al ., 2014 ), such as optimized epsilon encoding for backoff language models ( Roark et al ., 2011 ), and hierarchical phrase - based decoding with Pushdown Automata ( Allauzen et al ., 2014 ). The tools for disambiguation and WFST composition with bilingual models , along with a tutorial to replicate Section 4 . 2 , are all available at _CITE___label__Method|Tool|Produce
However , in contrast to our implementation and that of Shafran et al ( 2011 ), no expansion into an WFST with aligned input / output is described . Lexicographic semirings , used for PoS tagging disambiguation ( Shafran et al ., 2011 ), have been also shown to be useful in other tasks ( Sproat et al ., 2014 ), such as optimized epsilon encoding for backoff language models ( Roark et al ., 2011 ), and hierarchical phrase - based decoding with Pushdown Automata ( Allauzen et al ., 2014 ). The tools for disambiguation and WFST composition with bilingual models , along with a tutorial to replicate Section 4 . 2 , are all available at _CITE___label__Method|Algorithm|Produce
However , in contrast to our implementation and that of Shafran et al ( 2011 ), no expansion into an WFST with aligned input / output is described . Lexicographic semirings , used for PoS tagging disambiguation ( Shafran et al ., 2011 ), have been also shown to be useful in other tasks ( Sproat et al ., 2014 ), such as optimized epsilon encoding for backoff language models ( Roark et al ., 2011 ), and hierarchical phrase - based decoding with Pushdown Automata ( Allauzen et al ., 2014 ). The tools for disambiguation and WFST composition with bilingual models , along with a tutorial to replicate Section 4 . 2 , are all available at _CITE___label__Supplement|Document|Produce
Ancestor F1 : Measures the precision , recall , and F1 = 2PR /( P + R ) of correctly predicted ances12This is somewhat different from our general setup where we work with any given set of terms ; they start with a large set of leaves which have substantial Web - based relational information based on their selected , hand - picked patterns . Their data is available at _CITE_ downloads . html .__label__Material|Data|Use
Examples of the use of JSrealB , and a webbased development environment are available at : _CITE_ jsrealb - bilingual - text - realiser The javascript code of the realizer , the lexicon and tables are made available to the NLG community at : https :// github . com / rali - udem / JSrealB__label__Method|Tool|Produce
Examples of the use of JSrealB , and a webbased development environment are available at : _CITE_ jsrealb - bilingual - text - realiser The javascript code of the realizer , the lexicon and tables are made available to the NLG community at : https :// github . com / rali - udem / JSrealB__label__Supplement|Document|Produce
Overall and pairwise agreement lied within the range set by current related literature . From the four sets of annotations , we built a gold standard , where paragraphs were classified according to the opinion of the majority of annotators . This gold standard and annotated corpus , by all four annotators , are available to the community under a Creative Commons licence at _CITE_ We hope our efforts to be useful to other researchers in a number of ways , from deeper studies related to news texts to the application of machine learning techniques , also serving as a common ground for comparison amongst research that build on our corpus and gold standard . As for future work , we intend to use this corpus as one of the variables necessary to identify bias in newswire outlets , thereby determining not only if news from some outlet is biased , but also allowing for the identification of the way this bias is introduced in texts .__label__Material|Data|Produce
a set of directed , binary dependency relations holding exclusively between lexical units . This conversion is defined by Ivanova et al . ( 2012 ) and seeks to ( a ) project some aspects of construction semantics onto word - to - word dependencies ( for example introducing specific dependency types for compounding or implicit conjunction ) and ( b ) relate the linguistically informed ERG - internal tokenization to the conventions of the PTB . 3 Seeing as both is called the LOGON SVN trunk as of January 2014 ; see _CITE_ for detail . 2Conversely , semantically vacuous parts of the original input ( e . g . infinitival particles , complementizers , relative pronouns , argument - marking prepositions , auxiliaries , and most punctuation marks ) were not represented in the MRS in the first place , hence have no bearing on the conversion .__label__Method|Algorithm|Introduce
For the English and Swedish data sets , we obtained approval from the University of Pittsburgh IRB and the Regional Ethical Review Board in Stockholm ( Etikpr ¨ ovningsn ¨ amnden i Stockholm ). The study is part of the Interlock project , funded by the Stockholm University Academic Initiative and partially funded by NLM Fellowship 5T15LM007059 . Lexicons and probabilities will be made available and updated on the iDASH NLP ecosystem under Resources : _CITE___label__Supplement|Document|Produce
version has been already publically released . The whole VALLEX 1 . 0 can be downloaded from the Internet after filling the on - line registration form at the following address : _CITE_ From the very beginning , VALLEX 1 . 0 was designed with an emphasis on both human and machine readability . Therefore both linguists and developers of applications within the Natural Language Processing domain can use and critically evaluate its content . In order to satisfy different needs of these different potential users , VALLEX 1 . 0 contains the data in the following three formats : Browsable version .__label__Method|Algorithm|Produce
The meanings of the emoticons were taken from http :// en . wikipedia . org / wiki / List_of_emoticons . - Replacing acronyms : Each acronym is replaced by its meaning . The meanings of the acronyms were taken from _CITE___label__Supplement|Document|Extent
“−” denotes the data set with punctuations removed . reported so far . Our code will be available at _CITE___label__Method|Code|Produce
As a research testbed and target resource to expand / domain - tune , we use the LinGO English Resource Grammar ( LinGOERG ), a linguistically - precise HPSG - based grammar under development at CSLI ( Copestake and Flickinger , 2000 ; Flickinger , 2000 ). The particular MWE type we target for extraction is the English verb - particle construction . Verb - particle constructions (“ VPCs ”) consist of a l_CITE_ head verb and one or more obligatory particles , in the form of intransitive prepositions ( e . g . hand in ), adjectives ( e . g . cut short ) or verbs ( e . g .__label__Method|Algorithm|Introduce
clusters : The graph shows the cosine similarity between today ’ s English language cluster ( Final hole being drilled ...) and seven clusters identified during five previous days . Only clusters with a similarity above 0 . 5 will be retained . ( _CITE_ ) do not link related news over time either . NewsTin ( http :// www . newstin . com ) is the only one to offer more languages ( ten ) and to categorise news into a number of broad categories , but they , again , do not link related news over time or across languages .__label__Method|Tool|Introduce
This gives a higher weight to the extended word and retains its contribution to the sentiment of the tweet . Chat lingo normalization : Words used in chat / Internet language that are common in tweets are not present in the lexical resources . We use a dictionary downloaded from _CITE_ . A chat word is replaced by its dictionary equivalent .__label__Material|Data|Use
1 The windows is the number of intermediate words between two words . 2 Dataset of high quality English paragraphs containing over three billion words and it is available in _CITE___label__Material|Data|Use
DA refers to the spoken language used for daily communication in Arab countries . There are considerable geographical distinctions between DAs within countries , across country borders , and even between cities and villages as shown in Figure 1 . According to Ethnologue ( _CITE_ ), there are 34 variations of spoken Arabic or dialects in Arabic countries in addition to the Modern Standard Arabic ( MSA ). Some recent works ( Zbib et al ., 2012 ; Cotterell et al ., 2014 ) are based on a coarser classification of Arabic dialects into five groups namely : Egyptian ( EGY ), Gulf ( GLF ), Maghrebi ( MGR ), Levantine ( LEV ), and Iraqi ( IRQ ). Other dialects are classified as OTHER .__label__Supplement|Website|Extent
The ASR check condition also involved issuing a warning flag , but in contrast with the previous condition , every audio segment was compared to an expected input , and this time the expected transcript was produced by ASR . String overlap was also calculated using Equation 1 , but to account for the higher WER for the ASR output than a human transcript , we lowered the threshold of overlap to 20 % in comparison with the 30 % overlap for expert - produced transcripts . 1JavaScript implementation taken from : _CITE_ A total of 149 users participated in the transcription tasks of EGY audio . The average WER for each user was calculated based on comparing each transcript to the four other user - provided transcripts for each item . As shown in Figure 3 , there were different distributions of above - average and below - average users across conditions .__label__Method|Code|Use
In the Filter stage , the language and a hint of the polarity of a microblog post are detected based on the This work is licenced under a Creative Commons Attribution 4 . 0 International License . Page numbers and proceedings footer are added by the organizers . License details : _CITE___label__Supplement|License|Other
Its output is a ( time - varying ) 3d model that can be displayed by Partiview , an external data viewer . Future plans include adding more scalable embedding algorithms , and allowing other output formats . Ndaona , documentation , and examples of models created with it , can be found at _CITE___label__Method|Tool|Produce
Its output is a ( time - varying ) 3d model that can be displayed by Partiview , an external data viewer . Future plans include adding more scalable embedding algorithms , and allowing other output formats . Ndaona , documentation , and examples of models created with it , can be found at _CITE___label__Method|Algorithm|Produce
Its output is a ( time - varying ) 3d model that can be displayed by Partiview , an external data viewer . Future plans include adding more scalable embedding algorithms , and allowing other output formats . Ndaona , documentation , and examples of models created with it , can be found at _CITE___label__Supplement|Document|Produce
The starting point of the work reported here is Walenty , a valence dictionary for Polish described in Przepiórkowski et al . 2014 and available from _CITE_ ( see § 1 . 1 ). Walenty contains some valence schemata for verbal idioms ; e . g ., one of the schemata for xuc ‘ forge ’ says that it combines with a nominal subject , a nominal object and a prepositional phrase consisting of the preposition NA ` on ' and the accusative singular form of the noun PAMi � c ‘ memory ’ – this represents the idiom ktos kuje cos na pamigc ‘ somebody rote learns something ’, lit . ‘ somebody forges something onto memory ’.__label__Material|Data|Extent
We also define a number of markers of person ( such as Doctor , Engineer , etc .) and organization ( such as Corp .) names . We used the list of markers available at : _CITE_ 1439292 , that we extended manually . Contextual features : These features are related to the token ’ s local context within the NE . These include information about the current token ’ s surrounding tokens , its relative position in the NE ( beginning , middle or end ).__label__Material|Data|Use
Earlier work on Turkish indicates that starting with default Moses parameters and applying MERT to the resulting model does not even come close to the performance of the model with those two specific parameters set as such ( distortion limit - 1 and distortion weight 0 . 1 ), most likely because the default parameters do not encourage the range of distortions that are needed to deal with the constituent order differences . Earlier work on Turkish also shows that even when the weight - d parameter is initialized with this specific value , the space explored for distortion weight and other parameters do not produce any improvements on the test set , even though MERT claims there are improvements on the tune set . The other practical reasons for not using MERT were the following : at the time we performed this work , the discussion thread at _CITE_ indicated that MERT was not tested on multiple factors . The discussion thread at http :// www . mail - archive . com / moses - support @ mit . edu / msg00262 . html claimed that MERT does not help very much with factored models . With these observations , we opted not to experiment with MERT with the multiple factor approach we employed , given that it would be risky and time consuming to run MERT needed for 10 different models and then not necessarily see any ( consistent ) improvements .__label__Supplement|Document|Introduce
We have collected from Livejournal2 a total of 346723 weblogs ( mood - annotated by authors ) in z_CITE_ English , from which almost half are annotated with a mood belonging to one of the four quadrants , described as follows : Quadrant1 bellicose , tense , alarmed , envious , hateful , angry , enraged , defiant , annoyed , jealous , indignant , frustrated , distressed , disgusted , suspicious , discontented , bitter , insulted , distrustful , startled , contemptuous and impatient . Quadrant2 apathetic , disappointed , miserable , dissatisfied , taken aback , worried , languid , feel guilt , ashamed , gloomy , sad , uncomfortable , embarrassed , melancholic , depress , desperate , hesitant , bored , wavering , droopy , tired , insecured , anxious , lonely and doubtful .__label__Material|Data|Use
We hope our work will pave the way for new research on the generation and exploitation of large - scale sense - annotated corpora . Furthermore , our new type of pseudoword might also be used for a realistic , wide - coverage evaluation of other difficult tasks such as Word Sense Induction ( Bordag 2006 ; Di Marco and Navigli 2013 ; Navigli and Vannella 2013 ), Entity Linking ( Moro , Raganato , and Navigli 2014 ) and selectional preference acquisition ( Chambers and Jurafsky 2010 ; Erk , Pad ´ o , and Pad ´ o 2010 ), among others . We are releasing to the research community the entire set of 15 , 935 pseudowords of WordNet 3 . 0 polysemous nouns , including those selected for our WSD experiments ( _CITE_ ). Together with the pseudosense - annotated corpus , this will allow for future experimental comparisons and studies with other WSD systems , also in other languages . In fact , our pseudowords and our WSD framework are not language - dependent and can readily be applied to other languages with the help of multilingual semantic networks such as BabelNet ( Navigli and Ponzetto 2012a ) and the use of multilingual WSD algorithms ( Moro , Raganato , and Navigli 2014 ).__label__Material|Data|Produce
We are also indebted to the numerous researchers worldwide who helped us by providing advice , data , and documentation , and by proofreading the multitagged Corpus and MultiTreebank . This COLING Workshop paper is an an abridged version of a full paper published in ICAME Journal , ( Atwell et al 2000 ); we are grateful for the Journal ’ s permission to present our findings to this complementary Workshop audience . To get the full ICAME Journal paper , see _CITE___label__Supplement|Paper|Introduce
are added by the organizers . License details : _CITE___label__Supplement|License|Other
To avoid inconsistency between vote and perspective , we use data from pro - choice and pro - life nongovernmental organizations , NARAL and NRLC , that track legislators ’ votes on abortion - related bills , showing the percentage of times a legislator supported the side the organization deems consistent with its perspective . We removed 22 legislators with a mixed record , that is , those who gave 20 - 60 % support to one of the positions . 2 Death Penalty ( DP ) blogs : We use University of Maryland Death Penalty Corpus ( Greene and Resnik , 2009 ) of 1085 texts from a number of proand anti - death penalty websites . We report 4 - fold cross - validation ( DP - 4 ) using the folds in Greene and Resnik ( 2009 ), where training and testing data come from different websites for each of the sides , as well as 10 - fold cross - validation performance on the entire corpus , irrespective of the site . 3 Bitter Lemons ( BL ): We use the GUEST part of the BitterLemons corpus ( Lin et al ., 2006 ), containing 296 articles published in 2001 - 2005 on _CITE_ by more than 200 different Israeli and Palestinian writers on issues related to the conflict . Bitter Lemons International ( BL - I ): We collected 150 documents each by a different per__label__Material|Data|Use
We test its reliability on the WordNet sense inventory . Overall , the experimental results show high agreement , confirming our hypothesis that agreement at sense level might be higher than at the word level . The annotated sense inventory will be made publically available to other researchers at _CITE_ The remainder of this paper is organized as follows . Section 2 discusses previous related work .__label__Material|Data|Produce
Compared to existing frameworks , the S - Space Package supports a much wider variety of algorithms and provides significantly more reusable developer utilities for word spaces , such as tokenizing and filtering , sparse vectors and matrices , specialized data structures , and seamless integration with external programs for dimensionality reduction and clustering . We hope that the release of this framework will greatly facilitate other researchers in their efforts to develop and validate new word space models . The toolkit is available at _CITE_ , which includes a wiki__label__Method|Tool|Produce
On the basis of functions such as these , it is possible to construct the DAG shown in Fig . 1 . 3 The unidirectional edges between the various 2A completion may be thought of as the semantic equivalent of an utterance , an entity as the semantic equivalent of a noun , and an action as the semantic equivalent of a verb . 3The DAGs shown here were constructed with yEd ( _CITE_ about . html ), which generates a GraphML representation for each graph . For simplicity , we have ignored representation of tense and aspect in these examples , although the formalism permits this .__label__Method|Tool|Use
Unlike most computational grammars , the Core defines analyses for phenomena not restricted to one language , but for the union of all languages for which c - profiles have been defined . ( In this respect it resembles the HPSG Grammar Matrix (‘ the Matrix ’ - see Bender et . al , and _CITE_ ); we comment on its relationship to this system below .) The mediation between the Core and the cprofiles is induced by special type files : - one file for each c - profile ( of which there are currently three , for Ga , Norwegian and Kistaninya ) - one general file , called Labeltypes , for defining CL labels as types in terms of the Core types . This architecture can be summed up as follows ( with ‘ Ga c - types ’ meaning ‘ types corresponding to the templates constituting the c - profile for Ga ’, and items in boldface being items defined inside the TypeGram system ): Thus , what communicates between the Core and the construction specifications in the CL code is Labeltypes , which in turn feeds into the language specific template definition files .__label__Method|Algorithm|Introduce
In the US DARPA Communicator project which addresses spoken language and multimodal dialogue systems , for instance , all participants start from shared core technologies without having to build these themselves ( http :// fofoca . mitre . org /). In the German SmartKom project which addresses multimodal communication systems , the budget is large enough for the participants to build and integrate the technologies needed ( _CITE_ ). In the European Intelligent Information Interfaces ( i3 , http :// www . i3net . org /) and CLASS ( http :// www . class - tech . org /) initiatives , whilst the traditional 3 - year small - scale project topology has been preserved , major efforts are being made to promote cross - project collaboration , synergy , and critical mass . For reasons too obvious to mention , relatively small - scale research should continue to exist , of course .__label__Method|Tool|Introduce
This mapping is described in Henrich et al . ( 2011 ). The resulting resource consists of a web - harvested corpus WebCAGe ( short for : Web - Harvested Corpus Annotated with GermaNet Senses ), which is freely available at : _CITE_ The remainder of this paper is structured as follows : Section 2 provides a brief overview of the resources GermaNet and Wiktionary . Section 3 introduces the mapping of GermaNet to Wiktionary and how this mapping can be used to automatically harvest sense - annotated materials from the web . The algorithm for identifying the target words in the harvested texts is described in Section 4 .__label__Material|Data|Use
We thank Shahar Maoz , Rami Marelly , Yoav Goldberg and three anonymous reviewers for their insightful comments on an earlier draft . This research was supported by an Advanced Research Grant to D . Harel from the European Research Council ( ERC ) under the European Community ‘ s Seventh Framework Programme ( FP7 / 2007 - 2013 ), and by a grant to D . Harel from the Israel Science Foundation ( ISF ). visual editor are available via _CITE___label__Method|Tool|Produce
The Gaussian prior was set to 0 . 1 , and the maximum training iterations to 100 . In order to assess the performance of the final system , MAXENT was compared with support vector machines ( SVM ) using the SVMl ' ght toolkit ( Joachims , 1999 ). Since both the classifiers assign a confidence to each prediction , a varying threshold can be applied to the output of the classifier to provide a precision - recall 2_CITE_ tradeoff . Candidate relations were generated by considering entity pairs of the appropriate type , taking into account the distance between the entities . It was thought that inter - sentential and intra - sentential relations would require different feature sets and different models , so inter - and intra - sentential candidates were generated separately .__label__Method|Tool|Compare
After the SemEval task , we crawled the full articles from _CITE_ , cleaned the corpus and annotated it with the exact publication date of the article , its title and the URL from which it was retrieved . The Daikon Corpus is made up of articles from the British Spectator news magazine from year 828 to 2008 . The Daikon corpus can be used for future diachronic studies and epoch identification tasks ; it provides a complementary dataset to the gold standard provided by task .__label__Material|Data|Use
The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers ( Wu et al . 2013 ). The collected data set is publicly available at _CITE_ The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers . Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors ( Chen et al ., 2013 ).__label__Material|Data|Produce
In this paper , we propose an approach for designing the confirmation strategies and implementing error recover and user - modelling techniques in a Railway Information system . During last decade , the performance of spoken dialogue system for traveling information has improved substantially . One spoken dialogue project is the DARPA Communicator _CITE_ ( Pellom , 2000 ; Rudnicky , 2000 ) that enables to access information about airline flights , hotels and rental cars . In Europe , one important project concerning railway information is ARISE ( Lamel , 2000 ; Baggia , 2000 ).__label__Method|Tool|Introduce
Systems that can produce an appropriate semantic representation for a TTS are not many at an international level but they can be traced from the results of a Shared Task organized by members of SigSem and are listed here below in the corresponding webpage _CITE_ _2008_shared_task : _comparing_semantic_repre sentations ( see Bos & Delmonte , 2008 ). State of the art semantic systems are based on different theories and representations , but the final aim of the workshop was reaching a consensus on what constituted a reasonably complete semantic representation . Semantics in our case not only refers to predicate - argument structure , negation scope , quantified structures , anaphora resolution and other similar items , it refers essentially to a propositional level analysis .__label__Supplement|Website|Produce
Systems that can produce an appropriate semantic representation for a TTS are not many at an international level but they can be traced from the results of a Shared Task organized by members of SigSem and are listed here below in the corresponding webpage _CITE_ _2008_shared_task : _comparing_semantic_repre sentations ( see Bos & Delmonte , 2008 ). State of the art semantic systems are based on different theories and representations , but the final aim of the workshop was reaching a consensus on what constituted a reasonably complete semantic representation . Semantics in our case not only refers to predicate - argument structure , negation scope , quantified structures , anaphora resolution and other similar items , it refers essentially to a propositional level analysis .__label__Supplement|Document|Produce
In the preliminary experiment , 27 , 239 Thai utterances with a mix of sentences and phrases from a general domain corpus are tested . The input was word - segmented by JwordSeg ( _CITE_ ) and approved by linguists . In the test corpus , the longest utterance contains seventeen words , and the shortest utterance contains two words .__label__Method|Tool|Use
digestive endoscopy ). The data are thus homogeneous , and is about 25 0 pages long ( 15 0 , 000 + words ). Most of these practice guidelines are publicly available at : _CITE_ or http :// affsaps . sante . fr . Similar documents have been published in English and other languages ; the GEM DTD is languageindependent .__label__Supplement|Document|Produce
Whilst these initiatives have made good progress on written language and current coding practice , none of them have focused on the creation of standards and tools for cross - level spoken language corpus annotation . It is only recently that there has been a major effort in this domain . The project Multi - level Annotation Tools Engineering ( MATE ) ( _CITE_ ) was launched in March 1998 in response to the need for standards and tools in support of creating , annotating , evaluating and exploiting spoken language resources . The central idea of MATE has been to work on both annotation theory and practice in order to connect the two through a flexible framework which can ensure a common and user - friendly approach across annotation levels . On the tools side , this means that users are able to use level - independent tools and an interface representation which is independent of the internal coding file representation .__label__Method|Tool|Introduce
Since Sinhala words have not been added to this synset , it shows the available information in the English WordNet . In addition , it shows suggested Sinhala words obtained from linguistic resources as described in Section 4 . 2 . The web - based user interface is operational and can be accessed from _CITE_ The modifications made by the contributors have to be approved by an evaluator before being included in a release . How to effectively use a crowdsourcing technique to get a particular task done with acceptable quality is an open research question .__label__Supplement|Website|Produce
The grammar is implemented in the LKB using the T DL formalism ( Krieger and Schäfer , 1994 ), based on unification and on typed feature structures , and whose types are organized in a multiple inheritance hierarchy . For more information , please refer to a detailed implementation report ( Branco and Costa , 2008a ) or on pages 31 – 43 of this volume ( Branco and Costa , 2008b ). A free version of the grammar can also be obtained at _CITE_ , under an ELDA research license . Section 2 introduces the main features of the Minimal Recursion Semantics format , which is employed in the semantic representations produced by LXGram . In Section 3 , the sample text that the LXGram team submitted is described , together with an explanation of the representations derived by the grammar .__label__Method|Algorithm|Produce
We have performed two Reliability Tests in order to 1 ) to check the transferability and applicability of MIPVU , which was originally designed for English , to Russian - language material and 2 ) to assess the reliability of MIPVU on Russianlanguage material by measuring the rate of interannotator agreement . The Reliability Tests had the following setup : – 3 annotators ( PhDs and current PhD students with prior experience in conceptual metaphor studies ); – a collection of 4 text excerpts ( 500 - 600 words each ), representing the 4 genres : fiction , transcribed spoken , popular science / academic , and news texts ; – POS - tagged files from the National Russian Corpus ( _CITE_ ) in xhtmlformat ; – 2 dictionaries used to define the word meanings : ( Dictionary of the Russian Language , 1981 - 1984 , Dictionary of the Russian Language , 1999 ). The inter - annotator agreement was measured by Fleiss ' kappa ( Artstein and Poesio , 2008 ) using binary classification , i . e . 1 for any metaphorrelated word and 0 for otherwise .__label__Material|Data|Use
Observe that when either the INTR or CAUS feature is removed ( rows 2 and 3 , respectively , of Table 2 ), performance degrades considerably , with a decrease in accuracy of 8 - 10 % from the maximum achieved with the four features ( row 1 ). However , when the VBD feature is removed ( row 4 ), there is a smaller decrease in accuracy , of 4 - 6 %. When the ACT feature is removed ( row 5 ), there is an Available for a number of platforms from _CITE_ 6A 10 - fold cross - validation means that the system randomly divides the data into ten parts , and runs ten times on a different 90 %- training - data / 10 %- test - data split , yielding an average accuracy and standard error . This procedure is then repeated for 10 different random divisions of the data , and accuracy and standard error are again averaged across the ten runs .__label__Supplement|Website|Use
The test datasets ( T06 , T07 ) were used in CoNLL06 and CoNLL07 dependent parsing evaluation individually . The main difference between Sinica Treebank data and CoNLL data is that the CoNLL is in dependency format . Word Sense : With regard to semantic features , we use the head senses of words expressed in EHowNet ( _CITE_ ) as words ’ sense types . For example , the E - HowNet definition of 車 輛 ( Na ), is { LandVehicle | For training CDP in CDM model , we extract relevant features from each parse tree in training data , in accordance with features setting in Table 1 . Zhang ( 2004 ) provides a maximum entropy toolkit ( MaxEnt ) to help us training .__label__Material|Data|Extent
For the purposes of the example , the text to be processed is given inline . Our current implementation includes results from each step in a pipeline , where applicable , together with metadata describing the service applied in each step ( here , org . anc . lapps . stanford . SATokenizer : 1 . 4 . 0 ) and identified by an internally - defined type ( stanford ). The annotations include references to the objects defined in the WS - EV , in this example , Token ( defined at _CITE_ ) with ( inherited ) features id , start , end and specific feature string , defined at http :// vocab . lappsgrid . org / Token # id , http :// vocab . lappsgrid . org / Token # start , http :// vocab . lappsgrid . org / Token # end , and http :// vocab . lappsgrid . orgToken /# string , respectively . The web page defining these terms is shown in Figure 3 .__label__Supplement|Document|Use
We collected pairs of articles spanning from 1 / 1 / 2001 through 10 / 05 / 2005 . The corpus consists of 2 , 327 documents , with 0 - 8 documents per day . The corpus is available on our web page at _CITE_ cogcomp /. The English side was tagged with a publicly available NER system based on the SNoW learning architecture ( Roth , 1998 ), that is available on the same site . This set of English NEs was hand - pruned to remove incorrectly classified words to obtain 978 single word NEs .__label__Material|Data|Produce
We collected pairs of articles spanning from 1 / 1 / 2001 through 10 / 05 / 2005 . The corpus consists of 2 , 327 documents , with 0 - 8 documents per day . The corpus is available on our web page at _CITE_ cogcomp /. The English side was tagged with a publicly available NER system based on the SNoW learning architecture ( Roth , 1998 ), that is available on the same site . This set of English NEs was hand - pruned to remove incorrectly classified words to obtain 978 single word NEs .__label__Supplement|Website|Produce
Analysis scripts and primary data and results files are available for download from _CITE_ When no target segment is found by the algorithm for the speech of one caregiver , this results in missing data , as no recall , purity , or collocation can be calculated in these conditions . Therefore , we excluded from inspection all settings of the similarity threshold that resulted in missing data prior to carrying out statistical analyses .__label__Method|Code|Produce
Analysis scripts and primary data and results files are available for download from _CITE_ When no target segment is found by the algorithm for the speech of one caregiver , this results in missing data , as no recall , purity , or collocation can be calculated in these conditions . Therefore , we excluded from inspection all settings of the similarity threshold that resulted in missing data prior to carrying out statistical analyses .__label__Material|Data|Produce
Analysis scripts and primary data and results files are available for download from _CITE_ When no target segment is found by the algorithm for the speech of one caregiver , this results in missing data , as no recall , purity , or collocation can be calculated in these conditions . Therefore , we excluded from inspection all settings of the similarity threshold that resulted in missing data prior to carrying out statistical analyses .__label__Supplement|Document|Produce
Khaltar and Fujii ’ s method was also evaluated for comparison . We used Moses ( Koehn et al ., 2007 ) with the standard configuration and GIZA ++ ( Och et al ., 2003 ) with the grow - diag - final - and heuristic for word - alignment . Our parallel data set was collected from web sites ( _CITE_ and http :// mongolia . usembassy . gov /), and consists of law and news domains . Example En - Mn sentence pairs in our data are shown below . En1 : Occupational safety and health measures shall not involve any expenditure for the workers .__label__Material|Data|Use
Khaltar and Fujii ’ s method was also evaluated for comparison . We used Moses ( Koehn et al ., 2007 ) with the standard configuration and GIZA ++ ( Och et al ., 2003 ) with the grow - diag - final - and heuristic for word - alignment . Our parallel data set was collected from web sites ( _CITE_ and http :// mongolia . usembassy . gov /), and consists of law and news domains . Example En - Mn sentence pairs in our data are shown below . En1 : Occupational safety and health measures shall not involve any expenditure for the workers .__label__Supplement|Website|Use
Engelhardt , Bailey , and Ferreira ( 2006 ) found that subjects rated 5 This term is more commonly used in biology where it refers to a laboratory measurement of biological activity within the body that indirectly indicates the effect of treatment on disease state . For example , CD4 cell counts and viral load are examples of surrogate markers in HIV infection . ( _CITE_ ).__label__Supplement|Document|Introduce
A non - applicable parameter for a particular type of expression would receive a “ Null ” value . The annotation of text fragments has been guided by the presence of evaluative expressions and other criteria as explained in section III . For annotation purposes , we have collected the editorials from two online newspapers ( _CITE_ , http :// kantipuronline . com / ktmpost . php ) of different dates of the year 2007 , amounting to a total of 16 text files and approximately 320 sentences with an average of 20 sentences per editorial . Two annotators having a fairly good understanding of the English__label__Material|Data|Use
A non - applicable parameter for a particular type of expression would receive a “ Null ” value . The annotation of text fragments has been guided by the presence of evaluative expressions and other criteria as explained in section III . For annotation purposes , we have collected the editorials from two online newspapers ( _CITE_ , http :// kantipuronline . com / ktmpost . php ) of different dates of the year 2007 , amounting to a total of 16 text files and approximately 320 sentences with an average of 20 sentences per editorial . Two annotators having a fairly good understanding of the English__label__Supplement|Website|Use
This work is licensed under a Creative Commons Attribution 4 . 0 International Licence . Page numbers and proceedings footer are added by the organisers . Licence details : _CITE_ This document is a description paper , therefore , we focus the rest of it on the features and models we used for carrying out the experiments . A complete description of the task and the dataset used are given in Marelli et al . ( 2014a ) and in Marelli et al .__label__Supplement|License|Other
§ Please note that the analysis is based on a single native ‡ The standard deviation was 29 . 8 ( 9 . 3 %). speaker , thus we need further analysis by multiple subjects . ( _CITE_ ). The native speaker largely agreed with our gen † eration , determining correct choices ( type I ). The__label__Supplement|Document|Produce
They This work is licenced under a Creative Commons Attribution 4 . 0 International License . Page numbers and proceedings footer are added by the organizers . License details : _CITE___label__Supplement|License|Other
In order to test this hypothesis , our source and target texts were POS - tagged using Freeling 3 . 0 suite of language analyzers ( Padr ´ o and Stanilovsky , 2012 ). Freeling gives comparatively good results in English and Russian POS - tagging , using Markov trigram scheme trained on large disambiguated corpus . Freeling tag set for English follows that of Penn TreeBank , while Russian tag set , according to Freeling manual , corresponds to EAGLES recommendations for morphosyntactic annotation of corpora described on _CITE_ ( Monachini and Calzolari , 1996 ). It is not trivial to project one scheme onto another completely , except for the main content words – nouns , verbs and adjectives . Moreover , these three parts of speech are the ones used in the paper by Chen and Chen ( 1994 ), mentioned above .__label__Material|Data|Introduce
The Organising Committee would like to thank the Programme Committee , who responded with very fast but also substantial reviews for the workshop programme . This workshop would not have been possible iii without the support received from the EXPERT project ( FP7 / 2007 - 2013 under REA grant agreement no . 317471 , _CITE_ ).__label__Method|Tool|Introduce
Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre . Additionally , we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines . Tagging software , annotation guidelines , and large - scale word clusters are available at : _CITE_ This paper describes release 0 . 3 of the “ CMU Twitter Part - of - Speech Tagger ” and annotated data .__label__Method|Tool|Produce
Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre . Additionally , we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines . Tagging software , annotation guidelines , and large - scale word clusters are available at : _CITE_ This paper describes release 0 . 3 of the “ CMU Twitter Part - of - Speech Tagger ” and annotated data .__label__Method|Algorithm|Produce
Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre . Additionally , we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines . Tagging software , annotation guidelines , and large - scale word clusters are available at : _CITE_ This paper describes release 0 . 3 of the “ CMU Twitter Part - of - Speech Tagger ” and annotated data .__label__Supplement|Document|Produce
MAISE can be obtained from the author ’ s webpage : _CITE_ The release includes MAISE ’ s source code , instructions , documentation , and a tutorial . MAISE is an open - source tool , licensed under the terms of the GNU Lesser General Public License ( LGPL ).__label__Method|Tool|Produce
MAISE can be obtained from the author ’ s webpage : _CITE_ The release includes MAISE ’ s source code , instructions , documentation , and a tutorial . MAISE is an open - source tool , licensed under the terms of the GNU Lesser General Public License ( LGPL ).__label__Supplement|Website|Produce
We apply our semantic parser on two datasets : WEBQUESTIONS ( Berant et al ., 2013 ), which contains 5 , 810 question - answer pairs with common questions asked by web users ; and FREE917 ( Cai and Yates , 2013 ), which has 917 questions manually authored by annotators . On WEBQUESTIONS , we obtain a relative improvement of 12 % in accuracy over the state - of - the - art , and on FREE917 we match the current best performing system . The source code of our system PARASEMPRE is released at _CITE___label__Method|Code|Produce
The Appendix is available online at _CITE_ clpsych2_appendix . pdf .__label__Supplement|Document|Produce
In Canada , the Canadian Legal Information Institute project ( CANLII ) aims at gathering legislative and judicial texts , as well as legal commentaries , from federal , provincial and territorial jurisdictions in order to make primary sources of Canadian law accessible for free on the Internet ( _CITE_ ). The large volume of legal information in electronic form creates a need for the creation and production of powerful computational tools in order to extract relevant information in a condensed form . But why are we interested in the processing of previous legal decisions and in their summaries ?__label__Supplement|Document|Introduce
A more useful feature would be to label whether a pronoun refers to an entity in the previous questions or in the current question . However , the performances of currently available tools for anaphora resolution are quite limited for our task . The tools we tried , including GATE ( Cunningham et al ., 2002 ), LingPipe ( _CITE_ ) and JavaRAP ( Qiu et al ., 2004 ), tend to use the nearest noun phrase as the referents for pronouns . While in the TREC questions , pronouns tend to refer to the topic words ( focus ). As a result , unsupervised anaphora resolution introduced more noise than useful information .__label__Method|Tool|Use
ever , they are actually derived from datasets constructed by Cotterell et al . ( 2015 ), which are available with full descriptions at _CITE_ Briefly : EXERCISE Small datasets of Catalan , English , Maori , and Tangale , drawn from phonology textbooks . Each dataset contains 55 to 106 surface words , formed from a collection of 16 to 55 morphemes .__label__Material|Data|Extent
Furthermore , the optimal personality of the system is likely to be application - dependent ; it would thus be useful to evaluate how the user ’ s and the system ’ s personality affect task performance in different applications . The PERSONAGE language generator is available for download at http :// mi . eng . cam . ac . uk /∼ farm2 / personage , as well as the personality - annotated corpus collected for our experiments . An on - line demonstrator and a tutorial for customizing PERSONAGE for a new domain can be found at _CITE___label__Method|Tool|Produce
Furthermore , the optimal personality of the system is likely to be application - dependent ; it would thus be useful to evaluate how the user ’ s and the system ’ s personality affect task performance in different applications . The PERSONAGE language generator is available for download at http :// mi . eng . cam . ac . uk /∼ farm2 / personage , as well as the personality - annotated corpus collected for our experiments . An on - line demonstrator and a tutorial for customizing PERSONAGE for a new domain can be found at _CITE___label__Supplement|Document|Produce
This work is licensed under a Creative Commons Attribution 4 . 0 International Licence . Page numbers and proceedings footer are added by the organisers . Licence details : _CITE_ The purpose of this task is to enhance current research in Natural Language Processing ( NLP ) methods used in the clinical domain . The task is a continuation of the CLEF / eHealth ShARe 2013 Shared Task . In particular there were two specific tasks , viz .__label__Supplement|License|Other
Table 3 shows the performance of our Ngrambased system using the SMR technique . First row is the WMT07 baseline system which can be reproduced following the instructions in _CITE_ This baseline system uses a non - monotonic search . Second row shows the results of the Ngram - based system presented in section 2 using the weighted reordering graph trained with the best configuration found in the above section ( 200 statistical classes and an Ngram of length 5 ).__label__Method|Tool|Produce
Table 3 shows the performance of our Ngrambased system using the SMR technique . First row is the WMT07 baseline system which can be reproduced following the instructions in _CITE_ This baseline system uses a non - monotonic search . Second row shows the results of the Ngram - based system presented in section 2 using the weighted reordering graph trained with the best configuration found in the above section ( 200 statistical classes and an Ngram of length 5 ).__label__Supplement|Document|Produce
This work is licenced under a Creative Commons Attribution 4 . 0 International License . Page numbers and proceedings footer are added by the organizers . License details : _CITE___label__Supplement|License|Other
The NESPOLE interlingua has been under development for the last two years as part of the NESPOLE project ( _CITE_ ). FigI would like to make a hotel reservation for the fourth through the seventh of july c : request - action + reservation + temporal + hotel ( time =( start - time = md4 , end - time =( md7 , july )))__label__Method|Tool|Introduce
Unfortunately , the results for ME and TBL when training on one million words cannot be reported in this paper . The learning algorithms will still be occupied after the deadline for the final version of this manuscript . The results will be published on the author ' s web page _CITE_ as soon as the learners have finished their struggle . The total error rate , i . e . the percentage of erroneous tags , is shown in Figure 2 for each classifier .__label__Supplement|Website|Introduce
Unfortunately , the results for ME and TBL when training on one million words cannot be reported in this paper . The learning algorithms will still be occupied after the deadline for the final version of this manuscript . The results will be published on the author ' s web page _CITE_ as soon as the learners have finished their struggle . The total error rate , i . e . the percentage of erroneous tags , is shown in Figure 2 for each classifier .__label__Supplement|Document|Introduce
Theoretically , the algorithm should also distinguish between all literal senses so that the contexts of the same meaning appear in the same cluster and the contexts of different meanings - in different clusters . Therefore , ideally , the algorithm should solve word sense discrimination and non - literal usages detection tasks simultaneously . For each Russian word shown in Table 3 , we extracted from the Russian National Corpora ( _CITE_ ) several literal and non - literal occurences . Some of these words have more than one meaning in Russian , e . g . KnFo - i can be translated as a key or water spring and the word Koca as a plait , scythe or spit .__label__Material|Data|Use
tem . The dataset and code can be downloaded from _CITE___label__Method|Code|Produce
tem . The dataset and code can be downloaded from _CITE___label__Material|Data|Produce
To test the consistence of the results obtained by our method , they will be compared with the Edinburgh Association Thesaurus , a collection of 8000 words whose association norms were produced by presenting each of the stimulus words to about 100 subjects each , and by collecting their responses . The subjects were 17 to 22 year old British students . To perform the tests , we take a sample ( EAT : _CITE_ ) consisting in 100 words . For building a network to deal with the specific task of producing word associations we have used the British National Corpus ( BNC ) as a source . The way the network has been constructed has also some interest and impact in the final results .__label__Material|Data|Use
Large scale annotated corpora , e . g ., the Penn TreeBank ( PTB ) project ( Marcus et al . 1993 ), have played an important role in text - mining . The Penn Discourse Treebank ( PDTB ) ( _CITE_ ) ( Prasad et al . 2008a ) annotates the argument structure , semantics , and attribution of discourse connectives and their arguments . The current release of PDTB2 . 0 contains the annotations of 1 , 808 Wall Street Journal articles (~ 1 million words ) from the Penn TreeBank ( Marcus et al .__label__Material|Data|Introduce
This bakeoff followed a strict set of guidelines and a rigid timetable . The detailed instructions for the bakeoff can be found at _CITE_ The training material of simplified Chinese word segmentation was available starting April 1 , the training material of traditional Chinese word segmentation was available April 23 , testing material was available June 9 , and the results had to be returned to the organizer by email by June 11 no later than 18 : 00 Beijing time . The participating groups (“ sites ”) of CIPSSIGHAN CLP 2010 Bakeoff registered by email .__label__Supplement|Document|Produce
We have presented a new top - down left - to - right parsing model . Its performance of 89 . 4 % is a 20 % error reduction over the previous singleparser performance , and indeed is a small improvement ( 0 . 6 %) over the best combinationparser result . The code is publically available . 1 l_CITE___label__Method|Code|Produce
Research has addressed methods for reducing the time burden associated to these annotation This work is licenced under a Creative Commons Attribution 4 . 0 International License . Page numbers and proceedings footer are added by the organizers . License details : _CITE___label__Supplement|License|Other
The unsatisfying results for the speaking head are based hypothetically on the missing facial expression and on its location within the spelling game . Compared to the virtual language teacher Ville , which was developed specifically for educational purposes , the SitePal ' s talking head seems to have a rather entertaining function . The expressive lip movement that is 1Full questionnaire form can be downloaded from _CITE_ 112 that the words in the Kelly - list are too advanced for the intermediate level . Some of the advanced participants find the word level not challenging enough as the target words are displayed quickly before they are pronounced . This kind of spelling tip needs to be adapted to the proficiency level .__label__Supplement|Document|Introduce
As a consequence , the precisions at different cut - offs have a significantly higher value with Moby as reference than with WordNet as reference . Finally , the results of Table 2 are compatible with those of ( Lin , 1998 ) for instance ( R - prec . = 11 . 6 and MAP = 8 . 1 with WM as reference for all entries of the thesaurus at _CITE_ ) if we take into account the fact that the thesaurus of Lin was built from a much larger corpus and with syntactic co - occurrences .__label__Material|Data|Use
Note that the temporalOrdering property only says it is transitive , not that it is a transitive version of precedes . DAML + OIL does not currently allow us to express this relation . ( see _CITE_ ). Frame Elements may also inherit from each other . We use the rdfs : subPropertyOf to specify this dependences .__label__Method|Tool|Introduce
The MWE expression rules made here and the machine translation system that uses them are available through an open source code repository . Installation details can be found at _CITE_ The code to make the rules is undergoing constant revision , when it settles down we intend to also add it to the repository .__label__Supplement|Document|Produce
This property makes SVM highly competitive , compared with other traditional pattern recognition methods , in terms of computational efficiency and predictive accuracy ( Yang and Liu , 1999 ). In recent years , Joachims has done much research on the application of SVM to text categorization ( Joachims , 1998 ). His SVMlight system published via _CITE_ SVM_LIGHT / svmlight . eng . html is used in our benchmark experiments .__label__Method|Tool|Introduce
subordinate , attributive , and coordinate relations . In addition , each of these types can be endocentric or exocentric , depending of the presence ( endocentric ) or absence ( exocentric ) of a head constituent . In a project on automatic compound processing ( the AuCoPro project ; see _CITE_ ), we investigated various aspects related to the computational processing of compounds ( Verhoeven et al ., 2014 ). In a specific subpart of this project , we aimed to gain more insight in compound semantics in general by drawing from perspectives from computational semantics ( i . e . î Seaghdha , 2008 ), typological studies ( e . g .__label__Method|Tool|Introduce
The data for BWI was obtained using the TIES implementation [ tcc . itc . it / research / textec / toolsresources / ties . html ]. The data for the LP2 learning curve was obtained from ( Ciravegna , 2003 ). The results for ELIE were generated by the current implementation [ _CITE_ For the CRF results , we used MALLET ’ s SimpleTagger ( McCallum , 2002 ), with each token encoded with a set of binary features ( one for each observed literal , as well as the eight token generalizations ). Our results in Fig .__label__Method|Code|Use
CL : flat surfaces ( e . g . books , periodicals , files ): 一本書 yī běn shū a book ; 一間 房子 yí jiān fángzi a room CLI : for flat irregular shapes ( e . g . slices , tablets , tracts of land , areas of water , CDs , movies , DVDs ): 一 片樹葉 yípiàn shùyè a leaf ; 一片吐 司 yípiàn tǔsī a piece of toast ; 一片 火腿 yī piàn huǒ tuǐ a slice of ham Upper Concept kinds of ShapeAttribute selected Chinese shape classifiers metaphorical extension contained in the classifier acting as noun ( N ) metaphorical extension contained in the classifier acting as classifier ( CL ) ° Mindmap modified upon the original of Andrei Sobolevski , _CITE___label__Supplement|Document|Extent
For each mention , its head and extension were considered . The extension was learned by using the mention annotation provided in the training set ( 13th column ) whereas the head annotation was learned by exploiting the information produced by MaltParser ( Nivre et al ., 2007 ). In addition to the features extracted from the training set , such as prefixes and suffixes ( 1 - 4 characters ) and orthographic information ( capitalization and hyphenation ), a number of features extracted by using external resources were used : mentions recognized by TextPro ( _CITE_ ), gazetteers of generic proper nouns extracted from the Italian phone - book and Wikipedia , and other features derived from WordNet . Each of these features was extracted in a local context of f2 words .__label__Method|Tool|Use
They also separately tabulate the results achieved for unknown targets . Our full model , denoted by “ FullGraph ,” outperforms all the baselines for both tasks . Note that the Self - training model even falls 13_CITE_ software . html # comparator short of the supervised baseline SEMAFOR , unlike what was observed by Bejan ( 2009 ) for the frame identification task . The model using a graph constructed solely from the thesaurus ( LinGraph ) outperforms both the supervised and the self - training baselines for all tasks , but falls short of the graph constructed using the similarity metric that is a linear combination of distributional similarity and supervised frame similarity . This indicates that a graph constructed with some knowledge of the supervised data is more powerful .__label__Method|Algorithm|Introduce
Reitter , D . ( 2002 ), Rhetorical theory in LaTeX with the ` rse package , Technical report , _CITE_ Reitter , D . ( to appear ), Complex signals for rhetorics : On rhetorical analysis with richfeature support vector models , in ' in Proceedings of the GLDV conference 2003 '. Schilder , F . ( 2002 ), ' Robust discourse parsing via discourse markers , topicality and position ', Natural Language Engineering 8 ( 2 / 3 ).__label__Supplement|Document|Introduce
This work is licensed under a Creative Commons Attribution 4 . 0 International Licence . Page numbers and proceedings footer are added by the organisers . Licence details : _CITE___label__Supplement|License|Other
We extract further information indicating whether a named entity , as identified by the Stanford NE Recognizer ( Finkel et al ., 2005 ) begins at wi . These features are relevant as there 5Realization of the classes D and N as lexical items is straightforward . To convert I into a or an , we use the CMU pronouncing dictionary ( _CITE_ ) and select an if wi starts with a phonetic vowel .__label__Material|Data|Use
Further , prior tools are often in the form of discrete components , hard to extend or to integrate with other systems . Some good corpus resources are available , most recently the Copenhagen Dependency Treebank 1E . g . CST ’ s non - commercial - only anonymisation tool , at _CITE_ ( CDT ) ( Buch - Kromann and Korzen , 2010 ), which built on and included previously - released corpora for Danish . This 200K - token corpus is taken from news articles and editorials , and includes document structure , tokenisation , lemma , part - ofspeech and dependency relation information . The application demonstrated , DKIE , draws only on open corpus resources for annotation , and the annotations over these corpora are released openly .__label__Method|Tool|Introduce
We would like to thank Marc Brysbaert and his colleagues for making their excellent resources available to the research community . We also thank the anonymous reviewers for their useful feedback . This research was funded by LEAD Graduate School ( GSC 1028 , _CITE_ lead ), a project of the Excellence Initiative of the German federal and state governments .__label__Method|Tool|Introduce
This work has been partially funded by the European Union under the EuroMatrix Plus project ( _CITE_ , IST - 2007 . 2 . 2FP7 - 231720 )__label__Method|Tool|Introduce
The different classes ( types in Freebase ) have different properties . Although the Freebase types are not strict in inheriting properties , some types are still not mutually compatible ( intuitively ). For example , due to this misclassification , the instance of the United States of America ( _CITE_ ) is not only an instance of the types Country , Location but also of Food . We believe that such knowledge has to be represented in a different way . It is important to note that correcting such cases of instances classification to many disjoint types ( classes ) is outside the scope of the current version of FactForge .__label__Supplement|Document|Introduce
The weight computation is done by emergence along with the gaming activity . Obviously by intuition , the relation cat --> animal is stronger than cat --> ball of wool , none withstanding their types . The lexical network has been made available ( at _CITE_ ) and free to use by their authors , giving the research community a resource to play with . The question of the evaluation of its quality , usability in WSD and word recollection ( Tip of the Tongue problem ), and distributional properties are the main subjects of this article . One specific question is whether low weight but still important relations can be captured by some similar approaches and to which extend they are useful .__label__Material|Data|Use
We are also grateful to our colcan be dealt with using direct verbalisation ( in con - leagues in the Open University ’ s Natural Lantrast with low frequency of , e . g ., FactQ ). guage Generation group for stimulating discussions and feedback . The research reported in this paper was carried out as part of the CODA research project ( _CITE_ ) which was funded by the UK ’ s Engineering and__label__Method|Tool|Introduce
The first strategy constituted the creation of frame files using the existing data from both the corpus of the earlier version of PropbankBr and the lexical repository of the English Propbank plus some new data , which was incorporated for this purpose in an updated version of PropbankBr ; this strategy is described in the Subsection 2 . 2 . In the second strategy , described in Subsection 2 . 3 , we duplicated the structure of the framefiles from 1 http :// verbs . colorado . edu /~ mpalmer / projects / ace . html 2 As informed in the site of the Verb - Index , updated on 08 / 01 / 2013 . 3 _CITE_ 217 the English Propbank to Propbank - Br for every add the extra information required to the corpus verb which , in English , possessed a single sense . Propbank - Br . Aiming this , we created six “ word 2 . 1 The corpus Propbank - Br tags ” in corpus Propbank - Br , using the same annoThe corpus Propbank - Br ( Duran and Aluisio , tation tool used to annotate the original corpus 2012 ) was annotated by a sole linguist , aiming to ( SALTO – Burchardt et al .__label__Supplement|Website|Extent
Each entry includes an ID number and a Nonliteral , Literal , or Unannotated tag . Annotations are from testing or from active learning during example - base construction . The TroFi Example Base is available at _CITE_ Further unsupervised expansion of the existing clusters as well as the production of additional clusters is a possibility .__label__Material|Data|Use
We are able to extract high - quality information about temporal durations and to effectively classify tweets as to their habituality . It is clear that Twitter tweets contain a lot of unique data about different kinds of events and habits , and mining this data for temporal duration information has turned out to be a fruitful avenue for collecting the kind of worldknowledge that we need for robust temporal language processing . Our verb lexicon is available at : _CITE___label__Material|Data|Produce
The toolkit has been hosted and developed under sourceforge . net since inception . Moses has an active research community and has reached over 1000 downloads as of 1st March 2007 . The main online presence is at _CITE_ where many sources of information about the project can be found . Moses was the subject of this year ’ s Johns Hopkins University Workshop on Machine Translation ( Koehn et al . 2006 ).__label__Supplement|Document|Produce
author refers both to a book but also to his DNA ; this paradox is a good illustration of the difficulty translating a literary work ! The data in this article are available at _CITE_ There one can find :__label__Material|Data|Produce
[ The ginger ] stops constipation . Pechsiri and Kawtrakul ( 2007 ), proposed verb - pair rules learned by two different machine learning techniques ( NB and SVM ) to extract causality with multiple EDUs of a causative unit and multiple EDUs of an effect unit with the problems of the discourse marker ambiguity and the implicit discourse marker . This verb - pair rule has been represented by the following equation ( 1 ) ( Pechsiri and Kawtrakul , 2007 ) where Vc is the causative verb concept set , Ve is the effect verb concept set , C is the Boolean variables of causality and non - causality , and a causative verb concept ( vc , where vcEVj and an effect verb concept ( ve , where veEVe ) are referred to WordNet ( _CITE_ ) and the predefined plant disease information from Department of Agriculture ( http :// www . doa . go . th /). CausalityFunction : Vc A Ve 4 C ( 1 ) They also proposed using Vc and Ve to solve the boundary of the causative unit and using the Centering theory along with Ve to solve the boundary of the effect unit . The outcomes of their research were the verb - pair rule , Vc , Ve , and the multiple EDUs of causality ( extracted from textual data ) was at their highest precision of 89 % and their highest recall of 76 %.__label__Method|Tool|Use
where β is a weighted constant often set to 1 . We test the baseline system on the newswire test data for the 1999 IEER evaluation in Mandarin ( _CITE_ 99 . htm ). Table 2 in section 4 summarizes the result of baseline model .__label__Material|Data|Use
words for training and 100 thousand words for validation from the Harry Potter fan fiction database ( _CITE_ ). We restricted the vocabulary to the top 100 thousand words which covered all but 4 words from Chapter 9 of Harry Potter and the Sorcerer ’ s Stone . For the RNNLM , we trained models with different hidden layers and learning rates and found the RNNLM with 250 hidden units to perform best on the validation set .__label__Material|Data|Use
To test our approach for PLSA initialization we developed an LSA implementation based on the SVDLIBC package ( _CITE_ dr / SVDLIBC /) for computing the singular values of sparse matrices . The PLSA implementation was based on an earlier implementation by Brants et al . ( 2002 ).__label__Method|Code|Use
The training corpus is part of the JRC - ACQUIS ( _CITE_ - last accessed on 18 . 04 . 09 ). Two types of alignments are available on the corpus homepage : Vanilla and HunAlign . The alignments realized with the Vanilla aligner7 were used for the experiments presented here .__label__Material|Data|Use
Further information ( terms and frequencies ) is displayed thanks to tooltips ( see Figure 2 ), using the JavaScript overLIB libray ( _CITE_ ).__label__Method|Code|Use
The corpora are detailed in Table 1 . Links to descriptions of the corpora can be found at _CITE_ bakeoff_instr . html ; publications on specific corpora are ( Huang et al ., 1997 ) ( Academia Sinica ), ( Xia , 1999 ) ( Chinese Treebank ); the Beijing University standard is very similar to that outlined in ( GB / T 13715 – 92 , 1993 ). Table 1 lists the abbreviations for the four corpora that will be used throughout this paper . The suffixes “ o ” and “ c ” will be used to denote open and closed tracks , respectively : Thus “ ASo , c ” denotes the Academia Sinica corpus , both open and closed tracks ; and “ PKc ” denotes the Beijing University corpus , closed track .__label__Material|Data|Introduce
The corpora are detailed in Table 1 . Links to descriptions of the corpora can be found at _CITE_ bakeoff_instr . html ; publications on specific corpora are ( Huang et al ., 1997 ) ( Academia Sinica ), ( Xia , 1999 ) ( Chinese Treebank ); the Beijing University standard is very similar to that outlined in ( GB / T 13715 – 92 , 1993 ). Table 1 lists the abbreviations for the four corpora that will be used throughout this paper . The suffixes “ o ” and “ c ” will be used to denote open and closed tracks , respectively : Thus “ ASo , c ” denotes the Academia Sinica corpus , both open and closed tracks ; and “ PKc ” denotes the Beijing University corpus , closed track .__label__Supplement|Document|Introduce
These approaches are representative of different solutions that have been proposed in the literature This work is licensed under a Creative Commons Attribution 4 . 0 International Licence . Page numbers and proceedings footer are added by the organizers . License details : _CITE_ ( Pradhan et al ., 2013 ), which can be broadly classified in the following types :__label__Supplement|License|Other
More specifically , for a given source si and another sentence σ , we define LDA ( σ | si ) as follows ( d = si ), where w1 , ... , w | a | are now the words of σ , ignoring stop - words . 16The document - specific parameters of the first multinomial distribution are drawn from a Dirichlet distribution . 17We use MALLET ( _CITE_ ), with Gibbs sampling ( Griffiths and Steyvers , 2004 ). We set K = 800 , having first experimented with K = 200 , 400 , 600 , 800 , 1000 . 18We trained the LDA model on approximately 106 , 000 articles from the TIPSTER and AQUAINT corpora .__label__Method|Tool|Use
Experimental results show that though the compact decomposition requires more running time for each iteration , it achieves consistently tighter bounds and outperforms the naive dual decomposition . The two experiments demonstrate that our method works for general graphs , even if the graph can not be decomposed into a few spanning trees ( for example , if the graph has large complete subgraphs or large factors ). Our code is available at _CITE___label__Method|Code|Produce
The StandOff axis steps are part of release 0 . 10 of the open - source MonetDB / XQuery product , which can be downloaded from _CITE_ In addition to the StandOff axis steps , a keyword search function has been added to the XQuery system to allow queries asking for regions containing specific words . This function is called so - contains ($ node , $ needle ) which will return a boolean specifying whether $ needle occurs in the given region represented by the element $ node .__label__Method|Code|Produce
( Fujii et al ., 2008 ) We applied CaboCha ( Kudo and Matsumoto , 2002 ) to the reference sentences , and manually corrected the dependency trees because Japanese dependency parsers are not satisfactory in terms of sentence accuracy ( Tamura et al ., 2007 ). To support this manual correction , CaboCha ’ s XML output was automatically converted to dependency tree pictures by using cabochatrees package for LATEX . _CITE_ uploads / cabochatrees . pdf . Then , it is easy to find mistakes of the dependency trees . In addition , CaboCha ’ s dependency accuracy is very high ( 89 – 90 %) ( Kudo and Matsumoto , 2002 ).__label__Method|Code|Use
It therefore makes sense to think of this as a separable part of the theory , the “ upper ontology ”. At the top level , datastructures ( instances of Object ) belong to one of the concepts Ordered , Set and Primitive . Ordered structures are divided ' These are both available in full from _CITE_ up in terms of the number of components ( concepts Arity - 1 , Arity - 2 etc ) and whether they are Tuples or Sequences . For convenience , union types such as Arity - atleast - 2 are also defined . The RAGS NLG ontology ( see Figure 3 for an overview ) contains the main substance of the RAGS type definitions .__label__Method|Algorithm|Use
The difference between reading and dictation times ( SRT ) is statistically significant at p - value = 0 . 0022 measured across all participants . This is unsurprising when comparing SRT mean ( 128 . 3s ) and SD ( 29 . 57s ) to reading aloud . A Wilcoxon signed rank - test was used to calculate 4 The tool , developed by Peter Kleiweg , is available for free 5 Unfortunately , the other nine participants were no longer at : _CITE_ available to perform this task . Proceedings of the 20th Nordic Conference of Computational Linguistics ( NODALIDA 2015 ) 205 the p - value because normal distribution of task times cannot be assumed and , with a small sample size , a robust method is needed to calculate statistical significance .__label__Method|Tool|Introduce
It has been designed for corpus collecting , annotating , maintaining and analyzing . Additionally , it has been designed as the engine , which the end user could use with their data . ( See a service on _CITE_ ).__label__Supplement|Website|Produce
In future work , we hope to further extend the coverage of the provided system outputs as well as their analysis to cover all participants of all tasks in the BioNLP Shared Task 2011 . We also aim to use the compiled resource in further study of appropriate criteria for the evaluation of event extraction methods and deeper analysis of the remaining challenges in event extraction . To encourage further study of all aspects of event extraction , all resources and tools introduced in this study are provided freely to the community from _CITE___label__Method|Tool|Produce
In future work , we hope to further extend the coverage of the provided system outputs as well as their analysis to cover all participants of all tasks in the BioNLP Shared Task 2011 . We also aim to use the compiled resource in further study of appropriate criteria for the evaluation of event extraction methods and deeper analysis of the remaining challenges in event extraction . To encourage further study of all aspects of event extraction , all resources and tools introduced in this study are provided freely to the community from _CITE___label__Supplement|Document|Produce
We have successfully combined LXGram with a part - of - speech tagger and a morphological analyzer ( more in Section 6 ). The grammar code includes mappings from the input format ( XML ) to the feature structures that are manipulated by the grammar . Availability A version of LXGram is publicly available at _CITE_ LXGram can be used by applications without any knowledge of the grammar ’ s implementation or internal workings . The LKB allows for applications to communicate with the grammar via sockets , accepting parser input in XML or raw text and returning semantic representations in XML , for which a DTD is available .__label__Method|Tool|Produce
Training of a classifier (’ language annotator ’) in a supervised framework , requires a set of annotated entries with a distribution similar to the set of entries to be annotated . We know of only two such databases which can be freely accessed6 ; WALS and the library catalogue of MPI / EVA in Leipzig . WALS : The bibliography for the World Atlas of Language Structures book can now be accessed online ( _CITE_ ). This database contains 5633 entries annotated to 2053 different languages . MPI / EVA : The library catalogue for the library of the Max Planck Institute for Evolution Anthropology ( http :// biblio . eva . mpg . de /) is queryable online .__label__Material|Data|Introduce
http :// www . hf . uio . no / tekstlab / for an overview ). To our knowledge , the largest existing written corpus of Norwegian is the Norsk Aviskorpus ( Hofland 2000 , cf . _CITE_ ), an expanding newspaper - based corpus currently containing 700 million words . However , the Norsk Aviskorpus is only available though a dedicated web interface for non commercial use , and advanced research tasks cannot be freely carried out on its contents . Even though we have only worked on building a web corpus for Bokmål Norwegian , we intend to apply the same procedures to create web - corpora also for Nynorsk and North Sami , thus covering the whole spectrum of written languages in Norway .__label__Material|Data|Introduce
However , because text - processing is more advanced in this regard than image - processing , we shall concentrate on NER , which is performed with a system called Oscar . A preliminary overview of the system was presented by Corbett and Murray - Rust ( 2006 ). Oscar is open source and can be downloaded from _CITE_ As a first step in representing biomedical content , we identify Gene Ontology ( GO ) terms in full text . 1 ( The Gene Ontology Consortium , 2000 ) We have chosen a relatively simple starting point in order to gain experience in implementing useful semantic markup in a publishing workflow without a substantial word - sense disambiguation effort . GO terms are largely compositional ( Mungall , 2004 ), hence incomplete matches will still be useful , and that there is generally a low level of semantic ambiguity . For example , there are only 133 single - word GO terms , which significantly reduces the chance of polysemy for the 20000 or so others .__label__Method|Code|Use
We also investigated the contribution of the feature nodes by running HTP without them . In addition , we ran HTP on a bipartite graph , i . e ., one created from English - foreign phrase alignments only without any phrase alignments between foreign languages . We used Callison - Burch ( 2008 )’ s implementation of SBP that is publicly available at _CITE_ SBP is based on BCB ( Bannard and Callison - Burch , 2005 ) which computes the probability that English phrase E ' is a paraphrase of E using the following formula :__label__Method|Code|Use
Of course , the results on both tracks are welcomed . Scoring was done automatically using a combination of Perl and shell scripts . The scripts ( Sproat and Emerson , 2003 , 2005 ) used for scoring can be downloaded from _CITE_ The bakeoff organizer provided an on - line scoring system to all the participants who had submitted their bakeoff results for their follow - up experiments .__label__Method|Code|Use
The latter makes this extraction methodology suitable for inflecting languages ( Russian in our case ) where frequencies of ngrams are low . Porting the NP extractor from English to Russian consisted in substituting English stop lexicons of the tool with the Russian equivalents . We did this by translating each of the English stop lists into Russian using a free online system PROMT ( _CITE_ ) followed by manual brush - up . The NP extractor does not rely on a preconstructed corpus , works on small texts , does not miss low frequency units and can reliably extract all NPs from an input text . We excluded a lemmatizer from the original extraction algorithm and kept all extracted Russian NPs in their textual forms .__label__Method|Tool|Use
We have provided a qualitative analysis of the process and results of ZI based on our handannotated sample , with a view to strengthening the basis for the annotation scheme . We are now starting to use our sample as training data for machine learning , as well as creating more data in a systematic way , toward automatic annotation . We are also considering to use our ontology management tool ( Open Ontology Forge , _CITE_ ex . htm ) for these purposes ; 1 ) to define zone classes as ontology classes ; zone annotation is then expected to be a variant of named entity annotation , which we are familiar with , and 2 ) to link between expressions referring to results ( e . g . these results / our results ) and their antecedent ( i . e . the RSL zone providing a concrete description of the experimental results ), using the coreference tool .__label__Method|Tool|Use
Given our 72 animals and 54 features , we ask a human annotator to mark each animal - feature pair with a ‘ probability ’, expressed as a quantifier . Possible values are no , few , some , most , all . The guidelines for the annotation task can be seen at _CITE_ iwcs13 - annot . pdf .__label__Supplement|Document|Produce
in data recording , storage , and coding methods in recent decades . Thanks to corpora and tools such as those developed in the context of the CHILDES project ( _CITE_ ), researchers in areas such as morphology and syntax have enjoyed a convenient and powerful method to analyze the morphosyntactic properties of adult languages and their acquisition by first and second language learners . In the area of phonetics , the Praat system ( http :// www . fon . hum . uva . nl / praat /) has expanded our abilities to conduct phonological modeling , computational simulations based on a variety of theoretical approaches , and articulatory synthesis . In this rapidly - expanding software universe , phonologists interested in the organization of sound systems ( e . g .__label__Method|Tool|Introduce
in data recording , storage , and coding methods in recent decades . Thanks to corpora and tools such as those developed in the context of the CHILDES project ( _CITE_ ), researchers in areas such as morphology and syntax have enjoyed a convenient and powerful method to analyze the morphosyntactic properties of adult languages and their acquisition by first and second language learners . In the area of phonetics , the Praat system ( http :// www . fon . hum . uva . nl / praat /) has expanded our abilities to conduct phonological modeling , computational simulations based on a variety of theoretical approaches , and articulatory synthesis . In this rapidly - expanding software universe , phonologists interested in the organization of sound systems ( e . g .__label__Material|Data|Introduce
The sarcasm is ambiguous because of a likely hyperbole in the first sentence , and because 1We use irony and sarcasm interchangeably in this paper , as has been done in past work . Sarcasm has an element of criticism , while irony may not . 2_CITE_ sentiment associated with ‘ four hours cooking ’ depends on how much the author / speaker likes cooking . Such sarcasm is difficult to judge for humans as well as an automatic sarcasm detection approach . Essentially , we need more context related to the author of these sentences to identify sarcasm within them .__label__Supplement|Document|Introduce
Collocation segmentation is done on a separate line basis , i . e ., for each text line , which is usually a paragraph , the average and the minimum combinability values are determined and the threshold is set at 50 percent , midway between the average and the minimum . The Average Minimum Law is applied in tandem . The tool CoSegment for collocation segmentation is available at ( _CITE_ ). Table 3 presents the distribution of segments by length , i . e ., by the number of words . The length of collocation segments varies from 1 to 7 words .__label__Method|Tool|Produce
In this set of experiments , we investigate the influence of the text normalization strategies presented in section 3 on parsing and more specifically on our parse revision strategy . Thus , we first apply a partial normalization , using only the basic text normal11_CITE_ ization . For the full normalization , we combine the basic text normalization with the spell checker . For these experiments , we use the restricted APS reviser and the EWT treebank for training and testing .__label__Method|Tool|Use
The whole evaluation Wolf Moon T - shirt is the clearest example . This item corpus is integrated with 8 , 861 documents . It is became one of the most popular products , both in available at _CITE_ Amazon as well as in social networks , due to the 4 Model ironic reviews posted by people1 . We define a model with six categories which atOur positive data are thus integrated with reviews tempts to represent irony from different linguistic of five different products published by Amazon .__label__Material|Data|Introduce
lastly , the video has a comment ( _CITE_ < CommentID >) that reflects a positive opinion ( OpinionC01 ). The JSON - LD context in Listing 6 provides extra information the semantics of the document , and has been added for completeness .__label__Supplement|Media|Introduce
ArSenL - AWN , will have an AWN offset while an entry in ArSenL - Eng will have the same field set to N . A ( Not Available ). Furthermore , due to manual correction performed to ArSenL - AWN , the gold version of the union lexicon includes 28 , 780 lemmas with the corresponding number of 157 , 969 synsets . A public interface to browsing ArSenL is available at _CITE_ The interface allows the user to search for an Arabic word . The output would show the different scores for the Arabic word along with the corresponding sentiment scores , English glosses and examples that help in disambiguating different sentiment scores for the same Arabic lemma .__label__Supplement|Website|Produce
We use regular expression pattern to detect . errors in words by using word weight ( Wazn ) and affixes . For example we can detect that words with the 1 The script is named AkhtaBot , which is applied to arabic wikipedia , the Akhtabot is available on _CITE_ weight INFI ' AL لﺎﻌﻔﻧا must be written by Hamza Wasl , and we consider the form لﺎﻌﻔﻧإ * as wrong . Then , we represent all forms of this weight with all possible affixes .__label__Method|Code|Use
The preposition “ in ” also takes two arguments : the first argument is the verb phrase “ make payment ” and the second argument is the noun phrase “ his own domestic currency ”. For simplicity we use only the head word of each noun phrase , so the subjects and objects of the transitive verbs are nouns and the second arguments of the prepositions are also nouns . We further modify the output by incorporating the l_CITE___label__Method|Algorithm|Extent
It will be published in the upcoming leading ACL conference . ( _CITE_ )__label__Supplement|Paper|Produce
To evaluate the results , two native Romanian speakers labeled the system outputs as being “ DE ”, “ not DE ” or “ Hard ( to decide )”. The labeling protocol , which was somewhat complex to prevent bias , is described in the externallyavailable appendices (§ 7 . 1 ). The complete system output and annotations are publicly available at : _CITE___label__Supplement|Document|Produce
We thank the anonymous reviewers for their detailed comments . Our research was funded by the LEAD Graduate School ( GSC 1028 , _CITE_ ), a project of the Excellence Initiative of the German federal and state governments , and the European Commission ’ s 7th Framework Program under grant agreement number 238405 ( CLARA ).__label__Supplement|Website|Other
We also carried out experiments on Semeval 2014 aspect based sentiment analysis data obtained from _CITE_ Results are shown in Table 7 . We cannot perform a comparative evaluation of such experimental results as there is no state - of - art approach yet which used this dataset for the same kind of experiment .__label__Material|Data|Use
We use a CRF ++ based POS tagger for Hi , which is freely available from _CITE_ For En , we use the Twitter POS tagger ( Owoputi et al ., 2013 ). It also has an inbuilt tokenizer and can work directly on unnormalized text .__label__Method|Tool|Use
These are byproducts of protein - interaction extraction in our project . The corresponding ‘ official symbol ’ was searched using a partial match of registered names , and finally was checked manually . Compound names were gathered from the index of the biochemical dictionary , KEGG ( _CITE_ ), mesh terms , and UMLS ( http :// www . nlm . nih . gov / research / umls /) and were registered in GENA . Some high - concept terms were removed manually . Compound name searches were not evaluated in this study .__label__Material|Data|Use
The backbone of the LOGON prototype implements a relatively conventional architecture , orgaPhis demonstration reflects the work of a large group of people whose contributions we gratefully acknowledge . Please see ‘ _CITE_ for background .__label__Supplement|Document|Produce
All the documents are stemmed and all stopwords are removed with the SnowBall Stemmer ( _CITE_ ) for the Russian language . As it was mentioned above , this algorithm is aimed at providing word sense discrimination and non - literal usages detection simultaneously . So far we have paid attention only to the non - literal usages detection aspects .__label__Method|Tool|Use
Because LDA - SP generates a complete probabilistic model for our relation data , its results are easily applicable to many other tasks such as identifying similar relations , ranking inference rules , etc . In the future , we wish to apply our model to automatically discover new inference rules and paraphrases . Finally , our repository of selectional preferences for 10 , 000 relations is available at _CITE___label__Material|Data|Produce
Word categories are identified by using the LingPipe ’ s general English part - of - speech ( POS ) tagger trained on the Brown Corpus ( _CITE_ ). We leverage POS information to collect , for each sentence , nominal groups that are potential keyphrases .__label__Material|Data|Use
Having previously obtained a PhD at UMass Amherst under the supervision of David Smith and Mark Johnson , his current research aims to improve natural language understanding by performing task - specific training of word representations and parsing models . He is also interested in semi - supervised learning , joint inference , and semantic parsing . His web page is available at _CITE_ Sebastian Riedel is a senior lecturer at University College London and an Allen Distinguished Investigator , leading the Machine Reading Lab . Before , he was a postdoc and research scientist with Andrew McCallum at UMass Amherst , a researcher at Tokyo University and DBCLS with Tsujii Junichi , and a PhD student with Ewan Klein at the University of Edinburgh .__label__Supplement|Website|Introduce
It is often believed of natural language and speech applications that deployed commercial systems are about a generation behind the systems being developed in research laboratories . It would be interesting to know if this is true in the domain of Chinese word segmentation , which should be possible to find out if we get a good balance of both . For the present , we will make the training and test data for the bakeoff available via _CITE_ ( subject to the restrictions of the content providers ), so that others can better study the results of this contest .__label__Material|Data|Produce
The UNESCO - funded project The Tenth - Century Cyrillic Manuscript Codex Suprasliensis aims at digitizing the largest Old Church Slavonic manuscript , the Codex Suprasliensis ( _CITE_ ). This early Cyrillic manuscript has been dated to the end of the tenth or the beginning of the eleventh century and has been published three times on paper ( Miklošič , 1851 ; Severjanov , 1904 ; Zaimov and Capaldo , 1982 – 83 ). The most recent of these , the two - volume edition by Zaimov and Capaldo ( 1982 , 1983 ), was published more than two decades ago and contains photographic images of the entire manuscript ; a transcription reproduced from Severjanov , 1904 and corrected ( not entirely without error ) against the facsimile ; and a Greek text ( compiled from multiple Byzantine sources , which necessarily implies complications in its philological interpretation ; see also Abicht and Schmidt , 1896 ).__label__Method|Tool|Introduce
In this paper , we introduce a voice - based search system that allows users to specify search requests in a single natural language utterance . The output of ASR is then parsed by a query parser into three fields : LocationTerm , SearchTerm , and Filler . We use a local search engine , _CITE_ , which accepts the SearchTerm and LocationTerm as two query fields and returns the search results from a business listings database . We present two methods for parsing the voice query into different fields with particular emphasis on exploiting the ASR output beyond the 1 - best hypothesis . We demonstrate that by parsing word confusion networks , the accuracy of the query parser can be improved .__label__Method|Tool|Use
The experiments of this paper utilize the first medium - sized corpus for Amharic ( available at _CITE_ ). The corpus consists of all 1065 news texts ( 210 , 000 words ) from the Ethiopian year 1994 ( parts of the Gregorian years 2001 – 2002 ) from the Walta Information Center , a private news service based in Addis Ababa . It has been morphologically analysed and manually partof - speech tagged by staff at ELRC , the Ethiopian Languages Research Center at Addis Ababa University ( Demeke and Getachew , 2006 ).__label__Material|Data|Use
We now turn to the question of learning rules which reflect real - world relationships . Because of space constraints , we are unable to reproduce actual output from the system , but examples can be seen at _CITE_ As illustration , we consider here the classifiers produced for the feature aquatic a . The baseline classifier ( the one learnt from distributional data only ) does use distributional features that are associated with water ( mediterranean a , in p ()+ water n ), but overall , the decision tree is rather far from the way we might expect a human to attribute the feature aquatic a to an animal species , and mostly includes seemingly irrelevant features such as variant of or again fascinating .__label__Supplement|Document|Produce
Nor is it constrained to be acyclic : it may visit the same node more than once . An example of this is provided by a narration in which the same event is recounted twice . To take an extreme case , in the movie Groundhog Day ( _CITE_ ), the character Phil relives the same day and its events many times . In our DAGs , we represent threads by a dot__label__Supplement|Media|Introduce
In human evaluation , people judge the adequacy and the fluency of each translation . Denoual and Lepage ( 2005 ) pointed out that BLEU assumes word boundaries , which is ambiguous in Japanese and Chinese . Here , we assume the word boundaries given by ChaSen , one of the standard morphological analyzers ( _CITE_ ) following Fujii et al . ( 2008 ) In JE translation , most Statistical Machine Translation ( SMT ) systems translate the Japanese sentence ( J0 ) kare wa sono hon wo yonda node sekaishi ni kyoumi ga atta which means__label__Method|Tool|Use
is a rule and transfer - based Machine Translation System . More information on the software is available at _CITE_ category are also kept track of . The Nepali Stemmer and Morphological Analyzer is a rule - based one and makes use of the following resources :__label__Supplement|Document|Produce
In this paper we present a method of automatically learning word - phone mapping rules for synthesizing foreign words occurring in text . We show the effectiveness of the method by computing the accuracy of prediction and also by means of perceptual evaluations . The synthesized multilingual wave files are available for download at _CITE___label__Supplement|Document|Produce
Reddit comprises ‘ sub - reddits ’, which focus on specific topics . For example , http :// reddit . com / r / politics features articles ( and hence comments ) centered around political news . The current version of the corpus is available at : _CITE_ ACL - 2014 - irony . Data collection and annotation is ongoing , so we will continue to release new ( larger ) versions of the corpus in the future . The present version comprises 3 , 020 annotated comments scraped from the six subreddits enumerated in Table 1 .__label__Material|Data|Produce
In a typical domain adaptation scenario , there are in - domain texts that are manually annotated and that are used to train a general - language parser , and out - of - domain or target domain texts that are This work is licensed under a Creative Commons Attribution 4 . 0 International Licence . Page numbers and proceedings footer are added by the organisers . Licence details : _CITE___label__Supplement|License|Other
Regardless of actual performance , all submissions contribute to the common effort to produce an effective Chinese spell checker , and the individual reports in the Bake - off proceedings provide useful insight into Chinese language processing . We hope the data sets collected for this Bakeoff can facilitate and expedite the development of effective Chinese spelling checkers . All data sets with gold standards and evaluation tool are publicly available for research purposes at _CITE_ Based on the results of this Bake - off , we plan to build new language resources to improve existing and develop new techniques for computer__label__Method|Tool|Produce
Regardless of actual performance , all submissions contribute to the common effort to produce an effective Chinese spell checker , and the individual reports in the Bake - off proceedings provide useful insight into Chinese language processing . We hope the data sets collected for this Bakeoff can facilitate and expedite the development of effective Chinese spelling checkers . All data sets with gold standards and evaluation tool are publicly available for research purposes at _CITE_ Based on the results of this Bake - off , we plan to build new language resources to improve existing and develop new techniques for computer__label__Material|Data|Produce
Roughly speaking , the rules were extracted from a parallel English - Chinese corpus , based on the assumption that two English phrases 01 and 02 that are often aligned to the same Chinese phrase � are 5We trained GA - EXTR on approximately 1 , 050 pairs of source sentences and gold human extractive compressions , obtained from Edinburgh ’ s ‘ written ’ extractive dataset ; see _CITE_ The source sentences of that dataset are from 82 documents . The 1 , 050 pairs that we used had source sentences from 52 out of the 82 documents .__label__Material|Data|Use
The second part of the demonstration shows a pre - built multimodal application running on the iPhone ( http :// www . apple . com ) and Google Android phone ( http :// code . google . com // android ). This application allows the user to have a dialogue about places of interest using The List website ( _CITE_ ). Figure 4 shows screenshots of the iPhone , firstly with The List homepage and then a page with content on Bar Roma , an “ italian restaurant in Edinburgh ” as requested by the user through spoken dialogue . Figure 4 : DUDE - generated iPhone List Application pushing relevant web content Figure 5 shows the architecture of this system whereby the DUDE server runs the spoken dialogue system ( as outputted from the DUDE Development Environment ).__label__Supplement|Website|Use
The syntactic annotation procedure , which like the POS tagging is performed semi - automatically , uses the interactive annotation environment developed within the German NEGRA project ( http :// www . coli . unisb . de / sfb378 / negra - corp us / negra - corp us . html ). A simple visualisation tool ( Portray ) for the annotation graphs is freely available from the Utrecht CGN site ( _CITE_ ). In a later phase of the project , the CGN exploitation software ( COREX tools ) will provide more advanced display and search facilities for the syntactic annotation .__label__Method|Tool|Introduce
The syntactic annotation procedure , which like the POS tagging is performed semi - automatically , uses the interactive annotation environment developed within the German NEGRA project ( http :// www . coli . unisb . de / sfb378 / negra - corp us / negra - corp us . html ). A simple visualisation tool ( Portray ) for the annotation graphs is freely available from the Utrecht CGN site ( _CITE_ ). In a later phase of the project , the CGN exploitation software ( COREX tools ) will provide more advanced display and search facilities for the syntactic annotation .__label__Supplement|Website|Introduce
The DRI is an international & quot ; grassroots & quot ; effort that seeks to share corpora that have been tagged with the core features of interest to the discourse community . In order to use the core scheme , it is anticipated that each group will need to refine it for their particular purposes . A usable draft core scheme is now available for experimentation ( see _CITE_ ). Whereas several groups are working with the unadapted core DRI scheme ( Core and Allen , 1997 ; Poesio and Traum , 1997 ), we have attempted to adapt it to our corpus and particular research questions . First we describe our corpus , and the issue of tracking agreement .__label__Method|Algorithm|Use
Automatic evaluation of the results was performed against a gold standard lexicon of health - related terms that was obtained from the top - ranking nouns in the English health domain model of the initial corpus and that at the same time appeared in the comprehensive dictionary of medical terms mediLexicon2 and were missing from the general bilingual seed dictionary . The gold standard 2 _CITE_ [ 1 . 4 . 2010 ] contains 360 English single - word terms with their translations into Slovene . If more than one translation variant is possible for a single English term , all variants appear in the gold standard and any of these translations suggested by the algorithm is considered as correct . Below we present the results of three experiments that best demonstrate the performance and impact of the key parameters for bilingual lexicon extraction from comparable corpora that we were testing in this research .__label__Material|Data|Introduce
In other words , as long as a candidate has been confirmed once , it is assumed to be a protein throughout . In this way , there are two filtering alternatives M31 and M32 from M1 and M2 , respectively . To get more objective evaluation , we utilized another corpus of 101 abstracts used by Yapex [ _CITE_ Using the test corpus and answer keys supported in Yapex project , the evaluation results on filtering strategies are listed in Table 1 .__label__Material|Data|Use
Both annotators disagreed in only 2 % of the cases . The numbers of annotations , including their distribution over positive and negative instances , are summarized in Table 3 . The corpus is made publicly available at_CITE_ 2673424 ( Hartung and Zwick , 2014 ). In order to alleviate the imbalance of positive and negative examples in the data , additional positive examples have been gathered by manually searching PubMed7 . At this point , special attention has been paid to extract only instances denoting the correct gene / protein corresponding to the full long name , as we are interested in assessing the impact of examples of a particularly high quality .__label__Material|Data|Introduce
Syntactic tag % Gloss Syntactic tag % Gloss acl 1 . 89 adjectival clause advcl 0 . 70 adverbial clause modifier advmod 2 . 12 adverbial modifier amod 8 . 34 adjectival modifier appos 1 . 69 appositional modifier aux 4 . 35 auxiliary auxpass 0 . 71 passive auxiliary case 9 . 80 case marking cc 3 . 09 coordinating conjunction ccomp 1 . 03 clausal complement compound 3 . 02 compound conj 3 . 80 conjunct cop 1 . 41 copula csubj 0 . 12 clausal subject csubjpass 0 . 03 clausal passive subject dep 0 . 01 unspecified dependency det 0 . 98 determiner discourse 0 . 71 discourse element dislocated 0 . 01 dislocated elements dobj 3 . 92 direct object expl 0 . 00 expletive foreign 0 . 01 foreign words goeswith 0 . 08 goes with iobj 0 . 22 indirect object list 0 . 00 list mark 3 . 59 marker mwe 0 . 32 multi - word expression name 1 . 56 name neg 0 . 30 negation modifier nmod 17 . 05 nominal modifier nsubj 5 . 97 nominal subject nsubjpass 0 . 65 passive nominal subject nummod 2 . 05 numeric modifier parataxis 1 . 47 parataxis punct 12 . 86 punctuation remnant 0 . 14 remnant in ellipsis root 4 . 51 root vocative 0 . 00 vocative xcomp 1 . 50 open clausal complement Table 1 : Syntactic tags in Croatian UD , sorted alphabetically , and listed together with their relative frequencies and short glosses . The frequencies are calculated for Croatian only , and for the entire collection ( train , dev , test ). The syntactic tags are further explained in the UD documentation : _CITE_ TIMES . HR to provide Croatian UD with a clean , unbiased start , contrasting the manual creation experience of McDonald et al . ( 2013 ) to the one of automatic conversions within the HamleDT project of Zeman et al .__label__Supplement|Document|Produce
The semantic zone most frequently refers to ontological concepts , either directly or with property - based modifications , but can also describe word meaning extra - ontologically , for example , in terms of modality , aspect , time , etc . The current English lexicon contains approximately 25 , 000 senses , including most closed - class items and many of the most frequent and polysemous verbs , as targeted by corpus analysis . ( An extensive description of the lexicon , formatted as a tutorial , can be found at _CITE_ )__label__Supplement|Document|Produce
After we had obtained the extracted verb features , we then determined the probability of causal and non causal from the occurrences of the cartesian products of three verb feature concepts , shown in Table2 , by using Weka which is a software tool for machine learning ( _CITE___label__Method|Tool|Use
We used the Text :: Similarity v . 0 . 09 module to obtain the overlap value between two bags of words . Text similarity is based on counting the number of overlapping tokens between the two strings , normalized by the length of the strings . In the second approach , Sense Similarity , the basis for sense alignment is the Personalized Page Rank ( PPR ) algorithm ( Eneko and Soroa , 2009 ) relying on a lexical - semantic knowledge base model as a graph G = ( V , E ) as available in the UKB tool suite _CITE_ . As knowledge base we have used WN 3 . 0 extended with the “ Princeton Annotated Gloss Corpus ”. Each vertex v of the graph is a synset , and the edges represent semantic relations between synsets ( e . g .__label__Method|Tool|Introduce
Our initial Talking Head was based around the Stelarc Prosthetic Head which combines multiple off - the - shelf components : keyboard input to a chatbot ( AliceBot ) is linked to speech synthesis ( IBM ViaVoice ) and 3D face rendering ( Eyematic ). More recently we have adopted Head X5 which is capable of generating a continuous , synchronized , optionally subtitled audiovisual speech stream in many different languages , with the ability to switch and modify voices and morph different faces at the same time as interacting with the user . The system is designed to be able to use different speech and face technologies , and we in general use Microsoft ’ s SAPI for speech recognition and generation plus the FaceGen face generation technology _CITE_ . We have been predominantly exploring the application of our Talking Head as a virtual tutor of various subject areas . Initially our focus was language teaching / learning , but more recently demand for assistance with social teaching and assistant / companion applications has redirected our efforts .__label__Method|Tool|Use
A key factor in personal data management is the highly contextual nature of privacy related issues ; privacy concerns and practices are situated in their context ( Nissenbaum , 2009 ) and influenced by cultural issues ( Milberg et al ., 2000 ). The diversity of technology in the AAC sector is set to increase dramatically . Apple ’ s iPad _CITE_ has caused a huge investment in tablet technology . Multiple , third party applications ( e . g . proloque2go , myVoice , and verbally ) already exist that allow this new range of tablets to function as AAC devices .__label__Supplement|Website|Introduce
The system obtained an average precision of 77 % and an average recall of 35 . 1 % for all four drugs . To the best of our knowledge , the system described in ( Segura - Bedmar et al ., 2014 ) is the only one that has dealt with the detection of drugs and their effects from Spanish social media streams . The system used the Textalytics tool _CITE_ , which follows a dictionary - based approach to identify entities in texts . The dictionary was constructed based on the following resources : CIMA and MedDRA . CIMA is an online information center maintained by the Spanish Agency for Medicines and Health Products ( AEMPS ).__label__Method|Tool|Use
The authors constructed a corpus of 3000 parallel sentences , which were translated manually from monolingual online Farsi doc uments at New Mexico State University . More recently Qasemizadeh et al . ( 2007 ) participated in the Farsi part of MULTEXT - EAST _CITE_ project ( Erjavec , 2010 ) and developed about 6000 sentences . There is also a corpus available in ELRA consisting of about 3 , 500 , 000 English and Farsi words aligned at sentence level ( about 100 , 000 sentences ). This is a mixed domain dataset including a variety of text types such as art , law , culture , literature , poetry , proverbs , religion etc .__label__Method|Tool|Introduce
The synset 00198451 - n is translated as 晋什 jìnshén , which should have been 晋升 jìnsheng ‘ promotion ’. 00198451 - n promotion “ act of raising in rank or position ” ( iii ) Need M de / A de to match the English POS The synset 01089369 - a is an adjectival , but the translation 兼职 jidnzhí ‘ part time ’ is a verb / noun , so we add 的 de to it ( 1 . 3 ). 01089369 - a part - time ; part time “ involving less than the standard or customary time for an activity ”: parttime employees ; a part - time job To improve the coverage and accuracy of COW , we make reference not only to many authoritative bilingual dictionaries , such as The American Heritage Dictionary for Learners of English ( Zhao , 2006 ), The 21st Century Unabridged EnglishChinese Dictionary ( Li , 2002 ), Collins COBUILD Advanced Learner ' s English - Chinese Dictionary ( Ke , 2011 ), Oxford Advanced Learner ' s EnglishChinese Dictionary ( 7th Edition ) ( Wang , Zhao , & Zou , 2009 ), Longman Dictionary of Contemporary English ( English - Chinese ) ( Zhu , 1998 ), etc ., but also online bilingual dictionaries , such as iciba _CITE_ , youdao , lingoes10 , dreye11 and bing12 . For example , the English synset 00203866 - v can be translated as 变坏 biàn huài ‘ decline ’ and 恶化 èhuà ‘ worsen ’, which are not available in the current wordnet , so we added them to COW . 00203866 - v worsen ; decline “ grow worse ”: Conditions in the slum worsened PWN groups nouns , verbs , adjectived and adverbs into synonyms ( synsets ), most of which are linked to other synsets through a number of semantic relations .__label__Material|Data|Use
Emoticons : Another challenge has to do with the limited usefulness of emoticons , because Arabic ’ s smileys and sad emoticons are often mistakenly interchanged . Thus , many tweets have words and emoticons that are contradictory in sentiment . For example : meaning : I have a sister from which I seek the protection of Allah ( negative ) : followed by a smilie Use of dialects : Though most Arabic speakers can read and understand MSA , they generally use different Arabic dialects in their daily interactions including online social interaction _CITE_ . There are 6 dominant dialects , namely Egyptian , Moroccan , Levantine , Iraqi , Gulf , and Yemeni . Dialects introduce many new words into the language , particularly stopwords ( ex .__label__Supplement|Website|Introduce
See Figure 4 for details . Our part - of - speech tagging data set is the standard data set from Wall Street Journal included in PennIII ( Marcus et al ., 1993 ). We use the standard splits and construct our data set in the following way , following Søgaard ( 2010 ): Each word in the data wi is associated with a feature vector xi = ( x _CITE_i , x i ) where x _CITE_i is the prediction on wi of a supervised partof - speech tagger , in our case SVMTool _CITE_ ( Gimenez and Marquez , 2004 ) trained on Sect . 0 – 18 , and x i is a prediction on wi from an unsupervised part - ofspeech tagger ( a cluster label ), in our case Unsupos ( Biemann , 2006 ) trained on the British National Corpus . We train a semi - supervised condensed nearest neighbor classifier on Sect .__label__Method|Tool|Use
If V2 is on the list of 完Tt掉開壞 A , which is a subclass of the VV compounds that are often called resultative compounds , for there is a causal relation between the event represented by the first compound of such a compound and the event / state represented by the second component . In this section , we discuss the experiment we designed , the evaluation and error analysis . The first step is to create a list of term pairs , which a total of 561 , 703 words covered in CWN _CITE_ , Sinica BOW , and Ministry of Education Online Chinese Dictionary . In this experiment , we focus only on bi - syllabic words represented by two characters , which constitute the largest proportion of Chinese vocabulary repository . In order to filter out a coarse - grained bisyllabic word list , only both characters of a bi - syllabic word that could be found in the big word list , are preserved .__label__Material|Data|Use
First , word alignments in both directions are calculated . We used a multi - threaded version of the GIZA ++ tool ( Gao and Vogel , 2008 ). _CITE_ This speeds up the process and corrects an error of GIZA ++ that can appear with rare words . Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit . The parameters of Moses were tuned on newstest2008 , using the ‘ new ’ MERT tool .__label__Method|Tool|Use
We have proved the need of a temporal analysis at document level . For that , we have proposed a simple strategy that acquires implicit relations and it obtains a more complete time - anchoring . _CITE_ The approach has been evaluated on the TimeLine extraction task and the results show that the performance can be doubled when using implicit relations . As future work , we plan to explore in more detail this research line by applying more sophisticated approaches in the temporal analysis at document level . However , this is not the only research line that we want to go in depth .__label__Method|Algorithm|Produce
For these , the literature provides a section - level alignment only . The DDD builds on the earlier efforts of the TITUS project ( Thesaurus of Indo - European Text and Language Materials , Thesaurus Indogermanischer Text - und Sprachmaterialien ) that provided digitized editions of texts in old Germanic languages as well as other Indo - European and selected non - Indo - European languages ( Gippert , 2011 ). _CITE_ The annotations are mostly derived from the literature and existing glossaries that provide grammatical information for all known OHG and OS words , together with their exact source . These have been digitized , automatically applied to the text , manually refined using the annotation software ELAN , augmented with metadata , and finally published via the ANNIS database ( Linde and Mittmann , 2013 ). The annotated corpus is published under a CCBY - SA license over http :// www . laudatio .__label__Method|Tool|Introduce
For evaluation in the HOO framework , a distinction is made between scores and measures . The complete evaluation mechanism is described in detail in ( Dale and Narroway , 2012 ) and on the HOO - 2012 website . _CITE_ Scores Three different scores are used : Measures For each score , three measures are calculated : precision ( 1 ), recall ( 2 ) and F - score ( 3 ). where tp is the number of true positives ( the number of instances that are correctly found by the system ), fp the number of false positives ( the number of instances that are incorrectly found ), and fn the number of false negatives ( missing results ). where Q is used as a weight factor regulating the trade - off between recall and precision .__label__Supplement|Website|Introduce
This approach is denoted as PRvis + Per ( PMI ). This section discusses the experimental design for evaluating the proposed approaches to labelling topics with images . To our knowledge no data set for evaluating these approaches is currently available and consequently we developed one for this study _CITE_ . Human judgements about the suitability of images are obtained through crowdsourcing . We created a data set of topics from two collections which cover a broad thematic range : police , officer , crime , street , man , city , gang , suspect , arrested , violence game , season , team , patriot , bowl , nfl , quarterback , week , play , jet military , afghanistan , force , official , afghan , defense , pentagon , american , war , gates categories ( e . g .__label__Material|Data|Produce
Naturally , this is only an overall view of the results obtained . The new extended WordNet . PT version is also a crucial resource allowing for contrastive studies on lexicalization patterns depending on semantic domains or on frequency of use , for instance , for all or for specific Portuguese varieties . In order to make these data publicly available , a new WordNet . PT version , the WordNet . PTglobal has been released on the WWW _CITE_ . Releasing the WordNet . PT fragment extended to Portuguese varieties online involved developing an updated version of the web interface for wordnet online navigation . In Section 3 we present the main features of this web interface and how users can navigate and straightforwardly access the data on Portuguese varieties .__label__Method|Tool|Produce
These state charts model various subdialogues like question - answering , offer , threat , greetings , closings , etc . The DM also implements advanced features like topic - tracking and grounding ( Roque and Traum , 2009 ). The virtual human character de livers synthesized speech and corresponding nonverbal behavior , based on additional components of the ICT Virtual Human Toolkit _CITE_ . This work was sponsored by the U . S . Army Research , Development , and Engineering Command ( RDECOM ). The content does not necessarily reflect the position or the policy of the U . S . Government , and no official endorsement should be inferred .__label__Method|Tool|Use
We use the data that were recorded and preprocessed by Mitchell et al . ( 2008 ), available for download in their supporting online material . _CITE_ Full details of the experimental protocol , data acquisition and preprocessing can be found in Mitchell et al . ( 2008 ) and the supporting material . Key points are that there were nine right - handed adult participants ( 5 female , age between 18 and 32 ).__label__Material|Data|Use
We use the data that were recorded and preprocessed by Mitchell et al . ( 2008 ), available for download in their supporting online material . _CITE_ Full details of the experimental protocol , data acquisition and preprocessing can be found in Mitchell et al . ( 2008 ) and the supporting material . Key points are that there were nine right - handed adult participants ( 5 female , age between 18 and 32 ).__label__Supplement|Document|Use
Second , initial manual comparison between MWE lists ranked according to all measures implemented in the UCS toolkit revealed the most convincing results for the X test . For the time being , we focus on bigram MWE extraction . While the UCS toolkit readily supports work on Unicode - based languages such as Urdu , it does not support trigram extraction ; other freely available tools such as TEXT - NSP _CITE_ do come with trigram support , but cannot handle Unicode script . As a consequence , we currently implement our own scripts to overcome these limitations . The clustering approach taken in this paper is based on Urdu - specific syntactic information that can be gathered straightforwardly from the corpus .__label__Method|Tool|Introduce
We leave the exploitation of other mechanisms of incorporating prior knowledge into model training as future work . The document sentiment is classified based on P ( l | d ), the probability of sentiment label given document , which can be directly obtained from the document - sentiment distribution . We define that a document d is classified as positive We conducted experiments on the four corpora _CITE_ which were derived from product reviews harvested from the website IT168 with each corresponding to different types of product reviews including mobile phones , digital cameras , MP3 players , and monitors . All the reviews were tagged by their authors as either positive or negative overall . The statistics of the four corpora are shown in Table 2 .__label__Material|Data|Use
For this purpose , we created the Paraphrase for Plagiarism corpus ( P4P ) annotating a portion of the PAN - PC - 10 corpus for plagiarism detection ( Potthast et al . 2010 ) on the basis of a paraphrase typology , and we mapped the annotation results with those of the Second International Competition on Plagiarism Detection ( Pan - 10 competition , hereafter ). _CITE_ The results obtained provide critical insights for the improvement of automatic plagiarism detection systems . The rest of the article is structured as follows . Section 2 sets out the paraphrase typology used in this research work .__label__Material|Data|Use
Recognition is made with Sphinx and synthesis with Theta . The back - end applications are directly connected to the HUB through an included stub . Some of our recent developments are also inspired in Voice XML _CITE_ , in an effort to simplify the framework parameterization and development , required in the enterprise context . Voice XML provides standard means of declarative configuration of new systems reducing the need of coding to the related devices implementation ( Nyberg et al ., 2002 ). Our reengineering work aimed at : i ) making the framework more robust and flexible , enhancing the creation of new systems for different domains ; ii ) simplifying the system ’ s development , debug and deployment processes through common techniques from software engineering areas , such as design patterns ( Gamma et al ., 1994 ; Freeman et al ., 2004 ).__label__Method|Tool|Extent
In ( Alkhouli et al ., 2015 ), it is shown that approximate RNN integration into the phrase - based decoder has a slight advantage over n - best rescoring . Therefore , we apply RNNs in rescoring in this work , and to allow for a direct comparison between FFNNs and RNNs , we apply FFNNs in rescoring as well . We perform experiments on the largescale IWSLT 2013 ( Cettolo et al ., 2014 ) German → English , WMT 2015 _CITE_ German → English and the DARPA BOLT Chinese → English tasks . The statistics for the bilingual corpora are shown in Table 2 . Word alignments are generated with the GIZA ++ toolkit ( Och and Ney , 2003 ).__label__Material|Data|Use
We construct 1000 clusters employing the Brown method on 112 million words from the North American New York Times corpus . We keep the top 20 most frequent words for each cluster as paraphrases . To generate LSA paraphrases , we used the Infomap software _CITE_ on a 34 million word collection of articles from the American News Text corpus . We used the default parameter settings : a 20 , 000 word vocabulary , the 1000 most frequent words ( minus a stoplist ) for features , a 15 word context window on either side of a word , a 100 feature reduced representation , and the 20 most similar words as paraphrases . While we experimented with several parameter settings for LSA and Brown methods , we do not claim that the selected settings are necessarily optimal .__label__Method|Tool|Use
Figure 1 shows the interaction of the components in our final hybrid system , producing the results submitted to the CoNLL 2014 shared task . The following sections describe each of these components in detail . The rule - based system is a component of the SelfAssessment and Tutoring ( SAT ) system , a web service developed at the University of Cambridge aimed at helping intermediate learners of English in their writing tasks _CITE_ ( Andersen et al ., 2013 ). The original SAT system provides three main functionalities : 1 ) text assessment , producing an overall score for a piece of text , 2 ) sentence evaluation , producing a sentence - level quality score , and 3 ) word - level feedback , suggesting specific corrections for frequent errors . Since the focus of the shared task is on strict correction ( as opposed to detection ), we only used the word - level feedback component of the SAT system .__label__Method|Tool|Introduce
The formulae produce an unique score , the C - score , which gives an estimation of user ’ s reading comprehension of a certain text . The score can be used to evaluate the performance of a text simplification engine on pairs of complex and simplified texts , or to compare the performances of different TS methods using the same texts . The approach can be particularly useful for the modern crowdsourcing approaches , such as those employing the Amazon ’ s Mechanical Turk _CITE_ or CrowdFlower . The aim of this paper is thus to propose an evaluation approach and to motivate the TS community to start a relevant discussion , in order to come up with a common evaluation metrics for this task . Currently , the area of Text Simplification ( TS ) is getting more and more attention .__label__Supplement|Website|Use
For definitional QA task , Lin ( 2002 ) presented an approach in which web - based answer reranking is combined with dictionary - based ( e . g ., WordNet ) reranking , which leads to a 25 % increase in mean reciprocal rank ( MRR ). Xu et al . ( 2003 ) proposed a statistical ranking method based on centroid vector ( i . e ., vector of words and frequencies ) learned from the online encyclopedia ( i . e ., Wikipedia _CITE_ ) and the web . Candi date answers were reranked based on their similarity ( TFIDF score ) to the centroid vector . Similar techniques were explored in ( BlairGoldensohn et al ., 2003 ).__label__Supplement|Website|Use
The corpus comes from a collection of blog postings from 2004 on . These blog postings come from around the world , and in a variety of languages . We view this as an excellent example of an unstructured corpus for event detection since it is composed of blog articles harvested by BlogLines _CITE_ . The documents come from some standard news sources , but also from any blogging service which provides rss feeds , such as livejournal , local newspapers , wordpress , and many more . For this experiment , we collected only English articles from the blog corpus , but the algorithm could be used in practice with any language .__label__Material|Data|Use
The main source of our parallel data was CzEng 1 . 0 ( Bojar et al ., 2012b ). We also used Europarl ( Koehn , 2005 ) as made available by WMT13 organizers . _CITE_ The English - Czech part of the new Common Crawl corpus was quite small and very noisy , so we did not include it in our training data . Table 2 provides basic statistics of the data . Processing large parallel data can be challenging in terms of time and computational resources required .__label__Material|Data|Use
Surprisingly , there was not a significant change in our ROUGE scores over the three different summary lengths . This indicates that our system can create summaries of any length without losing its content . We also conduct manual evaluations utilizing a crowdsourcing tool _CITE_ . In this experiment , our system with 15 segments is compared with FUSION , human - authored summaries ( ABS ) and , humanannotated extractive summaries ( EXT ). After randomly selecting 10 meetings , 10 participants were selected for each meeting and given instructions to browse the transcription of the meeting so as to understand its gist .__label__Method|Tool|Use
Enclitics in Manipuri fall into six categories : determiners , case markers , the copula , mood markers , inclusive / exclusive and pragmatic peak markers and attitude markers . The role of the enclitics used and its meaning differs based on the context . Using factored approach , a tighter integration of linguistic information into the translation model is done for two reasons _CITE_ : ■ Translation models that operate on more general representations , such as lemma instead of surface forms of words , can draw on richer statistics and overcome the data sparseness problem caused by limited training data . ■ Many aspects of translation can be best explained at a morphological , syntactic or semantic level . Having such information available to the translation model allows the direct modeling of these aspects .__label__Method|Algorithm|Introduce
It helps avoiding misunderstandings and giving them a sense of being involved and committed . Annotation tools Although there exists many annotation tools , few are actually available , free , downloadable and usable . Among those tools are Callisto , MMAX2 , Knowtator or Cadixe _CITE_ which was used in the reported experiment . The features and the annotation language expressivity must be adapted to the targeted annotation task : is it sufficient to type the textual segments or should they also be related ? is it possible / necessary to have concurrent or overlapping annotations ?__label__Method|Tool|Use
This is good since we want similar words to be close together but not have exactly the same input vector . The words that are still clustered to the same input are mostly numbers or typing mistakes like “ YouTube ” and “ Youtube ”. The translation system for the German - to - English task was trained on the European Parliament corpus , News Commentary corpus , the BTEC corpus and TED talks _CITE_ . The data was preprocessed and compound splitting was applied for German . Afterwards the discriminative word alignment approach as described in Niehues and Vogel ( 2008 ) was applied to generate the alignments between source and target words .__label__Material|Data|Use
We formu late our problem in terms of MALLET ’ s SimpleTagger class which is a command line interface to the MALLET CRF class . We modify the SimpleTagger class in order to include the provision for producing corresponding posterior probabilities of the predicted labels which are used later for ranking sentences . We build the MaxEnt system using Dr . Dekang Lin ’ s MaxEnt package _CITE_ . To define the exponential prior of the A values in MaxEnt models , an extra parameter α is used in the package during training . We keep the value of α as default .__label__Method|Code|Use
Compared with its performance on CTB5 ( in Table 4 ), our Nonlocal & Cluster system also got 0 . 8 % improvement . All these results show that our approach can become more powerful when given more labeled training data . To better understand the linguistic behavior of our systems , we employed the berkeley - parseranalyser tool _CITE_ ( Kummerfeld et al ., 2013 ) to categorize the errors . Table 6 presents the average number of errors for each error type by our parsing systems . We can see that almost all the Worst numbers are produced by the Pipeline system .__label__Method|Tool|Use
In a nutshell , we analyse the entries of an encyclopedia with the aid of a noun hierarchy . Our motivation is that proper nouns that form entities can be obtained from the entries in an encyclopedia and that some features of their definitions in the encyclopedia can help to classify them into their correct entity category . The encyclopedia used has been Wikipedia _CITE_ . According to the English version of Wikipedia 2 , Wikipedia is a multi - lingual web - based , freecontent encyclopedia which is updated continuously in a collaborative way . The reasons why we have chosen this encyclopedia are the following : The noun hierarchy used has been the noun hierarchy from WordNet ( Miller , 1995 ).__label__Supplement|Website|Use
In fact , the adherence to one model rather than another has an impact on who should be performing the evaluation . Senseval - 2 was in line with Putnam ’ s view of ‘ division of linguistic labour ’ by relying on lexicographers ’ judgments to build a gold standard ( Kilgarrif , 1998 ). On the other hand , Senseval - 3 collected data via Open - Mind Initiative _CITE_ , which was much more in line with Fodor ’ s view that any common people can use their own similarity metric to disambiguate polysemous terms . Interestingly , a recent empirical study ( Murray and Green 2004 ) showed how judgments by ordinary people were consistent among themselves but different from the one of lexicographers . It is important to decide who the best judges are ; a decision which can certainly be based on the foreseen application , but also , as we suggest here , on some theoretical grounds .__label__Material|Data|Introduce
The algorithm parameters are tuned on the development set . In order to compare our approach with a nonsequential one , we perform the classification task also using Support Vector Machines ( Vapnik , 1998 ). We use YAMCHA _CITE_ , a tool developed for chunking tasks , in which SVMs are easily combined with different context window sizes and dynamic features ( Kudo and Matsumoto , 2001 ). Since this work is only a preliminary step towards the automatic identification and extraction of biographical information from Wikipedia , we first experiment with the simplest approach . Therefore , we consider a small set of shallow features extracted only from section titles , and we ignore the content of the sections .__label__Method|Tool|Use
Speakerindependent triphones are used as acoustic models . The Finnish , Estonian , and Turkish data sets contain planned speech , i . e ., written text read aloud . By contrast , the Arabic data consists of transcribed spontaneous telephone conversations , _CITE_ which are characterized by disfluencies and by the presence of “ non - speech ”, such as laugh and cough sounds . There are multiple speakers in the Arabic data , and online speaker adaptation has been performed . The n - gram language models are trained using the SRILM toolkit ( Stolcke , 2002 ) ( Fin1 , Fin2 , Tur1 , Tur2 , ECA ) or similar software developed at HUT ( Siivola and Pellom , 2005 ) ( Fin3 , Fin4 , Est ).__label__Material|Data|Introduce
First , we propose how to reduce paraphrase generation costs by early exclusion of low - accuracy crowdworkers . Second , we compare two HIT designs for evaluating phrase pairs on a continuous semantic similarity scale . In order to evaluate our crowdsourcing strategies , we conduct our own experiments via the CROWDFLOWER _CITE_ platform . The rest of the paper is structured as follows . Section 2 first gives an overview of related work and lines out current approaches .__label__Method|Tool|Use
Furthermore , the results indicate that target features is the most informative of the tested feature classes . The neural network is implemented in Theano ( Bergstra et al ., 2010 ), and is publicly available on Github . _CITE_ Nous avons cette banniere dans nos bureaux ˆ Palo Alto number of preceding POS tags . Window size is varied in a symmetrical fashion of n + n . When varying window size , 3 preceding POS tags are used .__label__Method|Code|Produce
Our results demonstrate that , despite their simplicity , deterministic models for coreference resolution obtain competitive results , e . g ., we obtained the highest scores in both the closed and open tracks ( 57 . 8 and 58 . 3 respectively ). The code used for this shared task is publicly released . _CITE_ We thank the shared task organizers for their effort . This material is based upon work supported by the Air Force Research Laboratory ( AFRL ) under prime contract no . FA8750 - 09 - C - 0181 .__label__Method|Code|Produce
In this paper , we obtained a subset of Slovnyk for two language pairs : English - Ukrainian , and Ukrainian - Spanish . This has been converted into RDF , with a separate lexicon for each language using the lemon model ( McCrae et al ., 2011 ), and a translation set for each language pair . As Slovnyk mainly contains nouns and noun phrases , we automatically extracted verbs from the Apertium Russian - Ukrainian bilingual lexicon _CITE_ . Apertium ( Corbí Bellot et al ., 2005 ) was an opensource rule - based Machine Translation platform , which therefore heavily relies on bilingual lexica and grammars . It is now supported by an online community10 .__label__Material|Data|Use
Two vectors of features are used , one where the seed set is the subjective seed set , and one where it is the objective seed set . In summary , we use the following features ( here , SS is the subjective seed set and OS is the objective one ). We perform 10 - fold cross validation experiments on several data sets , using SVM light ( Joachims , 1999 ) _CITE_ under its default settings . Based on our random sampling of WordNet , it appears that WordNet nouns are highly skewed toward objective senses . ( Esuli and Sebastiani , 2007 ) argue that random sampling from WordNet would yield a corpus mostly consisting of objective ( neutral ) senses , which would be “ pretty useless as a benchmark for testing derived lexical resources for opinion mining [ p . 428 ].” So , they use a mixture of subjective and objective senses in their data set .__label__Method|Tool|Use
Thanks to the fact that it is translated into English , it is open to international audiences . To the best of our knowledge , most correspondence seminars are organised in the area of former Czechoslovakia . The Slovak seminars include KMS ( mathematics ), FKS ( physics ), and STROM _CITE_ ( mathematics ). The last mentioned one claims to have the longest tradition in the area of former Czechoslovakia , having been established in 1976 . Correspondence seminars organised in the Czech Republic include MKS ( mathematics ; founded 1981 ), FYKOS ( physics ; 1986 ), and KSICHT ( chemistry ; 2002 ; cf .__label__Material|Data|Use
For example , the CUI for “ Original ,” another term mapped from the caption shown in Figure 1 , is “ C0205313 .” Our results indicate that “ C0205313 ,” which occurs 19 times in our evaluation data , never identifies a useful indexing term . F . 2 Semantic Type ( nominal ): The concept ’ s semantic categorization . There are currently 132 different semantic types _CITE_ in the UMLS Metathesaurus . For example , The semantic type of “ Original ” is “ Idea or Concept .” F . 3 Presence in Caption ( nominal ): true if the phrase that generated the concept is located in the image caption ; false if the phrase is located in the image mention . F . 4 MeSH Ratio ( real ): The ratio of words ci in the concept c that are also contained in the Medical Subject Headings ( MeSH terms ) M assigned to the document to the total number of words in the concept .__label__Material|Data|Introduce
As our last experiment , we evaluate the word spaces on a dialogue act tagging task ( Stolcke et al ., 2000 ) over the Switchboard corpus ( Godfrey et al ., 1992 ). Switchboard is a collection of approximately 2500 dialogs over a telephone line by 500 speakers from the U . S . on predefined topics . _CITE_ The experiment pipeline follows ( Milajevs and Purver , 2014 ). The input utterances are preprocessed so that the parts of interrupted utterances are concatenated ( Webb et al ., 2005 ). Disfluency markers and commas are removed from the utterance raw texts .__label__Material|Data|Use
Specifically , we take the PCFG encoding of the LDA topic model described above , but modify it so that the Topici nodes generate sequences of words rather than single words . Then we adapt each of the Topici nonterminals , which means that we learn the probability of each of the sequences of words it can expand to . In order to demonstrate that this model works , we implemented this using the publicallyavailable adaptor grammar inference software , _CITE_ and ran it on the NIPS corpus ( composed of published NIPS abstracts ), which has previously been used for studying collocation - based topic models ( Griffiths et al ., 2007 ). Because there is no generally accepted evaluation for collocation - finding , we merely present some of the sample analyses found by our adaptor grammar . We ran our adaptor grammar with ` = 20 topics ( i . e ., 20 distinct Topici nonterminals ).__label__Method|Tool|Use
Instead of returning a long list of documents more or less related to the query parameters , the aim of a QA system is to isolate the exact answer as accurately as possible , and to provide the user only a short text clip containing the required information . One of the major development challenges is evaluation . The conferences such as TREC , CLEF _CITE_ and NTCIR have provided valuable QA evaluation methods , and in addition produced and distributed corpora of questions , answers and corresponding documents . However , these conferences have focused mainly on fact - based questions with short answers , so called factoid questions . Recently more complex tasks such as list , definition and discoursebased questions have also been included in TREC in a limited fashion ( Dang et al ., 2007 ).__label__Supplement|Website|Introduce
In this paper we detail our submission to SemEval Task 2 . We use an improved and revised version of the system presented in our SemEval 2014 submission ( Gupta et al ., 2014 ). As in Gupta et al ., 2014 , we employ a Machine Learning ( ML ) method which exploits available NLP technology , adding features inspired by deep semantics ( such as parsing and paraphrasing ) with distributional Similarity Measures , Conceptual Similarity Measures , Semantic Similarity Measures and Corpus Pattern Analysis _CITE_ ( CPA ). The remainder of the paper is structured as follows . Section 2 describes our approach , i . e .__label__Method|Algorithm|Extent
Currently , there are 109 composers in this category . We evaluated manually the matching accuracy for the articles in this category , obtaining 95 . 5 % and 89 . 1 % matching accuracy for the 15th edition and 11th edition respectively , with the lower matching accuracy for 11th edition mainly caused by segmentation errors . We used Web - based Analysis and Visualization Environment ( WEAVE ) _CITE_ to visualize and analyze the relative importance , rank , and its change over time for the historical figures in this category . Figure 3 illustrates the change in importance . The legend on the left lists the composers alphabetically .__label__Method|Tool|Use
We define context words to be 5 words to the left / right for all considered methods . We use three word similarity datasets each containing 353 , 3000 , and 2034 word pairs . _CITE_ We report the average similarity score across these datasets under the label AVG - SIM . We use two word analogy datasets that we call SYN ( 8000 syntactic analogy questions ) and MIXED ( 19544 syntactic and semantic analogy questions ). We implemented the template in Figure 2 in C ++.__label__Material|Data|Use
We selected two genres where music descriptions between pieces were common , jazz and classical music . The programmes we used were broadcast on BBC Radio Three . We transcribed sixty - four discussions ; to maintain uniformity , we followed the Linguistic Data Consortium ’ s transcription guidelines _CITE_ . This was not a thorough corpus collection ; the purpose of collecting examples was to gain a sense of what disc jockeys tend to discuss and compare . Based on the transcribed examples , we selected and hand - wrote twelve database entries for music tracks , using the authoring tool developed by the M - PIRO project ( Androutsopoulos et al ., 2007 ).__label__Supplement|Document|Extent
Since the output is generated via templates , other formats could also be generated to feed into alternate tools . In addition to generating existing XML formats it is also useful to generate data in a form that is easily consumed by custom applications . JSON ( Javascript Object Notation _CITE_ ) is a data format that is frequently used to transport data in modern web applications and is easily parsed by libraries in many target languages . The DADA JSON interface will deliver descriptions of any kind of object in the data store in a way that makes it easy to implement clients that present interactive displays of the data . As a demonstration of the web based API allowing remote clients to read annotation data from the server , we have implemented a Javascript based browser for annotation data that is able to show data aligned with the source video data .__label__Method|Tool|Use
The disambiguated items with high confidence correspond to more than 50 % of all the bank & apos ; bank & apos ; number & apos ; number & apos ; hood & apos ; hood & apos ; content words . As a result of the disambiguation step , we obtain sense - annotated data comprising around one billion tagged words with at least five occurrences and 2 . 5 million unique word senses . The disambiguated text is processed with the Word2vec ( Mikolov et al ., 2013a ) toolkit _CITE_ . We applied Word2vec to produce continuous representations of word senses based on the distributional information obtained from the annotated corpus . For each target word sense , a representation is computed by maximizing the log likelihood of the word sense with respect to its context .__label__Method|Tool|Use
The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter ( Tseng et al ., 2005 ). The regularity parameters Sk are set to be 3 , the same as the Stanford segmenter , because no significant performance improvements were observed by tuning that parameter . To extract features for the word splitter , the Stanford named entity recognizer ( Finkel et al ., 2005 ) _CITE_ was employed to obtain the tags of named entities . Word frequencies were caculated from the source side of SMT training corpus . The character - level unsupervised alignment was conducted using GIZA ++ ( Och and Ney , 2003 ) .__label__Method|Tool|Use
Metric : We use the case - insensitive 4gram NIST BLEU as our evaluation metric , with statistical significance test with signtest ( Collins et al ., 2005 ) between the proposed models and two baselines . We use the tagCNN and inCNN joint language models as additional decoding features to a dependency - to - string baseline system ( Dep2Str ), and compare them to the neural network joint model with 11 source context words ( Devlin et al ., 2014 ). We use the implementation of an open source toolkit _CITE_ with default configuration except the global settings described in Section 5 . 1 . Since our tagCNN and inCNN models are source - totarget and left - to - right ( on target side ), we only take the source - to - target and left - to - right type NNJM in ( Devlin et al ., 2014 ) in comparison . We call this type NNJM as BBN - JM hereafter .__label__Method|Code|Use
It is also possible to view the definitions that originated the path . FrameNet ( Baker et al ., 1998 ) is a manually built knowledge base structured on semantic frames that describe objects , states or events . There are several means for exploring FrameNet easily , including FrameSQL ( Sato , 2003 ) _CITE_ , which allows searching for frames , lexical units and relations in an integrated interface , and FrameGrapher , a graphical interface for the visualization of frame relations . For each frame , in both interfaces , a textual definition , annotated sentences of the frame elements , lists of the frame relations , and lists with the lexical units in the frame are provided . ReVerb ( Fader et al ., 2011 ) is a Web - scale information extraction system that automatically acquires binary relations from text .__label__Method|Tool|Use
Otherwise , we may alter the coherence of the text and decrease its readability . This leaves us with 19 simplification rules . To apply them , the candidate structures for simplification first need to be detected using regular expressions , via Tregex _CITE_ ( Levy and Andrew , 2006 ) that allows the retrieval of elements and relationships in a parse tree . In a second step , syntactic trees in which a structure requires simplification are modified according a set of operations implemented through Tsurgeon . The operations to perform depend on the type of rules : 1 .__label__Method|Algorithm|Use
The development corpus and test corpus were taken from the evaluation dataset in IWSLT 2006 ( 489 tuning and 500 test sentences with 7 references ). The EUROPARL model was trained using the EUROPARL corpora with approximately 1 . 3M sentence pairs , leaving out 1K sentences for tuning and another 1K sentences for tests . In the IWSLT experiment , word alignments were generated using an HMM model ( Vogel et al ., 1996 ), with symmetric posterior constraints ( V . Grac ¸ a et al ., 2010 ), using the Geppetto toolkit _CITE_ . This setup was used in the official evaluation in ( Ling et al ., 2010 ). For the EUROPARL experiment the word alignments were generated using IBM model 4 .__label__Method|Tool|Use
A goal of this paper is to investigate a particular way in which Natural Language Processing ( NLP ) can usefully contribute to SLA . In terms of existing work , the subfield of Native Language Identification ( NLI ) has been quite active recently , which looks at predicting the L1 of writers writing in a common L2 within a classification task framework ; see for example the recent NLI shared task with 29 entrants ( Tetreault et al ., 2013 ). _CITE_ From within linguistics , there has been much interest in how data - driven approaches can contribute to SLA . Granger ( 2011 ) discusses a body of work based on the the methodology of carrying out corpus - based approaches to SLA with a focus on NLP tools ; Jarvis and Crossley ( 2012 ) in an edited collection present recent work by linguists who extend the corpus - based setup by using a text classification approach , looking at what feature selection might say for SLA . From within NLP , Swanson and Charniak ( 2013 ) and Swanson and Charniak ( 2014 ) take a data - driven approach to SLA investigations much in the spirit of this work .__label__Supplement|Website|Introduce
Having discussed the ideas driving the web - based teaching platform and exemplified one of the tools , we now return to the courses which have informed our work on the three core modules currently being developed in terms of their content and the use of a web - and implementation environment they make . ALE ( Carpenter and Penn , 1996 ) is a conservative extension of Prolog based on typed feature structures , with a built - in parser and semantic - headdriven generator . The demand for such a utility was so great when it was beta - released in 1992 that it immediately became the subject of early work in graphical front - end development for large constraint - based grammars : first with the Pleuk system ( Calder , 1993 ), then as one of several systems supported by Gertjan van Noord ’ s HDrug _CITE_ , followed by an ALE - mode Emacs user interface ( Laurens , 1995 ). It also provided the computational support for one of the very first web - based computational linguistics courses , Colin Matheson ’ s widely used HPSG Development in ALE . A follow - up course on computational morphology , also by Colin Matheson , was based on ALE - RA , a morphological extension of ALE by Tomaz Erjavec .__label__Method|Tool|Introduce
See table 1 for the complete list of all words selected for the Spanish lexical sample task . The words can belong only to one of the syntactic categories . The fourteen words selected to be translation - equivalents to English has been : The corpus was collected from two different sources : " El Periodico " _CITE_ ( a Spanish newspaper ) and LexEsp ( a balanced corpus of 5 . 5 million words ). The length of corpus samples is the sentence . The lexicon provided was created specifically for the task and it consists of a definition for each sense linked to the Spanish version of EuroWordNet and , thus , to the English WordNet 1 . 5 .__label__Material|Data|Use
Table 1 provides a summary of this corpus . We use mate - tools to perform morphological analysis and parse German sentences ( Bohnet , 2010 ). Then MaltParser _CITE_ converts a parse result into a projective dependency tree ( Nivre and Nilsson , 2005 ). In this paper , we mainly compare our system ( DGST ) with HPB in Moses ( Koehn et al ., 2007 ). We implement our model in Moses and take the same settings as Moses HPB in all experiments .__label__Method|Tool|Use
We built several corpora using two different strategies . The first set was built using Wikipedia and the interlingual links available on articles ( that points to another version of the same article in another language ). We started from the list of all French articles _CITE_ and randomly selected articles that provide a link to Spanish and English versions . We downloaded those , and clean them by removing the wikipedia formatting tags to obtain raw UTF8 texts . Articles were not selected based on their sizes , the vocabulary used , nor a particular topic .__label__Material|Data|Use
However , the task is slightly more difficult than simply mining for a “ similar_to ” relation , which is addressed by our approach in section 5 . As the goal of this paper is to supply the tools for creating a large corpus of analogies from the Web , we require a reliable mechanism for automatically classifying if a text snippet contains an analogy or not . Such classification requires a Gold dataset which we construct in this section and which we make available to the community for download _CITE_ . As we expect the number of analogies in a completely random collection of web documents to be extremely low , we first start by collecting a set of web documents that are likely to contain an analogy by applying some easy - to - implement but rather coarse techniques as follows : In order to obtain a varied set of text snippets ( i . e . short excerpts from larger Web documents ), we first used a Web search engine ( Google Search API ) with simple Hearst - like patterns for crawling potentially relevant websites .__label__Material|Data|Produce
The subphrases were then labeled by human annotators in the same way as the sentences were labeled . Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in ( Le and Mikolov , 2014 ; Kalchbrenner et al ., 2014 ). Sentiment140 _CITE_ ( Go et al ., 2009 ). This is a large - scale dataset of tweets about sentiment classification , where a tweet is automatically labeled as positive or negative depending on the emoticon that occurs in it . The training set consists of 1 . 6 million tweets with emoticon - based labels and the test set of about 400 hand - annotated tweets .__label__Method|Tool|Extent
This often involves user interaction to support the system ’ s interpretation , as in ( Damljanovic et al ., 2010b ). A crucial question is to figure out whether open domain QA techniques , which leverage statistical methods and require minimal ( if any ) intervention at design stage , can be successfully combined with semantic search techniques , in order to leverage the benefits of both . In this perspective , the development and widespread usage of vast knowledge bases such as DBPedia , GeoNames _CITE_ and YAGO ( Suchanek et al ., 2007 ) demonstrates that semantics can encompass universal vocabularies and domains , opening the way to open - domain search . Recent initiatives such as the W3C Linked Open Data community project are fostering the adoption of Linked Data ( LD ) as a best practice . Clearly , the widespread presence of data services makes them a valuable resource for IR ; however , the problems typically addressed in this field concern service discovery ( Carenini et al ., 2008 ), automatic composition ( Martin et al ., 2007 ; Fensel et al ., 2011 ) and mediation ( Manolescu et al ., 2005 ) rather than the issue of interfacing to data services .__label__Material|Data|Introduce
To perform the above analysis , the key is how to construct an experimental data collection which relates hot trends to corresponding related products . We jointly consider microblogs and e - commerce platforms : we obtain hot trends in microblogs and manually identify trend - related products in e - commerce websites . In this paper , we adopt Sina Weibo as the microbloging platform and Taobao _CITE_ as the e - commerce platform , which are the biggest microblogging service and the largest C2C company in China respectively . The analysis method is general and can equally apply to other platforms . For both two data signals , we consider a two - month time span , i . e .__label__Supplement|Website|Use
The pre - processing step connects our MRS elements to a domain ontology and it can create additional states and roles . The pipeline can be reused by other grammars from the Delph - In network . NorSource _CITE_ ( Beermann and Hellan , 2004 ; Hellan and Beermann , 2005 ), a grammar for Norwegian , is a Head - Driven Phrase Structure Grammar ( HPSG ) ( Sag et al ., 2003 ), developed and maintained with the Linguistic Knowledge Builder ( LKB ) tool ( Copestake , 2002 ), and originally based on the HPSG Grammar Matrix , which is a starter kit for developing HPSG grammars ( Bender et al ., 2002 ). An HPSG grammar can use Minimal Recursion Semantics ( MRS ) as meaning representation ( Copestake et al ., 2005 ). In order to speed up the parsing process ( the unification algorithm ), a HPSG grammar can be compiled and run ( parsing ) with the PET tool ( Callmeier , 2001 ).__label__Method|Algorithm|Introduce
Nouns were extracted from WordNet ’ s noun list . Words starting with lower case and upper case letters were determined as NN and NNP , respectively . Nouns in NNS and NNPS categories were collected from the results of POS tagging articles from Plos Biology Journal _CITE_ with TreeTagger . Verbs were extracted from WordNet ’ s verb list . We manually curated VBD , VBN , VBG and VBZ verbs with irregular inflections based on WordNet .__label__Material|Data|Use
We also describe an interface that displays multiple parses compactly and facilitates users to select the desired parse among various possible solutions with a maximum of n − 1 choices for a sentence with n words . Past decade has witnessed a lot of dynamism and upsurge of activities in the field of Sanskrit Computational Linguistics . Several computational tools became available to the Sanskrit community as a web service through the internet _CITE_ . With the availability of a wide coverage grammar for Sanskrit in the form of As . t .¯ adhy ¯ ay ¯ ı , there was a natural tendency to follow the grammar based approach towards the development of these tools ( Huet , 2009 ; Kulkarni et al ., 2010 ; Kulkarni and Ramakrishnamacharyulu , 2013 ; Goyal and Huet , 2013 ). Nevertheless , there were also notable efforts to use pure machine learning approaches for building these tools with a small manually tagged corpus as a boot - strap ( Hellwig , 2009 ).__label__Method|Tool|Introduce
We also describe an interface that displays multiple parses compactly and facilitates users to select the desired parse among various possible solutions with a maximum of n − 1 choices for a sentence with n words . Past decade has witnessed a lot of dynamism and upsurge of activities in the field of Sanskrit Computational Linguistics . Several computational tools became available to the Sanskrit community as a web service through the internet _CITE_ . With the availability of a wide coverage grammar for Sanskrit in the form of As . t .¯ adhy ¯ ay ¯ ı , there was a natural tendency to follow the grammar based approach towards the development of these tools ( Huet , 2009 ; Kulkarni et al ., 2010 ; Kulkarni and Ramakrishnamacharyulu , 2013 ; Goyal and Huet , 2013 ). Nevertheless , there were also notable efforts to use pure machine learning approaches for building these tools with a small manually tagged corpus as a boot - strap ( Hellwig , 2009 ).__label__Supplement|Website|Introduce
For our goal , it is essential to normalize nouns to their singular form . This task is non - trivial , because there are numerous words with irregular plural forms and there exist even word forms that can be either the singular form of one word or the plural form of another . By collecting these exceptions systematically from WordNet , we were able to stem most of them correctly with our Plural - toSingular Stemmer ( PlingStemmer _CITE_ ). For the nongrammatical files , we provide a pseudo - parsing , which links each two adjacent items by an artificial connector . As a result , the uniform output of the preprocessing is a sequence of linkages , which constitutes the input for the core algorithm .__label__Method|Tool|Use
Further variants include different formalizations of norms for parameter regularization , e . g ., f1 , 2 regularization ( Obozinski et al ., 2010 ) or Ei regularization ( Quattoni et al ., 2009 ), where only the features that are most important across all tasks are kept in the model . In our experiments , we apply parameter regularization for multi - task learning to minimum error rate training for patent translation . Our work on patent translation is based on the MAREC _CITE_ patent data corpus . MAREC contains over 19 million patent applications and granted patents in a standardized format from four patent organizations ( European Patent Office ( EP ), World Intellectual Property Organization ( WO ), United States Patent and Trademark Office ( US ), Japan Patent Office ( JP )), from 1976 to 2008 . The data for our experiments are extracted from the EP and WO collections which contain patent documents that include translations of some of the patent text .__label__Material|Data|Use
The corpus is a collection of 30 , 033 sentence pairs and consists of dialogs in travel situations ( 10 , 061 ) and parts of the BTEC corpus ( 19 , 972 ). Details about the provided corpus are described in ( Paul , 2009 ). We used the Stanford Parser _CITE_ to obtain word - level dependency structures of Chinese sentences , and GIZA ++ to obtain word alignments of the biligual corpus . We extracted the SCFG - MWU from the biligual corpus with word alignment . In order to investigate the coverage of the extracted rule , we counted the number of the recovered sentences , i . e .__label__Method|Tool|Use
However , since the heads of nominal mentions are not trivially derivable , they are manually marked in Rich ERE . Furthermore , Light ERE lumped regions , landforms , buildings , and other structures into the Location entity type . Following ACE and to better align with TAC KBP evaluation tasks _CITE_ , Rich ERE separates the Light ERE Location entity type into Facility as well as Location types . Man - made structures and infrastructure are considered Facilities , while regions , landforms , and other non ­ descript sites fall under Locations . Examples include ( note that the heads of nominal mentions are indicated by underscoring ): In addition , we created a new class called Argument Fillers , which are entity - like participants in relations and events that are not annotated at the entity level .__label__Supplement|Website|Extent
We create a graph from a dictionary in the following way . The entries constituted the vertices . Edges between two vertices A and B were added if and only if B appears in A ’ s lemmatized definition _CITE_ as illustrated in Figure 4 . 1 We proceed in this way for each entry and obtained a graph of the dictionary . By extracting the subgraph composed only of verbs , the ’ neighborhood ’ we get for the verb ’ écorcer ’ is illustrated by Figure 4 . 1 . Then we render the graph symmetric and reflexive .__label__Supplement|Document|Use
Three distinct human summaries were produced for each chain . For each chain , one summary was produced for each of the three queries , where the person producing the summary was not shown the next steps in the chain when answering the first query . To simulate the exploratory search of the user we provided the annotators with a Solr _CITE_ query interface for each document collection . The interface allowed querying the document set , reading the documents and choosing sentences which answer the query . After choosing the sentences , annotators can copy and edit the resulting summary in order to create an answer of up to 250 words .__label__Supplement|Website|Produce
First , training data are linguistically annotated . In order to achieve robustness the same tools have been used to linguistically annotate both languages . The SVMTool _CITE_ has been used for PoS - tagging ( Gim ´ enez and M ` arquez , 2004 ). The Freeling package ( Carreras et al ., 2004 ) has been used for lemmatizing . Finally , the Phreco software ( Carreras et al ., 2005 ) has been used for shallow parsing .__label__Method|Tool|Use
An additional data - set was added with random data to act as an outlier to root the tree . PHILOLOGICON software was then used to calculate the lexical metrics corresponding to the individual data files and to measure KL divergences and Rao distances between them . The program NEIGHBOR from the PHYLIP _CITE_ package was used to construct trees from the results . The tree based on Rao distances is shown in figure 1 . The discussion follows this tree except in those few cases mentioning differences in the KL tree .__label__Method|Code|Use
One limitation of our method is that it cannot achieve high yield for PHvst whenever only a small number of paraphrase patterns can be extracted from the bilingual corpus ( see also Figure 5 ). Both the ratio of PHvst to PSeed and the relative yield could probably be increased by scaling up the monolingual corpus . For instance , in the patent domain , monolingual documents 10 times larger than the one used in the above experiments are available at the NTCIR project _CITE_ . It would be interesting to compare the relative gains brought by in - domain versus general - purpose corpora . ( left : probability - based ( 0 . 01 < the < 0 . 9 , th , g = e ), right : similarity - based ( e < th , g < 0 . 9 , the = 0 . 01 )) Finally , we investigated how the number of paraphrase pairs varies depending on the values for the two thresholds , i . e ., thp on the conditional probability and ths on the contextual similarity , respectively .__label__Method|Tool|Introduce
We use the best detokenization technique presented by El Kholy and Habash ( 2010 ). Evaluation Setup All of the training data we use is available from the Linguistic Data Consortium ( LDC ). _CITE_ For SMT training and language modeling ( LM ), we use 200M words from the Arabic Gigaword corpus ( LDC2007T40 ). We use 5 - grams for all LMs implemented using the SRILM toolkit ( Stolcke , 2002 ). MADA + TOKAN ( Habash and Rambow , 2005 ; Habash et al ., 2009 ) is used to preprocess the Arabic text for generation and language modeling .__label__Material|Data|Use
The data set consists of 100 files taken from the Chinese Treebank ( Xue et al ., 2005 ). The source of these files is Xinhua newswire . The annotation is carried out within the confines of the Brandeis Annotation Tool ( BAT ) _CITE_ ( Verhagen , 2010 ). Table 1 reports the inter - annotator agreement of temporal annotation , both between the two annotators ( A and B ) and between each annotator and the judge ( J ), over a training period of ten weeks . Each week , 10 files are assigned , averaging about 315 event pairs for annotation .__label__Method|Tool|Use
We evaluate the quality of our linguistic vectors on a number of tasks that have been proposed for evaluating distributional word vectors . We show that linguistic word vectors are comparable to current state - ofthe - art distributional word vectors trained on billions of words as evaluated on a battery of semantic and syntactic evaluation benchmarks . _CITE_ We construct linguistic word vectors by extracting word level information from linguistic resources . Table 1 shows the size of vocabulary and number of features induced from every lexicon . We now describe various linguistic resources that we use for constructing linguistic word vectors .__label__Method|Algorithm|Produce
Both these systems also use variants of Wattenberg and Vi ´ egas ( 2008 )’ s word tree visualization , which gives a sequential word frequencies as a tree ( i . e ., what computational linguists might call a trie representation of a high - order Markov model ). The “ God bless ” word sense example from § 2 indicates that such statistical summarization of local contextual information may be useful to integrate ; it is worth thinking how to integrate this against the important need of document covariate analysis , while being efficient with the use of space . Many other systems , especially ones designed for literary content analysis , emphasize concordances and keyword searches within a text ; for example , Voyeur / Voyant ( Rockwell et al ., 2010 ), _CITE_ which also features some document covariate analysis through temporal trend analyses for individual terms . Another class of approaches emphasizes the use of document clustering or topic models ( Gardner et al ., 2010 ; Newman et al ., 2010 ; Grimmer and King , 2011 ; Chaney and Blei , 2013 ), while Overview emphasizes hierarchical document clustering paired with manual tagging . Finally , considerable research has examined exploratory visual interfaces for information retrieval , in which a user specifies an information need in order to find relevant documents or passages from a corpus ( Hearst ( 2009 ), Ch .__label__Method|Tool|Introduce
Development and test sets are automatically tagged by the tagger trained on the training set . We use the training data set to train a supertagger of each model using Conditional Random Fields ( CRF ) and the test data set to evaluate the accuracies . We use version 0 . 12 of CRFsuite _CITE_ for our CRF implementation . First - order transitions , and word / POS of uni , bi and trigrams in a 7 - word window surrounding the target word are used as features . Table 3 shows the result of the supertagging accuracies .__label__Method|Tool|Use
The Berkeley parser split / merge capabilities provide a way of smoothing over these differences . For parser evaluation , we use our own implementation of the PARSEVAL metrics . _CITE_ We report labeled precision ( LP ), labeled recall ( LR ), and the labeled F - score ( LF1 ). Note that the labeled evaluation does not only look at constituent labels but also at grammatical functions attached to the constituents , e . g . NP - SBJ for a subject NP .__label__Method|Code|Produce
Section 2 includes an overview of these debate forum data sets . In the experiments , classification accuracy was estimated via five repeats of 5 - fold crossvalidation . In each fold , we ran logistic regression using the scikit - learn software package , _CITE_ using the default settings , except for the L1 regularization trade - off parameter C which was tuned on a within - fold hold - out set consisting of 20 % of the discussions within the fold . For the collective models , weight learning was performed on the same in - fold tuning sets . We trained via 700 iterations of structured perceptron , and ran the ADMM MAP inference algorithm to convergence at test time .__label__Method|Code|Use
Therefore , there is no forward jump from hCode , codei to h .,. i , but a monotone step to hein , εi followed by h .,. i . As the JTR sequence gK 1 is a unique interpretation of a bilingual sentence pair and its alignment , the probability p ( f1J , eI1 , bI1 ) can be computed as : Within this work , we first estimate the Viterbi alignment for the bilingual training data using GIZA ++ ( Och and Ney , 2003 ). Secondly , the conversion presented in Algorithm 1 is applied to obtain the JTR sequences , on which we estimate an n - gram model with modified Kneser - Ney smoothing as described in ( Chen and Goodman , 1998 ) using the KenLM toolkit _CITE_ ( Heafield et al ., 2013 ). tokens gk corresponding to Figure 2 . The right side shows the source and target tokens sk and tk obtained from the JTR tokens gk .__label__Method|Tool|Use
The sampled data is then used for one - iteration training . Later experiments will investigate the effect of the weighting proportion . In this work , we use b = 30 , and follow the implementation in CRFsuite _CITE_ to decide ηk . To evaluate different methods on annotation conversion , we build the first dataset that contains 1 , 000 sentences with POS tags on both sides of CTB and PD . The sentences are randomly sampled from PD .__label__Method|Code|Extent
They also prove that our approach , employing MLN to automatically learn the patterns of semantic triple grouping , is effective . Our system can answer more questions and obtain better performance than the traditional methods based on manually designed heuristic rules . DBpedia _CITE_ and some classes from Yago . These knowledge bases ( KBs ) are composed of many ontological and instance statements , and all statements are expressed by SPO triple facts . Figure 1 shows some triple fact samples from DBpedia .__label__Material|Data|Use
The model uses standard features : lemma and part of speech in a narrow context window ( 2 words either side ) and a wide context window ( 50 words either side ), as well as dependency labels leading to parent , children , and siblings of the target word , and lemmas and part of speech of parent , child , and sibling nodes . Table 3 shows sample model features for an occurrence of add in the British National Corpus ( BNC ) ( Leech , 1992 ). The model uses a maximum entropy learner _CITE_ , training one binary classifier per sense . ( With n - ary classifiers , the model ’ s performance is slightly worse .) The model is thus not highly optimized , but fairly standard .__label__Method|Algorithm|Use
the Web has changed from a static container of information into a live environment in which any user , in a very simple manner , can publish any type of information . This simplified means of publication has led to the rise of several different websites specialized in the publication of users opinions . Some of the most well - known sites include Epinions , RottenTomatoes _CITE_ and Muchocine , where users express their opinions or criticisms on a wide range of topics . Opinions published on the Internet are not limited to certain sites , but rather can be found in a blog , forum , commercial website or any other site allowing posts from visitors . On of the most representative tools of the Web 2 . 0 are social networks , which allow millions of users to publish any information in a simple way and to share it with their network of contacts or “ friends ”.__label__Supplement|Website|Introduce
We also combine this system with a baseline system consisting of effective surface features . A second contribution of the paper is the release of a new data set for QE . _CITE_ This data set comprises a set of 4 . 5K sentences chosen from customer support forum text . The machine translation of the sentences are not only evaluated in terms of adequacy and fluency , but also manually post - edited allowing various metrics of interest to be applied to measure different aspects of quality . All experiments are carried out on this data set .__label__Material|Data|Produce
Criterion , developed by ( Burstein et al ., 2004 ) is another Web - based learning tool which uses an automatic scoring engine to rate the input learner ’ s composition . It also shows detailed stylistic and grammatical feedback to the learner for educational purposes . Other than these , one can also find many other free or commercial English writing assistance systems including Grammarly _CITE_ , WhiteSmoke , and Ginger to name a few . However , all these systems assume rather static input , i . e ., focus on post - processing learners ’ compositions already finished . However , as stated in the previous section , many errors could be avoided by presenting appropriate feedback while the user is composing sentences .__label__Method|Tool|Introduce
Requests involve an imposition on the addressee , making them a natural domain for studying the inter - connections between linguistic aspects of politeness and social variables . Requests in online communities We base our analysis on two online communities where requests have an important role : the Wikipedia community of editors and the Stack Exchange question - answer community . On Wikipedia , to coordinate on the creation and maintenance of the collaborative encyclopedia , editors can interact with each other on user talk - pages ; _CITE_ re quests posted on a user talk - page , although public , are generally directed to the owner of the talkpage . On Stack Exchange , users often comment on existing posts requesting further information or proposing edits ; these requests are generally directed to the authors of the original posts . Both communities are not only rich in userto - user requests , but these requests are also part of consequential conversations , not empty social banter ; they solicit specific information or concrete actions , and they expect a response .__label__Supplement|Website|Introduce
00Equation 3 does not require that a symbol represent the entire right - hand side of each non - lexical rule , but does ensure that each right - hand side se Finally , Equation 5 ensures that if a pair is used , each 2 3 4 5 6 7 8 9 10 + member of the pair is included . This program can be optimized with an off - the - shelf ILP solver . _CITE_ Figure 4 shows the number of intermediate grammar symbols needed for the four binarization policies described above for a short sentence . Our ILP solver could only find optimal solutions for very short sentences ( which have small grammars after relativization ). Because greedy requires very little time to compute and generates symbol counts that are close to optimal when both can be computed , we use it for our remaining experiments .__label__Method|Tool|Use
To handle the OUT class , we need clustering algorithm . The basic idea is still using the Similarity formula . The detail algorithm is following : We followed the formula given by the organizers to calculate the precision rate , recall rate and FB1 _CITE_ . We directly list the best test result based on the given so called train set ( Table 3 ): And for the competition , our result is in Table 4 : We only get overall score , not in detail . All these data show that our recall rate is obviously larger than the precision rate .__label__Method|Algorithm|Extent
The Web - based interface allows the use of the translation system to any computer connected to the Internet . Given a string f in the source language , the goal of the statistical machine translation is to select the string e in the target language which maximizes the posterior distribution Pr ( e | f ). By introducing the hidden word alignment variable a , the following approximate optimization criterion can be applied for that purpose : At this time , Statistical Machine Translation ( SMT ) has empirically proven to be the most competitive approach in international competitions like the NIST Evaluation Campaigns and the International Workshops on Spoken Language Translation ( IWSLT - 2004 _CITE_ and IWSLT - 2005 ). In this paper we describe our multi - lingual phrase - based Statistical Machine Translation system which can be accessed by means of a Web page . Section 2 presents the general log - linear framework to SMT and gives an overview of our phrase - based SMT system .__label__Supplement|Website|Introduce
This discriminative module can flexibly incorporate extra features and it is implemented with the ME package given by Zhang Le . All training experiments are done with Gaussian prior 1 . 0 and 200 iterations . The character - based generative module is a character - tag - pair - based trigram model ( Wang et al ., 2009 ) and can be expressed as below : In our experiments , SRI Language Modeling Toolkit _CITE_ ( Stolcke , 2002 ) is used to train the generative trigram model with modified Kneser - Ney smoothing ( Chen and Goodman , 1998 ). The character - based joint model combines the above discriminative module and the generative module with log - linear interpolation as follows : Where the parameter α ( 0 . 0 ≤ α ≤ 1 . 0 ) is the weight for the generative model . Score ( tk ) will be directly used during searching the best sequence .__label__Method|Tool|Use
Then each bookmarks is represented as a feature vector whose each column value is the number of times that the word occurred in the document plus a constant multiple of the times it occurs in selected text , tags / labels ( constant multiplied to give higher weight to the selected text , tags / labels where this constant multiple is found by trial and error ). All the feature vectors are stored in the database . Porter stemmer _CITE_ is used for stemming the words . To make the system suitable for a online application , every activity like generating feature vector , training the classifier is done offline . To be clear and precise , when a user bookmarks a web page with selected text , tags / labels , then bookmarked content ( web page with selected text , tags / labels ) are stored in the database .__label__Method|Tool|Use
Unstructured Information Management Architecture ( UIMA ) ( Ferrucci and Lally , 2004 ) is a framework that supports the interoperability of mediaprocessing software components by defining common data structures and interfaces the components exchange and implement . The architecture has been gaining interest from academia and industry alike for the past decade , which resulted in a multitude of UIMA - supporting repositories of analytics . Notable examples include METANET4U components ( Thompson et al ., 2011 ) featured in U - Compare _CITE_ , DKPro ( Gurevych et al ., 2007 ), cTAKES ( Savova et al ., 2010 ), BioNLP - UIMA Component Repository ( Baumgartner et al ., 2008 ), and JULIE Lab ’ s UIMA Component Repository ( JCoRe ) ( Hahn et al ., 2008 ). However , despite conforming to the UIMA standard , each repository of analytics usually comes with its own set of type systems , i . e ., representations of data models that are meant to be shared between analytics and thus ensuring their interoperability . At present , UIMA does not facilitate the alignment of ( all or selected ) types between type systems , which makes it impossible to combine analytics coming from different repositories without an additional programming effort .__label__Method|Algorithm|Compare
Unstructured Information Management Architecture ( UIMA ) ( Ferrucci and Lally , 2004 ) is a framework that supports the interoperability of mediaprocessing software components by defining common data structures and interfaces the components exchange and implement . The architecture has been gaining interest from academia and industry alike for the past decade , which resulted in a multitude of UIMA - supporting repositories of analytics . Notable examples include METANET4U components ( Thompson et al ., 2011 ) featured in U - Compare _CITE_ , DKPro ( Gurevych et al ., 2007 ), cTAKES ( Savova et al ., 2010 ), BioNLP - UIMA Component Repository ( Baumgartner et al ., 2008 ), and JULIE Lab ’ s UIMA Component Repository ( JCoRe ) ( Hahn et al ., 2008 ). However , despite conforming to the UIMA standard , each repository of analytics usually comes with its own set of type systems , i . e ., representations of data models that are meant to be shared between analytics and thus ensuring their interoperability . At present , UIMA does not facilitate the alignment of ( all or selected ) types between type systems , which makes it impossible to combine analytics coming from different repositories without an additional programming effort .__label__Method|Algorithm|Introduce
Just as choosing corpora of articles that are controversial ( in the English - speaking world ) may have helped finding articles interesting to annotate it is possible that some other choice , e . g ., technical articles , may have helped select articles likely to be translated in full Thus further study may be required to choose the right Wikipedia balance for a set of priorities agreed upon by the annotation community . The American National Corpus has taken great pains to establish that the open subset of the corpus is freely usable by the community . The open license _CITE_ makes it clear that these corpora can be used for any reason and are freely distributable . In contrast , some aspects of the licensing agreement of corpora derived from Wikipedia are unclear . Wikipedia is governed by the GNU Free Document License which includes a provision that “ derived works ” are subject to this license as well .__label__Supplement|License|Other
We downloaded the dump dated February 27 , 2012 and extracted the textual contents using the wikipedia2text tool . The final plaintext file contains approximately 10 million words . We extracted word n - grams ( n ranging from 1 to 3 ) and their frequencies from this corpus thanks to the Text - NSP Perl module _CITE_ and its count . pl program , which produces the list of n - grams of a document , with their frequencies . Table 1 gives the number of n - grams produced . Some of these n - grams are invalid , and result from problems when extracting plain text from Wikipedia , such as “ 27 | ufc 1 ”, which corresponds to wiki syntax .__label__Method|Tool|Use
It will also be invaluable in gathering oral data from speakers of endangered languages for the production of monolingual talking dictionaries . The first of these projects is planned for the Arrernte language in central Australia . Several technological resources provide good data - gathering solutions for individual lexicographic projects , including Max Planck ’ s LEXUS ; TLex ; _CITE_ WeSay ; and SIL ’ s triad of Lexique Pro , Toolbox , and FLEx . Yet each of these solutions leaves gaps for the individual projects making use of them , and none is suitable for development of sophisticated multilingual dictionaries as envisioned by Kamusi . The learning curve can be steep , particularly the initial effort to set up an effective structure for a language .__label__Method|Tool|Introduce
This is also the method applied during the recent WMTs , where humans are asked to rank machine translation output by using APPRAISE ( Federmann , 2012 ), a software tool that integrates facilities for such a ranking task . In WMT , human MT evaluation is carried out by the MT development teams , usually computer scientists or computational linguists , sometimes involving crowd - sourcing based on Amazon ’ s Mechanical Turk . Being aware of the two communities , machine translation and translation studies , we took the available online data from the WMT2013 _CITE_ and tried to reproduce the ranking task with translation studies students for the English to German translations . The three questions we want to answer are : We concentrate on English - German data since the majority of our evaluators were native speakers of German and since , from a translation studies point of view , professional translation should be performed only into the mother tongue . 2 The WMT2013 English - German Data Before presenting the experimental setting and outcomes , we present the WMT data .__label__Material|Data|Use
One of the most successful approaches in the last years is the supervised learning from examples , in which statistical or Machine Learning classification models are induced from semantically annotated corpora ( M ` arquez et al ., 2006 ). Generally , supervised systems have obtained better results than the unsupervised ones , as shown by experimental work and international evaluation exercises such This paper has been supported by the European Union under the projects QALL - ME ( FP6 IST - 033860 ) and KYOTO ( FP7 ICT - 211423 ), and the Spanish Government under the project Text - Mess ( TIN2006 - 15265 - C06 - 01 ) and KNOW ( TIN2006 - 15049 - C03 - 01 ) as Senseval . These annotated corpora are usually manually tagged by lexicographers with word senses taken from a particular lexical semantic resource – most commonly WordNet _CITE_ ( WN ) ( Fellbaum , 1998 ). WN has been widely criticized for being a sense repository that often provides too fine – grained sense distinctions for higher level applications like Machine Translation or Question & Answering . In fact , WSD at this level of granularity has resisted all attempts of inferring robust broadcoverage models .__label__Method|Tool|Use
In our experiments we show an increase in the performance of TED based approach to textual entailment , by optimizing the cost of edit operations . In the following subsections , the framework and dataset of our experiments are elaborated . Our experiments were conducted on the basis of the Recognizing Textual Entailment ( RTE ) datasets _CITE_ , which were developed under PASCAL RTE challenge . Each RTE dataset includes its own development and test set , however , RTE - 4 was released only as a test set and the data from RTE - 1 to RTE - 3 were used as development set . More details about the RTE datasets are illustrated in Table 5 . 1 .__label__Material|Data|Use
Members are typically affiliated with either the Democratic Party or the Republican Party . Congressional election and Presidential election coincide every four years . The Corpus We use a corpus of public statements released by members of Congress in both the Senate and The House of Representatives , collected by Project Vote Smart _CITE_ . An example of a public statement is presented in Figure 1 . In this work we use all individual statements and press releases in a span of four years ( 20102013 ), a total of 134000 statements made by 641 representatives .__label__Material|Data|Use
Next , we used our Scasim measure to calculate the pair - wise similarities of all these regression patterns . This can be done with a function called Scasim which is freely available from the authors . _CITE_ This function takes a data frame ( basically a table ) as input which contains , chronologically ordered , a line for every fixation in the data set . One column identifies the trial to which a fixation belongs , other columns specify the x and y coordinates and the duration of a fixation . The resulting matrix of similarity scores was then used to fit a map of scanpath space , i . e ., a n - dimensional vector space with a vector for each regressive scanpath ( see fig .__label__Method|Tool|Use
It contains organoleptic labels and the chemical compounds — or more accurately the perfume raw materials ( PRMs )— that produce them . By automatically scraping the catalog we obtained a total of 137 organoleptic smell labels from SAFC , with a total of 11 , 152 associated PRMs . We also experimented with Flavornet _CITE_ and the LRI and odour database , but found that the data from these were more noisy and generally of lower quality . For each of the smell labels in SAFC we count the co - occurrences of associated chemical compounds , yielding a bag of chemical compounds ( BoCC ) model . Table 2 shows an example subspace of this model .__label__Material|Data|Use
We used gold - standard ( manually annotated ) morphemes , named entities , dependency structures and coreference relations to focus on the A / R detection and the zero reference resolution . We used SVMrank for the learning - to - rank method of the A / R detection and the PAS analysis . The categories of words are given by the morphological analyzer JUMAN _CITE_ . Named entities and predicate features ( e . g ., honorific expressions , modality ) are given by the syntactic parser KNP . We show the results of the author and reader mention detection in Table 6 and Table 7 .__label__Method|Tool|Use
In the aftermath , Chinese media accused Western media of “ softpedaling the attack and failing to state clearly that it was an act of terrorism ”. In particular , regarding the statement by the US embassy that referred to this incident as the “ terrible and senseless act of violence in Kunming ”, a Weibo user posted “ If you say that the Kunming attack is a ‘ terrible and senseless act of violence ’, then the 9 / 11 attack can be called a ‘ regrettable traffic incident ”’. _CITE_ This example is striking but not an isolated case , for settings in which one party is trying to convince another are pervasive ; scenarios range from court trials to conference submissions . Since the strength and scope of an argument can be a crucial factor in its success , it is important to understand the effects of statement strength in communication . A first step towards addressing this question is to be able to distinguish between strong and weak statements .__label__Supplement|Document|Produce
In this step , the set of core frame elements which function as the obligatory arguments of the required lexeme are matched with their corresponding ontology concepts . The algorithm that is applied to carry out this process utilizes the FE Taxonomy and the ontology class hierarchy . _CITE_ Matching is based on the class hierarchies . For example : Actor , which is a subclass of Person is matched with the core element Creator , which is a subclass of Agent because they are both characterized as animate objects that have human properties . Similarly , Represented Object , which is a subclass of Conceptual Object , is matched with the core element Represented , which is a subclass of Entity because they are both characterized as the results of a human creation that comprises nonmaterial products of the human mind .__label__Material|Data|Use
We conclude by summarizing plans for further development of our prototype . The initial input to the SurfShop system consists of a product database and a product ontology with node labels . All products were indexed for fast retrieval by the application _CITE_ . A chart of application components is presented in Figure1 . Raw product descriptions from our data would constitute a large corpus including meta - data such as shipping or manufacturer information , which are not relevant to our task .__label__Method|Tool|Use
Efforts in building large Chinese corpora started in the 90s , for example , the Sinica corpus ( CKIP , 1995 ) and the Chinese Penn Tree Bank ( Xia et al ., 2000 ). However , these two corpora concentrate on the tagging of parts - of - speech and syntactic structures , while little work has been done on semantic annotation . Of the few efforts that were carried out , Lua _CITE_ annotated 340 , 000 words with semantic classes defined in a thesaurus ( Mei , 1983 ). This resource , however , was not publicly accessible . With the release of HowNet ( Dong , 1999 ; Dong , 2000 ) in YANG Yongsheng Department of Computer Science , HKUST , Clear Water Bay , Hong Kong .__label__Material|Data|Introduce
Because it is primarily expressed as text , Debatepedia is a corpus of debate topics , but it is organized hierarchically , with multiple issues in each debate topic , questions within each issue , and arguments on two sides of each question . An important feature of the corpus is the widespread quotation and linking to external articles on the web , including news stories , blog postings , wiki pages , and social media forums ; here we use these external articles in evaluation (§ 4 ). Table 1 shows excerpts from a debate page _CITE_ from Debatepedia . Each debate contains “ questions ,” which reflect the different aspects of a debate . In this particular debate , there are 13 questions ( 2 shown ), ranging from economic benefits to enforceability to social impacts .__label__Supplement|Website|Introduce
All of them are derived from Altaic language family , belonging to Turkic languages , and mostly spoken by people in Central Asia . There are about 24 million people take these languages as mother tongue . All of the tasks are derived from the evaluation of China Workshop of Machine Translation ( CWMT ) _CITE_ . Table 1 shows the statistics of data sets . For the language model , we use the SRI Language Modeling Toolkit ( Stolcke , 2002 ) to train a 5 - gram model with the target side of training corpus .__label__Supplement|Website|Extent
We define a set of token and semantic features to train the CRF model . Token features : The word , the part - of - speech tag ( pos - tags ) and the lemma ; two tokens after and two tokens before the word , their lemmas and their pos - tags . We used StanfordTagger _CITE_ to obtain the words of clinical texts as well as their lemmas and their part - of - speech tags . StanfordTagger recognizes the word 1 / word 2 token as one word . Since , many UMLS terms contain either the word 1 or the word 2 , we separate the word 1 / word 2 phrase into three words : word 1 , / and word 2 .__label__Method|Tool|Use
However , due to the extreme level of variation of the topics in these corpora , we applied a filtering algorithm to select a subset of the corpora . Our approach to make the text similar involved reducing the corora based on matching Named Entities . Named Entities of English and Hindi corpus were listed using LingPipe _CITE_ and a Hindi NER system built at IIT Kharagpur ( Saha et al ., 1999 ). The listed Named Entities of the two corpora were compared to find the matching Named Entities . Named Entities in Hindi Unicode were converted to iTRANS format and matched with English Named Entities using edit distance .__label__Method|Tool|Use
Since the classification accuracies deceased to be quite low for source spans with more than 10 words , only { C1 , ..., C10 } were integrated into the HPB translation system . For each translation task , the recent version of Moses HPB decoder ( Koehn et al ., 2007 ) with the training scripts was used as the baseline ( Base ). We used the default parameters for Moses , and a 5 - gram language model was trained on the target side of the training corpus by IRST LM Toolkit _CITE_ with improved Kneser - Ney smoothing . { C1 ,..., C10 } were integrated into the baseline with different weights , which were tuned by MERT ( Och , 2003 ) together with other feature weights ( language model , word penalty ,...) under the log - linear framework ( Och and Ney , 2002 ). represents a significant difference at the p & lt ; 0 . 01 level and - represents a significant difference at the p & lt ; 0 . 05 level against the BLM .__label__Method|Tool|Use
The two computational semantics annotators had to tag each English constituent noun with its corresponding WordNet sense and each instance with the corresponding semantic category . If the word was not found in WordNet the instance was not considered . Whenever the annotators found an example encoding a semantic category other than those provided or they didn ’ t know what interpretation to give , they had to tag it as “ OTHER - SR ”, and respectively “ OTHER - PP ” _CITE_ . The details of the annotation task and the observations drawn from there are presented in a companion paper ( Girju , 2007 ). The corpus instances used in the corpus analysis phase have the following format : < NPEn ; NPEs ; NPIt ; NPFr ; NPPort ; NPRo ; target >.__label__Material|Data|Produce
The list of modals and auxilliaries is deterministic . Syntactic Chunk ( CHUNK ): This feature explicitly models the syntactic phrases in which our tokens occur . The possible phrases are shallow syntactic representations that we obtain from the TreeTagger chunker : _CITE_ ADJC ( Adjective Chunk ), ADVC ( Adverbial Chunk ), CONJC ( Conjunctional Chunk ), INTJ ( Interjunctional Chunk ), LST ( numbers 1 , 2 , 3 etc ), NC ( Noun Chunk ), PC ( Prepositional Chunk ), PRT ( off , out , up etc ), VC ( Verb Chunk ). Since the data is very small , we tested our automatic annotation using 5 fold cross validation where 10 % of the data is set aside as development data , then 70 % is used for training and 20 % for testing . The reported results are averaged over the 5 folds for the Test data for each of our experimental conditions .__label__Method|Tool|Use
( Roche et al ., 2009 ) The aim of checking a report for its completeness of content includes the need to determine what content is required . The CAP ( College of American Pathologists ) offers some cancer protocols , which specify the content of pathology reports for different cancer types . Moreover , the ICCR group ( International Collaboration on Cancer Reporting ) has published five datasets _CITE_ for reporting different types of cancer . These determine which information is required in a report and which information is just considered to be recommended . Daniel and Macary ( 2011 ) created a terminology called PathLex , which covers the scope of anatomic pathology observations and speci men collection procedures .__label__Material|Data|Introduce
The dataset is composed of two test sets , A and B . They both contain the same instances , but the test B was processed with a Named Entity Recogniser ( NER ) We used the training set to learn probabilities and the corresponding machine learning models . We tested our methods with the development set using the Weka GUI _CITE_ ( Witten and Frank , 2005 ). We built a Java application to predict documents in the test set by using the models previously learned with Weka . In the following sections we explain the specific approach for both open and close submissions .__label__Method|Tool|Use
To compute the preference of a word w in the grammatical context of a PMW t ( the target ) towards each of t ’ s possible senses , we consider each relation ( w , R , t ), where R is the grammatical relation . The set C of word collocations are extracted from the BNC and used to compute a preference score Psi for each sense si E S : where supersense ( wj , si ) is true if si is a supersense of one of wj ’ s senses ; isa ( wj , si ) is true if si is a hypernym of one of wj ’ s senses in WordNet , or is a fact extracted from Wikipedia . To determine the supersense and isa relation we use WordNet 3 . 0 , and a set of 7 , 578 , 112 isa relations extracted by processing the page and category network of Wikipedia _CITE_ ( Nastase and Strube , 2008 ). The collocations extracted from BNC contain numerous named entities , most of which are not part of WordNet . If an isa relation between a collocate from the corpus wj and a possible sense of a PMW si cannot be established using supersense information ( for the supersenses ) or through transitive closure in the hypernymhyponym hierarchy in WordNet ( for company and organization ) for any sense of wj , it is tried against the Wikipedia - based links .__label__Material|Data|Use
IBM models 1 , 2 and 3 yielded subpar results , so we will not discuss them . To evaluate the alignments , we used 484 goldaligned sentences from Och and Ney ( 2000 ). _CITE_ We used the F - score of correct sure and possible links ( Fraser and Marcu , 2007 ) for a general evaluation , which we will call Fall . 6 In order to specifically evaluate pronoun alignment , we used the F - score of the subset of links that align the two sets of pronouns we are interested in , Fpro . For all alignment models , grow - diag - final - and symmetrization performed best on the pronoun metric , followed by grow - diag and intersection , which also performed best for general alignments . Table 7 shows the results for different models with grow - diag - final - and symmetrization .__label__Material|Data|Use
Novel research directions we investigated include : neural network language models and bilingual neural network language models , a comprehensive use of word classes , and sparse lexicalized reordering features . The Edinburgh / JHU phrase - based translation systems for our participation in the WMT 2015 shared translation task are based on the open source Moses toolkit ( Koehn et al ., 2007 ). We built upon Edinburgh ’ s strong baselines from WMT submissions in previous years ( Durrani et al ., 2014a ) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT and EU - BRIDGE _CITE_ ( Birch et al ., 2014 ; Freitag et al ., 2014a ; Freitag et al ., 2014b ). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh / JHU submission . Next we give a general system overview with details on our training pipeline and decoder configuration .__label__Method|Tool|Use
Here , we consider an important dimension of style , namely , simplicity . Systems that can rewrite text into simpler versions promise to make information available to a broader audience , such as non - native speakers , children , laypeople , and so on . One major effort to produce such text is the Simple English Wikipedia ( henceforth SimpleEW ) _CITE_ , a sort of spin - off of the well - known English Wikipedia ( henceforth ComplexEW ) where human editors enforce simplicity of language through rewriting . The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories , thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW . Importantly , not all the changes on SimpleEW are simplifications ; we thus also make use of ComplexEW edits to filter out non - simplifications .__label__Material|Data|Produce
During fold partition , all posts that are in the same post sequence are assigned to the same fold . All reason and stance classifiers are domain - specific , meaning that each of them is trained on sentences / posts from exactly one domain and is applied to classify sentences / posts from the same domain . We use the Stanford maximum entropy classifier for classification and solve ILP programs using lpsolve _CITE_ . Results are shown in Table 3 . Each row corresponds to one of our seven RC systems , showing its SC accuracy as well as its sentence - and postlevel RC F - scores for each domain .__label__Method|Tool|Use
Task 2 on resolving in - sentence scopes of hedge cues , was performed by a memorybased system that relies on information from syntactic dependencies . This system scored the highest F1 ( 57 . 32 ) of Task 2 . In this paper we describe the machine learning systems that CLiPS _CITE_ submitted to the closed track of the CoNLL - 2010 Shared Task on Learning to Detect Hedges and Their Scope in Natural Language Text ( Farkas et al ., 2010 ). The task consists of two subtasks : detecting whether a sentence contains uncertain information ( Task 1 ), and resolving in - sentence scopes of hedge cues ( Task 2 ). To solve Task 1 , systems are required to classify sentences into two classes , “ Certain ” or “ Uncertain ”, depending on whether the sentence contains factual or uncertain information .__label__Method|Tool|Introduce
We performed feature selection using the metric Chisquare , to select the top 500 features to represent documents . Since tweets are very short we incorporated a binary representation for BoW instead of term frequency . For classification we used a multiclass SVM classifier _CITE_ and all the experiments were conducted using the data mining software Weka . We used standard metrics such as Precision , Recall and F - measure to compare the performance of the different algorithms . In the following section we analyse the experimental results for TF - lex ( Sec 4 . 1 ), EMallclass - lex ( Sec 4 . 2 . 2 ), EMclass - corpuslex ( Sec 4 . 2 . 3 ), PMI - lex ( Mohammad , 2012a ), WNA - lex ( Strapparava and Valitutti , 2004 ), NRClex ( Saif M . Mohammad , 2013 ) and BoW in an emotion classification task .__label__Method|Tool|Use
The semantic textual similarity prediction problem involves finding a function f approximating the semantic textual similarity score given two sentences , S1 and S2 : We approach f as a supervised learning problem with ( S1 , S2 , q ( S1 , S2 )) tuples being the training data and q ( S1 , S2 ) being the target similarity score . We model the problem as a translation task where one possible interpretation is obtained by translating S1 ( the source to translate , S ) to S2 ( the target translation , T ). Since linguistic processing can reveal deeper similarity relationships , we also look at the translation task at different granularities of information : plain text ( R for regular ) , after lemmatization ( L ), after part - of - speech ( POS ) tagging ( P ), and after removing 128 English stop - words ( S ) _CITE_ . Thus , and the Shared Task , pages 234 – 240 , Atlanta , Georgia , June 13 - 14 , 2013 . c � 2013 Association for Computational Linguistics we obtain 4 different perspectives on the binary relationship between S1 and S2 . Referential translation machines ( RTMs ) we develop provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data ( Bic ¸ ici and Yuret , 2011a ; Bic ¸ ici , 2011 ) as interpretants for reaching shared semantics ( Bic ¸ ici , 2008 ).__label__Method|Algorithm|Introduce
We train our model on a dataset with ˜ 1 . 5M sentence pairs from the LDC dataset . We use the 2002 NIST MT evaluation test data ( 878 sentence pairs ) as the development data , and the 2003 , 2004 , 2005 , 2006 - news NIST MT evaluation test data ( 919 , 1788 , 1082 , and 616 sentence pairs , respectively ) as the test data . To find heads , we parse the source sentences with the Berkeley Parser ( Petrov and Klein , 2007 ) trained on Chinese TreeBank 6 . 0 and use the Penn2Malt toolkit _CITE_ to obtain ( unlabeled ) dependency structures . We obtain the word alignments by running GIZA ++ ( Och and Ney , 2000 ) on the corpus in both directions and applying “ grow - diag - final - and ” refinement ( Koehn et al ., 2003 ). We use the SRI language modeling toolkit to train a 5 - gram language model on the Xinhua portion of the Gigaword corpus and standard MERT ( Och , 2003 ) to tune the feature weights on the development data .__label__Method|Tool|Use
Because we do not know the relative quality of different permutations , our corpus includes only pairwise rankings that comprise the original document and one of its permutations . Given k original documents , each with n randomly generated permutations , we obtain k · n ( trivially ) annotated pairwise rankings for training and testing . Using the technique described herein , we collected data _CITE_ in two different genres : newspaper articles and accident reports written by government officials . The first collection consists of Associated Press articles from the North American News Corpus on the topic of earthquakes ( Earthquakes ). The second includes narratives from the National Transportation Safety Board ’ s aviation accident database ( Accidents ).__label__Material|Data|Use
Furthermore , it uses portable formats and format converters that would allow for combining several software components . There exist a lot of platforms dedicated to NLP , but none are fully satisfactory for various reasons . Intex ( Silberztein , 1993 ), FSM ( Mohri et al ., 1998 ) and Xelda _CITE_ are closed source . Unitex ( Paumier , 2003 ), inspired by Intex has its source code under LGPL license but it does not support standard formats for Language Resources ( LR ). Systems like NLTK ( Loper and Bird , 2002 ) and Gate ( Cunningham , 2002 ) do not offer functionality for Lexical Resource Management .__label__Method|Tool|Introduce
We experimented with various combinations of E1 - and E2 - loss SVMs , with both E1 and E2 - regularization , but in the end opted to use the E2 - regularized logistic regression due to slightly superior performance and the ease with which we could extract eleven values of P ( H ) for inclusion in our ensemble . Another component that was tested in development of our ensemble systems was a maximum entropy classifier . This particular effort used the implementation from JCarafe , _CITE_ which uses L - BFGS for optimization . We approached the NLI task as document classification , following a typical JCarafe recipe ( Gibson et al ., 2007 ). The class of the document is the native language of the author .__label__Method|Code|Use
However , their methods are handicapped by the built - in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base , leading to false negatives . In reality , knowledge bases are often incomplete , giving rise to numerous false negatives in the training data . We sampled 1834 sentences that contain two entities in the New York Times 2006 corpus and manually evaluated whether they express any of a set of 50 common Freebase _CITE_ relations . As shown in Figure 1 , of the 133 ( 7 . 3 %) sentences that truly express one of these relations , only 32 ( 1 . 7 %) are covered by Freebase , leaving 101 ( 5 . 5 %) false negatives . Even for one of the most complete relations in Freebase , Employee - of ( with more than 100 , 000 entity pairs ), 6 out of 27 sentences with the pattern ‘ PERSON executive of ORGANIZATION ’ contain a fact that is not included in Freebase and are thus mislabeled as negative .__label__Material|Data|Use
Discourse Topic is an under - theorized notion in linguistic theory : not all linguists agree that the notion of Discourse Topic is required in discourse analysis at all ( cf . Asher , 2004 ). For our purposes , however , we formulated a set of patterns for identifying Discourse Topics on the basis of the output of the CMU Link Parser _CITE_ the system uses . Paradigmatically , we counted ordinary subjects of the first sentence of a passage as expressive of the Discourse Topic . So , if we found an expression of the topic there , either in full or reduced form , we took that as an instance of the topic appearing as Discourse Topic in that passage and ranked that passage highly .__label__Method|Tool|Use
For morph / label pairs that were never observed in the training set , a maximum distance value is assigned . A good segmentation algorithm will find segments that are good building blocks of entirely new word forms , and thus the maximum distance values will occur only rarely . We compared the two proposed methods as well as Goldsmith ’ s program Linguistica _CITE_ on both Finnish and English corpora . The Finnish corpus consisted of newspaper text from CSC . A morphosyntactic analysis of the text was performed using the Conexor FDG parser .__label__Method|Tool|Compare
This is due at least to the crucial need of the domain knowledge and also of the linguistic knowledge . Our approach considers that for some specific domains a semantic annotation can be achieved by a light parsing of the text which is based on the user of certain cue - words as a heuristic for describing its semantic structure . The availability of a large collection of annotated telephone calls for querying the Swiss phone - book database ( i . e the Swiss French PolyPhone corpus ) allowed us to experiment our recent findings in robust text analysis obtained in the context of the Swiss National Fund research project ROTA ( Robust Text Analysis ), and in the recent Swisscom funded project ISIS ( Interaction through Speech with Information Systems ) _CITE_ ( Chappelier et al ., 1999 ). This database contains 4293 simulated recordings related to the would produce the following query frame filling for the Swiss Phone - book database : Nom de famille / Firme : MOTTAZ Prenom / Autres informations : MONIQUE Rue , numero : rue du PRINTEMPS , 4 NPA , localito : SAIGNELEGIER . The goal of semantic annotation is to provide a tree structure which can be superposed over the flat sentence .__label__Method|Tool|Introduce
Also , we observe that the best fusion scheme consistently outperforms the Lexical + Lexical approach for 10 − 100 neighbors . Here , we compare the performance of semantic and affective features ( described in Section 6 ) for the discrimination of word pairs the fall into two categories , synonyms and antonyms . The word pairs were taken from two sets of WordNet synonyms and opposites _CITE_ We retained those pairs that were included in the networks described in Section 7 . 1 . In total , 172 pairs are contained in each category for a total of 344 pairs . The experimental dataset include pairs such as ( happiness , felicity ) and ( comedy , tragedy ) that correspond to synonyms and antonyms , respectively .__label__Material|Data|Use
The topic model infers the topic distribution features of each excerpt and the RF classifier predicts the section ( s1 , s2 , etc .) of the excerpt . All web automation tasks are performed using HTMLUnit _CITE_ . In the second stage , our ILP based summarization approach synthesizes information from multiple excerpts assigned to a section and presents the most informative and linguistically well - formed summary as the corresponding content for each section . A wordgraph is constructed that generates several sentences ; only a few of the sentences are retained based on the ILP solution .__label__Method|Tool|Use
Besides the common features used in traditional Named Entity Recognition ( NER ) systems , we also utilize extensive external resources to build various name lists and word clusters . Following the traditional BIO scheme used in sequential labeling , we assign a label for each word in the sentence , where “ B - TERM ” indicates the start of an aspect term , “ I - TERM ” indicates the continuation of an aspect term , and “ O ” indicates not an aspect term . All sentences are tokenized and parsed using the Stanford Parser _CITE_ . The parsing information is used to extract various syntactic features ( e . g . POS , head word , dependency relation ) described in the next section .__label__Method|Tool|Use
This result is much lower than the best F1 score for English reported at the CoNLL - 2000 chunking competition : 94 . 13 %. However , this comparison should be treated with caution since we did no special adaptation of the features to Bulgarian . We should also note that the ChunkLink _CITE_ script , used at CoNLL - 2000 , was tailored to the Penn Treebank tagset , and was thus not very suitable to our collapsed BulTreeBank tagset . The syntactic parser : Unfortunately , we were unable to evaluate our parser with the full morphosyntactic tagset of the BulTreeBank ; this would have required coding efforts for some parts of speech , e . g ., nouns , that go beyond simple adaptation . On our collapsed tagset , we achieved an F1 score of 77 . 56 %.__label__Method|Code|Use
In this paper we model this sub - structure and share parameters among related dependency paths , using a unified loss function learning entity and relation representations to maximize performance on the knowledge base link prediction task . We evaluate our approach on the FB15k - 237 dataset , a knowledge base derived from the Free base subset FB15k ( Bordes et al ., 2013 ) and filtered to remove highly redundant relations ( Toutanova and Chen , 2015 ). The knowledge base is paired with textual mentions for all entity pairs derived from ClueWeb12 _CITE_ with Freebase entity mention annotations ( Gabrilovich et al ., 2013 ). We show that using a convolutional neural network to derive continuous representations for textual relations boosts the overall performance on link prediction , with larger improvement on entity pairs that have textual mentions . There has been a growing body of work on learning to predict relations between entities without requiring sentence - level annotations of textual mentions at training time .__label__Material|Data|Extent
We show good mining performance for En - Hi and En - Ta . We perform error analysis for En - Ar , and identify sources of error ( Section 6 . 5 ). To understand the various issues in mining MWNE equivalents from comparable corpora , we took a random sample of 100 comparable En - Hi news article pairs from the Indian news portal WebDunia _CITE_ . The English articles had 682 unique NEs of which 252 ( 37 %) were person names , 130 ( 19 %) were location names , and 300 ( 44 %) were organization names . A substantial percentage of the names comprised of more than one word : locations 25 %, person names 96 %, and organizations 98 %.__label__Material|Data|Use
We show good mining performance for En - Hi and En - Ta . We perform error analysis for En - Ar , and identify sources of error ( Section 6 . 5 ). To understand the various issues in mining MWNE equivalents from comparable corpora , we took a random sample of 100 comparable En - Hi news article pairs from the Indian news portal WebDunia _CITE_ . The English articles had 682 unique NEs of which 252 ( 37 %) were person names , 130 ( 19 %) were location names , and 300 ( 44 %) were organization names . A substantial percentage of the names comprised of more than one word : locations 25 %, person names 96 %, and organizations 98 %.__label__Supplement|Website|Use
These heuristics handled the alignment of named entities ( e . g ., George Bush ) and definite descriptions ( e . g ., the president ), tenses ( e . g ., had been and shall be ), noun phrases with mismatching determiners ( e . g ., a man and the man ), verb complexes ( e . g ., was developed and had been developed ), phrasal verbs ( e . g ., take up and accept ), genitives ( e . g ., Bush ’ s infrequent speeches and the infrequent speeches by Bush ), pronouns , repetitions , typographic errors , and approximate correspondences . For more details , we refer the interested reader to our annotation guidelines . _CITE_ Figure 1 shows the alignment for two sentence pairs from the MTC corpus . The first pair ( Australia is concerned with the issue of carbon dioxide emissions . & lt ;- 4 The problem of greenhouse gases has attracted Australia ’ s attention .)__label__Supplement|Document|Produce
However , context models , user models or plan based mechanisms could also be used for this purpose . During the activation , the appropriate dialogue description is translated into internal data structures appropriate for the dialogue management in SesaME . This process is performed through JAXB _CITE_ which provides a fast way to create a twoway mapping between XML documents and Java objects ( Pakucs , 2002 ). During the dialogue management , the generated internal data structures are used for frame - based dialogue management . The actual dialogue management process does not follow the VoiceXML specifications .__label__Method|Tool|Use
If � kXL + U is equal to kXL , then there is no new sense in XU . Otherwise (� kXL + U > kXL ) new senses of w may be represented by the groups in which there is no instance from XL . We evaluated the ELP based model order identification algorithm on the data in English lexical sample task of SENSEVAL - 3 ( including all the 57 English words ) _CITE_ , and further empirically compared it with other state of the art classification methods , including SVM 10 ( the state of the art method for supervised word sense disambiguation ( Mihalcea et al ., 2004 )), a one - class partially supervised classification algorithm ( Liu et al ., 2003 ) 11 , and a semi - supervised k - means clustering based model order identification algorithm . The data for English lexical samples task in SENSEVAL - 3 consists of 7860 examples as official training data , and 3944 examples as official test data for 57 English words . The number of senses of each English word varies from 3 to 11 .__label__Material|Data|Use
Integrating work from psychology and computational linguistics , we develop and compare three approaches to detecting deceptive opinion spam , and ultimately develop a classifier that is nearly 90 % accurate on our gold - standard opinion spam dataset . Based on feature analysis of our learned models , we additionally make several theoretical contributions , including revealing a relationship between deceptive opinions and imaginative writing . With the ever - increasing popularity of review websites that feature user - generated opinions ( e . g ., TripAdvisor _CITE_ and Yelp ), there comes an increasing potential for monetary gain through opinion spam — inappropriate or fraudulent reviews . Opinion spam can range from annoying self - promotion of an unrelated website or blog to deliberate review fraud , as in the recent case of a Belkin employee who hired people to write positive reviews for an otherwise poorly reviewed product . While other kinds of spam have received considerable computational attention , regrettably there has been little work to date ( see Section 2 ) on opinion spam detection .__label__Supplement|Website|Introduce
These relations form a hierarchical structure with the most general relation at the root . There are various argument relations like subject , object , objects of prepositions , and clausal complements , modifier relations like adjectival , adverbial , participial , and infinitival modifiers , and other relations like coordination , conjunct , expletive , and punctuation . UNL : The 44 UNL relations _CITE_ include relations such as agent , object , co - agent , and partner , temporal relations , locative relations , conjunctive and disjunctive relations , comparative relations and also hierarchical relationships like part - of and aninstance - of . Comparison : Unlike the Stanford parser which expresses the semantic relationships through grammatical relations , UNL uses attributes and universal words , in addition to the semantic roles , to express the same . Universal words are used to disambiguate words , while attributes are used to express the speaker ’ s point of view in the sentence .__label__Material|Data|Introduce
We also collected article titles from Project Gutenberg for segmentation and evaluation . We use the 15th edition gender metadata to identify and extract articles about people from the current edition . In order to identify people articles in the historical editions , we use the Stanford CoreNLP named entity recognizer ( NER ) with pre - trained models , _CITE_ on the first sentence of the article . The common format of a person name is “ last name , first name ” in the 9th and 11th edition and “ last name ( first name )” in the 3rd edition . The first token always serves as the article title and is prone to OCR errors , since it is usually all - capitalized , and in some editions , uses a special font .__label__Method|Tool|Use
We evaluate the advantages / drawbacks of each HIT design and show that , in our case , the use of non - expert annotations is a viable and costeffective alternative to expert annotations . Obtaining reliable human annotations to train datadriven AI systems is often an arduous and expensive process . For this reason , crowdsourcing platforms such as Amazon ’ s Mechanical Turk _CITE_ , Crowdflower and others have recently attracted a lot of attention from both companies and academia . Crowdsourcing enables requesters to tap from a global pool of non - experts to obtain rapid and affordable answers to simple Human Intelligence Tasks ( HITs ), which can be subsequently used to train data - driven applications . A number of recent papers on this subject point out that non - expert annotations , if produced in a sufficient quantity , can rival and even surpass the quality of expert annotations , often at a much lower cost ( Snow et al ., 2008 ), ( Su et al ., 2007 ).__label__Supplement|Website|Introduce
The respective dependency parse tree is included through following the shortest dependency path hypothesis ( Bunescu and Mooney , 2005 ), by using the syntactical and dependency information of edges ( e ) and vertices ( v ). So - called v - walks and e - walks of length 3 are created as well as n grams along the shortest path ( Miwa et al ., 2010 ). One of the most important source of publications in the biomedical domain is MEDLINE _CITE_ , currently containing more than 21 million citations . The initial step is annotation of named entities – in our case performed by ProMiner ( Hanisch et al ., 2005 ), a tool proving state - of - the - art results in e . g . the BioCreative competition ( Fluck et al ., 2007 ). Based on the named entity recognition , only sentences containing co - occurrences are further processed .__label__Material|Data|Introduce
Features of the first group are only given as examples . We then applied machine learning in order to build an automatic classifier for detecting nonreferential instances of it , given a vector of features as described above . We used JRip , the WEKA _CITE_ reimplementation of Ripper ( Cohen , 1995 ). All following figures were obtained by means of ten - fold cross - validation . Table 3 contains all results discussed in what follows .__label__Method|Tool|Use
DD / TD data sets are used as information for format that the system responds the retrieval results and so on . In the next section , we describe how application developers prepare DD / TI and DD / TD data sets . We used Chasen as Japanese morphological analysis and PostgreSQL _CITE_ as a database retrieval management system . We have constructed Semantics Modules based on the Mt . Fuji sightseeing guidance system ( Nakagawa et al ., 2000 ) with separating task dependent and independent parts .__label__Material|Data|Use
( 6 ) We experimentally test our M method in the context of the HOO shared task . The HOO test data consists of text fragments from NLP papers together with manually - created gold - standard corrections ( see ( Dale and Kilgarriff , 2011 ) for details ). We test our method by re - scoring the best runs of the participating teams in the HOO shared task with our M scorer and comparing the scores with the official HOO scorer , which simply uses GNU wdiff _CITE_ to extract system edits . We obtain each system ’ s output and segment it at the sentence level according to the gold standard sentence segmentation . The source sentences , system hypotheses , and corrections are tokenized using the Penn Treebank standard ( Marcus et al ., 1993 ).__label__Method|Algorithm|Use
In practice , it is no slower than running token - role filtering on its own . We extract dependency structures from the Penn Treebank using the head rules of Yamada and Matsumoto ( 2003 ). _CITE_ We divide the Treebank into train ( sections 2 – 21 ), development ( 22 ) and test ( 23 ). We part - of - speech tag our data using a perceptron tagger similar to the one described by Collins ( 2002 ). The training set is tagged with jack - knifing : the data is split into 10 folds and each fold is tagged by a system trained on the other 9 folds .__label__Supplement|Document|Use
By providing scalar factuality judgments for events , our models enable more fine - grained reasoning than previously considered . The corpus and learned models are available online . _CITE_ While event definitions have been proposed in several prior studies , existing approaches vary in how they model various linguistic forms such as nominal events , stative events , generic events , and light verbs ( Pustejovsky et al ., 2003 ; Palmer et al ., 2005 ; Meyers et al ., 2004 ; Kim et al ., 2009 ; Song et al ., 2015 ). Even with a formal and precise account of events , training annotators to learn all such linguistic intricacies remains a practical challenge . Instead of definition - driven instructions , we propose example - driven instructions and show their effectiveness .__label__Material|Data|Produce
By providing scalar factuality judgments for events , our models enable more fine - grained reasoning than previously considered . The corpus and learned models are available online . _CITE_ While event definitions have been proposed in several prior studies , existing approaches vary in how they model various linguistic forms such as nominal events , stative events , generic events , and light verbs ( Pustejovsky et al ., 2003 ; Palmer et al ., 2005 ; Meyers et al ., 2004 ; Kim et al ., 2009 ; Song et al ., 2015 ). Even with a formal and precise account of events , training annotators to learn all such linguistic intricacies remains a practical challenge . Instead of definition - driven instructions , we propose example - driven instructions and show their effectiveness .__label__Method|Algorithm|Produce
The experimental results show the effectiveness of our system , for instance , the F3 / NR improvement of our system over the baseline and translation - based model reaches 7 . 9 %/ 11 . 1 %, and 5 . 1 %/ 5 . 6 %, respectively . Recently launched social QA websites such as Yahoo ! Answer and Baidu Zhidao _CITE_ provide an interactive platform for users to post questions and answers . After questions are answered by users , the best answer can be chosen by the asker or nominated by the community . The number of Q & A pairs on such sites has risen dramatically .__label__Supplement|Website|Introduce
The opinion score is calculated using the number of opinion words normalized by the total number of words in candidate sentence . For lexicon - based opinion analysis , the selection of opinion thesaurus plays an important role in the final performance . HowNet _CITE_ is a knowledge database of the Chinese language , and provides an online word list with tags of positive and negative polarity . We use the English translation of those sentiment words as the sentimental lexicon . SentiWordNet ( Esuli and Sebastiani , 2006 ) is another popular lexical resource for opinion mining .__label__Material|Data|Introduce
In this task we followed the tagging schema of “ Specification for Corpus Processing at Peking University ” in the design of our model . In this word , under the assumption that a segmentation system for general text is already good , for a special domain we only need to do some modification to make the segmentation result better . The main frame of this system is using ICTCLAS _CITE_ as the segmentation tool . Based on it result , we do a group of preprocessing and postprocessing to get a better result . After analyzed the 500 sentences train corpora , we found that there are some rules in the segmentation that it is very difficult to use other approaches to recognize them .__label__Method|Tool|Use
YourDictionary . com lists about five such dictionaries . Most of them are narrowdomain dictionaries . The Google directory _CITE_ lists seven dictionaries . Rjecnik . com is the oldest one in these language pairs and is still active and expanding . Tkusmic . com was created in 2003 and has a very similar interface .__label__Material|Data|Introduce
We have developed a first prototype of the system which uses simulated data to produce handover reports . This runs on standard desktop PCs . For our second prototype , which is currently being developed , we port the NLG algorithm onto a GETAC Z710 tablet _CITE_ which has been chosen for it ’ s robustness , capacitative touch screen , and long battery life ( Figure 2 ). Our research also includes the establishment of a connection between the tablet and sensors , the recording of the incoming data stream and the development of an interface for the tablet , which can be used by the CFR to enter observations and actions taken or any other useful information . At the ENLG workshop we will present our first hardware prototype alongside the desktop computer version , highlighting the challenges that the project faces in developing a handover report generator for pre - hospital care .__label__Method|Algorithm|Use
We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation . This design enables learning for mapping high - level instructions , which previous statistical methods cannot handle . _CITE_ In this paper , we introduce a novel method for mapping high - level instructions to commands in an external environment . These instructions specify goals to be achieved without explicitly stating all the required steps . For example , consider the first instruction in Figure 1 — “ open control panel .” The three GUI commands required for its successful execution are not explicitly described in the text , and need to be inferred by the user .__label__Method|Algorithm|Produce
The complete set of rules can be found in ( Hwa et al ., 2002 ). Because our error analysis and subsequent algorithm refinements made use of our original Chinese - English data set , we created a new test set based on 88 new Chinese sentences from the Penn Chinese Treebank , already manually translated into English as part of the NIST MT evaluation preview . _CITE_ These sentences averaged 19 . 0 words in length . As described above , parses ort the English side were created semi - automatically , and word alignments were acquired manually . However , in order to reduce our reliance ort linguistically sophisticated human annotators for Chinese syntax , we adopted art alternative strategy for obtaining the gold standard : we automatically converted the Treebank & apos ; s constituency parses of the Chinese sentences into syntactic dependency representations , using art algorithm similar to the one described in Section 2 of the paper by Xia and Palmer ( 2001 ).__label__Supplement|Website|Introduce
We sketch a formal analysis of the response categories in the framework of KoS . Responding to a query with a query is a common occurrence , representing on a rough estimate more than 20 % of all responses to queries found in the British National Corpus . _CITE_ Research on dialogue has long recognized the existence of such responses . However , with the exception of one of its subclasses — albeit a highly substantial one — the class of query responses has not been characterized empirically in previous work . The class that has been studied in some detail are Clarification Requests ( CRs ) ( Rodriguez and Schlangen , 2004 ; Rieser and Moore , 2005 ).__label__Material|Data|Use
When the training data is much smaller , the upper bound of the CL approach would decrease tremendously , whereas the upper bound of the CoRef approach remains the same . As mentioned before , most existing language ID algorithm falls into this category . We chose TextCat , _CITE_ an implementation of Cavnar - Trenkle ’ s algorithm ( 1994 ), as an example of these algorithms . In order to take advantage of the context information , we trained several classifiers ( e . g ., decision tree , Naive Bayes , and maximum entropy ) using the Mallet package ( McCallum , 2002 ) and a SVM classifier using the libSVM package ( Chang and Lin , 2001 ). The result is in Table 7 .__label__Method|Code|Use
This may be due to the frequency of relative clauses in GENIA . 4 Parsing system and extraction of imperative and question sentences We introduce the parser and the POS tagger whose performances are examined , and the extraction of imperative or question sentences from GTREC treebank on which the performances are measured . The Enju parser ( Ninomiya et al ., 2007 ) _CITE_ is a deep parser based on the HPSG formalism . It produces an analysis of a sentence that includes the syntactic structure ( i . e ., parse tree ) and the semantic structure represented as a set of predicate - argument dependencies . The grammar is based on the standard HPSG analysis of English ( Pollard and Sag , 1994 ).__label__Method|Tool|Introduce
Table 1 presents statistics of these in - domain data . The data extracted from HAL were used to adapt a generic system to the scientific literature domain . The generic system was mostly trained on data provided for the shared task of Sixth Workshop on Statistical Machine Translation _CITE_ ( WMT 2011 ), described in Table 2 . Table 3 presents results showing , in the English – French direction , the impact on the statistical engine of introducing the resources extracted from HAL , as well as the impact of domain adaptation techniques . The baseline statistical engine is a standard PBSMT system based on Moses ( Koehn et al ., 2007 ) and the SRILM tookit ( Stolcke , 2002 ).__label__Material|Data|Use
We modeled our approach based on the features motivated by the morphological complexity of Estonian . To our knowledge , this is the first work that studies the role of morphology based features for proficiency classification in general and in Estonian in particular . The Estonian Interlanguage Corpus ( EIC ) _CITE_ was created by the Talinn University . It is a collection of written texts produced by learners of Estonian as a second language . Most of the learners were native speakers of Russian .__label__Material|Data|Introduce
All the sentences are collected from students ’ written essays . All the data are in traditional Chinese . The dictionary used in SSSP algorithm is SogouW _CITE_ dictionary from Sogou inc ., which is in simplified Chinese . The OpenCC converter is used to convert it into traditional Chinese . For similar character map the data set provided by ( Liu et al ., 2011 ) is used .__label__Material|Data|Use
Both STACKING and 2STEPSML systems rely on several kinds of features , which vary from lexical to semantic ones . Features are grouped in seven main classes , as follows : We adopt several similarity measures using semantic distributional models ( see Section 2 . 5 ), the Resnik ’ s knowledge - based approach ( Resnik , 1995 ) and the point - wise mutual information as suggested by Turney ( Turney , 2001 ) computed on British National Corpus . For all the features , the idf is computed relying on UKWaC corpus _CITE_ ( Baroni et al ., 2009 ). measure which counts the number of possible paraphrasings belonging to the two texts . Given two texts T1 and T2 , for each token in T1 a list of paraphrasings is extracted using a dictionary .__label__Material|Data|Use
We first evalaute the sentence level plagirism detection using the PAN corpus in English . We then evaluate the capability of the full system to detect on - line plagiarism cases using annotated results in Chinese . We want to compare our model with the state - ofthe - art methods , in particular the winning entries in plagiarism detection competition in PAN _CITE_ . However , the competition in PAN is designed for off - line plagiarism detection ; the entries did not exploit an IR system to search the Web like we do . Nevertheless , we can still compare the core component of our system , the sentence - based measuring model with that of other systems .__label__Method|Tool|Compare
Ai are the parameters need to be estimated which reflects the importance of fi ( c , d ) in prediction . Li and Roth ( 2002 ) have developed a machine learning approach which uses the SNoW learning architecture . They have compiled the UIUC question classification dataset _CITE_ which consists of 5500 training and 500 test questions . All questions in the dataset have been manually labeled according to the coarse and fine grained categories as shown in Table 1 , with coarse classes ( in bold ) followed by their fine classes . The UIUC dataset has laid a platform for the follow - up research including ( Hacioglu and Ward , 2003 ; Zhang and Lee , 2003 ; Li and Roth , 2006 ; Krishnan et al ., 2005 ; Moschitti et al ., 2007 ).__label__Material|Data|Introduce
We train ℓ2 - regularized logistic regression classifiers using the LIBLINEAR package ( Fan et al , 2008 ) with the learned embeddings . To build the component - enhanced character embeddings , we employ the GB2312 character set and extract all their component lists . It is easy to obtain the first components ( i . e ., the radicals ), as they are readily available in the online Xinhua Dictionary _CITE_ . For the rest radical - like components , we extract them by matching the patterns like “� k ( from )+ X ” in the Xinhua dictionary . Such a pattern indicates that a character has a component of X .__label__Material|Data|Use
1 . Some information cannot be deduced from the already used databases and thus we require additional means of gathering extra information of the form : Background knowledge was built semiautomatically , for the named entities ( NEs ) and for numbers from the hypothesis without correspondence in the text . For these NEs , we used a module to extract from Wikipedia _CITE_ snippets with information related to them . Subsequently , we use this file with snippets and some previously set patterns of relations between NEs , with the goal to identify a known relation between the NE for which we have a problem and another NE . If such a relation is found , we save it to an output file .__label__Supplement|Website|Use
– Subclauses : The mean number of subclauses in each sentence , normalized by sentence length in words . The mean subclause length in words . Subclauses are labeled as “ SBAR ” in the parser tree generated by a commonly used NLP tool , Stanford Core NLP ( Klein and Manning , 2003 ), which is an integrated suite of natural language processing tools for English in Java _CITE_ , including part - of - speech tagging , parsing , co - reference , etc .. – Sentence level : The sum of the depth of all nodes in a parser tree generated by Stanford Core NLP . The height of the parser tree is also incorpo rated into the feature set . – Mode , preposition , comma : The number of modes , prepositions and commas in each sentence respectively , normalized by sentence length in words .__label__Method|Tool|Use
Furthermore , while some discrete classifiers provide a degree of confidence or probability for a relation classification , there is no a priori reason that such values would correspond to human prototypicality judgments . Our proposed task is distinct from these past tasks in that we focus on measuring the degree of relational similarity . _CITE_ A graded measure of the degree of relational similarity would tell us that dog : bark is more similar to cat : meow than to floor : squeak . The discrete classification ENTITY : SOUND drops this information . Systems that are successful at identifying degrees of relation similarity can have a significant impact where an application must choose between multiple instances of the same relation .__label__Method|Algorithm|Compare
The English corpus we used was the New York Times ( 1994 - 2002 ) material available in the LDC Gigaword Corpus ( NYT ) ( Graff , 2003 ). For Dutch we used both the ILK Corpus and the Twente Corpus ( TWC ). For French we used 8 years (' 91 -' 98 ) of Roularta Magazines _CITE_ . Statistics on these corpora are presented in table 2 . A TISC lexicon is derived from a large corpus of tokenised , but otherwise raw text , from which all xmL or other tags have been discarded .__label__Material|Data|Use
( 2004 ). The dataset contains 1000 positive and 1000 negative movie reviews with size varying between 700 to 1000 words . As summary generation is time consuming task ( DUC _CITE_ only used 25 summaries to evaluate the performance of systems ), we picked 100 positive and 100 negative reviews randomly from the dataset and their abstract summaries are generated manually with 200 words limit as budget for evaluation . These 200 summaries are used as gold standard for estimating ROUGE scores of system generated summaries . In the experiment , the partial enumeration based greedy algorithm ( Khuller et al ., 1999 ) is used for summary generation of 200 test documents within budget of 200 words .__label__Method|Tool|Introduce
However , the hope is that by choosing the right value of i , these estimates will be accurate enough to affect the search quality only slightly , which is analogous to “ almost admissible ” heuristics in A * search ( Soricut , 2006 ). We test our methods on two large - scale English - toChinese translation systems : a phrase - based system and our tree - to - string system ( Huang et al ., 2006 ). We implemented Cubit , a Python clone of the Pharaoh decoder ( Koehn , 2004 ), _CITE_ and adapted cube pruning to it as follows . As in Pharaoh , each bin i contains hypotheses ( i . e ., + LM items ) covering i words on the source - side . But at each bin ( see Figure 5 ), all + LM items from previous bins are first partitioned into − LM items ; then the hyperedges leading from those − LM items are further grouped into hyperedge bundles ( Figure 6 ), which are placed into the priority queue of the current bin .__label__Method|Code|Produce
The linear interpolation coefficients are grouped into equivalence classes ( tied ) based on the range into which the count falls ; the count ranges for each equivalence class , “ buckets ,” are set such that a statistically sufficient number of events fall within that range . In our experiments , we set the count ranges to be the intervals of 2i , i = 0 , 1 , · · · , 10 ( i . e ., 0 , 1 , 2 , 4 , 8 , 16 , 32 , 64 , 128 , 256 , 512 , 1024 , and ∞). These “ tied ” interpolation weights are determined by the maximum likelihood estimate from cross - validation data through the EM algorithm ( Dempster , Laird , and Rubin 1977 ) where we use a public available parser in the openNLP software _CITE_ to parse sentences in cross - validation data , and we run LSA to extract N most likely topics for each document in cross - validation data , then we gather joint counts for each model component , WORDPREDICTOR , TAGGER , CONSTRUCTOR used to determine interpolation weights . In the M - step , assuming that the count ranges and the corresponding interpolation values for each order are kept fixed to their initial values , the only parameters to be re - estimated using the EM algorithm are the maximal order counts for each model component . The interpolation scheme outlined here is then used to obtain a smooth probability estimate for each model component .__label__Method|Tool|Use
A related issue is that of annotation tools . One possibility would be to use a generic tool supporting the annotation of either of the levels we are concerned with ( and any levels that we may want to add in the future ). The most ambitious projects we are familiar with are the MATE Workbench and the follow - up NITE Workbench _CITE_ for multi - level , cross - level and cross - modality annotation of language data . The MATE Workbench has been developed as a highly customizable tool for parallel annotation of arbitrary and possibly non - hierarchical layers of linguistic description . It is an open source tool written in Java and handles XML - encoded data .__label__Method|Tool|Introduce
We use two machine learning methods in this section . They are maximum entropy method ( ME ) ( Beger et al . 96 ) and support vector machine ( SVM ) ( cristianini00 ) _CITE_ , both of which have been shown to be quite effective in natural language processing . The task of a machine learning method is to make a classifier that can decide whether a response is paraphrasable by te - hoshii or not . A response X is tagged possible if it is paraphrasable Given training data , a machine learning method produces a classifier that outputs possible or impossible according to a given feature vector .__label__Method|Algorithm|Use
In addition , dependency information in the spirit of ( Shen et al ., 2008 ) is included . Jane features models for string - to - dependency language models and computes various scores based on the well - formedness of the resulting dependency tree . Jane supports the Stanford parsing format , _CITE_ but can be easily extended to other parsers . In the standard formulation of the hierarchical phrase - based translation model two additional rules are added : This allows for a monotonic concatenation of phrases , very much in the way monotonic phrasebased translation is carried out . It is a well - known fact that for phrase - based translation , the use of additional reordering models is a key component , essential for achieving good translation quality .__label__Method|Algorithm|Extent
Moreover , ( Hahn et al ., 2010 ) have shown that the structured information gathered from Wikipedia infoboxes can be used to answer complex questions , like “ Which Rivers flow into the Rhine and are longer than 50 kilometers ?” For this purpose , text documents need to be previously annotated using DBpedia Spotlight ( Mendes et al ., 2011 ), which automatically annotates text with links to articles in Wikipedia . The process of semantic enrichment is still largely domain - dependent ; therefore , apart from the available general - purpose knowledge bases and ontologies ( DBpedia , FOAF , DublinCore ...), the EUMSSI platform needs specialized resources for categorizing videos on different dimensions . Linked Data technologies ( Heath and Bizer , 2011 ) and the Linked Open Data cloud _CITE_ provide access to several of these resources , including geodata , movie databases and program information . The semantically enriched information is then used by the EUMSSI system to make personalized content - based recommendation . We propose a novel recommender system that leverages matrix factorization ( Koren , 2008 ) with implicit feedback in order to integrate content - based similarity , usage history ( i . e .__label__Method|Tool|Introduce
Shared task participants were required to provide the data collected as part of their experiments . All of the shared task data is available on the workshop website . Amazon ’ s Mechanical Turk _CITE_ is an online marketplace for work . Amazon ’ s tag line for Mechanical Turk is artificial artificial intelligence , and the name refers to a historical hoax from the 18th cen tury where a chess - playing automaton appeared to be able to beat human opponents using a mechanism , but was , in fact , controlled by a person hiding inside the machine . These hint at the the primary focus of the web service , which is to get people to perform tasks that are simple for humans but difficult for computers .__label__Supplement|Website|Introduce
It was created by manually identifying the emotions of a few seed words and then marking all their WordNet synonyms as having the same emotion . The General Inquirer ( Stone et al ., 1966 ) has 11 , 788 words labeled with 182 categories of word tags , including positive and negative semantic orientation . _CITE_ It also has certain other affect categories , such as pleasure , arousal , feeling , and pain but these have not been exploited to a significant degree by the natural language processing community . Work in emotion detection can be roughly classified into that which looks for specific emotion denoting words ( Elliott , 1992 ), that which determines tendency of terms to co - occur with seed words whose emotions are known ( Read , 2004 ), that which uses hand - coded rules ( Neviarouskaya et al ., 2009 ), and that which uses machine learning and a number of emotion features , including emotion denoting words ( Alm et al ., 2005 ). Much of this recent work focuses on six emotions studied by Ekman ( 1992 ).__label__Material|Data|Introduce
The system is domain - independent , using a primary task description vocabulary and training data to learn the task , but domain resources can be incorporated as additional features when available , as described here . The approach can be broken down into 4 components : an automated annotation pipeline to provide the basis for features , classification - based trigger identification and argument identification components , and a post - processing component to apply semantic constraints . The UIMA framework _CITE_ is used to integrate the components into a pipeline architecture . A definition of the events to be extracted is used to define candidates for classification and post - process the results of the classification . First a list of domain - specific entity classes is given .__label__Method|Tool|Use
We give a full spectrum evaluation of all three stages of IR + QA : document retrieval , passage retrieval and answer extraction , to examine thoroughly the effectiveness of the method . All of our code and datasets are publicly available . _CITE_ Besides Predictive Annotation , our work is closest to structured retrieval , which covers techniques of dependency path mapping ( Lin and Pantel , 2001 ; Cui et al ., 2005 ; Kaisser , 2012 ), graph matching with Semantic Role Labeling ( Shen and Lapata , 2007 ) and answer type checking ( Pinchak et al ., 2009 ), etc . Specifically , Bilotti et al . ( 2007 ) proposed indexing text with their semantic roles and named entities .__label__Method|Code|Produce
We give a full spectrum evaluation of all three stages of IR + QA : document retrieval , passage retrieval and answer extraction , to examine thoroughly the effectiveness of the method . All of our code and datasets are publicly available . _CITE_ Besides Predictive Annotation , our work is closest to structured retrieval , which covers techniques of dependency path mapping ( Lin and Pantel , 2001 ; Cui et al ., 2005 ; Kaisser , 2012 ), graph matching with Semantic Role Labeling ( Shen and Lapata , 2007 ) and answer type checking ( Pinchak et al ., 2009 ), etc . Specifically , Bilotti et al . ( 2007 ) proposed indexing text with their semantic roles and named entities .__label__Material|Data|Produce
On of the most representative tools of the Web 2 . 0 are social networks , which allow millions of users to publish any information in a simple way and to share it with their network of contacts or “ friends ”. These social networks have also evolved and become a continuous flow of information . A clear example is the microblogging platform Twitter _CITE_ . Twitter publishes all kinds of information , disseminating views on many different topics : politics , business , economics and so on . Twitter users regularly publish their comments on a particular news item , a recently purchased product or service , and ultimately on everything that happens around them .__label__Supplement|Website|Introduce
We already described the training data for supervised and semi - supervised classifiers in previous sections . In this section we will compare their dialect classification accuracies . We select two test sets : 9 . 5K sentences from the AOC corpus as the AOC test set and 2 . 3K sentences from the Facebook data set as the FB test set _CITE_ . Both test sets have the dialect of each sentence labeled by human . The accuracy is computed as the percentage of sentences whose classified label is the same as the human label .__label__Material|Data|Use
At the initial step , we form three genetic semantic categories – disease , drugs , and symptoms – as was discussed in Section 2 ; see Table 1 for examples . For diseases and drugs , we concentrate on extraction of their names . The category contents were derived from Webster ’ s New World Medical Dictionary [ 15 ], the International Classification of Diseases ( ICD9 codes ) _CITE_ , the Medical Dictionary for Regulatory Activities ( MedDRA ) 10 and Canadian Drug Product Database ( Active and Inactive ) 11 . ICD9 codes [ 2 ] are used by health care professionals to tag and classify morbidity data from inpatient and outpatient records , physician offices , as well as most of the National Center for Health Statistics ( NCHS ) 12 and the Canada Institute for Health Information13 surveys . The codes are divided into two sections : one containing diseases and injuries ( ICD9CM Disease and Injury ), and another containing surgical , diagnostic , and therapeutic procedures ( ICD9CM Procedures ).__label__Material|Data|Extent
According to the latest Twitter entry in Wikipedia , the number of Twitter users has climbed to 190 million and the number of tweets published on Twitter every day is over 65 million . As a result of the rapidly increasing number of tweets , mining people ’ s sentiments expressed in tweets has attracted more and more attention . In fact , there are already many web sites built on the Internet providing a Twitter sentiment search service , such as Tweetfeel , Twendz , and Twitter Sentiment _CITE_ . In those web sites , the user can input a sentiment target as a query , and search for tweets containing positive or negative sentiments towards the target . The problem needing to be addressed can be formally named as Target - dependent Sentiment Classification of Tweets ; namely , given a query , classifying the sentiments of the tweets as positive , negative or neutral according to whether they contain positive , negative or neutral sentiments about that query .__label__Supplement|Website|Introduce
The output layer is a SoftMax classifier that predicts , after the “ GO ” symbol is read , one of the following three labels : 1 , if a word is to be retained in the compression , 0 if a word is to be deleted , or EOS , which is the output label used for the “ GO ” input and the end - of - sentence final period . Input representation : In the simplest implementation , that we call LSTM , the input layer has 259 dimensions . The first 256 contain the embedding - vector representation of the current in put word , pre - trained using the Skipgram model _CITE_ ( Mikolov et al ., 2013 ). The final three dimensions contain a one - hot - spot representation of the goldstandard label of the previous word ( during training ), or the generated label of the previous word ( during decoding ). For the LSTM + PAR architecture we first parse the input sentence , and then we provide as input , for each input word , the embedding - vector representation of that word and its parent word in the dependency tree .__label__Method|Algorithm|Use
We use Python 2 . 7 ’ s multiprocessing module in all experiments . We base our experiments on our dynamic programming incremental dependency parser ( Huang and Sagae , 2010 ). _CITE_ Following Huang et al . ( 2012 ), we use max - violation update and beam size b = 8 . We evaluate on the standard Penn Treebank ( PTB ) using the standard split : Sections 02 - 21 for training , and Section 22 as the held - out set ( which is indeed the test - set in this setting , following McDonald et al .__label__Method|Tool|Use
Budanitsky and Hirst ( 2001 ) evaluated the performance of five different methods that measure the semantic distance between words in the WordNet Hierarchy , which Patwardhan et al . ( 2003 ) have then implemented and made available for general use as the Perl package distance - 0 . 11 . _CITE_ We focused in particular on the following three measures , the first two of which are based on information theoretic principles , and the third on sense topology : • Resnik ( 1995 ) combined WordNet with corpus statistics . He defines the similarity between two words as the information content of the lowest superordinate in the hierarchy , defining the information content of a concept c ( where a concept is the WordNet class containing the word ) to be the negative of its log likelihood . This is calculated over a corpus of text .__label__Method|Code|Introduce
Table 1 shows the percentage of discharge letters with available standard sections in the corpus of 1 , 375 EHRs . Having the potential to identify automatically the Anamnesis section , which contains the patient history , we can plan further research tasks related to its temporal segmentation . We have developed extractors of ICD - 10 codes ( the International Classification of Diseases , v . 10 _CITE_ ) and ATC codes ( the Anatomical Therapeutic Chemical Classification System ) from discharge letter texts [ 9 , 10 , 11 ]. The tool for diagnosis extraction assigns ICD - 10 codes to Bulgarian nominal phrases designating disease names with 84 . 5 % precision [ 9 ]. The drug extractor assigns ATC codes to Bulgarian drug names with f - measure 98 . 42 % and achieves 93 . 85 % f measure for the dosage recognition [ 10 ].__label__Method|Tool|Extent
For policy training , we train a linear SVM classifier using Liblinear ( Fan et al ., 2008 ). For all languages , we run DAgger for 20 iterations and se lect the best policy evaluated on the development set among the 20 policies obtained from each iteration . We use the publicly available implementation of MSTParser _CITE_ ( with modifications to the feature computation ) and its default settings , so the feature weights of the projective and non - projective parsers are trained by the MIRA algorithm ( Crammer and Singer , 2003 ; Crammer et al ., 2006 ). Our feature set contains most features proposed in the literature ( McDonald et al ., 2005a ; Koo and Collins , 2010 ). The basic feature components include lexical features ( token , prefix , suffix ), POS features ( coarse and fine ), edge length and direction .__label__Method|Code|Use
As a post - processing , in the En2Es direction we used a POS target language model as a feature ( instead of the target language model based on classes ) that allowed to recover the segmentations ( de Gispert , 2006 ). Language Model Interpolation . In other to better adapt the system to the out - of - domain condition , the target language model feature was built by combining two 5 - gram target language models ( using SRILM _CITE_ ). One was trained from the EuroParl training data set , and the other from the available , but much smaller , newscommentary data set . The combination weights for the EuroParl and news - commentary language models were empirically adjusted by following a minimum perplexity criterion .__label__Method|Tool|Use
Moreover , it creates a Web interface ( called Spinet ) where WSs can be tested and used with input forms . All these features make Soaplab a suitable tool for our project . Moreover , its numerous successful stories make it a safe choise ; e . g ., it has been used by the European Bioinformatics Institute _CITE_ to deploy their tools as WSs . Once the WSs are deployed by WSPs , some means to find them becomes necessary . Biocatalogue ( Belhajjame et al ., 2008 ) is a registry where WSs can be shared , searched for , annotated with tags , etc .__label__Supplement|Website|Introduce
A classification based approach and a dictionary based approach are employed to calculate the accuracy of the queries translated . 400 sentences with their corresponding translations ( English - Hindi ) have been used as test set to evaluate the performance of the query formation . The sentence pairs are provided by FIRE _CITE_ . These sentences contain all types of words ( Named entities , Verbs etc ) and will be referred to as samples . The English language sentences are used as queries and are translated to Hindi using the approach described .__label__Method|Tool|Use
Second , we consider two methods for predicting comment polarity from post content : support vector machine classification , and sLDA , a topic - modeling - based approach . Finally , we demonstrate that emotional reactions are indeed community - specific , compare the accuracy of this approach to the more traditional approach of predicting sentiment of a text from the text itself , and present our conclusions . In this study , we use a collection of blog posts from five blogs : Carpetbagger ( CB ) , Daily Kos ( DK ) _CITE_ , Matthew Yglesias ( MY ) , Red State ( RS ) , and Right Wing News ( RWN ) , that focus on American politics made available by ( Yano et al ., 2009 ). The posts were collected during November 2007 to October 2008 , which preceded the US presidential elections held in November 2008 . The blogs included in the dataset vary in political idealogy with blogs like Daily Kos that are Democrat - leaning and blogs like Red State tending to be much more conservative .__label__Material|Data|Use
Each synset is accompanied by a gloss describing its meaning and , when present , one or more examples of use . Only 3 , 177 glosses ( 8 , 21 %) are in Italian and , in particular , 402 for verbs and 2 , 481 for nouns . The SCDM lexicon is part of a larger research initiative , Senso Comune _CITE_ ( Oltramari et al . ( 2013 )). Senso Comune aims at building an open knowledge base for the Italian language , designed as a crowd - sourced initiative that stands on the solid ground of an ontological formalization and wellestablished lexical resources .__label__Material|Data|Introduce
These results are obtained using 10 - folds cross validation (* Recall has been inherited from the definition classification task , since no indication has been reported in their contribution ). In this section we present the evaluation of our technique on both the tasks of classifying definitional sentences and extracting hypernym relations . Notice that our approach is susceptible from the errors given by the POS - tagger _CITE_ and the syntactic parser . In spite of this , our approach demonstrates how syntax can be more robust for identifying semantic relations . Our approach does not make use of the full parse tree , and we are not dependent on a complete and correct result of the parser .__label__Method|Tool|Use
In ( Bunt , 2006 ); ( Bunt and Girard , 2005 ) a dimension in dialogue act analysis is defined as an aspect ofparticipating in dialogue which can be addressed : The independence of dimensions , required by this definition , has the effect that an utterance may have a function in one dimension independent of the functions that it may have in other dimensions , and helps to explain why utterances may have multiple functions . Moreover , it leads to more manageable and more adaptable annotation schemas ( compared to , for instance , DAMSL and its derivatives ), since it allows annotators to leave out certain dimensions that they are not interested in , or to extend the schema with additional dimensions ; and it allows restricting or modifying the set of tags in a particular dimension without affecting the rest of the schema . Based on the above definition and extensive theoretical and empirical studies , 10 dimensions are defined in the DIT ++ dialogue act annotation scheme _CITE_ : the domain or task / activity ( Task ); feedback on the processing of previous utterances by the speaker ( Auto - feedback ) or by other interlocutors ( Allofeedback ); managing difficulties in the speaker ’ s utterance production ( Own - Communication Management , OCM ) or that of other interlocutors ( Partner Communication Management , PCM ); the speaker ’ s need for time to continue the dialogue ( Time Management ); establishing and maintaining contact ( Contact Management ); the allocation of the next turn ( Turn Management ); the way the speaker is planning to structure the dialogue ( Dialogue Structuring ); and attention for social aspects of the interaction ( Social Obligations Management , SOM ). This paper investigates the independence of these ten dimensions . In Section 2 we discuss the notion of independence of dimensions and how it can be tested .__label__Method|Algorithm|Use
We divided them evenly and randomly into two parts and use one half for a training set and the other for a test set . In experiments described in section 4 . 4 and 4 . 5 , we used other portion of the corpus to scale experiments . For tokenization and pos - tagging , we used MeCab to Japanese texts and SS Tagger _CITE_ to English texts . Because SS Tagger doesn ’ t act as lemmatizer , we used morphstr () function in WordNet library . Figure 3 shows the results of experiments on several conditions .__label__Method|Tool|Use
In order to do so , we index target documents t in the collection thanks to an indexing strategy φ that will be described shortly . Then , for a source document s , we first index it , that is , we compute φ ( s ), and query the retrieval engine with φ ( s ), which in turn returns the N most similar target documents found in the collection . In our experiments , we used the Lucene _CITE_ retrieval library . We tested two indexing strategies : one reduces a document to the sequence of hapax words it contains ( φ - hap ), the other one reduces it to its sequence of numerical entities ( φ - num ). Hapax words have been found very useful in identifying parallel pairs of documents ( Enright and Kondrak , 2007 ) as well as for word - aligning bitexts ( Lardilleux and Lepage , 2007 ).__label__Method|Code|Use
For aspects , however , there is not a pre - defined set . We observe that these topic aspects are usually named entities or noun phrases frequently mentioned . We therefore use the OpenNLP toolkit to perform chunking and obtain noun phrases and the Standford NER tagger _CITE_ to identify named entities from the posts . Some of the candidate aspect phrases identified above actually refer to the same actual aspect , e . g . “ Obama voter ,” “ Obama voters ” and “ the Obama voter .” We remove stop words from each candidate phrase and use the WordNet by Miller ( 1995 ) to obtain the lemma of each word such that we can normalize the candidate aspect phases to some extent .__label__Method|Tool|Use
For lexicographers , the computational environment fills the need for a corpus workbench which supports WSD . Results under simulated lexicographic use on the English lexical - sample task show precision comparable with supervised systems ', without using the laboriously - prepared training data . WASP - Bench _CITE_ is a web - based tool supporting both corpus - based lexicography and Word Sense Disambiguation . The central premise behind the initiative is that deciding what the senses for a word are , and developing a WSD program for it , should be tightly coupled . In the course of the corpus analysis , the lexicographer explores the textual clues that indicate a word is being used in one sense or another ; given an appropriate computational environment , these clues can be gathered and used to seed a bootstrapping WSD program .__label__Method|Tool|Introduce
The second article in this part , “ Multidimensional Dialogue Management ” ( Simon Keizer , Harry Bunt , and Volha Petukhova ), is more theoretical and presents a dialog manager built using the framework of Dynamic Interpretation Theory ( Bunt 2000 ) which is able to both interpret and generate utterances using dialog acts . The article also presents briefly the way in which this dialog manager was integrated in the IMIX demonstrator . In my opinion , the editors of the book could have chosen a better title for the third part of the book : “ Fusing Text , Speech , and Images .” Both articles in this part present work done in the IMOGEN ( Interactive Multimodal Output GENeration ) project , _CITE_ one of the subprojects embedded in the IMIX Programme that focused on producing multimodal presentations that combine text , speech , and graphics . Only the first article focuses on the multimodal aspect of the project , however . The other one discusses only text processing .__label__Method|Tool|Introduce
Thus , we build a sentiment classifier that takes a node as input and outputs a positive and a negative score . It is built from widely - used , freely available resources : the OpinionFinder ( Wilson et al ., 2005 ) and General Inquirer ( Stone et al ., 1966 ) lexicons and the OpinionFinder system . _CITE_ We also use a new Opinion Extraction system ( Johansson and Moschitti , 2013 ) that shows better performance than previous work on fine - grained sentiment analysis , and a new automatically developed connotation lexicon ( Feng et al ., 2013 ). We implement a weighted voting method among these various sentiment resources . After that , for nodes that have not yet been assigned polar values ( positive or negative ), we implement a simple local discourse heuristic to try to assign them polar values .__label__Method|Tool|Use
While topics of the first kind can be directly rephrased into essay topics , from topics of the second kind one of the available interpretations was chosen . To give the oDesk writers a familiar search experience while maintaining reproducibility at the same time , we developed a tailored search engine called ChatNoir ( Potthast et al ., 2012b ). Besides ours , the only other public search engine for the ClueWeb is Carnegie Mellon ’ s Indri , _CITE_ which , unfortunately , is far from our efficiency requirements . Moreover , its search interface does not follow the standard in terms of result page design , and it does not give access to interaction logs . Our search engine is on the order of milliseconds in terms of retrieval time , its interface follows industry standards , and it features an API that allows for user tracking .__label__Method|Tool|Compare
It is a common practice in the Hindi community to use the characters zF [ k ], a [ kʰ ], wr [ g ], ur [ ʤ ], � [ ɖ ], - a [ ɖʰ ] and w [ p ] instead of the characters zF [ q ], w [ x ], ar [ ɣ ], 7 [ z ], , [ ɽ ], -,� [ ɽʰ ] and ? F [ f ] respectively , due to their shape similarities . In Test Set 2 , the extracted Hindi sentences were edited and corrected for these typographical errors . Then , we translated the extracted Hindi sentences into Urdu by using an online Hindi – Urdu transliteration system _CITE_ . These translated Urdu sentences were post - edited to remove errors , and all necessary diacritical marks were introduced in the Urdu text . Diacritical marks are vital for Urdu to Hindi transliteration , but they are sparingly used by people in writing .__label__Method|Tool|Use
Although the general ways are essentially the same for English and Chinese , the implementation details are different . It is also nontrivial to optimize these methods for Chinese NLP tasks . There are also some toolkits to be used for NLP , such as Stanford CoreNLP _CITE_ , Apache OpenNLP , Curator and NLTK . But these toolkits are developed mainly for English and not optimized for Chinese . In order to customize an optimized system for Chinese language process , we implement an open source toolkit , FudanNLPS , which is written in Java .__label__Method|Tool|Use
( 2008 ) proposed two different methods to extract term translations based on the observation that authors of many bilingual web pages , especially those whose primary language is Chinese , Japanese or Korean , sometimes annotate terms with their English translations inside a pair of parentheses , like “ c1c2 ... cn ( e1 e2 ... em )” ( c1c2 ... cn is a primary language term and e1 e2 ... em is its English translation ). Actually , in addition to the parenthesis pattern , there is another interesting phenomenon that in many bilingual web pages bilingual data appear collectively and follow similar surface patterns . Figure 1 shows an excerpt of a page which introduces different kinds of dogs _CITE_ . The page provides a list of dog names in both English and Chinese . Note that those bilingual names do not follow the parenthesis pattern .__label__Supplement|Document|Introduce
We show how MDD techniques and tools facilitate working with different data formats , adapting to new languages and domains , managing UIMA type systems , and accessing the external knowledge bases . Modern architectures of knowledge - based computing ( cognitive computing ) require HLT components to interact with increasingly many sources and services , such as Open Data and APIs , which may not be known before the system is designed . IBM ’ s Watson _CITE_ , for instance , works on textual documents to provide question answering and other knowledge - based services by integrating lexical resources , ontologies , encyclopaedic data , and potentially any available information source . Also , they combine a variety of analytical procedures , which may use search , reasoning services , database queries , to provide answers based on many kinds of evidence ( IJRD , 2012 ). Development platforms such as UIMA or GATE facilitate the development of HLT components to a great extent , by providing tools for annotating texts , based on vocabularies and ontologies , training and evaluating pipeline components , etc .__label__Supplement|Website|Introduce
The auto - encoder eliminates the need to have access to the original training data and the vector training model , requiring only the trained distributed vectors . In this sense , it can be considered computationally lighter than the above mentioned information fusion methods . _CITE_ Our approach is inspired by Ngiam et al . ( 2011 ) and Glorot et al . ( 2011 ), where auto - encoders are efficiently deployed to generate improved features for domains or modalities that are different from those of its inputs .__label__Method|Algorithm|Compare
We present a method to analyze calibration , and apply it to compare the miscalibration of several commonly used models . We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task . _CITE_ Natural language processing systems are imperfect . Decades of research have yielded analyzers that mis - identify named entities , mis - attach syntactic relations , and mis - recognize noun phrase coreference anywhere from 10 - 40 % of the time . But these systems are accurate enough so that their outputs can be used as soft , if noisy , indicators of language meaning for use in downstream analysis , such as systems that perform question answering , machine translation , event extraction , and narrative analysis ( McCord et al ., 2012 ; Gimpel and Smith , 2008 ; Miwa et al ., 2010 ; Bamman et al ., 2013 ).__label__Method|Algorithm|Produce
Our multilingual book collection consists of around 800k books in German and English languages . It is a subset of a larger Internet Archive collection of books in over 200 languages . The whole collection consists of OCRed books incorporating a small number of human transcribed books from Project Gutenberg _CITE_ . The collection was initially annotated with author and language information using the existing database obtained from the Internet Archive . This database originally contained incorrect language metadata .__label__Material|Data|Use
Thanks to the fact that it is translated into English , it is open to international audiences . To the best of our knowledge , most correspondence seminars are organised in the area of former Czechoslovakia . The Slovak seminars include KMS ( mathematics ), FKS _CITE_ ( physics ), and STROM ( mathematics ). The last mentioned one claims to have the longest tradition in the area of former Czechoslovakia , having been established in 1976 . Correspondence seminars organised in the Czech Republic include MKS ( mathematics ; founded 1981 ), FYKOS ( physics ; 1986 ), and KSICHT ( chemistry ; 2002 ; cf .__label__Material|Data|Introduce
The weights associated to feature functions are optimally combined using a discriminative training framework ( Och , 2003 ) ( Minimum Error Rate Training ( ME RT ), see details in Section 5 . 4 ), using the provided newstest2009 data as development set . om agiven word - aligned pair of sentences ( top ). 310 a word - aligned corpus ( using MGIZA ++ _CITE_ with default settings ) in such a way that a unique segmentation of the bilingual corpus is achieved , allowing to estimate the n - gram model . Figure 1 presents a simple example illustrating the unique tuple segmentation for The resulting sequence of tuples is further refined to avoid NULL words in the source side of the tuples ( 2 ). Once the whole bilingual training data is segmented into tuples , n - gram language model probabilities can be estimated .__label__Method|Tool|Use
The file format was also fixed for the plain spreadsheet one , keeping into consideration the discomfort faced by the linguists and data entry persons with the XML data format . The latest size of the lexicon is 37 , 000 root words with their parts of speech category specified . Wherever more than one category is possible , multiple categories have been entered with the comma as the separator . The Nepali POS Tagset designed in the beginning consisted of 112 tags _CITE_ . These tags were used to manually and semi - automatically annotate the written corpus as well . Experiences , however , showed that error rates of annotation could be much higher when the size of the tagset was a big one , the reason primarily being the chances of assigning incorrect tags to the words out of confusion while manually annotating the the training data itself .__label__Method|Algorithm|Use
The Simple NPs derived from our definition are highly coherent units , but are also more complex than the non - recursive English base NPs . As can be seen in Table 1 , our definition of Simple NP yields chunks which are on average considerably longer than the English chunks , with about 20 % of the chunks with 4 or more words ( as opposed to about 10 % in English ) and a significant portion ( 6 . 22 %) of chunks with 6 or more words ( 1 . 67 % in english ). Moreover , the baseline used at the CoNLL shared task _CITE_ ( selecting the chunk tag which was most frequently associated with the current PoS ) 3 For readers familiar with Hebrew and feel that n � lvn ; t is an adjective and should be inside the NP , we note that this is not the case – n � lvn ; t here is actually a Verb in the Beinoni form and the definite marker is actually used as relative marker . gives far inferior results for Hebrew SimpleNPs ( see Table 3 ). We have experimented with different known methods for English NP chunking , which resulted in poor results for Hebrew .__label__Method|Algorithm|Compare
In this section , we introduce the two tasks Paraphrase Identification and Semantic Similarity in Twitter , then we describe the set of simple features which enables us to achieve competitive performance in both tasks . This is a shared - task proposed as the Task # 1 & quot ; Paraphrase and Semantic Similarity in Twitter & quot ; at SemEval 2015 ( Xu et al ., 2015 ). _CITE_ In this task , the first common ground for development and comparison of Paraphrase Identification ( PI ) and Semantic Similarity ( SS ) systems for the Twitter data is provided . Given a pair of sentences from Twitter trends , systems are required to produce a binary yes / no judgment and an optionally graded similarity score in the scale [ 0 - 1 ] to measure their semantic equivalence . This task is used to promote this line of research in the new challenging setting of social media data , and help to advance other NLP techniques for noisy user - generated text in the long run .__label__Supplement|Website|Introduce
Thus , we designed the process to be gated by cost , and keeping the costs low was a high priority . Crowd - sourcing seemed particularly appropriate , given the nature of the task , so we opted to use Amazon Mechanical Turk ( AMT ). With over 500 , 000 workers _CITE_ , it provides the work force required to both achieve scalability and , equally importantly , to provide diversity in the stories and types of questions . We restricted our task to AMT workers ( workers ) residing in the United States . The average worker is 36 years old , more educated than the United States population in general ( Paolacci et al ., 2010 ), and the majority of workers are female .__label__Method|Tool|Use
This is particularly true with respect to Indian languages . In the last 15 years or so , MT into Indian languages ( especially Hindi ) has gained tremendous research interest in India and elsewhere . Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed , for example AnglaBharati ( Sinha et al ., 1995 ), Anusaaraka ( Chaudhury et al ., 2010 ), Anuvadaksh , Google , Sampark , MaTra _CITE_ ( Ananthakrishnan et al ., 2006 ), to name just a few . However , the issue of evaluating the output of these MT systems has remained rather unexplored . The state - of - the - art methods for automatic MT evaluation are represented by BLEU ( Papineni et al ., 2002 ) and closely related NIST ( Doddington , 2002 ), METEOR ( Banerjee and Lavie , 2005 ; Lavie and Agarwal , 2007 ) and TER ( Snover et al ., 2006 ).__label__Method|Tool|Introduce
To estimate the strength of each of them in a single utterance , we collect user ratings for three data sets that were collected under different conditions and are freely available . The utterances consist of user queries for restaurants , such as “ I need an Italian restaurant with a moderate price range .” Our joint dataset consists of 1 , 361 human utterances , 450 from the LIST , 334 from MAI , and 577 from CLASSIC . We asked users on the CrowdFlower crowdsourcing platform _CITE_ to read utterances and rate their colloquialism , politeness and naturalness on a 1 - 5 scale ( the higher the better ). The following questions were asked . could have been produced by a human .__label__Supplement|Website|Use
Systems which can link local semantic argument structures can create more complete meaning representations of a text than semantic role labellers restricted to the local domain . In order to stimulate research in this direction , we are organising a Shared Task at SemEval2010 on finding links between locally uninstantiated roles and the discourse context . _CITE_ To our knowledge , the data we are creating for this task will be the first publicly available reference data set containing information about global linking of semantic argument structures . ally , he was cleared . While discourse information can be beneficial for the computation of sematic argument structures , the reverse is also true : the semantic argument structures in a text and their relations can provide vital cues about the coherence of the discourse .__label__Supplement|Website|Produce
Finally , we picked Finnish – English for the rich agglutinative morphology of Finnish . Statistical machine translation systems are typically trained on sentence - aligned parallel corpora . We selected Europarl _CITE_ , a freely available parallel corpus in eleven languages . In addition , we also made a word alignment available , which was derived using a variant of the current default method for word alignment – Och and Ney ( 2003 )’ s refined method . Figure 1 details some properties of the parallel corpora .__label__Material|Data|Use
Table 1 shows the feature templates used in our experiments and we call the features in the bottom two rows “ non - local ” features . We conducted experiments for NTCIR - 9 and 10 patent data using a Japanese - English language pair . Mecab _CITE_ was used for the Japanese morphological analysis . The data are summarized in Table 2 . We used Enju ( Miyao and Tsujii , 2008 ) for parsing the English training data and converted parse trees into HFE trees by a head - finalization scheme .__label__Method|Tool|Use
In recent years , the adoption of standardized terminologies for the representation of clinical concepts – and their textual instantiations – has enabled meaning - based retrieval of information from electronic health records ( EHRs ). By identifying and linking key facts in health records , the ever - growing stores of clinical documentation now available to us can more readily be processed and , ultimately , leveraged to improve the quality of care . SNOMED CT _CITE_ has emerged as the de facto international terminology for representing clinical concepts in EHRs and is today used in more than fifty countries , despite only being available in a handful of languages . Translations into several other languages are , however , under way . This translation effort is essential for more widespread integration of SNOMED CT in EHR systems globally .__label__Method|Tool|Introduce
The data from the tweets was cleaned by removing the tweets that were not in English as well as the retweets ; i . e ., re - publications of a tweet by a different user . We deduplicated the 16 , 000 extracted URLs into 6 , 003 unique addressed , then extracted and preprocessed their contents . The newspaper package _CITE_ was used to extract article text and the title from the web page . Since we are interested in text articles that can serve as the source text for summarization algorithms , we needed to remove photos and video links such as those from Instagram and YouTube . To do so , we removed those links that contained fewer than a threshold of 150 words .__label__Method|Code|Use
These heterogeneous practices prevent the automated processing of rights information . Recently , we witness the proliferation of repositories collecting LRs and their metadata descriptions from various communities and sources according to different harvesting methodologies , and publishing them into homogeneous catalogs . The most relevant initiatives for our discussion are : META - SHARE ( Piperidis , 2012 ), CLARIN _CITE_ , LRE - Map ( Calzolari et al ., 2012 ), OLAC ( Simons and Bird , 2003 ) and Datahub . io . Taking a closer look at the rights metadata present in these catalogs , we see the following tendencies : censes are not available over the internet ( e . g . resources from older times , when licenses were not standardised and providers asked legal experts to draft specific contracts for each resource , which were made available only to interested parties upon request ); for the LRE Map , this practice has been dictated by the fact that the metadata are submitted by authors of papers in conferences ( e . g .__label__Method|Tool|Introduce
This mismatch will be interpreted in view of our translation - oriented research . In the following subsection we will see how these two phenomena can be retrieved automatically . Since the multi - dimensional annotation and alignment is realised in XML , the queries are posed using XQuery _CITE_ . This query language is particularly suited to retrieve information from different sources like for instance individual annotation and alignment files . The use for multilayer annotation is shown in ( Teich et al .__label__Method|Tool|Use
Most existing HLT pipelines assume the input is pure text or , at most , HTML and either ignore ( logical ) document structure or remove it . We argue that identifying the structure of documents is essential in digital library and other types of applications , and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved . Many off - the - shelf Human Language Technology ( HLT ) pipelines are now freely available ( examples include LingPipe , OpenNLP , GATE ( Cunningham et al ., 2002 ), TextPro _CITE_ ( Pianta et al ., 2008 )), and although they support a variety of document formats as input , actual processing ( mostly ) takes no advantage of structural information , i . e . structural information is not used , or stripped off during preprocessing . Such processing can be considered safe , e . g .__label__Method|Tool|Introduce
PCFG parsing features were generated on the output of the Berkeley Parser ( Petrov and Klein , 2007 ), trained over an English and a Spanish treebank ( Mariona Taul ´ e and Recasens , 2008 ). Ngram features have been generated with the SRILM toolkit ( Stolcke , 2002 ). The Acrolinx IQ was used to parse the source side , whereas the Language Tool _CITE_ was applied on both sides . The feature selection and learning algorithms were implemented with the Orange ( Demˇsar et al ., 2004 ) and Weka ( Hall et al ., 2009 ) toolkits . The methods explained in the previous section provide a wide range of experiment parameters .__label__Method|Tool|Use
Its support is 66 . 7 % and its confidence is 100 %. As another example , LSP p2 Generating Sequence Database . We generate the database by applying Part - Of - Speech ( POS ) tagger to tag each training sentence while keeping function words and time words _CITE_ . After the processing , each sentence together with its label becomes a database tuple . The function words and POS tags play important roles in both grammars and sentence structures .__label__Method|Tool|Use
We also point to some additional problems that arise when one moves on to OWL DL and OWL FULL . The discussion is based on experiments we conducted with more than a dozen of existing OWL ontologies . _CITE_ One of the main difficulties is that OWL ( all versions ) allows multiple inheritance , while M - PIRO does not ( section 2 ). Importing an ontology with multiple inheritance currently causes the process to fail . The need for multiple inheritance has also been noted by authors , who often encounter cases where , for example , a person has to be categorized as both painter and potter .__label__Material|Data|Use
In particular , we will show an initial version of a GUI - based top - level for the development environment , a tool that supports graphical debugging of unification grammars by cutting and pasting of derivation trees , and various functionalities that support systematic development of speech translation and spoken dialogue applications built using Regulus . The Regulus platform is a comprehensive toolkit for developing grammar - based speech - enabled systems that can be run on the commercially available Nuance recognition environment . The platform has been developed by an Open Source consortium , the main partners of which have been NASA Ames Research Center and Geneva University , and is freely available for download from the SourceForge website _CITE_ . Regulus has been used to build several large systems , including Geneva University ’ s MedSLT medical speech translator ( Bouillon et al ., 2005 ) and NASA ’ s Clarissa procedure browser ( Rayner et al ., 2005b ) . Regulus is described at length in ( Rayner et al ., 2006 ), the first half of which consists of an extended tutorial introduction .__label__Supplement|Website|Produce
We worked with their unigram model ( Purandare and Pedersen , 2004 ) to cluster the web pages using the text content between the title tags . Our web people clustering approach is presented in Figure 1 and consists of the following steps : away , the javascript code is eliminated , the non closed WePS tags are repaired , the missing begin / end body tags are included and then the content between the title , the body and the anchor tags is extracted . • name matching : the location , person and organization names in the body texts are identified with the GATE _CITE_ system ( Cunningham , 2005 ). Each named entity of a document is matched with its corresponding named entity category from the rest of the web pages . This information is used to calculate the social semantic similarity of the person , the location and the organization names .__label__Method|Tool|Use
However , the PMI is known to be sensitive to low count words and bigrams , overemphasising them over high frequency words . To account for this , we express the mutual information of a word bigram by means of Lexicographer ’ s Mutual Information ( LMI ). _CITE_ The LMI , introduced by Kilgarriff et al . ( 2004 ), offers an advantage to Pointwise Mutual Information ( PMI ), as the scores are multiplied by the bigram frequency , boosting more frequent combinations of word ( w ) and context ( c ). We compute the LMI over a corpus of positive , respectively negative tweets , in order to obtain positive ( LMIpos ) and negative ( LMIneg ) bigram scores .__label__Method|Algorithm|Use
Figure 2 gives an overview of the general architecture . The eHumanities Desktop is implemented as a client / server system which can be used via any JavaScript / Java capable Web Browser . The GUI is based on the ExtJS Framework _CITE_ and provides a look and feel similar to Windows Vista . The server side is based on Java Servlet technology using the Tomcat Servlet Container . The core of the system is the Command Dispatcher which manages the communication with the client and the execution of tasks like downloading a document for example .__label__Method|Code|Extent
According to Reyes et . al ( 2013 ), these hashtags were selected for three main reasons : ( i ) to avoid manual selection of tweets , ( ii ) to allow irony analysis beyond literary uses , and because ( iii ) irony hashtag may “ reflect a tacit belief about what constitutes irony .” Another corpora is employed in our approach to measure the frequency of word usage . We adopted the Second Release of the American National Corpus Frequency Data _CITE_ ( Ide and Suderman , 2004 ), which provides the number of occurrences of a word in the written and spoken ANC . From now on , we will mean with “ frequency of a term ” the absolute frequency the term has in the ANC . In order to process the tweets we use the freely available vinhkhuc Twitter Tokenizer which allows us to recognise words in each tweet .__label__Material|Data|Use
They argue that information extraction techniques can be used to mine large text datasets for relevant information , such as relations between specific types of entities . Inspired in the previews works the system we propose makes use of machine learning methods too , using some of the common features described above , such as the n - grams and keywords and co - occurrences , but we also add some semantic information to enrich those features . As it has been mentioned before , the system was developed to detect and classify drugs in biomedical texts , so the process is performed in two main phases : Both phases are determined by the following stages , described in Figure 1 : Given a biomedical sentence , the system obtains the lemmas and POS - tag of every token of the sentence , by means of Freeling tool _CITE_ . After that , it is able to generate candidates according to certain parameters ( see section 3 . 3 ). Then , all the generated candidates are processed to extract the features needed for the learning methods , in order to determine which candidates are drugs .__label__Method|Tool|Use
Due to MaxEnt ’ s capability to combine multiple and dependent knowledge sources , we employed MaxEnt as our machine learning model . Features we used to train the model include meta information features and collocation features . Meta Information Features include three features : We use the OpenNLP MaxEnt _CITE_ Java library as the MaxEnt trainer and classifier . For each hedge cue , the training is iterated 100 times , with no cut off threshold for events . We first ran experiments to evaluate the performance of the entire system .__label__Method|Code|Use
The results show an overall significant improvement over the other methods , with the added advantage of being computationally efficient . Next we evaluated our method on a word translation task , introduced in ( Mikolov et al ., 2013b ) and used in ( Gouws et al ., 2015 ). The words were extracted from the publicly available WMT11 _CITE_ corpus . The experiments were done for two sets of translation : English to Spanish and Spanish to English . ( Mikolov et al ., 2013b ) extracted the top 6K most frequent words and translated them with Google Translate .__label__Material|Data|Use
We use the German corpus ( which was developed first ) as our example throughout . The procedure was carried on a server running RH Fedora Core 3 with 4 GB RAM , Dual Xeon 4 . 3 GHz CPUs and about 2 . 5 TB hard disk space . We are making the tools we develop as part of the project freely available , _CITE_ in the hope of stimulating public sharing of resources and know - how . We would like a “ balanced ” resource , containing a range of types of text corresponding , to some degree , to the mix of texts we find in designed linguistic corpora ( Atkins et al ., 1992 ), though also including text types found on the Web which were not anticipated in linguists ’ corpus design discussions . We do not want a “ blind ” sample dominated by product listings , catalogues and computer scientists ’ bulletin boards .__label__Method|Tool|Produce
Method : To evaluate the performance of our system , we measure how well the relationships discovered compare with manually selected PPI sentences . To do so , we follow the same procedure and data sets used to evaluate semi - supervised classification of PPI sentences ( Erkan et al ., 2007 ). The two data sets are AIMED and CB , which have been marked for protein entities and interaction phrases _CITE_ . For each sentence in which n proteins appear , we build ( 2 ) phrases . Each phrase consists of the words between each entity combination , and is labeled as positive if it describes a PPI , or negative otherwise .__label__Material|Data|Use
In order to perform the task , participants are required to resolve entity coreference , as timelines should contain events involving all coreferring textual mentions of the target entities ( including pronominal mentions ). For example , in Figure 1 , the event fighting involving the target entity Steve Jobs mentioned as he is included in the timeline together with other events also referring to Steve Jobs . The dataset released for this task is composed of 120 Wikinews _CITE_ articles and 44 target entities . 30 documents and 6 target entities ( each associated to a timeline ) are provided as trial data , while the evaluation dataset consist of 90 documents and 38 target entities ( each associated to a timeline ). We manually selected a set of target entities that appeared in at least two different documents and were involved in more than two events .__label__Material|Data|Introduce
The connection to the sense modalities of the words might not be mutually exclusive , that is to say a word can be associated with more than one sense . For instance , the adjective sweet could be associated with both taste and smell . The description of one kind of sense impression by using words that normally describe another is commonly referred to as linguistic synaesthesia _CITE_ . As an example , we can consider the slogans “ The taste of a paradise ” where the sense of sight is combined with the sense of taste or “ Hear the big picture ” where sight and hearing are merged . Synaesthesia strengthens creative thinking and it is commonly exploited as an imagination boosting tool in advertisement slogans ( Pricken , 2008 ).__label__Supplement|Document|Introduce
In particular , given an optimal pairwise bilingual ranking , we show that simple heuristics can effectively approximate the optimal monolingual ranking . Using these heuristics and our learned pairwise scoring function , we can derive a ranking for new , unseen bilingual queries . We develop and test our bilingual ranker on English and Chinese with two large , publicly available query logs from the AOL search engine _CITE_ ( English query log ) ( Pass et al ., 2006 ) and the Sougou search engine ( Chinese query log ) ( Liu et al ., 2007 ). For both languages , we achieve significant improvements over monolingual Ranking SVM ( RSVM ) baselines ( Herbrich et al ., 2000 ; Joachims , 2002 ), which exploit a variety of monolingual features . We designate a query as bilingual if the concept has been searched by users of both two languages .__label__Supplement|Website|Use
While Romanian normally has diacritical markings , this particular newspaper does not include those in their online edition , so the alphabet used was the same as English . The Bulgarian data is from the Sega 2002 news corpus , which was originally prepared for the CLEF competition . _CITE_ This is a corpus of news articles from the Newspaper Sega , which is based in Sofia , Bulgaria . The Bulgarian text was transliterated ( phonetically ) from Cyrillic to the Roman alphabet . Thus , the alphabet used was the same as English , although the phonetic transliteration leads to fewer cognates and borrowed English words that are spelled exactly the same as in English text .__label__Material|Data|Introduce
We found that MAD , a recently proposed graph - based SSL algorithm , is consistently the most effective across the various experimental conditions . We also showed that class - instance acquisition performance can be significantly improved by incorporating additional semantic constraints in the class - instance acquisition process , which for the experiments in this paper were derived from instance - attribute pairs available in an independently developed knowledge base . All the data used in these experiments was drawn from publicly available datasets and we plan to release our code _CITE_ to foster reproducible research in this area . Topics for future work include the incorporation of other kinds of semantic constraint for improved class - instance acquisition , further investigation into per - node sparsity constraints in graph - based SSL , and moving beyond bipartite graph constructions . We thank William Cohen for valuable discussions , and Jennifer Gillenwater , Alex Kulesza , and Gregory Malecha for detailed comments on a draft of this paper .__label__Method|Code|Produce
In our experiments , we find that the dense NN model and our combined model achieve better performances by using dropout , but the other models do not benefit from dropout . The final results across web domains are shown in Table 2 . Our logistic regression linear parser and re - implementation of Chen and Manning ( 2014 ) give comparable accuracies to the perceptron ZPar _CITE_ and Stanford NN Parser , respectively . It can be seen from the table that both Turian and Guo outperform L by incorporating embed ding features . Guo gives overall higher improvements , consistent with the observation of Guo et al .__label__Method|Tool|Compare
To verify that the annotation rules were reasonable and led to a problem that could potentially be solved by a computer , we had each of the annotators mark up a small shared set of a few hundred words from each of eight documents , in order to measure the inter - annotator agreement . The average actual agreement was 0 . 988 , with 0 . 5 agreement expected by chance for a kappa of 0 . 975 . Following Scannell ( 2007 ), we collected small monolingual samples of 643 languages from four sources : the Universal Declaration of Human Rights , non - English Wikipedias _CITE_ , the Jehovah ’ s Witnesses website , and the Rosetta project ( Landsbergen , 1989 ). Only 30 of these languages ended up being used in experiments . Table 3 shows the sizes of the monolingual samples of the languages used in this paper .__label__Material|Data|Use
The final weight is of the form : This term is proportional to perplexities , as the exponent of entropy is perplexity by definition . One could also use filtering for TM adaptation , but , as shown in ( Mansour and Ney , 2012 ), filtering for TM could only reduce the size and weighting performs better than filtering . The experiments are done on the recent Germanto - English WMT 2013 translation task _CITE_ . For test data statistics : the number of sentence pairs ( Sent ), German ( De ) and English ( En ) words are given . German - English WMT 2013 , the common - crawl bilingual corpus was introduced , enabling more impact for TM adaptation on the SMT system quality .__label__Material|Data|Use
While some progress has been made toward enabling syntactic interoperability via the development of standard representation formats ( e . g ., ISO LAF / GrAF ( Ide and Suderman , 2014 ; ISO - 24612 , 2012 ), NLP Interchange Format ( NIF ) ( Hellmann et al ., 2013 ), UIMA Common Analysis System ( CAS )) which , if not identical , can be trivially mapped to one another , semantic interoperability among NLP tools remains problematic ( Ide and Pustejovsky , 2010 ). A few efforts to create repositories , type systems , and ontologies of linguistic terms ( e . g ., ISOCat , OLiA , various repositories for UIMA type systems , GOLD , NIF Core Ontology ) have been undertaken to enable ( or provide ) a mapping among linguistic terms , but none has yet proven to include all requisite terms and relations or be easy to use and reference . General repositories such as Dublin Core _CITE_ , schema . org , and the Friend of a Friend This work is licensed under a Creative Commons Attribution 4 . 0 International License . Page numbers and proceedings footer are added by the organizers . License details : http :// creativecommons . org / licenses / by / 4 . 0 / See , for example , proceedings of the recent LREC workshop on “ Language Technology Service Platforms : Synergies , Standards , Sharing ” ( http :// www . ilc . cnr . it / ltsp2014 /).__label__Material|Data|Introduce
The examples are out of domain , so the language model doesn ’ t help us at all . So let ’ s try an in - domain example of newswire text . The following is almost the first text I found by searching Arabic Web pages for the Arabic for “ Google Machine Translation ,” simply because I had already read the English reference document , _CITE_ and I was pretty sure it would be out there somewhere . It is a human - authored Arabic translation of a recent Reuters story about the launching of Google Language Tools , taken from Al Jazeera : Here is the SMT translation , delivered in about the time it would take a native speaker to read the original : The German Franz Ouch which leads efforts Google translation computer feeds hundreds of millions of words of parallel texts such as Arabic , English , using documents of the United Nations and the European Union key sources . And how a new translation Ouch said that although the quality would not be complete That was a good in the previous translation mechanism , and that the correct translation mostly might be good enough for some tasks .__label__Supplement|Document|Use
2 , where the solid line represents training process and the dotted line represents testing process . The Korean analyzer at the center of the figure takes Korean texts as an input and generates several raw features as an output , such as results of morphological analysis , Part - Of - Speech ( POS ) tags , Named - Entity ( NE ) tags , and results of dependency parsing ( Lim et al ., 2006 ). The number of possible POS tags is 45 , which follows the definition of Sejong Treebank _CITE_ . The number of possible NE tags is 178 , where each of them belongs to one of 15 super NE tags . The generated raw features are used to define a set of features for machine - learning models and a set of hand - crafted rules .__label__Supplement|Document|Use
For word pairs , we extract the ordered pairs of words that occur in the same sentence , and similarly for POS pairs . To derive VIN features , we take each word bigram w1 , w2 and further represent it as two patterns p1 , w2 and w1 , p2 each consisting of a word and a POS tag . In all of our experiments , we train logistic regression classifiers using the liblinear toolkit _CITE_ . This choice was partly motivated by our earlier summarization research , where logistic regression classifiers were compared alongside support vector machines . The two types of classifier yielded very similar results , with logistic regression classifiers being much faster to train .__label__Method|Tool|Use
Rule 3 . 1 then infers that the writer is positive toward the agent of E4 , Obama . In summary , we infer that the writer is positive toward E1 , health care reform , E2 , patients , E4 , and Obama , and negative toward E3 and private insurance companies . We use the data described in ( Deng et al ., 2013 ), _CITE_ which consists of 134 documents about a controversial topic , “ the Affordable Care Act .” The documents are editorials and blogs , and are full of opinions . In the data , gfbf triples are annotated specifying the spans of the gfbf event , its agent , and its object , as well as the polarity of the gfbf event ( GOODFOR or BADFOR ), and the writer ’ s attitude toward the agent and object ( positive , negative , or neutral ). Influencers are also annotated .__label__Material|Data|Use
However , when this happens , the propositions grouped together in the RS must remain consecutive in the TS ; solutions in which p3 comes in between p1 and p2 are prohibited . Our procedure for generating candidate solutions is based on a technique for formulating text structuring as a constraint satisfaction problem ( CSP ) ( van Hentenryck , 1989 ), using the ECLIPSE logic programming environment . _CITE_ In general , a CSP is characterized by the following elements : A solution assigns to each variable Vi a value from its domain Di while respecting all constraints . For instance each node of the rhetorical structure is annotated with a TEXT – LEVEL variable with the domain 0 ... Lmax and an ORDER variable with the domain 1 ... N , where N is the number of sisters . Depending on the constraints , there may be multiple solutions , or there may be no solution at all .__label__Method|Tool|Use
We then investigated whether lexical normalization can decrease the number of out - of - vocabulary words . For the 793 ill - spelled words , we counted how many of their surface forms and normal forms were not registered in the JUMAN dictionary . _CITE_ The result suggests that 411 ( 51 . 8 %) and 74 ( 9 . 3 %) are not registered in the dictionary . This indicates the effectiveness of lexical normalization for decreasing out - of - vocabulary words . This section gives an overview of our joint model with lexical normalization for accurate word segmentation and POS tagging .__label__Material|Data|Use
PARSEVAL results on the development and test set are presented in Tables 5 and 6 . We see that the reranked models outperform the generative baseline model in terms of F1 , and that the reranked model that uses extra - sentential context outperforms the version that does not use extra - sentential context in the development set , but not in the test set . Using Bikel ’ s randomized parsing evaluation comparator _CITE_ , we find that both reranking models outperform the baseline generative model to statistical significance for recall and precision . The context - ignorant reranker outperforms the context - aware reranker on recall ( p & lt ; 0 . 01 ), but not on precision ( p = 0 . 42 ). However , the context - aware model has the highest exact match scores in both the development and the test set .__label__Method|Tool|Use
As it may be observed , the results obtained for Twitter and SMS sentiment classification are good considering that our proposal is unsupervised . The explosion of Web 2 . 0 has marked a new age for the human society . The huge use of Social Media such as Facebook _CITE_ , MySpace , LinkedIn and Twitter , offers a place for people to share information in real time . Twitter is one of the most popular social network websites and has been growing at a very fast pace . The number of active users exceeds 500 million and the number of tweets posted by day exceeds 500 million ( as of May 2012 ) .__label__Supplement|Website|Introduce
First , we prepared source text in the four languages from Wikipedia dumps following ( Baroni et al ., 2014 ). We extracted plain text from the XML dumps by using wp2txt . Since words are concatenated in Japanese and Chinese , we used MeCab and Stanford Word Segmenter _CITE_ to tokenize the text . Since inflection occurs in English , Spanish , and Japanese , we used Stanford POS tagger , Pattern , and MeCab to lemmatize the text . Next , we induced count - based word vectors from the obtained text .__label__Method|Tool|Use
The hypotheses for our experiments is that the selection of high - quality dependency trees is a crucial precondition for the successful use of selftraining in dependency parsing . Therefore , we explore a confidence - based method to select highquality dependency trees from newly parsed sentences . Our self - training approach consists of a single iteration with the following steps : We use the freely available Mate tools _CITE_ to implement the self - training approach . This tool set contains a part - of - speech ( PoS ) tagger , morphologic tagger , lemmatizer , graph - based parser and an arc - standard transition - based parser . The arcstandard transition - based parser has the option to use a graph - based model to rescore the beam which seems to be a sort - of reranking ( Bohnet and Kuhn , 2012 ).__label__Method|Tool|Use
To obtain the Viterbi alignments , which are required for phrase extraction ( Koehn et al ., 2003 ), we select for each aj the most frequent value in the M collected samples . For TurkishHEnglish experiments , we used the 20K - sentence travel domain BTEC dataset ( Kikui et al ., 2006 ) from the yearly IWSLT evaluations for training , the CSTAR 2003 test set for development , and the IWSLT 2004 test set for testing . For CzechHEnglish , we used the 95K - sentence news commentary parallel corpus from the WMT shared task _CITE_ for training , news2008 set for development , news2009 set for testing , and the 438M - word English and 81 . 7M - word Czech monolingual news corpora for additional language model ( LM ) training . For ArabicHEnglish , we used the 65K - sentence LDC2004T18 ( news from 2001 - 2004 ) for training , the AFP portion of LDC2004T17 ( news from 1998 , single reference ) for development and testing ( about 875 sentences each ), and the 298M - word English and 215M - word Arabic AFP and Xinhua subsets of the respective Gigaword corpora ( LDC2007T07 and LDC2007T40 ) for additional LM training . All language models are 4 - gram in the travel domain experiments and 5 - gram in the news domain experiments .__label__Material|Data|Use
These rules rely on the use of affixes that classify drugs according to their chemical structure , indication or mechanism of action . For example , analgesics substances can receive affixes such as - adol -, - butazone , -� enine , - eridine and fentanil . In the present work , we focus , particulary , on the implementation of a set of 531 affixes approved by the USAN Council and published in 2007 _CITE_ . The affixes allow a specific classification of drugs on pharmacological families , which ULMS Semantic NetWork is unable to provide . The system consists of four main modules : a basic text processing module , WordNet look - up module , UMLS look - up module and the USAN rules module , as shown in Figure 1 .__label__Method|Code|Produce
In this setting , we assume that we do not have access to any French tagger that we can exploit to improve projection . Hence , all we can do is to employ the three steps involved in the projection approach as described at the beginning of this section to create coreference - annotated data for French . Specifically , we translate a French text to an English text using GoogleTranslate , and create coreference chains for the translated English text using Reconcile _CITE_ ( Stoyanov et al ., 2010 ). To project mentions from English to French , we first align the English and French words in each pair of parallel sentences , and then project the English mentions onto the French text using the alignment . However , since the alignment is noisy , the French words to which the words in the English mention are aligned may not form a contiguous text span .__label__Method|Tool|Use
For example , target word selection is possible based on co - occurrence relationship extracted from a monolingual corpus ( Suzuki et al ., 2005 ). Furthermore , we have developed a word sense disambiguation based on a monolingual corpus in the target domain , and it has been applied to Japanese - Korean and KoreanJapanese translation systems ( Kumano 2013 , Tanaka et al ., 2014 ). On the other hand , open Asian parallel corpora including ASPEC , NTCIR PatentMT and JPO Patent Corpus _CITE_ are available for the research of machine translation systems . By using the parallel corpora , we have confirmed advantages which apply statistical post editing ( SPE ) to RBMT in domain adaptation ( Suzuki , 2011 ). In the last workshop ( Nakazawa et al ., 2014 ), we participated in Japanese - English and Japanese - Chinese tasks with SPE approach and obtained higher evaluation results than RBMT .__label__Material|Data|Use
Working with the official key file and scoring software , the intersection combination with MFS backoff gives an F - measure of 0 . 78713 corresponding to the 6th best result . The same combination method but without MFS backoff achieves a precision of 0 . 80559 but at the cost of a very low F - measure ( 0 . 41492 ). For this task , LexPar and SynWSD were further trained on a 12 million POS tagged and lemmatized balanced corpus _CITE_ . The run that was submitted was the intersection combination with the MFS backoff strategy which obtained an F - measure of 0 . 527 . This score puts our algorithm on the 8th position out of 14 competing systems .__label__Material|Data|Use
BLEU should be calculated on a large test set with multiple reference texts . We used BLEU - 4 ( that is , BLEU calculated using n - grams of size up to n = 4 ) because this version of BLEU is the main metric used in recent NIST Machine Translation evaluations ( and indeed seems to have become a standard in the MT community ). We also used the NIST _CITE_ MT evaluation score ( Doddington 2002 ); this is an adaptation of BLEU which gives more weight to less frequent n - grams which are assumed to be more informative . There are several different ROUGE metrics . The simplest is ROUGE - N , which computes the highest proportion in any reference text of n - grams of length N that are matched by the generated text .__label__Method|Algorithm|Use
However these are difficult to develop and domain sensitive . To surmount these obstacles , application of machine learning approaches to NER became a research subject . Various state - of - the - art machine learning algorithms such as Maximum Entropy ( Borthwick , 1999 ), AdaBoost ( Carreras et al ., 2002 ), Hidden Markov Models ( Bikel et al ., ), Memory - based Based learning ( Tjong Kim Sang , 2002b ), have been used _CITE_ . ( Klein et al ., 2003 ), ( Mayfield et al ., 2003 ), ( Wu et al ., 2003 ), ( Kozareva et al ., 2005c ) among others , combined several classifiers to obtain better named entity coverage rate . Nevertheless all these machine learning algorithms rely on previously hand - labeled training data .__label__Method|Algorithm|Introduce
Firstly , a corpus of articles was created ( Section 3 . 1 ), after which the documents were automatically annotated with named entities ( Section 3 . 2 ). We then extracted a number features relevant to the named entities present in the corpus ( Section 3 . 3 ). Our corpus was created by first searching the NLM Catalog _CITE_ for journals whose Broad Subject Term attributes contain only cell biology or pharmacology , and then narrowing down the results to those which are in English and available via PubMed Central . Also , since we are concentrating on full - text documents , we retained only those journals that are available within the PubMed Open Access subset . According to this procedure , we obtained a final list of two journals for cell biology and six for pharmacology .__label__Supplement|Website|Use
In addition to evaluating factuality detection in isolation , we also evaluate its impact on a system for event detection . The two components for factuality detection and event detection form part of a system for identifying negative factual events , or counterfacts , with top - ranked results in the * SEM 2012 shared task . The First Joint Conference on Lexical and Computational Semantics (* SEM 2012 ) is hosting a shared task _CITE_ ( Morante and Blanco , 2012 ) on identifying various elements of negation , and one of the subtasks is to identify negated events . However , only events occurring in factual statements should be labeled . This paper describes pilot experiments on how to train a factuality classifier by taking advantage of implicit information on factuality in annotations of negation .__label__Supplement|Website|Introduce
We present a novel method to adapt a supervised coreference resolution system trained on newswire to short narrative stories without retraining the system . The idea is to perform inference via an Integer Linear Programming ( ILP ) formulation with the features of narratives adopted as soft constraints . When testing on the UMIREC and N2 _CITE_ corpora with the - stateof - the - art Berkeley coreference resolution system trained on OntoNotes , our inference substantially outperforms the original inference on the CoNLL 2011 metric . Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes ( or ‘ chains ’) corresponding to those referents ( Stede , 2011 ). To solve the problem , contextual and grammatical clues , as well as semantic information and world knowledge are necessary for either learning - based ( Bengtson and Roth , 2008 ; Stoyanov et al ., 2010 ; Haghighi and Klein , 2010 ) or rule - based ( Haghighi and Klein , 2009 ; Lee et al ., 2011 ) coreference systems .__label__Material|Data|Use
Identifying functional zones in email messages is a challenging task , due in large part to the diversity in syntax used by different email software , and the dynamic manner in which people employ different styles in authoring email messages . Zebra , our system for segmenting and classifying email message text into functional zones , achieves per formance that exceeds comparable systems , and that is at a level to be practically useful to email researchers and system builders . In addition to releasing our annotated email dataset , the Zebra system will also be available for others to use _CITE_ . Because we employ a non - sequential learning algorithm , we encode sequence information into the feature set . In future work , we plan to determine the effectiveness of using a sequential learning algorithm like Conditional Random Fields ( CRF ).__label__Method|Tool|Use
Since the text categorization task requires that multiple categories are assigned if appropriate , we constructed a binary categorizer , pJ ( y ∈ {+ 1 , − 1 }| d ), for each category c . If the probability pJ (+ 1 | d ) is greater than 0 . 5 , the category is assigned . To construct a conditional maximum entropy model , we used the feature function of the form ( 22 ), where hi ( d ) returns the TFIDF value of the i - th word of the document vector . We implemented the estimation algorithms as an extension of an ME estimation tool , Amis , _CITE_ using the Toolkit for Advanced Optimization ( TAO ) ( Benson et al ., 2002 ), which provides the LMVM and the BLMVM optimization modules . For the inequality ME estimation , we added a hook that checks the KKT conditions after the normal convergence test . We compared the following models : For the inequality ME models , we compared the two methods to determine the widths , single and bayes , as described in Section 5 .__label__Method|Tool|Extent
Generally their paper , which is submitted to a conference and may be rejected not because of their research works but because of the English writing , which makes the paper harder for the reviewer to understand intention of author . This kind of problem will be faced in any field where someone has to provide material in a language other than his / her first language . The mentoring _CITE_ service of Association for Computational Linguistics ( ACL ) is one part of a response . This service can address a wider range of problems than those related purely to writing . The aim of this service is that a research paper should be judged only on its research content .__label__Supplement|Website|Introduce
In addition , BLUE allows propositions to themselves be arguments to other propositions as a nested structure , e . g ., for modals : ;;; " The man wanted to leave the house " As described earlier , BLUE currently uses two alternative conceptual vocabularies , namely the concepts in WordNet ( with minor extensions ) or the Component Library . BLUE ’ s relational vocabulary is approximately 100 semantic relations drawn from the Component Library . _CITE_ We illustrate our system using an example from Project Halo ( Clark et al ., 2007 ), where the system is used to interpret multi - sentence science questions posed to a knowledge - based system . While BLUE produces a slightly better output for this text using the Component Library ontology , we illustrate it using WordNet ’ s ontology for consistency with our output for the other shared task texts ( we use WordNet for these as WordNet has broader coverage ). We also discuss our system further in Section 4 on additional sentences .__label__Material|Data|Use
Since this data set has predefined training and testing partitions , our results are comparable to those obtained by other researchers . There are 50 documents per author for training and 50 documents per author for testing . We performed experiments with LOWBOW _CITE_ representations at word and character - level . For the experiments with words , we took the top 2 , 500 most common words used across the training documents and obtained LOWBOW representations . We used this setting in agreement with previous work on AA ( Houvardas and Stamatatos , 2006 ).__label__Method|Tool|Use
In this paper , we report the experiments carried out by the NLP CT Laboratory at University of Macau for WMT2014 medical sentence translation task on six language pairs : Czech - English ( cs - en ), French - English ( fr - en ), German - English ( de - en ) and the reverse direction pairs ( i . e ., en - cs , en - fr and en - de ). As data in specific domain are usually relatively scarce , the use of web resources to complement the training resources provides an effective way to enhance the SMT systems ( Resnik and smith , 2003 ; Esplà - Gomis and Forcada , 2010 ; Pecina et al ., 2011 ; Pecina et al ., 2012 ; Pecina et al ., 2014 ). In our experiments , we not only use all available training data provided by the WMT2014 standard translation task ( generaldomain data ) and medical translation task _CITE_ ( indomain data ), but also acquire addition indomain bilingual translations ( i . e . dictionary ) and monolingual data from online sources . First of all , we collect the medical terminologies from the web .__label__Material|Data|Use
Extract entities from collection of documents We started with one corpus of documents ( the test documents in the online approach or a pre – compiled set in the offline version ): the seed documents ( SD ). Then we applied the statistical implementation of DBPedia Spotlight ( Daiber et al ., 2013 ) in order to obtain entities and their corresponding links to DBPedia . With this we compile the EA corpus , which contains all the Wikipedia texts associated to the DBpedia links extracted _CITE_ . We experimented with some filtering techniques on the list of DBPedia links in order to keep just domain specific ones , such as considering only those DBPedia links tagged with an ontological concept which is a leaf of the ontology tree . Nevertheless , we found a better performance when using all the DBpedia links without any filtering .__label__Material|Data|Produce
Figure 1 is a sample lexical entry from the main output of CPA , the Pattern Dictionary of English Verbs ( PDEV ). This tells us that , for the verb abolish , three patterns were found . For each pattern it tells us the percentage of the data that it accounted for , its grammatical structure and the semantic type ( drawn from a shallow ontology of 225 semantic types _CITE_ ) of each of the arguments in this structure . For instance , pattern 1 means : i ) that the subject is preferably a word referring to [[ Human ]] or [[ Institution ]] ( semantic alternation ), and ii ) that the object is preferably [[ Action ]], [[ Rule ]] or [[ Privilege ]]. It also tells us the implicature ( which is similar to a “ definition ” in a traditional dictionary ) of a sentence exemplifying the pattern : that is , if we have a sentence of the pattern [[ Institution | Human ]] abolish [[ Action = Punishment | Rule | Privilege ]], then we know that [[ Institution | Human ]] formally declares that [[ Action = Punishment | Rule | Privilege ]] is no longer legal or operative .__label__Material|Data|Use
Otherwise , we may alter the coherence of the text and decrease its readability . This leaves us with 19 simplification rules . _CITE_ To apply them , the candidate structures for simplification first need to be detected using regular expressions , via Tregex ( Levy and Andrew , 2006 ) that allows the retrieval of elements and relationships in a parse tree . In a second step , syntactic trees in which a structure requires simplification are modified according a set of operations implemented through Tsurgeon . The operations to perform depend on the type of rules : 1 .__label__Supplement|Document|Introduce
Consequently , CUI - less facts are ignored in our evaluation framework . In this work , we will focus only on the fact types of Disorders and Procedures , and use SNOMED as our medical taxonomy . We also use Linkbase _CITE_ as our knowledge - base for descriptions of the fact codes . Some of the unique characteristics of medical fact coding compared to the traditional entity recognition are as follows : In the official data of the Semeval task , it is reported that at least a quarter of the annotated facts are CUI - less ( Pradhan et al ., 2014 ). Hence ignoring these facts essentially renders a comparison of our evaluation numbers with the official Semeval numbers meaningless .__label__Material|Data|Use
Metadata - based features As a metadata - based indicator , the Q & A identical user identifies if the user who posted the question is the same user who wrote the answer . This indicator is useful for detecting irrelevant dialogue answers . Rank - based features We employ SVM Rank _CITE_ to compute ranking scores of answers with respect to their corresponding questions . After generating all other features , SVM Rank is run to produce ranking scores for each possible answer . For training SVM Rank , we convert answer labels to ranks according to the following heuristic : good answers are ranked first , potential ones second , and bad ones third .__label__Method|Tool|Use
There exist a lot of platforms dedicated to NLP , but none are fully satisfactory for various reasons . Intex ( Silberztein , 1993 ), FSM ( Mohri et al ., 1998 ) and Xelda are closed source . Unitex ( Paumier , 2003 ), inspired by Intex has its source code under LGPL license _CITE_ but it does not support standard formats for Language Resources ( LR ). Systems like NLTK ( Loper and Bird , 2002 ) and Gate ( Cunningham , 2002 ) do not offer functionality for Lexical Resource Management . All the operations described below are implemented in C ++ independent modules which interact with each others through XML streams .__label__Supplement|License|Other
The third stage puts together all possible tags for a sequence of tokens and chooses the best one according to the probability which was computed from the output of the classifiers ( before thresholding ) via a Sigmoid function . The paper reports evaluation results on three corpora covering different IE tasks – named entity recognition ( CoNLL - 2003 ) and template filling or scenario templates in different domains ( Jobs and CFP ). The CoNLL - 2003 _CITE_ provides the most recent evaluation results of many learning algorithms on named entity recognition . The Jobs corpus has also been used recently by several learning systems . The CFP corpus was created as part of the recent Pascal Challenge for evaluation of machine learning methods for IE .__label__Material|Data|Introduce
In this section we discuss the data and features used in our experiments . The data for our experiments comes from three sources : ( i ) grapheme - phoneme mappings from an online encyclopedia , ( ii ) translations of the Universal Declaration of Human Rights ( UDHR ) , and ( iii ) entries from the World Atlas of Language Structures ( WALS ) ( Haspelmath and Bibiko , 2005 ). To start , we downloaded and transcribed image files containing grapheme - phoneme mappings for several hundred languages from an online en http :// www . ohchr . org / en / udhr / pages / introduction . aspx cyclopedia of writing systems _CITE_ . We then crossreferenced the languages with the World Atlas of Language Structures ( WALS ) database ( Haspelmath and Bibiko , 2005 ) as well as the translations available for the Universal Declaration of Human Rights ( UDHR ). Our final set of 107 languages includes those which appeared consistently in all three sources and that employ a Latin alphabet .__label__Method|Tool|Use
In contrast to using expert annotators , crowd - workers are readily and cheaply available even for ad - hoc tasks . In this paper , we used micro - task crowd - sourcing , i . e . a central platform like for example Amazon Mechanical Turk _CITE_ or CrowdFlower assigns small tasks ( called HITs , human - intelligence tasks ) to workers for monetary compensation . HITs usually consist of multiple work units taking only a few minutes to process , and therefore pay few cents . Crowd - sourcing has been shown to be effective for language processing related tasks , e . g .__label__Supplement|Website|Use
We assume that using better candidate - reference alignment results in better MT evaluation . Research in the area of monolingual alignment demonstrates that exploiting syntactic context to discriminate between candidate pairs for alignment significantly improves the results ( MacCartney et al ., 2008 ; Thadani et al ., 2012 ; Yao et al , 2013 ; Sultan et al ., 2014 ). The alignment module of UPF - Cobalt builds on an existing system Monolingual Word Aligner ( MWA ) _CITE_ which takes context information into account and has been shown to significantly improve on state - of - the - art results ( Sultan et al ., 2014 ). MWA exploits lexical similarity and contextual evidence to make alignment decisions . Lexical similarity component identifies possible candidates for alignment .__label__Method|Tool|Extent
Consider the adjective “ frigid ”, which may be judged to be simpler than “ gelid ” if referring to temperature , but perhaps less simple than “ ice - cold ” when characterizing someone ’ s personality . These differences in word sense are taken into account by measuring the similarity between corpus documents and substitution contexts and use these values to provide a weighted average of the syntactic complexity measures . The unlabeled data set was generated by a threestep procedure involving synonyms extracted from Wordnet _CITE_ and sentences from the UKWAC corpus . mas in Wordnet . The synsets must have more than three synonyms .__label__Material|Data|Use
Unfortunately , their method requires lengthy guidelines and substantial annotator training effort , which are time consuming and costly . Thus , a simple , robust and replicable evaluation method is needed . Recently , crowdsourcing services such as Amazon Mechanical Turk ( AMT ) and CrowdFlower ( CF ) _CITE_ have been employed for semantic inference annotation ( Snow et al ., 2008 ; Wang and CallisonBurch , 2010 ; Mehdad et al ., 2010 ; Negri et al ., 2011 ). These works focused on generating and annotating RTE text - hypothesis pairs , but did not address annotation and evaluation of inference rules . In this paper , we propose a novel instance - based evaluation framework for inference rules that takes advantage of crowdsourcing .__label__Supplement|Website|Introduce
While ACE 2005 and ACE 2004 ( Ratinov et al ., 2011 ) fit our target scenario most ( both common and proper nouns are annotated ), MSNBC ( Cucerzan , 2007 ) and TAC 2011 ( Ji et al ., 2011 ) are only annotated for proper nouns . The training , development and testing data are all preprocessed in the same way . We perform POS tagging , syntactic parsing and named entity recognition using the Stanford CoreNLP pipeline _CITE_ . For identifying mentions we extract all noun phrases ( excluding discontinuous phrases and determiners ) and look them up in our lexicon . Our lexicon and also all other information we obtained from Wikipedia are extracted from the same English Wikipedia dump .__label__Method|Tool|Use
Let us define sp ( d ) and sq ( d ) the scores assigned by S to a document d for the query p and q , respectively . Then , the similarity score is calculated as : simIR ( p , q ) = 1 | Lp n Lq | if | Lp n Lq |=� 0 , 0 otherwise . For the participation in the English sub - task we indexed a collection composed by the AQUAINT26 and the English NTCIR - 8 document collections , using the Lucene _CITE_ 4 . 2 search engine with BM25 similarity . The Spanish index was created using the Spanish QA @ CLEF 2005 ( agencia EFE1994 - 95 , El Mundo 1994 - 95 ) and multiUN ( Eisele and Chen , 2010 ) collections . The K value was set to 70 after a study detailed in ( Buscaldi , 2013 ).__label__Method|Tool|Use
Fourth , in SemEval the words to be substituted come from various syntactic or semantic categories , while we only suggest appropriate emotion words to the learners . For writing assessment , existing works are known as automatic essay assessment ( AEA ) systems , which analyze user compositions in terms of wording , grammar and organization . PIGAI _CITE_ , targeted at generating suggested revisions , suggests unranked synonyms for words . However , unranked synonyms easily confuse Chinese learners ( Ma , 2013 ). E - rater ( Leading et al .__label__Method|Tool|Introduce
We omit to mention the argument / 1 , hasRole / 2 and role / 3 modules because they are present for all languages . A more detailed description of the formulae can be found in our MLN model files . They can be used both as a reference and as input to our Markov Logic Engine , _CITE_ and thus allow the reader to easily reproduce our results . Global formulae relate several hidden ground atoms . We use them for two purposes : to ensure consis tency between the decisions of all SRL stages and to capture some of our intuition about the task .__label__Method|Tool|Produce
With these two resources combined , there are four stages of word level matching in our system : exact match , stem match , WordNet match and unigram paraphrase match . The stemming module uses Porter ’ s stemmer implementation and the WordNet module uses the JAWS WordNet interface . Our metric only considers unigram paraphrases , which are extracted from the paraphrase database in TERP using the script in the METEOR _CITE_ metric . The metric described in ( Owczarzak et al ., 2007 ) does not explicitly consider word order and fluency . METEOR , on the other hand , utilizes this information through a chunk penalty .__label__Method|Code|Use
We have used a Bengali news corpus ( Ekbal and Bandyopadhyay , 2008 ) developed from the webarchives of a widely read Bengali newspaper . A portion of the Bengali news corpus containing 1500 sentences have been POS tagged using a Maximum Entropy based POS tagger ( Ekbal et al ., 2008 ). The POS tagger was developed with a tagset of 26 POS tags _CITE_ , defined for the Indian languages . The POS tagger demonstrated an accuracy of 88 . 2 %. We have also developed a rulebased chunker to chunk the POS tagged data with an overall accuracy of 89 . 4 %.__label__Method|Algorithm|Use
We also use the same development set and test set provided by the shared task . The in - domain test set includes 2 , 000 sentences and the out - of - domain test set includes 1 , 064 sentences for each language . To perform phrase - based SMT , we use Koehn ' s training scripts _CITE_ and the Pharaoh decoder ( Koehn , 2004 ). We run the decoder with its default settings and then use Koehn ' s implementation of minimum error rate training ( Och , 2003 ) to tune the feature weights on the development set . The translation quality was evaluated using a well - established automatic measure : BLEU score ( Papineni et al ., 2002 ).__label__Method|Code|Use
She has been waiting for him for 20 years . The reduplication scanner will be integrated to vnTokenizer - an open source and highly accurate tokenizer for Vietnamese texts ( Le et al ., 2008 ). The software and related resources will be distributed under the GNU General Public Lisence and it will be soon available online _CITE_ . We have presented for the first time a computational model for the reduplication of the Vietnamese language . We show that a large class of reduplicative words can be modeled effectively by sequential finite - state string - to - string transducers .__label__Method|Tool|Produce
She has been waiting for him for 20 years . The reduplication scanner will be integrated to vnTokenizer - an open source and highly accurate tokenizer for Vietnamese texts ( Le et al ., 2008 ). The software and related resources will be distributed under the GNU General Public Lisence and it will be soon available online _CITE_ . We have presented for the first time a computational model for the reduplication of the Vietnamese language . We show that a large class of reduplicative words can be modeled effectively by sequential finite - state string - to - string transducers .__label__Supplement|License|Produce
She has been waiting for him for 20 years . The reduplication scanner will be integrated to vnTokenizer - an open source and highly accurate tokenizer for Vietnamese texts ( Le et al ., 2008 ). The software and related resources will be distributed under the GNU General Public Lisence and it will be soon available online _CITE_ . We have presented for the first time a computational model for the reduplication of the Vietnamese language . We show that a large class of reduplicative words can be modeled effectively by sequential finite - state string - to - string transducers .__label__Supplement|Document|Produce
Similarly , we hypothesize that helpful peer reviews are closely related to domain topics that are shared by all students papers in an assignment . Our domain topic set contains 288 words extracted from the collection of student papers using topic - lexicon extraction software ; our feature ( domainWord ) counts how many words of a given review belong to the extracted set . For sentiment polarities , we extract positive and negative sentiment words from the General Inquirer Dictionaries _CITE_ , and count their appearance in reviews in terms of their sentiment polarity ( posWord , negWord ). The section of the essay on African Americans needs more careful attention to the timing and reasons for the federal governments decision to stop protecting African American civil and political rights . The review has only one sentence , in which one regular expression is matched with “ the section of ” thus regTag % = 1 ; no demonstrative determiner , thus dDeterminer = 0 ; “ African ” and “ Americans ” are domain words appearing between the subject “ section ” and the object “ attention ”, so soDomain is true for this sentence and thus soDomain % = 1 for the given review .__label__Material|Data|Use
Our apThis work is licensed under a Creative Commons Attribution 4 . 0 International Licence . Page numbers and proceedings footer are added by the organisers . Licence details : http :// creativecommons . org / licenses / by / 4 . 0 / proach relies on features inspired by deep semantics ( such as parsing and paraphrasing ), machine translation quality estimation , machine translation evaluation and Corpus Pattern Analysis ( CPA _CITE_ ). We use the same features to measure both semantic relatedness and textual entailment . Our hypothesis is that each feature covers a particular aspect of implicit similarity and entailment information contained within the pair of sentences .__label__Method|Algorithm|Extent
We then perform POS tagging using the Stanford POS tagger ( Toutanova et al ., 2003 ) committee member . Here V stands for verb ( possibly + preposition and / or + particle ), P for preposition and C for coordinating conjunction ; 1 → 2 means committee precedes the feature and member follows it ; 2 → 1 means member precedes the feature and committee follows it . and shallow parsing with the OpenNLP tools _CITE_ , and we extract the following types of features : Verb : We extract a verb if the subject NP of that verb is headed by one of the target nouns ( or an inflected form ), and its direct object NP is headed by the other target noun ( or an inflected form ). For example , the verb include will be extracted from “ The committee includes many members .” We also extract verbs from relative clauses , e . g ., “ This is a committee which includes many members .” Verb particles are also recognized , e . g ., “ The committee must rotate off 1 / 3 of its members .” We ignore modals and auxiliaries , but retain the passive be . Finally , we lemmatize the main verb using WordNet ’ s morphological analyzer Morphy ( Fellbaum , 1998 ).__label__Method|Tool|Use
“ noooo ” is reduced to “ no ”). In addition , according to the Internet slang dictionary , we convert each slang to its complete form , for example , “ aka ” is rewritten as “ also known as ”. After that , we use the Stanford parser _CITE_ for tokenization and the Stanford POS Tagger for POS tagging . Finally , Natural Language Toolkit is used for WordNet based Lemmatization . Typically , punctuation may express user ’ s sentiment to a certain extent .__label__Method|Tool|Use
The first set of the experiments was performed on the base of training data released by the organisers in May 2015 . The second set consisted of evaluation runs on test data released in June and the results for these experiments were provided by the organizers . For the DSL shared task 2015 edition , the organizers released two new versions of the DSL corpus collection _CITE_ ( DSLCC ), the version 2 . 0 and 2 . 1 . The version 2 . 0 is the standard shared task training material whereas the version 2 . 1 can be used for the unshared task track or as additional training material . The collection is described in ( Tan et al ., 2014 ).__label__Material|Data|Use
Each sentence is manually tagged with opinionatedness , polarity , and relevance to the topic by three annotators from a pool of six annotators . For preprocessing , no removal or stemming is performed on the data . Each sentence was processed with the Stanford English parser _CITE_ to produce a dependency parse tree . Only the Title fields of the topics were used . For performance evaluations of opinion and polarity detection , we use precision , recall , and Fmeasure , the same measure used to report the official results at the NTCIR MOAT workshop .__label__Method|Tool|Use
We use a manually constructed polarity lexicon ( Wilson et al ., 2005 ), in which each entry is annotated with its degree of subjectivity ( strong , weak ), as well as its sentiment polarity ( positive , negative and neutral ). We only take into account the subjective terms with the degree of strong subjectivity . We consider two baselines : All experiments were carried out using the SVMLight - TK toolkit _CITE_ with default parameter settings . All results reported are based on 10 - fold cross validation . Table 2 lists the results of the different kernel type combinations .__label__Method|Tool|Use
For each emotion class , the judge extracted expressions that reflect the emotion , and then made pairs that were conceptually equivalent . It was not feasible to ask a second judge to do the same task , because the process is time - consuming and tedious . In Information Retrieval , Precision and Recall are defined in terms of a set of retrieved documents and a set of relevant documents _CITE_ . In the following sections we describe how we compute the Precision and Recall for our algorithm compared to the manually extracted paraphrases . From the paraphrases that were extracted by the algorithm from the same texts , we counted how many of them were also extracted by the human judge .__label__Supplement|Document|Introduce
For Croatian , preliminary work on tagger evaluation for tagger voting has been conducted ( Agi ´ c et al ., 2010 ). SETIMES . HR is a new manually lemmatized and MSD - tagged corpus of Croatian . It is built on top of the SETimes parallel newspaper corpus involving 10 languages from the SEE region , _CITE_ Croatian and Serbian included . This initial dataset selection was deliberate in terms of enabling us with possibility of cross - lingual annotation projection and other cross - lingual experiments . SETIMES . HR was annotated by experts using the Croatian Lemmatization Server ( HML ) ( Tadi ´ c , 2005 ) to facilitate the process .__label__Material|Data|Use
Both allow the user to search for documents targeted to a population ( e . g ., patient - oriented documents ). We also queried known relevant websites for documents dealing with our chosen topics . Those were French governmental websites , including that of the HAS which issues guidelines for health professionals , and that of the INPES which provides educational material for the general public ; as well as health websites dedicated to the general public , including Doctissimo , Tabac Info Service _CITE_ , Stoptabac and Diabète Québec10 . The corpus dealing with the topic of diabetes served as our development corpus for the first type of paraphrases we extracted , the other two corpora were used as test corpora . Once collected , a corpus needs to be cleaned and converted into an appropriate format to allow further processing , i . e .__label__Supplement|Website|Introduce
We detected emotions using four feature types : 1 ) interjections , 2 ) profanity , 3 ) emoticons and 4 ) overall sentiment of the tweet . Interjections , profanity , and emoticons are widely used by individuals to convey emotion , such as anger , surprise , happiness , etc . To identify these three feature types , we used a combination of POS tags in the English tagger ( which contains tags for interjections , emoticons , etc ), compiled lists of interjections and profanity from the web for both English and Spanish _CITE_ and regular expression patterns for emoticons . We also included sentiment features using the sentiment140 API ( Go et al ., 2009 ). This API provides a sentiment label ( positive , negative or neutral ) for a tweet corresponding to its overall sentiment .__label__Material|Data|Produce
In this paper , we mainly compare our system ( DGST ) with HPB in Moses ( Koehn et al ., 2007 ). We implement our model in Moses and take the same settings as Moses HPB in all experiments . In addition , translation results from a recently open - source dependency tree - to - string system , Dep2Str _CITE_ ( Li et al ., 2014 ), which is implemented in Moses and improves the dependencybased model in Xie et al . ( 2011 ), are also reported . All systems use the same sets of features defined in Section 4 .__label__Method|Tool|Use
We train an SVM classifier on the training set , using the sentence vectors composed by a model as features , and report accuracy on the test set . State of the art is obtained by Le and Mikolov ( 2014 ) with the Paragraph Vector approach we describe below . The source corpus we use to build the lexical vectors is created by concatenating three sources : ukWaC , a mid - 2009 dump of the English Wikipedia _CITE_ and the British National Corpus ( about 2 . 8B words in total ). We build vectors for the 180K words occurring at least 100 times in the corpus . Since our training procedure requires parsed trees , we parse the corpus using the Stanford parser ( Klein and Manning , 2003 ).__label__Material|Data|Extent
One is in spoken language domain while the other is on newswire corpus . Both experiments are on Chinese - to - English translation . Experiments on spoken language domain were carried out on the Basic Traveling Expression Corpus ( BTEC ) ( Takezawa et al ., 2002 ) Chinese - to - English data augmented with HITcorpus _CITE_ . BTEC is a multilingual speech corpus which contains sentences spoken by tourists . 40K sentence - pairs are used in our experiment .__label__Material|Data|Use
In addition , they present a very userfriendly graphical interface that makes easy the development of very complex dialogues , besides the incorporation of predefined libraries for typical dialogues states such as requesting card or social security numbers , etc ., and additional assistants for debugging , logging and simulate the service . In contrast to commercial platforms , research or academic platforms ( e . g . CSLU - RAD , DialogDesigner _CITE_ , Olympus , Trindi - kit , etc .) do not necessary incorporate all the above - mentioned features ; especially because they are limited to the number of standards that they are able to handle and to the integration level with other platforms , as well as the number of capabilities that they can offer to the users and programmers . However , they allow more complex dialogue interactions , most of them are freely available as open source , and using third party modules it is possible to extend their functionalities .__label__Supplement|Website|Compare
Getting all the resources into one single compilation is a challenge . These resources were brought together and suitably compiled into a format that can be easily processed by Semantex ( Srihari , 2008 ), a text extraction platform provided by Janya Inc . Lists of places , organizations and names of famous personalities in Pakistan were also compiled using the Urdu - Wikipedia and NationalMaster _CITE_ . A list of most common names in Pakistan was composed by retrieving data from the various name databases available on the internet . The word segmentation model uses the Urdu corpus released by CRULP as the training data .__label__Supplement|Website|Use
Unless noted otherwise , we use CONSTITUENT - LEVEL inference . All our experiments are based on the publicly available BerkeleyParser . _CITE_ A great deal has been written on the topic of products versus sums of probability distributions for joint prediction ( Genest and Zidek , 1986 ; Tax et al ., 2000 ). However , those theoretical results do not apply directly here , because we are using multiple randomly permuted models from the same class , rather models from different classes . To shed some light on this issue , we addressed the question empirically , and combined two grammars into an unweighted product model , and also an unweighted sum model .__label__Method|Tool|Use
We used Reuters , an English - Japanese bilingual corpus , and Conversation , an English - Vietnamese corpus ( Table 4 ). These corpora were split into data sets as shown in Table 3 . Japanese sentences were analyzed by ChaSen _CITE_ , a word - segmentation tool . A number of tools were used in our experiments . Vietnamese sentences were segmented using a wordsegmentation program ( Nguyen et al ., 2003 ).__label__Method|Tool|Use
It is available as a free software project , thus enabling its practical re - use in other systems . Some research is also underway to explore the reverse direction , i . e . from XML schema to ontology content _CITE_ . The motivation for that is twofold : firstly to enable reasoning about XML content for DAML - enabled software and secondly to create DAML content from XML in a quick and automated fashion . The main objective of our approach is , however , to bring semantics to XML documents , i . e ., derive appropriate interface specifications from the given domain model , thereby enabling highquality reasoning immediately on the XMLS level .__label__Supplement|Document|Introduce
Links between aligned words in the sentence pairs are then classified as positive or negative based on their scores , a technique which has previously been applied to extract paraphrase fragments from non - parallel bilingual corpora and has been shown to improve a state of the art machine translation system ( Munteanu and Marcu , 2006 ). Word pairs containing punctuation or stop words are excluded from the alignment prior to scoring . _CITE_ Afterwards , the alignment is refined by removing all negatively - scored word pairs , such that only very strong alignments survive . We then smooth the alignment by recomputing scores for each word , averaging over a window of five words . In this way we often capture context words that are left out of the alignment process ( e . g .__label__Supplement|Document|Introduce
It provides access to the Archaeological articles in the APSAT / ALPINET repository , and therefore , dedicated content extraction resources needed to be created , tuned on the specificities of the domain . The corpus of articles in the repository consists of a complete collection of the journal Preistoria Alpina published by the Museo Tridentino di Scienze Naturali . In order to make those articles accessible through the portal , they are tokenized , PoS tagged and Named Entity ( NE ) annotated by the TEXTPRO _CITE_ pipeline ( Pianta et al ., 2008 ). The first version of the pipeline included the default TEXTPRO NE tagger , EntityPro , trained to recognize the standard ACE entity types . However , the final version of the portal is based on an improved version of the NEtagger capable of recognising all relevant entities in the APSAT / ALPINET collection ( Poesio et al ., 2011b ; Ekbal et al ., 2012 ) A close collaboration with the University of Trento ’ s “ B .__label__Method|Tool|Use
KB BIO 101 is organized into a set of concept maps , where each concept map corresponds to a biological entity or process . It was encoded by biology teachers and contains around 5 , 000 concept maps . KB BIO 101 is available for download for academic purposes in various formats including OWL _CITE_ . To test and evaluate our approach , we focus on the subpart of KB BIO 101 isolated for the KBGEN surface realisation shared task by ( Banik et al ., 2013 ). In this dataset , content units were semiautomatically selected from KB BIO 101 in such a way that ( i ) the set of relations in each content unit forms a connected graph ; ( ii ) each content unit can be verbalised by a single , possibly complex sentence which is grammatical and meaningful and ( iii ) the set of content units contain as many different relations and concepts of different semantic types ( events , entities , properties , etc ) as possible .__label__Material|Data|Introduce
For each word in the tweet , its corresponding binary prefix string representation is used as the feature value . K - means clusters are generated using two different methods . The first method uses the word2vec tool ( Mikolov et al ., 2013 ) _CITE_ . By varying the minimum occurrences ( 15 , 10 , 201 ), word vector size ( 150 , 100 , 200 , 500 , 10001 ), cluster size ( 150 , 100 , 200 , 500 , 10001 ) and sub - sampling threshold ( 10 . 00001 , 0 . 001 }), different cluster files are generated and tested . Similar to the Brown cluster feature , the name of the cluster that each word belongs to is used as the feature value .__label__Method|Tool|Use
96 . 5 % of English Pos , 100 % of Japanese Pos , and 99 . 5 % of Chinese Pos were definitions . As the source of Neg , we used 600 million Japanese Web pages ( Akamine et al ., 2010 ) and the ClueWeb09 corpus for English ( about 504 million pages ) and Chinese ( about 177 million pages ). _CITE_ From each Web corpus , we collected the sentences satisfying following conditions : 1 ) they contain 5 to 50 words and at least one verb , 2 ) less than half of their words are numbers , and 3 ) they end with a period . Then we randomly sampled sentences from the collected sentences as Neg so that INegl was about twice as large as lPosl : 5 , 000 , 000 for English , 1 , 400 , 000 for Japanese , and 600 , 000 for Chinese . In Section 3 . 1 . 3 , we use 10 % of the Web corpus as the input to the definition classifier .__label__Material|Data|Use
Using these words can express the richest meaning with the less characters . Chinese micro - blog texts are unrestrained . In this task we followed the tagging schema of “ Specification for Corpus Processing at Peking University ” _CITE_ in the design of our model . In this word , under the assumption that a segmentation system for general text is already good , for a special domain we only need to do some modification to make the segmentation result better . The main frame of this system is using ICTCLAS as the segmentation tool .__label__Method|Algorithm|Extent
( 2011 ) argue that it is harder to distinguish sarcastic from non - sarcastic messages where the nonsarcastic messages contain sentiment . Our results support this argument ( 97 % F1 measure for the best result for S vs . L , compared to 84 % F1 for the best result for S vs . Laent ; Section 4 ). _CITE_ To collect a set of target words that can have either literal or sarcastic meaning depending on context , we propose a two step approach : 1 ) a crowdsourcing task to collect a parallel dataset of sarcastic utterances and their re - phrasings that convey the authors ’ intended meaning ; and 2 ) unsupervised alignment techniques to detect semantically opposite words / phrases . Crowdsourcing Task . Given a sarcastic message ( SM ), Turkers were asked to re - phrase the message so that the new message is likely to express the author ’ s intended meaning ( IM ).__label__Supplement|Document|Produce
Results of WPT 2005 showed an improvement of at least 0 . 3 BLEU point when exploiting different resources for the FrenchEnglish pair of languages . In addition to the training resources used in WPT 2005 for the French - English task , i . e . Europarl and Hansard , we used a bilingual dictionary , Le Grand Dictionnaire Terminologique ( GDT ) _CITE_ to train translation models and the English side of the UN parallel corpus ( LDC2004E13 ) to train an English language model . Integrating terminological lexicons into a statistical machine translation engine is not a straightforward operation , since we cannot expect them to come with attached probabilities . The approach we took consists on viewing all translation candidates of each source term or phrase as equiprobable ( Sadat et al , 2006 ).__label__Material|Data|Use
The first level of interpretation of a Sanskrit text is its word - to - word segmentation , and our tagger will be able to assist a philology specialist to achieve complete morphological mark - up systematically . This will allow the development of concordance analysis tools recognizing morphological variants , a task which up to now has to be performed manually . At some point in the future , one may hope to develop for Sanskrit the same kind of informative repository that the Perseus web site provides for Latin and Classical Greek _CITE_ . Such resources are invaluable for the preservation of the cultural heritage of humanity . The considerable classical Sanskrit corpus , rich in philosophical texts but also in scientific , linguistic and medical knowledge , is an important challenge for computational linguistics .__label__Supplement|Website|Introduce
We define a Twitter conversation as a chain of tweets where two users are consecutively replying to each other ’ s tweets using the Twitter reply button . We identify dyads of English - tweeting users with at least twenty conversations and collect their tweets . We use an open source tool for detecting English tweets _CITE_ , and to protect users ’ privacy , we replace Twitter userid , usernames and url in tweets with random strings . This dataset consists of 101 , 686 users , 61 , 451 dyads , 1 , 956 , 993 conversations and 17 , 178 , 638 tweets which were posted between August 2007 to July 2013 . To measure the accuracy of our model , we randomly sample 101 conversations , each with ten or fewer tweets , and ask three judges , fluent in English , to annotate each tweet with the level of self - disclosure .__label__Method|Tool|Use
A current research interest concerns consistency of performance across different domains . From our experiments , we show that monolingual segmenters cannot produce consistently good results when applied to a new domain . Our pilot investigation into the influence of word segmentation on SMT involves three offthe - shelf Chinese word segmenters including ICTCLAS ( ICT ) Olympic version , LDC segmenter and Stanford segmenter version 2006 - 0511 _CITE_ . Both ICTCLAS and Stanford segmenters utilise machine learning techniques , with Hidden Markov Models for ICT ( Zhang et al ., 2003 ) and conditional random fields for the Stanford segmenter ( Tseng et al ., 2005 ). Both segmentation models were trained on news domain data with named entity recognition functionality .__label__Method|Tool|Use
state - of - the - art paraphrase typologies to cover the phenomena described in them ; and it was used to annotate ( i ) the plagiarism paraphrases in the P4P corpus ( cf . Section 3 ), ( ii ) 3 , 900 paraphrases from the news domain in the Microsoft Research Paraphrase corpus ( MSRP ) ( Dolan and Brockett 2005 ), and ( iii ) 1 , 000 relational paraphrases ( i . e ., paraphrases expressing a relation between two entities ) extracted from the Wikipediabased Relational Paraphrase Acquisition corpus ( WRPA ) ( Vila , Rodr ´ ıguez , and Mart ´ ı Submitted ). _CITE_ P4P and MSRP are English corpora , whereas WRPA is a Spanish one . The success in the annotation of such diverse corpora with our paraphrase typology guarantees its adequacy for general paraphrasing not only in English . The typology is displayed in Figure 1 .__label__Material|Data|Use
Therefore , we suspect that DMV training assigns an increased amount of probability mass to dependency paths along structures which are truly related to these relations . We used the DMV implementation from Cohen and Smith ( 2009 ) 4 . For the supervised Nivre arc - eager parser we used MALT ( Nivre et al ., 2007 ) with a pre - trained Penn Treebank ( Marcus et al ., 1993 ) model _CITE_ . As a baseline , we tested left branching parses i . e . dependency trees solely consisting of head - todependent edges from the right to the left .__label__Method|Algorithm|Use
Over last decades , there has been increasing interest on coreference resolution within NLP community . The task of coreference resolution is to identify expressions in a text that refer to the same discourse entity . This year , CoNLL _CITE_ holds a shared task aiming to model unrestricted coreference in OntoNotes . The OntoNotes project has created a large - scale , accurate corpus for general anaphoric coreference that covers entities and events not limited to noun phrases or a limited set of entity types . And Pradhan et al .__label__Supplement|Website|Introduce
English is the source language in all the experiments . In expASR we used tst2011 – the official test set of the SLT track of the IWSLT 2011 evaluation campaign on the English - French language pair ( Federico et al ., 2011 ). _CITE_ This test set comprises reference transcriptions of 8 talks ( approximately 1 . 1h of speech , segmented to 818 utterances ), 1 - best hypotheses from five different ASR systems , a ROVER combination of four systems ( Fiscus , 1997 ), and three sets of lattices produced by the participants of the IWSLT 2011 ASR track . In this set of experiments we compare baseline systems performance to a performance of systems augmented with synthetic phrases on ( 1 ) reference transcriptions , ( 2 ) 1 - best hypotheses from all released ASR systems , and ( 3 ) a set of ASR lattices produced by FBK ( Ruiz et al ., 2011 ). Experiments with individual systems are aimed to validate that MT augmented with synthetic phrases can better translate ASR outputs with recognition errors and sequences that were not observed in the MT training data .__label__Material|Data|Use
First one is derived from ConceptNet . In the second technique sub - graph is identified consisting of a nearest verb and roles are assigned accordingly . A ConceptNet API _CITE_ written in Java has been used to extract pair wise relation from ConceptNet . A Bengali - English dictionary ( approximately 102119 entries ) has been developed using the Samsad Bengali - English dictionary used here for equivalent lookup of English meaning of each Bengali lexicon . Obtained semantic relations from ConceptNet for any lexicon English pair are assigned to source Bengali pair lexicons .__label__Supplement|Website|Use
Articles might be deleted only by Wikipedia administrators if they are subject to copyright violations , vandalism , spam or other conditions that violate Wikipedia policies . As a consequence , they are removed from the public view along with all their revision information , which makes it impossible to recover them from any future publicly available dump . _CITE_ Even though about five thousand pages are deleted every day , only a small percentage of those pages actually corresponds to meaningful articles . Most of the affected pages are newly created duplicates of already existing articles or spam articles . Even though article revisions are available from the official Wikipedia revision dumps , accessing this information on a large scale is still a difficult task .__label__Material|Data|Introduce
Therefore , the number of Google hits for every word was obtained and only the 20 % most common ones were kept , resulting in a list of 13 , 413 words . To this , the entries of the Brazilian Open Mind Common Sense network ( Anacleto et al ., 2008 ) were added , in order to allow future comparison with this resource . Apertium ’ s lt - toolbox _CITE_ was used in order to obtain the most frequent POS tag for each entry , resulting in 5 , 129 nouns , 3 , 672 verbs , 1 , 176 adjectives , and 201 adverbs . The union with the preceding dictionary resulted in a final seed list of 20 , 854 words . Once the game is deployed , one of the big challenges is to gather volunteer players .__label__Method|Tool|Use
Opinion cues ( or Q - elements ) are words , emoticons and phrases that convey the actual attitude of the speaker towards the topics and targets , and a strength and polarity can be attributed to them , both a priori and in context . Our modular processing approach allows customizing the annotation for each domain or genre , since , for example , regular expressions to detect emoticons will be useful for twitter microblogging , but less so for more conventional blogs where such sentiment - expression devices are less frequent ; Also pre - compiled lists of known entities can provide good target precision while customised distributional models will help discover unlisted names and concepts in text . The output of the semantic and syntactic processing pipeline is indexed using the Apache Solr framework , _CITE_ which is based on the Lucene engine . This setup allows the implementation of clustering and classification algorithms , allowing us to obtain reliable statistical correlations between documents and entities . We also developed or adapted a number of visualization components in order to present the data stored in Solr in an interactive page that is conducive to data exploration and discovery by the system ’ s corporate users .__label__Method|Tool|Use
Nonetheless , automatic metrics are far from perfection : when used in isolation , they tend to stress specific aspects of the translation quality and neglect others ( particularly during tuning ); they are often unable to capture little system improvements ( enhancements in very specific aspects of the translation process ); and they may make unfair comparisons when they are not able to reflect real differences among the quality of different MT systems ( Gim ´ enez , 2008 ). ASIYA , the core of our approach , is an opensource suite for automatic machine translation evaluation and output analysis . _CITE_ It provides a rich set of heterogeneous metrics and tools to evaluate and analyse the quality of automatic translations . The ASIYA core toolkit was first released in 2009 ( Gim ´ enez and M ` arquez , 2010a ) and has been continuously improved and extended since then ( Gonz ` alez et al ., 2012 ; Gonz ` alez et al ., 2013 ). In this paper we first describe the most recent enhancements to ASIYA : ( i ) linguistic - based metrics for French and German ; ( ii ) an extended set of source - based metrics for English , Spanish , German , French , Russian , and Czech ; and ( iii ) the integration of mechanisms to exploit the alignments between sources and translations .__label__Method|Code|Produce
A word unit is more or less fixed and there is no syntactic interference in the inside of the word unit . In the practical sense , it is useful for the further syntactic parsing because it is not decomposable by syntactic processing and also frequently occurred in corpora . There are a series of linguistic annotation standards in ISO : MAF ( morpho - syntactic annotation framework ), SynAF ( syntactic annotation framework ), and others in ISO / TC37 / SC4 _CITE_ . These standards describe annotation methods but not for the meaningful units of word segmentation . In this aspect , MAF and SynAF are to annotate each linguistic layer horizontally in a standardized way for the further interoperability .__label__Method|Algorithm|Introduce
This system scored the highest F1 ( 57 . 32 ) of Task 2 . In this paper we describe the machine learning systems that CLiPS submitted to the closed track of the CoNLL - 2010 Shared Task on Learning to Detect Hedges and Their Scope in Natural Language Text ( Farkas et al ., 2010 ). _CITE_ The task consists of two subtasks : detecting whether a sentence contains uncertain information ( Task 1 ), and resolving in - sentence scopes of hedge cues ( Task 2 ). To solve Task 1 , systems are required to classify sentences into two classes , “ Certain ” or “ Uncertain ”, depending on whether the sentence contains factual or uncertain information . Three annotated training sets are provided : Wikipedia paragraphs ( WIKI ), biological abstracts ( BIO - ABS ) and biological full articles ( BIO - ART ).__label__Supplement|Website|Introduce
It is basically the same as the vanilla Viterbi search often used for Japanese morphological analysis ( Kudo et al ., 2004 ), except that it runs on a consonant sequence . The key change to this Viterbi search is to make it possible to look up the dictionary directly by consonant substrings . To do this , we convert dictionary entries to possible consonant sequences referring to Microsoft IME Kana Table _CITE_ when the dictionary structure is loaded onto the memory . For example , for a dictionary entry ; KR / 7 hukubukuro , possible consonant sequences such as “ hkbkr ,” “ hukbkr ,” “ hkubkr ,” “ hukubkr ,” “ hkbukr ,”... are stored in the index structure . As for the conversion model , we employed the discriminative Kana - Kanji conversion model by Tokunaga ( 2011 ).__label__Supplement|Document|Extent
It output the top 5 translations for the out - of - five evaluation . For the out - of - five evaluation , if the query returned less than 5 answers , the first fallback appended the lemma of the Most Frequent Sense ( according to Wordnet ) of the target polysemous word in their respective language from the Open Multilingual Wordnet . _CITE_ If the first fallback was insufficient , the second fallback appended the most frequent translation of the target polysemous word to the queries ’ responses . We also constructed a baseline for matching sentences by cosine similarity between the lemmas of the query sentence and the lemmas of each English sentence in the training corpus . The baseline system is named XLING_SnT ( Similar and Translate ).__label__Method|Tool|Use
Explicit Semantic Analysis ( ESA ) similarity . ESA ( Gabrilovich and Markovitch , 2007 ) represents input documents as vectors of Wikipedia concepts . To compute ESA features we use Lucene _CITE_ to index documents extracted from a Wikipedia dump . Given a text pair we retrieve k top documents ( i . e . Wikipedia concepts ) and compute the metric by looking at the overlap of the concepts between the documents : esak ( a , b ) = | W - � Wb | k , where Wa is the set of concepts retrieved for document a .__label__Method|Tool|Use
VerbNet is a lexicon and by definition it does not list optional modifiers ( the arguments labelled AM - X in PropBank ). In order to support the joint use of both these resources and their comparison , SemLink has been developed ( Loper et al ., 2007 ). SemLink _CITE_ provides mappings from PropBank to VerbNet for the WSJ portion of the Penn Treebank . The mapping have been annotated automatically by a two - stage process : a lexical mapping and an instance classifier ( Loper et al ., 2007 ). The results were handcorrected .__label__Supplement|Website|Introduce
A distributional semantic space is a matrix whose rows represent target elements in terms of ( functions of ) their patterns of co - occurrence with contexts ( columns or dimensions ). Several parameters must be manually fixed or tuned to instantiate the space . Our source corpus is given by the concatenation of the ukWaC corpus , a mid - 2009 dump of the English Wikipedia and the British National Corpus , _CITE_ for a total of about 2 . 8 billion tokens . The corpora have been dependency - parsed with the MALT parser ( Hall , 2006 ), so it is straightforward to extract all cases of adjective - noun modification . We use part - of - speech - aware lemmas as our representations both for target elements and dimensions .__label__Material|Data|Use
The core of the system is the Command Dispatcher which manages the communication with the client and the execution of tasks like downloading a document for example . The Master Data include information about all objects managed by the system , for example users , groups , documents , resources and their interrelations . All this information is stored in a transactional Relational Database ( using MySQL _CITE_ ). The underlying data model is described later in more detail . Another important component is the Storage Handler : Based on an automatic mime type detection it decides how to store and retrieve documents .__label__Material|Data|Use
This may be because the other WordNet methods will only work for nouns and verbs . Our final experiment concerns sentence relatedness . We worked with a data set from ( Li et al ., 2006 ) _CITE_ . They took a subset of the term pairs from ( Rubenstein and Goodenough , 1965 ) and chose sentences to represent these terms ; the sentences are definitions from the Collins Cobuild dictionary ( Sinclair , 2001 ). Thirty people were then asked to assign relatedness scores to these sentences , and the average of these similarities was taken for each sentence .__label__Material|Data|Use
For our submissions we applied the approach proposed by Espl ` a - Gomis et al . ( 2015b ), who use black - box bilingual resources from the Internet for word - level MTQE . In particular , we combined two on - line MT systems , Apertium and Google Translate , and the bilingual concordancer Reverso Context _CITE_ to spot sub - segment correspondences between a sentence S in the source language ( SL ) and a given translation hypothesis T in the target language ( TL ). To do so , both S and T are segmented into all possible overlapping sub segments up to a certain length and translated into the TL and the SL , respectively , by means of the sources of bilingual information mentioned above . These sub - segment correspondences are used to extract a collection of features that is then used by a binary classifier to determine the final word - level MTQE labels .__label__Method|Tool|Extent
We have used two standard datasets . The first one , RG , consists of 65 pairs of words collected by Rubenstein and Goodenough ( 1965 ), who had them judged by 51 human subjects in a scale from 0 . 0 to 4 . 0 according to their similarity , but ignoring any other possible semantic relationships that might appear between the terms . The second dataset , WordSim353 _CITE_ ( Finkelstein et al ., 2002 ) contains 353 word pairs , each associated with an average of 13 to 16 human judgements . In this case , both similarity and re ll never forget the & apos ; on his face when grin , 2 , smile , 10 he had a giant & apos ; on his face and grin , 3 , smile , 2 room with a huge & apos ; on her face and grin , 2 , smile , 6 the state of every & apos ; will be updated every automobile , 2 , car , 3 repair or replace the & apos ; if it is stolen automobile , 2 , car , 2 located on the north & apos ; of the Bay of shore , 14 , coast , 2 areas on the eastern & apos ; of the Adriatic Sea shore , 3 , coast , 2 Thesaurus of Current English & apos ; The Oxford Pocket Thesaurus slave , 3 , boy , 5 , shore , 3 , string , 2 latedness are annotated without any distinction . Several studies indicate that the human scores consistently have very high correlations with each other ( Miller and Charles , 1991 ; Resnik , 1995 ), thus validating the use of these datasets for evaluating semantic similarity .__label__Material|Data|Use
We are using GIZA ++ ( Och and Ney , 2003 ) and Anymalign ( Lardilleux and Lepage , 2009 ) to generate phrase tables from a collection of four Japanese English parallel corpora and one bilingual dictionary . The corpora are the Tanaka Corpus ( 2 , 930 , 132 words : Tanaka ( 2001 )), the Japanese Wordnet Corpus ( 3 , 355 , 984 words : Bond et al . ( 2010 )), the Japanese Wikipedia corpus ( 7 , 949 , 605 ), _CITE_ and the Kyoto University Text Corpus with NICT translations ( 1 , 976 , 071 words : Uchimoto et al . ( 2004 )). The dictionary is Edict , a Japanese English dictionary ( 3 , 822 , 642 words : Breen ( 2004 )).__label__Material|Data|Use
This interest has given rise to the TweetLID shared task ( Zubiaga et al ., 2014 ), which asked participants to recognize the language of tweet messages , focusing on English and on languages spoken on the Iberian peninsula such as Basque , Catalan , Spanish , and Portuguese . The Shared Task on Language Identification in CodeSwitched Data held in 2014 ( Solorio et al ., 2014 ) is another related competition , where the focus was on tweets in which users were mixing two or more languages in the same tweet . For the first edition of the task , we compiled the DSL Corpus Collection ( Tan et al ., 2014 ), or DSLCC v . 1 . 0 , which included excerpts from journalistic texts from sources such as the SETimes Corpus ( Tyers and Alperen , 2010 ), HC Corpora _CITE_ and the Leipzig Corpora Collection ( Biemann et al ., 2007 ), written in thirteen languages divided into the following six groups : Group A ( Bosnian , Croatian , Serbian ), Group B ( Indonesian , Malay ), Group C ( Czech , Slovak ), Group D ( Brazilian Portuguese , European Portuguese ), Group E ( Peninsular Spanish , Argentine Spanish ), and Group F ( American English , British English ). In 2014 , eight teams built systems and submitted results to the DSL language identification shared task ( eight teams participated in the closed and two teams took part in the open condition ), and five participants wrote system description papers . The results are summarized in Table 1 , where the best - performing submissions , in terms of testing accuracy , are shown in bold .__label__Material|Data|Use
( 2006 ) ( see Section 3 ). We show that metrics based on deeper linguistic information ( syntactic / shallow - semantic ) are able to produce more reliable system rankings than those produced by metrics which limit their scope to the lexical dimension , specially when the systems under evaluation are of a different nature . For our experiments , we have compiled a representative set of metrics _CITE_ at different linguistic levels . We have resorted to several existing metrics , and we have also developed new ones . Below , we group them according to the level at which they operate .__label__Method|Algorithm|Produce
For future work , one could investigate whether the latter can be addressed by domain - adaptation techniques ( e . g . Satpal and Sarawagi ( 2007 )). To cope with DrugN entities , one could implement features derived from those resources that were used by the annotators for deciding whether a substance is approved for use in humans , e . g ., Drugs @ FDA and the WHO ATC _CITE_ classification system . We thank Philippe Thomas for preparing a simplified format of the corpora . We thank him and Roman Klinger for fruitful discussions .__label__Method|Tool|Introduce
Chemical compound names , i . e . names of molecules , are terms which prominently occur in scientific publications , patents and in biochemical databases . Any chemical compound can be unambiguously denoted by its molecular structure , either graphically or by certain representation standards . Established representation formats are SMILES strings ( Simplified Molecular Input Line Entry System ( Weininger , 1988 )) and InChIs _CITE_ . For example , a SMILES string such as CC ( OH ) CCC unambiguously describes a chain of five carbon ( C ) atoms connected by single bonds having an oxygen ( O ) and a hydrogen ( H ) atom connected to the second carbon atom by another single bond ( Figure 1 ). However , for communication purposes , e . g . in scientific publications and even in databases , it is common to use names for chemical compounds instead of a structural representation .__label__Method|Algorithm|Introduce
The main advantage was to outsource operations such as web crawling and website quality filtering , which are considered to be too costly or too complicated to deal with while the main purpose is actually to build a corpus . In fact , it is not possible to start a web crawl from scratch , so the main issue to tackle can be put this way : where may we find web pages which are bound to be interesting for corpus linguists and which in turn contain many links to other interesting web pages ? Researchers in the machine translation field have started another attempt to outsource competence and computing power , making use of data gathered by the CommonCrawl project _CITE_ to find parallel corpora ( Smith et al ., 2013 ). Nonetheless , the quality of the links may not live up to their expectations . First , purely URL - based approaches are a trade - off in favor of speed which sacrifices precision , and language identification tasks are a good example of this phenomenon ( Baykan et al ., 2008 ).__label__Material|Data|Use
SVM - based chunking is performed using the features to yield a protein name tagged sentence . Our morphological analysis gives ( a ) sophisticated tokenization , ( b ) part - of - speech tagging and ( c ) annotation of value - added information such as the stemmed form of a word , accession numbers to biomedical resources . Our morphological analyzer for biomedical English , cocab _CITE_ , is inspired by the work of Yamashita and Matsumoto ( 2000 ). We first define terms used in this paper with an illustration in Figure 2 . A lexeme is an entry in a dictionary .__label__Method|Tool|Produce
Another aspect we would like to distinguish is whether an instance of a pattern is a simile or not . We plan to tackle this using machine learning . Semantic features from an ontology like the one used in PDEV _CITE_ , or a more comprehensive work such as WordNet , can carry the information whether T and V belong to similar semantic categories . We expect other information , such as distributional and distributed word vector representations , to be of use . It may also be of interest to decide whether an instance is conventional or creative .__label__Method|Algorithm|Extent
No word sense disambiguation was performed and all synsets for a particular lemma were considered . We firstly recorded length information of given sentences pairs using following eight measure function where | A | stands for the number of non - repeated http :// nltk . org / words in sentence A , | A − B | means the number of unmatched words found in A but not in B , | A u B | stands for the set size of non - repeated words found in either A or B and | A n B | means the set size of shared words found in both A and B . Motivated by the hypothesis that two texts are considered to be more similar if they share more strings , we adopted the following five types of measurements : ( 1 ) longest common sequence similarity on the original and lemmatized sentences ; ( 2 ) Jaccard , Dice , Overlap coefficient on original word sequences ; ( 3 ) Jaccard similarity using n - grams , where n - grams were obtained at three different levels , i . e ., the original word level ( n = 1 , 2 , 3 ), the lemmatized word level ( n = 1 , 2 , 3 ) and the character level ( n = 2 , 3 , 4 ); ( 4 ) weighted word overlap feature ( ˇSari ´ c et al ., 2012 ) that takes the importance of words into consideration , where Web 1T 5 - gram Corpus _CITE_ was used to estimate the importance of words . ( 5 ) sentences were represented as vectors in tf * idf schema based on their lemmatized forms and then these vectors were used to calculate cosine , Manhattan , Euclidean distance and Pearson , Spearmanr , Kendalltau correlation coefficients based on different perspectives . Totally , we got thirty - one string based features .__label__Material|Data|Use
The overall system yields the precision , recall and F - measure values of 90 . 26 %, 71 . 91 % and 80 . 05 % respectively for the test dataset . Twitter has seen a phenomenal growth in the number of users during the last few years . Over 500 million user accounts have been registered with it with approx 302 million active users _CITE_ . Amount of user generated contents over the web would be unarguably enormous i . e . almost 500 million tweets per day .__label__Supplement|Website|Introduce
Concretely , the following features were computed from the original Twitter messages : We used the L2 - regularized logistic regression implementation from scikit - learn . Given a set of m instance - label pairs ( xi , yi ), with i = 1 , ... , m , xi E Rn , and yi E {− 1 , + 1 }, learning the classifier involves solving the following optimization problem , where C & gt ; 0 is a penalty parameter . In scikit - learn , the problem is solved through a trust region Newton method , using a wrapper over the implementation available in the liblinear _CITE_ package . For multi - class problems , scikitlearn uses the one - vs - the - rest strategy . This particular implementation also suports the introduction of class weights , which we set to be inversely proportional to the class frequency in the training data , thus making each class equally important .__label__Method|Code|Use
In this sense , our work aims to take a first step towards closing the gap between the detection and comparative approaches to cross - linguistic transfer . The second area of research , language typology , deals with the documentation and comparative study of language structures ( Song , 2011 ). Much of the descriptive work in the field is summarized in the World Atlas of Language Structures ( WALS ) _CITE_ ( Dryer and Haspelmath , 2013 ) in the form of structural features . We use the WALS features as our source of typological information . Several previous studies have used WALS features for hierarchical clustering of languages and typological feature prediction .__label__Method|Algorithm|Introduce
Our model achieved the best performance on both joint segmentation and tagging as well as joint phrase - structure parsing . Our final performance on constituent parsing is by far the best that we are aware of for the Chinese data , and even better than some state - of - the - art models with gold segmentation . For example , the un - lexicalized PCFG model of Petrov and Klein ( 2007 ) achieves 83 . 45 % _CITE_ in parsing accuracy on the test corpus , and our pipeline constituent parsing model achieves 83 . 55 % with gold segmentation . They are lower than the performance of our character - level model , which is 84 . 43 % without gold segmentation . The main differences between word - based and character - level parsing models are that character - level model can exploit character features .__label__Method|Algorithm|Compare
Each translation was evaluated by the BLEU and NIST metrics first with the single reference , then with the multiple references for each sentence using the paraphrases automatically generated from the source - reference mini corpus . A subset of a 100 sentences was randomly extracted from each test set and evaluated by two independent human judges with respect to accuracy and fluency ; the human scores were then compared to the BLEU and NIST scores for the single - reference and the automatically generated multiple - reference files . We used the GIZA ++ word alignment software _CITE_ to produce initial word alignments for our miniature bilingual corpus consisting of the source French file and the English reference file , and the refined word alignment strategy of ( Och and Ney , 2003 ; Koehn et al ., 2003 ; Tiedemann , 2004 ) to obtain improved word and phrase alignments . For each source word or phrase fi that is aligned with more than one target words or phrases , its possible translations ei1 , ..., ein were placed in a list as equivalent expressions ( i . e . synonyms , near - synonyms , or paraphrases of each other ).__label__Method|Tool|Use
All of the systems we present use the lattice input format to Moses ( Dyer et al ., 2008 ), including the baselines which do not need them . We do not report on the non - lattice baselines , but in initial experiments we conducted , they did not perform as well as the degenerate lattice version . The Devtest Set Our devtest set consists of sentences containing at least one non - MSA segment ( as annotated by LDC ) _CITE_ in the Dev10 audio development data under the DARPA GALE program . The data contains broadcast conversational ( BC ) segments ( with three reference translations ), and broadcast news ( BN ) segments ( with only one reference , replicated three times ). The data set contained a mix of Arabic dialects , with Levantine Arabic being the most common variety .__label__Material|Data|Use
The inter - tagger agreement rate was 0 . 80 , with a Kappa score of 0 . 77 . This annotation was used to group the pairs in three categories : similar pairs ( those classified as synonyms , antonyms , identical , or hyponym - hyperonym ), related pairs ( those classified as meronym - holonym , and pairs classified as none - of - the - above , with a human average similarity greater than 5 ), and unrelated pairs ( those classified as none - of - the - above that had average similarity less than or equal to 5 ). We then created two new gold - standard datasets : similarity ( the union of similar and unrelated pairs ), and relatedness ( the union of related and unrelated ) _CITE_ . Table 5 shows the results on the relatedness and similarity subsets of WordSim353 for the different methods . Regarding WordNet methods , both WN30 and WN30g perform similarly on the similarity subset , but WN30g obtains the best results by far on the relatedness data .__label__Material|Data|Produce
These pairs were classified manually in Cognates , Unrelated words , and False Friends for two annotators , the first native of Portuguese and the other native of Spanish . These data are illustrated in Table 1 . The word pairs were selected from the following resources : online Spanish - Brazilian Portuguese dictionary ; online Spanish - Portuguese dictionary ; list of most frequent words in Portuguese and Spanish ; and online list of different words in Portuguese and Spanish _CITE_ . As illustrated in Table 1 , there is an unbalance in the training set favoring the classes Cognates / False Friends . There are no multiword expressions in the data set .__label__Material|Data|Use
DM is a general distributional semantic resource which allows the generation of vectorbased semantic models ( Turney and Pantel , 2010 ) from the distribution of words in context . In general , distributional semantic models are two - dimensional , relating a word with other words in its context giving DM is a three - dimensional extension of such a two - dimensional matrix which includes the syntactically derived relation between the two words as an extra dimension . It is derived from the concatenation of the ukWaC _CITE_ , the English Wikipedia , and the BNC , resulting altogether in a 2 . 83 billion - token corpus . We use the TypeDM variant of DM , which contains over 130M links between nouns , verbs and adjectives , covering generic syntactic relations as well as lexicalized relations ( see Baroni and Lenci ( 2010 ) for details ). In DM , each triple of words w1 , w2 and relation r , ( w1 r w2 ), is scored by the Local Mutual Information ( LMI , Evert ( 2005 ), Equation 1 ) between its three elements .__label__Supplement|Website|Extent
For these reasons , to obtain a more interesting comparison , we modified CoMiC - EN to perform scoring instead of meaning comparison . This means that the memory - based learning approach CoMiCEN had employed so far was no longer applicable and had to be replaced with a regression - capable learning strategy . We chose Support Vector Regression ( SVR ) using libSVM _CITE_ since that is one of the methods employed by Mohler et al . ( 2011 ). However , all other parts of CoMiC - EN such as the processing pipeline and the alignment approach and the extracted features remained the same .__label__Method|Tool|Use
http :// nlp . stanford . edu / software / dependencies Data set : We use the data set from ( Fader et al ., 2011 ) which consists of 500 sentences sampled from the Web using Yahoo ’ s random link service . The sentences were labeled both with facts found with KRAKEN and the current version of REVERB . _CITE_ We then paired facts for the same sentence that overlap in at least one of the fact phrase words , in order to present to the judges two different versions of the same fact - often one binary ( REVERB ) and one Nary ( KRAKEN ). Measurements / Instructions : Given a sentence and a fact ( or fact - pair ), we asked two human judges to label each fact as either 1 ) true and complete , 2 ) true and incomplete , or 3 ) false . True and incomplete facts either lack contextual information in the form of arguments that were present in the sentence , or contain underspecified arguments , but are nevertheless valid statements in themselves ( see our examples in Section 1 ).__label__Method|Tool|Use
Features are pruned to less than 10 , 000 according to their pairwise Pearson correlation with the labels . The code is available on GitHub . _CITE_ Section 2 presents related work . Section 3 describes the dataset the methods are evaluated on . Section 4 provides an overview of the architecture of our method and describes the classification method .__label__Method|Code|Produce
They have been built on Reuters English newswire with case left intact . We test versions with 100 , 320 , 1000 and 3 , 200 clusters for Brown , with 25 , 50 , 100 and 200 dimensions for CnW and with 50 and 100 dimensions for HLBL . The Hellinger PCA ( HPCA ) embeddings come from ( Lebret and Collobert , 2014 ) _CITE_ and have been built over the entire English Wikipedia , the Reuters corpus and the Wall Street Journal with all words in lower case . The vocabulary corresponds to the words that appear at least 100 times and normalized frequency is computed with the 10 , 000 most frequent words as context words . We test versions with 50 , 100 and 200 dimensions for H - PCA .__label__Material|Data|Use
− Inkurdish : a new and high - quality translation between Sorani Kurdish and English . − English Kurdish Translation : especially can translate words in Kurmanji and English together . − Freelang _CITE_ : supports 4000 words in kurmanji . It currently has more than 12 , 000 Sorani and 20 , 000 Kurmanji10 articles . One useful application of these entries is to build a parallel collection of named entities across both dialects .__label__Method|Tool|Introduce
, ( p ,,,, r ,,,)} be the set of such problem - reply pairs from across threads in the discussion forum . We are interested in finding a subset C ' of C such that most of the pairs in C ' are problem - solution pairs , and most of those in C − C ' are not so . In short , we would like to find problemsolution pairs from C such that the F - measure _CITE_ for solution identification is maximized . Central to our approach is the assumption of lexical correlation between the problem and solution texts . At the word level , this translates to assuming that there exist word pairs such that the presence of the first word in the problem part predicts the presence / absence of the second word in the solution part well .__label__Method|Algorithm|Use
In addition , we employ additional online word lists to distinguish named entities and function words from potential informal words . As shown in Table 1 , alphabetic sequences in microblogs may refer to Chinese Pinyin or Pinyin abbreviations , rather than English ( e . g ., “ bs ” for bi shi ; “ to despise ”). Hence , we added dictionarybased features to indicate the presence of Pinyin initials , finals and standard Pinyin expansions , using a UK English word list _CITE_ . The final list of dictionary - based features employed are : Statistical Features . We use pointwise mutual information ( PMI ) variant ( Church and Hanks , 1990 ) to account for global , corpus - wide information .__label__Material|Data|Use
A current research interest concerns consistency of performance across different domains . From our experiments , we show that monolingual segmenters cannot produce consistently good results when applied to a new domain . Our pilot investigation into the influence of word segmentation on SMT involves three offthe - shelf Chinese word segmenters including ICTCLAS ( ICT ) Olympic version _CITE_ , LDC segmenter and Stanford segmenter version 2006 - 0511 . Both ICTCLAS and Stanford segmenters utilise machine learning techniques , with Hidden Markov Models for ICT ( Zhang et al ., 2003 ) and conditional random fields for the Stanford segmenter ( Tseng et al ., 2005 ). Both segmentation models were trained on news domain data with named entity recognition functionality .__label__Method|Tool|Use
This paper deal with Arabic language and its variants for the analysis of social media and the collaborative construction of linguistic tools , such as lexical dictionaries and grammars and their exploitation in NLP applications , such as translation technologies . Basically , Arabic is considered as morphologically rich and complex language , which presents significant challenges for NLP and its applications . It is the official language in 22 countries spoken by more than 350 million people around the world _CITE_ . Moreover , Arabic language exists in a state of diglossia where the standard form of the language , Modern Standard Arabic ( MSA ) and the regional dialects ( AD ) live side - by - side and are closely related ( Elfardy and Diab , 2013 ). Arabic has more than 22 variants , refereed a as dialects ; some countries share the same dialects , while many dialects may exist alongside MSA within the same Arab country .__label__Supplement|Document|Introduce
The main system consists of three major constituents – tokenization of the source text , the acquisition of a translation via online APIs and the selection of the best translation from the candidate hypotheses . A visualized workflow of the system is presented in Figure 1 . Currently the system uses three translation APIs ( Google Translate , Bing Translator _CITE_ and LetsMT ), but it is designed to be flexible and adding more translation APIs has been made simple . Also , it is initially set to translate from English into Latvian , but the source and target languages can also be changed to any language pair supported by the APIs . Currently there are three online translation APIs included in the project – Google Translate , Bing Translator and LetsMT .__label__Method|Tool|Use
( Lin and Pantel , 2001 ; Shinyama et al ., 2002 ; Szpektor et al ., 2004 ; Bhagat and Ravichandran , 2008 ), identifying appropriate contexts for their application ( Pantel et al ., 2007 ) and utilizing them for inference ( de Salvo Braz et al ., 2005 ; BarHaim et al ., 2007 ). Although current available rule bases are still quite noisy and incomplete , the progress made in recent years suggests that they may become increasingly valuable for text understanding applications . Overall , applied knowledge - based inference is a prominent line of research gaining much interest , with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions ( KRAQ ) and the planned evaluation of knowledge resources in the forthcoming 5th Recognizing Textual Entailment challenge ( RTE - 5 ) _CITE_ . While many applied systems utilize semantic knowledge via such inference rules , their use is typically limited , application - specific , and quite heuristic . Formalizing these practices seems important for applied semantic inference research , analogously to the role of well - formalized models in parsing and machine translation .__label__Method|Algorithm|Introduce
Data weighting approaches ( Matsoukas et al ., 2009 ; Foster et al ., 2010 ; Huang and Xiang , 2010 ; Phillips and Brown , 2011 ; Sennrich , 2012 ) use a rich feature set to decide on weights for the training data , at the sentence or phrase pair level . For instance , a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight , but sentences in this corpus that appear to be of a general nature might receive higher weights . The 2012 JHU workshop on Domain Adaptation for MT _CITE_ proposed phrase sense disambiguation ( PSD ) for translation model adaptation . In this approach , the context of a phrase helps the system to find the appropriate translation . All of the above work focuses on either TM or LM domain adaptation .__label__Supplement|Website|Introduce
The system is best on Greek and worst on Upper Midwest corpus , and its overall performance for place names is higher than the most of other applications . The KIM Platform provides a novel Knowledge and Information Management ( KIM ) infrastructure and services for automatic semantic annotation , indexing and retrieval of unstructured and semi - structured content . The ontologies and knowledge bases are kept in Semantic repositories based on cutting edge Semantic Web technology and standards , including RDF ( S ) repositories _CITE_ , ontology middleware ( Kiryakov et al , 2002 ) and reasoning . It provides a mature infrastructure for scalable and customizable information extraction as well as annotation and document management , based on GATE ( Cunningham et al ., 2002 ). GATE , a General Architecture for Text Engineering , is developed by the Sheffield NLP group and has been used in many language processing projects ; in particular for Information Extraction in a variety of languages ( Maynard and Cunningham , 2003 ).__label__Material|Data|Introduce
Our final system achieved an improvement of 0 . 79 BLEU points on the development set and 0 . 33 BLEU points on the test set . TER scores are also shown on test set of our final system in table 3 . Note that these results are state - of - the - art when compared to the official results of the 2008 NIST evaluation _CITE_ . The weights of the different corpora are shown in table 4 for the IWSLT and NIST task . In both cases , the weights optimized by CONDOR are substantially different form those obtained when creating an interpolated LM on the source side of the bitexts .__label__Supplement|Document|Compare
Wikipedia articles are freely available under a Creative Commons license , thus providing a convenient source of bilingual comparable corpus . Note that while the training corpus is English – Malay , the trained lookup tool can be applied to texts of any language included in the multilingual dictionary . Malay Wikipedia articles _CITE_ and their corresponding English articles of the same topics were first downloaded . To form the bilingual corpus , each Malay article is concatenated with its corresponding English article as one document . The TDM constructed from this corpus contains 62 993 documents and 67 499 terms , including both English and Malay items .__label__Material|Data|Use
Specifically , for each term , usage examples were extracted from large amounts of Twitter data . Tweets for the video game and film categories were extracted from the TREC Twitter corpus . _CITE_ The less common book and camera cases were extracted from a subset of all tweets from September 1st - 9th , 2012 . For each term , two annotators were given the term , the corresponding topic domain , and 10 randomly selected tweets containing the term . They were then asked to make a binary judgment as to whether the usage of the term in the tweet referred to an instance of the given category .__label__Material|Data|Use
Thus label propagation is especially suited when annotation data is extremely sparse . One reason for mincuts performing badly with few seeds is because they generate degenrate cuts . In order to demonstrate the ease of adaptability of our method for other languages , we used the Hindi WordNet _CITE_ to derive the adjective synonym graph . We selected 489 adjectives at random from a list of 10656 adjectives and this list was annotated by two native speakers of the language . The annotated list was then split 50 - 50 into seed and test sets .__label__Method|Tool|Use
However , the various providing institutions do not use consistent coding schemes to populate these fields which makes it difficult to compare items provided by different institutions . These differences in the information provided by the various institutions form a significant challenge in processing the Europeana data automatically . A data set was created by selecting 300 pairs of items added to Europeana by two providers : Culture Grid and Scran _CITE_ . The items added to Europeana by these providers represent the majority that are in English and they contain different types of items such as objects , archives , videos and audio files . We removed five pairs that did not have any images associated with one of the items .__label__Method|Tool|Use
Then , for each domain we produced two kinds of gold standard taxonomies . WordNet taxonomy Concepts and relationships in the WordNet hypernym - hyponym hierarchy rooted on the corresponding root concept . Combined taxonomy Domain - specific terms and relations from well - known , publicly available , tax onomies other than WordNet : CheBI for Chemicals , “ The Google product taxonomy ” _CITE_ for Foods , the “ Material Handling Equipment ” taxonomy for Equipment , and the “ Taxonomy of Fields and their Subfields ” for Science . Hypernym - hyponym relationships were also gathered from a general purpose resource , the Wikipedia Bitaxonomy ( WiBi ) ( Flati et al ., 2014 ), using a semi - automatic approach . For each domain we first manually identified domain sub - hierarchies from WiBi ( W ); Second we automatically searched for the terms of W in common with the corresponding gold standard G . For each common term t we added in G the taxonomy rooted on t from W . Table 1 shows the resulting number of vertices JV J , i . e ., the number of terms given to the participants , and the number of edges JEJ of the produced gold standard taxonomies for the four target domains .__label__Supplement|Document|Introduce
Therefore , genes should provide a relevant domainspecific description of documents from the genetics literature . The second indexing algorithm that we describe in this paper , know as the Gene Reference Into Function ( GeneRIF ) Related Citations ( GRC ) algorithm , uses “ GeneRIF ” links ( defined in the paragraph below ) to retrieve neighbors for a query document . To form a specific representation of the document , gene names are retrieved by ABGene ( Tanabe and Wilbur , 2002 ) and mapped to Entrez Gene _CITE_ unique identifiers . The mapping was performed with a version of SemRep ( Rindflesch and Fiszman , 2003 ) restricted to human genes . It consists in normalizing the gene name ( switch to lower case , remove spaces and hyphens ) and matching the resulting string to one of the gene names or aliases listed in Entrez Gene .__label__Method|Tool|Use
We implemented this functionality on top of the coreference resolution error analysis toolkit cort ( Martschat et al ., 2015 ). Hence , this toolkit now provides functionality for devising , implementing , comparing and analyzing approaches to coreference resolution . cort is released as open source and is available from the Python Package Index _CITE_ . In this section we briefly describe a structured prediction framework for coreference resolution . The popular mention pair approach ( Soon et al ., 2001 ; Ng and Cardie , 2002 ) operates on a list of mention pairs .__label__Method|Code|Produce
The last row in Table 4 lists the performance of humans on the same task , presented in the next section . Is it difficult for humans to find the word in a sentence that induces bias , given the subtle , often implicit biases in Wikipedia . We used Amazon Mechanical Turk _CITE_ ( AMT ) to elicit annotations from humans for the same 230 sentences from the test set that we used to evaluate the bias detector in Section 3 . 3 . The goal of this annotation was twofold : to compare the performance of our bias detector against a human baseline , and to assess the difficulty of this task for humans . While AMT labelers are not trained Wikipedia editors , under standing how difficult these cases are for untrained labelers is an important baseline .__label__Supplement|Website|Use
All the sentences in the other sets were parsed successfully . For the Japanese side of the data , we first concatenate the function words in the tokenized sentences using a script published by the author of the dataset . Then we re - segment and POStag them using MeCab _CITE_ version 0 . 996 and parse them using CaboCha version 0 . 66 ( Kudo and Matsumoto , 2002 ), both with UniDic . Finally , we modify the CoNLL - format output of CaboCha where some kind of symbols such as punctuation marks and parentheses have dependent words . We chose this procedure for a reasonable compromise between the dataset ’ s default tokenization and the dependency parser we use .__label__Method|Tool|Use
2 [ foot ] Cyber - Physical Systems , DFKI GmbH , Bremen , Germany Aspect terms can be product features but they can also include conditions such as ambience that influences an opinion which have not been addressed in Hu and Liu ( 2004 ). Our system is based on Natural Language Processing ( NLP ) libraries such as the Stanford CoreNLP . _CITE_ The system is heavily based on the Stanford sentiment tree . The sentiment treebank introduced by Socher et al . ( 2013 ) was developed at the University of Stanford to predict the sentiment of movie reviews .__label__Method|Code|Extent
Our maximum entropy classifier is implemented with Maximum Entropy Modeling Toolkit . The classifier parameters : gaussian prior and iterations , are tuned with the development data for different stages respectively . lp solve 5 . 5 _CITE_ is chosen as our ILP problem solver during the post inference stage . The training time of the syntactic and the semantic parsers are 22 and 5 hours respectively , on all training data , with 2 . 0GHz Xeon CPU and 4G memory . While the prediction can be done within 10 and 5 minutes on the development data .__label__Method|Tool|Use
In releasing this data we hope to equip researchers with the data to support numerous research directions going forward . The JCLC is freely available to the research community and accessible via our website . _CITE_ It can be used via a web - based interface for querying the data . Alternatively , the original texts can be downloaded in text format for more advanced tasks . Interest in learning Chinese is rapidly growing , leading to increased research in Teaching Chinese as a Foreign Language ( TCFL ) and the development of related resources such as learner corpora ( Chen et al ., 2010 ).__label__Material|Data|Produce
In releasing this data we hope to equip researchers with the data to support numerous research directions going forward . The JCLC is freely available to the research community and accessible via our website . _CITE_ It can be used via a web - based interface for querying the data . Alternatively , the original texts can be downloaded in text format for more advanced tasks . Interest in learning Chinese is rapidly growing , leading to increased research in Teaching Chinese as a Foreign Language ( TCFL ) and the development of related resources such as learner corpora ( Chen et al ., 2010 ).__label__Supplement|Website|Produce
A review summary database entry generated by the proposed approaches is exemplified in Figure 4 . { restaurant " dali restaurant and tapas bar " : atmosphere ( " wonderful evening ", " cozy atmosphere ", " fun decor ", " romantic date " ) : atmosphere_rating " 4 . 1 " In this project , we substantiate the proposed approach in a restaurant domain for our spoken dialogue system ( Gruenstein and Seneff , 2007 ), which is a web - based multimodal dialogue system allowing users to inquire about information about restaurants , museums , subways , etc . We harvested a data collection of 137 , 569 reviews on 24 , 043 restaurants in 9 cities in the U . S . from an online restaurant evaluation website _CITE_ . From the dataset , 857 , 466 sentences were subjected to parse analysis ; and a total of 434 , 372 phrases ( 114 , 369 unique ones ) were extracted from the parsable subset ( 78 . 6 %) of the sentences . Most pros / cons consist of well - formatted phrases ; thus , we select 3 , 000 phrases extracted from pros / cons as training data .__label__Material|Data|Use
A review summary database entry generated by the proposed approaches is exemplified in Figure 4 . { restaurant " dali restaurant and tapas bar " : atmosphere ( " wonderful evening ", " cozy atmosphere ", " fun decor ", " romantic date " ) : atmosphere_rating " 4 . 1 " In this project , we substantiate the proposed approach in a restaurant domain for our spoken dialogue system ( Gruenstein and Seneff , 2007 ), which is a web - based multimodal dialogue system allowing users to inquire about information about restaurants , museums , subways , etc . We harvested a data collection of 137 , 569 reviews on 24 , 043 restaurants in 9 cities in the U . S . from an online restaurant evaluation website _CITE_ . From the dataset , 857 , 466 sentences were subjected to parse analysis ; and a total of 434 , 372 phrases ( 114 , 369 unique ones ) were extracted from the parsable subset ( 78 . 6 %) of the sentences . Most pros / cons consist of well - formatted phrases ; thus , we select 3 , 000 phrases extracted from pros / cons as training data .__label__Supplement|Website|Use
Experiments on a large - scale real - world Twitter dataset show that Twitter - BTM outperforms several stateof - the - art baselines . In recent years , short texts are increasingly prevalent due to the explosive growth of online social media . For example , about 500 million tweets are published per day on Twitter _CITE_ , one of the most popular online social networking services . Probabilistic topic models ( Blei et al ., 2003 ) are broadly used to uncover the hidden topics of tweets , since the low - dimensional semantic representation is crucial for many applications , such as product recommendation ( Zhao et al ., 2014 ), hashtag recommendation ( Ma et al ., 2014 ), user interest tracking ( Sasaki et al ., 2014 ), sentiment analysis ( Si et al ., 2013 ). However , the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts .__label__Supplement|Website|Introduce
The second evaluation task consisted in using our baseline system for classifying user product reviews into positive or negative opinions . For this task we used a corpus of 400 user reviews of products such as cars , hotels , dishwashers , books , cellphones , music , computers and movies , extracted from the Spanish website Ciao . es . _CITE_ This is the same corpus used by Brooke ( 2009 ), who employed sentiment analysis tools in English to score Spanish texts after performing machine translation . On Ciao . es , users may enter their written reviews and associate a numeric score to them , ranging from 1 to 5 stars . For this evaluation task , we made the assumption that there was a strong relation between the written reviews and their corresponding numeric scores .__label__Material|Data|Use
The second evaluation task consisted in using our baseline system for classifying user product reviews into positive or negative opinions . For this task we used a corpus of 400 user reviews of products such as cars , hotels , dishwashers , books , cellphones , music , computers and movies , extracted from the Spanish website Ciao . es . _CITE_ This is the same corpus used by Brooke ( 2009 ), who employed sentiment analysis tools in English to score Spanish texts after performing machine translation . On Ciao . es , users may enter their written reviews and associate a numeric score to them , ranging from 1 to 5 stars . For this evaluation task , we made the assumption that there was a strong relation between the written reviews and their corresponding numeric scores .__label__Supplement|Website|Use
We experimented on six languages from diverse language families . The segmentation data for English , Finnish and Turkish was taken from MorphoChallenge 2010 ( Kurimo et al ., 2010 ). _CITE_ Despite typically being used for UMS tasks , the MorphoChallenge datasets do contain morpheme level labels . The German data was extracted from the CELEX2 collection ( Baayen et al ., 1993 ). The Zulu data was taken from the Ukwabelana corpus ( Spiegler et al ., 2010 ).__label__Material|Data|Use
Z & C08 refers to the results of Zhang and Clark ( 2008b ). They use a hybrid model to combine the advantages of both graph - based and transition - based models . We also do experiments with two publicly available and widely - used parsers , MSTParser and MaltParser _CITE_ . MSTParser1 refers to the first - order graph - based model of McDonald et al . ( 2005 ), while MSTParser2 is the second - order model of McDonald and Pereira ( 2006 ).__label__Method|Tool|Use
Finally , the non - text consuming & lt ; EmLink & gt ; link puts in relation the cause emotion event or phrase with the emotion keyword . The annotation has been performed by two expert linguists and validated by a judge . The tool used for the annotation is the Brandeis Annotation Tool ( BAT ) _CITE_ . The corpus is currently under annotation and we concentrated mainly on the development of a test set . Not all markables and attributes have been annotated in this phase .__label__Method|Tool|Use
While user session segmentation can be improved with more sophisticated algorithms , this simple low - cost heuristic performs adequately for our purposes . We then move on to map queries to Freebase and empirically filter sessions that are less entity - centric . We use an annotation tool especially for short text ( Ferragina and Scaiella , 2012 ) called Tagme _CITE_ to recognize entities and observe only 16 % of all the queries are exactly an entity itself , which means most of queries do have refiner words to convey information need . To ensure the precision of recognized entities , we set a significant threshold and bottom line threshold , queries should have at least one recognized entity with a likelihood above significant level , and those below bottom line are ignored . They are 0 . 19 and 0 . 05 in our work , which may vary with entity recognition method .__label__Method|Tool|Use
In this task , Yamcha obtained the best performance for a quadratic kernel with a c value of 0 . 5 . All results presented here use this setting . For CRF , we used the Mallet _CITE_ software package . Experiments are done only with order - 0 CRFs . CRFs proved to marginally improve the prediction accuracy while substantially improving the speed .__label__Method|Code|Use
The current version of the system does not include really language - specific techniques : we neither split German compounds , nor do we address the peculiarities of Czech mentioned above . Our translation system is built around Moses ( Koehn et al ., 2007 ). Two - way word alignment was computed using GIZA ++ _CITE_ ( Och and Ney , 2003 ), and alignment symmetrization using the grow - diag - final - and heuristic ( Koehn et al ., 2003 ). Weights of the system were optimized using MERT ( Och , 2003 ). No lexical reordering model was trained .__label__Method|Tool|Use
FreebaseEasy is a processed version of Freebase ( Bollacker et al ., 2008 ), which contains a unique meaningful name for every entity , together with canonical binary relations . For our experiments , we selected only the sentences containing at least two entities linked to FreebaseEasy , which corresponded to 1 . 2 million sentences . With the full articles set , we computed word embeddings with the skip - gram model using the word2vec _CITE_ implementation from Mikolov et . al . ( 2013a ).__label__Method|Code|Use
Finally , the corpus is applied in several tasks , such as generation of emotion object ontology or retrieval of emotional and moral consequences of actions . There is a lack of large corpora for Japanese applicable in sentiment and affect analysis . Although there are large corpora of newspaper articles , like Mainichi Shinbun Corpus , or corpora of classic literature , like Aozora Bunko _CITE_ , they are usually unsuitable for research on emotions since spontaneous emotive expressions either appear rarely in these kinds of texts ( newspapers ), or the vocabulary is not up to date ( classic literature ). Although there exist speech corpora , such as Corpus of Spontaneous Japanese , which could become suitable for this kind of research , due to the difficulties with compilation of such corpora they are relatively small . In research such as the one by Abbasi and Chen ( 2007 ) it was proved that public Internet services , such as forums or blogs , are a good material for affect analysis because of their richness in evaluative and emotive information .__label__Material|Data|Introduce
Section 3 . 3 shows how to rank event causality candidates . We extract the event causality between two events represented by two phrases from single sentences that are dependency parsed . _CITE_ We obtained sentences from 600 million web pages . Each phrase in the event causality must consist of a predicate with an argument position ( template , hereafter ) like conduct X and a noun like slash - and - burn agriculture that completes X . We also require the predicate of the cause phrase to syntactically depend on the effect phrase in the sentence from which the event causality was extracted ; we guarantee this by verifying the dependencies of the original sentence .__label__Material|Data|Use
The detailed definition of this measure as applied for each formalism is provided in ( Clark and Curran , 2003 ; Miyao and Tsujii , 2008 ; Cahill et al ., 2004 ). For CCG , we use the evaluation script from the C & C tools . _CITE_ For HPSG , we evaluate all types of dependencies , including punctuations . For LFG , we consider the preds - only dependencies , which are the dependencies between pairs of words . Secondly , we also evaluate using unlabeled PARSEVAL , a standard measure for PCFG parsing ( Petrov and Klein , 2007 ; Charniak and Johnson , 2005 ; Charniak , 2000 ; Collins , 1997 ).__label__Method|Code|Use
We use SVMUght ( Joachims , 1999 ) to learn a linear - kernel classifier on pairwise examples in the training set . When resolving pronouns , we select the candidate with the farthest positive distance from the SVM classification hyperplane . Our training set is the anaphora - annotated portion of the American National Corpus ( ANC ) used in Bergsma ( 2005 ), containing 1270 anaphoric pronouns _CITE_ . We test on the ANC Test set ( 1291 instances ) also used in Bergsma ( 2005 ) ( highest resolution accuracy reported : 73 . 3 %), the anaphoralabelled portion of AQUAINT used in Cherry and Bergsma ( 2005 ) ( 1078 instances , highest accuracy : 71 . 4 %), and the anaphoric pronoun subset of the MUC7 ( 1997 ) coreference evaluation formal test set ( 169 instances , highest precision of 62 . 1 reported on all pronouns in ( Ng and Cardie , 2002 )). These particular corpora were chosen so we could test our approach using the same data as comparable machine - learned systems exploiting probabilistic information sources .__label__Material|Data|Use
We conclude this paper with a discussion of the experimental methodology and an outlook . Current survey articles cover the spectrum of recent methods and results for biomedical named entity recognition and identification ( Cohen and Hersh , 2005 ; Leser and Hakenberg , 2005 ). A recent assessment of named entity recognition and identification was done during the BioCreAtIvE 2 evaluation _CITE_ . Official results will be available in April 2007 . Naturally , a number of systems proposed before are highly related to the method presented in this paper .__label__Method|Algorithm|Use
We only need to parse the source side of the tuning and test sets , and the only features that look at the source - side parse are those from § 4 . 3 . To obtain Brown clusters for the target tree features in § 4 . 1 , we used code from Liang ( 2005 ). _CITE_ We induced 100 clusters from the English side of the parallel corpus concatenated with 10M words of randomly - selected Gigaword sentences . Only words that appeared at least twice in this data were considered during clustering . An additional cluster was created for all other words ; this allowed us to use phrase dependency cluster features even for out - ofvocabulary words .__label__Method|Code|Use
To verify the semantics captured by the proposed models during training and to tune the hyperparameters , we used the WordSim - 353 word similarity data set ( Finkelstein et al ., 2001 ). The learned models were evaluated on four tasks of measuring the semantic similarity between short phrases . We performed evaluation using the three tasks ( AN , NN , and VO ) in the dataset _CITE_ provided by Mitchell and Lapata ( 2010 ), and the SVO task in the dataset provided by Grefenstette and Sadrzadeh ( 2011 ). The datasets include pairs of short phrases extracted from the BNC . AN , NN , and VO contain 108 phrase pairs of adjective - noun , nounnoun , and verb - object .__label__Material|Data|Use
The strategy was tested on both English and Bengali . The intension behind choosing two distinct language families is to establish the credibility of the proposed methods . For English we choose the widely used MPQA _CITE_ corpus , but for the Bengali we had to create our own corpus as discussed in the following section . The remainder of the paper then concentrates on the problems with using prior polarity values only , in Section 4 , while the Sentimantics concept proper is discussed in Section 5 . Finally , some initial conclusions are presented in Section 6 .__label__Material|Data|Use
We call this model Relgrams , as it can be seen as a relational analog to the n - grams language model . We extract relational triples from each sentence in a large corpus using the OLLIE Open IE system ( Mausam et al ., 2012 ). _CITE_ This provides relational tuples in the format ( Arg1 , Relation , Arg2 ) where each tuple element is a phrase from the sentence . The sentence “ He cited a new study that was released by UCLA in 2008 .” produces three tuples : Relational triples provide a more specific representation which is less ambiguous when compared to ( subj , verb ) or ( verb , obj ) pairs . However , using relational triples also increases sparsity .__label__Method|Tool|Use
In particular , in this paper we will show how vectorial and structured data can be exploited by KELP in three NLP tasks : Twitter Sentiment Analysis , Text Categorization and Question Classification . KELP is a machine learning library completely written in Java . The Java language has been chosen in order to be compatible with many Java NLP / IR tools that are developed by the commu nity , such as Stanford CoreNLP , OpenNLP _CITE_ or Lucene . KELP is released as open source software under the Apache 2 . 0 license and the source code is available on github . Furthermore it can be imported via Maven .__label__Method|Tool|Use
In the third step , the algorithm reduces pairs with non - domainspecific problem targets from the set PWTs using semantic knowledge . The pair pair ( pwi7 tj ) is reduced if there is no term with a domain category in the hierarchy of WordNet synsets induced by the hypernym , hyponym , or holonym relations with target tj . For our experiments we collected 734 sentences from the HP website _CITE_ . We employed 953 sentences from Amazon reviews about automobile products . Of the total sentences , 1 , 288 sentences ( 506 + 782 from electronic and automobile domains ) were classified as problem sentences , and 399 ( 228 + 171 ) sentences were labeled as part of the no - problem class .__label__Material|Data|Use
In the third step , the algorithm reduces pairs with non - domainspecific problem targets from the set PWTs using semantic knowledge . The pair pair ( pwi7 tj ) is reduced if there is no term with a domain category in the hierarchy of WordNet synsets induced by the hypernym , hyponym , or holonym relations with target tj . For our experiments we collected 734 sentences from the HP website _CITE_ . We employed 953 sentences from Amazon reviews about automobile products . Of the total sentences , 1 , 288 sentences ( 506 + 782 from electronic and automobile domains ) were classified as problem sentences , and 399 ( 228 + 171 ) sentences were labeled as part of the no - problem class .__label__Supplement|Website|Use
Each label candidate was rated in this way by at least 10 annotators , and ratings from annotators who passed the filter were combined by averaging them . A sample of topics , label candidates , and the average rating is presented in Table 1 . _CITE_ Finally , we train the regression model over all the described features , using the human rating - based ranking . In this section we present our experimental results for the topic labelling task , based on both the unsupervised and supervised methods , and the methodology of Mei et al . ( 2007 ), which we denote MSZ for the remainder of the paper .__label__Supplement|Document|Produce
( 2014 ), BITEXTOR and ILSP - FC have shown to be complementary , and combining both tools leads to a larger amount of parallel data . ILSP - FC ( Papavassiliou et al ., 2013 ) is a modular crawling system allowing to easily acquire domain - specific and generic corpora from the Web . _CITE_ This crawler includes a de - duplicator which checks all documents in a pairwise manner to identify near - duplicates . This is achieved by comparing the quantised word frequencies and the paragraphs of each pair of candidate duplicate documents . A document - pair detector also examines each document in the same manner and identifies pairs of documents that could be considered parallel .__label__Method|Tool|Introduce
Organizing bioscience images is not a new task . Related work includes the building of domainspecific image databases . For example , the Protein Data Bank ( PDB ) _CITE_ ( Sussman et al ., 1998 ) stores 3 - D images of macromolecular structure data . WebPath is a medical web - based resource that has been created by physicians to include over 4 , 700 gross and microscopic medical images . Textbased image search systems like Google ignore image content .__label__Material|Data|Introduce
Recent studies show that language varieties can be discriminated automatically using words or characters as features ( Zampieri and Gebre , 2012 ; Lui and Cook , 2013 ) . However , due to performance limitations , state - of - the - art general - purpose language identification systems do not distinguish texts from different national varieties , modelling pluricentric languages as unique classes . To evaluate how state - of - the - art systems perform in identifying similar languages and varieties , we decided to organize the Discriminating between Similar Languages ( DSL ) _CITE_ shared task . This shared task was organized within the scope of the workshop on Applying NLP Tools to Similar Languages , Varieties and Dialects ( VarDial ) in the 2014 edition of COLING . The motivation behind the DSL shared task is two - fold .__label__Supplement|Website|Produce
In this paper , the YAiTRON dictionary is exploited to assign the entity tags . YAiTRON : Yet Another ( Lex ) iTRON is a Thai - English and English - Thai dictionary data , stored in a wellformed XML format . YAiTRON is a homogeneous structure dictionary , adapted from National Electronics and Computer Technology Center ( NECTEC _CITE_ )’ s LEXiTRON dictionary . YAiTRON covers 32 , 350 unique words with 13 parts - of - speech i . e ., adjective ( ADJ ), adverb ( ADV ), auxiliary verb ( AUX ), classifier ( CLAS ), conjunction ( CONJ ), determiner ( DET ), end ( END ), interjection ( INT ), noun ( NOUN ), preposition ( PREP ), pronoun ( PRON ), question phrase ( QUE ), and verb ( VERB ). There are some tokens that always have only one PoS when beginning with some specific texts .__label__Material|Data|Extent
Grefenstette ( 1995 ) compared this approach to Ingle ( 1978 ), based on the frequency of short words . The interested reader is referred to Zampieri ( 2013 ) for a review of some statistical and machine learning proposals and to both Baldwin and Lui ( 2010 ) and Lui and Baldwin ( 2011 ) for an overview of some linguistically motivated models . As Baldwin and Lui ( 2010 ) or Tiedemann and Ljubeˇsi ´ c ( 2012 ) point out , language identification is erroneously considered an easy and solved problem , in part because of some general purpose systems being available , notably TextCat _CITE_ , Xerox Language Identifier and , more recently , langid . py ( Lui and Baldwin , 2012 ). While it is true that it is possible to obtain brilliant results for a small number of languages ( Baldwin and Lui , 2010 ) or typologically distant languages ( Zampieri et al ., 2013 ), accurately discriminating among closely related languages or varieties of the same language has been repeatedly reported as a bottleneck for language identification systems , in particular for those based on n - grams . Back in 2004 , Padr ´ o and Padr ´ o concluded that “ since the tested systems tend to fail when distinguishing similar languages ( e . g .__label__Method|Tool|Introduce
( 2006 ) presented a probabilistic model for taxonomy induction which considers as features paths in parse trees between related taxonomy nodes . They show that the best performing taxonomy was the one adding 30 , 000 hyponyms to WordNet . We created an entailment rule for each new hyponym added to WordNet by their algorithm _CITE_ . LCC ’ s extended WordNet ( XWN & apos ;°): In ( Moldovan and Rus , 2001 ) WordNet glosses were transformed into logical form axioms . From this representation we created a rule e ’ ==& gt ;- e for each e0 in the gloss which was tagged as referring to the same entity as e . CBC : A knowledgebase of labeled clusters generated by the statistical clustering and labeling algorithms in ( Pantel and Lin , 2002 ; Pantel and Ravichandran , 2004 ) .__label__Method|Algorithm|Use
The algorithm works by “ performing a global minimization of a Levenshtein distance function which weights the cost of correct words , insertions , deletions and substitutions as 0 , 75 , 75 and 100 respectively . The computational complexity of DP is O ( MN ).” The Carnegie Mellon University provides an implementation of the algorithm in its speech recognition toolkit . We use an adaptation of it which allows working on lists of strings _CITE_ rather than directly on strings ( as sequences of characters ). Each email is processed as a sequence of sentences . We choose to define the segmentation problem as a sequence labelling task whose aim is to assign the globally best set of labels for the entire sequence at once .__label__Method|Code|Use
It is an Eclipse based ontology development environment . There are many plugins available that extend NeOn toolkit ’ s functionality . For instance , the GATE Webservice plugin _CITE_ and its TermRaider component automatically generate ontological information . One of the plugins for NeOn is the work by Cimiano and V ¨ olker , who proposed Text2Onto ( Cimiano and V ¨ olker , 2005 ), which is a framework that allows to apply ontology learning and change discovery algorithms . Its central data structure is called probabilistic ontology model ( POM ).__label__Supplement|Website|Introduce
Statistical Machine Translation and Training Dataset For our translation task , we use the statistical translation toolkit Moses ( Koehn et al ., 2007 ), where the word alignments were built with the GIZA ++ toolkit ( Och and Ney , 2003 ). The SRILM toolkit ( Stolcke , 2002 ) was used to build the 5 - gram language model . For a broader domain coverage of the generic training dataset necessary for the SMT system , we merged parts of JRC - Acquis 3 . 0 ( Steinberger et al ., 2006 ), Europarl v7 _CITE_ ( Koehn , 2005 ) and OpenSubtitles201310 ( Tiedemann , 2012 ), obtaining a training corpus of 1 . 9M sentences , con taining around 38M running words ( Table 1 ). 11 The generic SMT system , trained on the concatenated 1 . 9 sentences , is used as a baseline , which we compare against the domain - specific models generated with different sentence selection methods . Furthermore we use the generic SMT system in combination with the smaller domainspecific models to evaluate different approaches when combining generic and domain - specific data together . We additionally compare our results to an SMT system built on an existing domain - specific parallel dataset , i . e .__label__Material|Data|Extent
The HDP and UKP systems use Wikipedia as raw text for sampling word counts ; DULUTH - SYS9 - PK2 uses the first 10 , 000 paragraphs of the Associated Press wire service data from the English Gigaword Corpus ( Graff , 2003 , 1st edition ), whereas DULUTH - SYS1 - PK2 and DULUTH - SYS7PK2 both use the snippets for inducing the query senses . Finally , the UKP systems were the only ones to retrieve the Web pages from the corresponding URLs and exploit them for WSI purposes . They also use WaCky ( Baroni et al ., 2009 ) and a distributional thesaurus obtained from the Leipzig Corpora Collection _CITE_ ( Biemann et al ., 2007 ). SATTYAPPROACH1 just uses snippets . The only participating WSD system , RAKESH , uses the YAGO hierarchy ( Suchanek et al ., 2008 ) together with DBPedia abstracts ( Bizer et al ., 2009 ).__label__Material|Data|Use
by Fleischman et al . ( 2003 ). The model is trained with the estimate software , which implements the LMVM algorithm ( Malouf , 2002 ) _CITE_ . The second learner is an instance of a memory - based learning ( MBL ) algorithm , the nearest neighbour algorithm . We use the implementation provided by TiMBL ( Daelemans et al ., 2003 ) with the recommended parameters , namely , adopting modified value difference with gain ratio feature weighting as similarity metric .__label__Method|Tool|Use
As they offer fast and simple means for adding and editing content , they are used for various purposes such as creating encyclopedias ( e . g . Wikipedia ), constructing dictionaries ( e . g . Wiktionary _CITE_ ), or hosting online communities ( e . g . ACLWiki ). However , as wikis do not enforce their users to structure pages or add complementary metadata , wikis often end up as a mass of unmanageable pages with meaningless page titles and no usable link structure ( Buffa , 2006 ).__label__Supplement|Website|Introduce
We sampled between 10 and 1000 training triples from the other half ( Figure 4 ). Distributional semantic models ( Turney and Pantel , 2010 ) encode word meaning in a vector format by counting co - occurrences with other words within a specified context window . We constructed the vectors from the October 2013 dump of Wikipedia articles , which was tokenised using the Stanford NLP tools _CITE_ , lemmatised with the Morpha lemmatiser ( Minnen et al ., 2001 ), and parsed with the C & C parser ( Clark and Curran , 2007 ). In this paper we use sentence boundaries to define context windows and the top 10 , 000 most frequent lemmatised words in the whole corpus ( excluding stopwords ) as context words . The raw co - occurrence counts are re - weighted using the standard tTest weighting scheme ( Curran , 2004 ), where fwicj is the number of times target noun wi occurs with context word cj : and p (, cj ) Ek El fwkcl .__label__Method|Tool|Use
Since this is generation from bag - of - words , the task is known to be at the high - complexity extreme of the run - time behavior of our algorithms . As such , we consider it a good test for the ability of our algorithms to scale up to increasingly complex inputs . We use a state - of - the - art , publicly available toolkit _CITE_ to train a trigram language model using Kneser - Ney smoothing , on 10 million sentences ( 170 million words ) from the Wall Street Journal ( WSJ ), lower case and no final punctuation . The test data is also lower case ( such that upper - case words cannot be hypothesized as first words ), with final punctuation removed ( such that periods cannot be hypothesized as final words ), and consists of 2000 unseen WSJ sentences of length 3 - 7 , and 2000 unseen WSJ sentences of length 10 - 25 . The algorithms we tested in this experiments were the ones presented in Section 3 . 2 , plus two baseline algorithms .__label__Method|Tool|Use
We believe that performance of this algorithm on our data reflects the state - of - the - art in content error correction . Next , we show how learner data and distribution of confusion pairs can be used to improve the performance of this algorithm . In our experiments , we use three publicly - available datasets of learner errors in AN combinations : the AN dataset extracted from the Cambridge Learner Corpus ( CLC ) _CITE_ and annotated with respect to the learner errors in the choice of adjectives and nouns ; the AN dataset extracted from the CLCFCE dataset ; and the set of errors in ANs that we have extracted for the purposes of this work from the training and development sets used in the CoNLL2014 Shared Task on Grammatical Error Correction . We discuss these datasets below . We use the dataset of AN combinations released by Kochmar and Briscoe ( 2014 ).__label__Material|Data|Use
One important pedagogical consequence of the notion of scenario is that it allows users to learn both a work process and a foreign language . Another scenario , we are currently working on , is targeted to people working at hotels reception desk . This scenario is developed in the framework of an eContent European project , Thetis ( http :// www . thetis - project . org /), which groups together a language publisher ( Q group ), a training company specialized in tourism ( Grupo GDT _CITE_ ) and a research center ( Xerox Research Centre Europe ). To summarize , while using linguistic technologies , students can process online texts on the fly and can work on their own documents . This strengthens the personalization aspect of the solution .__label__Supplement|Website|Introduce
In this work , we focus on the contents of a user ’ s posts and replies and their timeline of activities , rather than the network of friendship links ( which is sparse in forum - based social networks as compared to friendship - based social networks like Facebook ). We also focus on active participation , such as initiating a thread or posting a reply , and not on passive participation , such as simply viewing the forum ( since such passive information is only available to administrators of the support group service ). Our data is collected from DailyStrength _CITE_ , one of the largest support group based online social networks with more than 500 support groups based on the physical and mental ailments of its users . Users in these support groups can either post , creating a new thread on a new topic , or they can reply to a thread that someone else has created . We focused on 20 support groups : Acne , ADHD ( Attention Deficit Hyperactivity Disorder ), Alcoholism , Asthma , Back Pain , Bipolar Disorder , Bone Cancer , COPD ( Chronic Obstructive Pulmonary Disease ), Diets and Weight Maintenance , Fibromyalgia , Gastric and Bypass Surgery , Immigration Law , Infertility , Loneliness , Lung Cancer , Migraine , Miscarriage , Pregnancy , Rheumatoid Arthritis and War in Iraq .__label__Material|Data|Use
To identify metaphorical use , we assume that it results in unusual semantic patterns between the metaphor and its dependencies . To identify these cases , we use SVMs with tree - kernels on a balanced corpus of 3872 instances , created by bootstrapping from available metaphor lists . _CITE_ We outperform two baselines , a sequential and a vectorbased approach , and achieve an F1 - score of 0 . 75 . A metaphor is a figure of speech used to transfer qualities of one concept to another , as in “ He is such a sweet person ”. Here , the qualities of “ sweet ” ( the source ) are transferred to a person ( the target ).__label__Material|Data|Use
We avoid distinguishing between Argument and Non - Argument segments at this stage , instead assuming that any segments left unconnected are after the structure has been identified are Non - Argument . http :// www . nltk . org / In order to establish these links , we first consider that in many cases an argument can be represented as a tree . This assumption is supported by around 95 % of the argument analyses contained in AIFdb ( Lawrence et al ., 2012 ) as well as the fact that many manual analysis tools including Araucaria ( Reed and Rowe , 2004 ), iLogos _CITE_ , Rationale ( Van Gelder , 2007 ) and Carneades ( Gordon et al ., 2007 ), limit the user to a tree format . Furthermore , we assume that the argument tree is generated depth first , specifically that the conclusion is presented first and then a single line of supporting points is followed as far as possible before working back up through the points made . The assumption is grounded in work in computational linguistics that has striven to produce natural - seeming argument structures ( Reed and Long , 1997 ).__label__Method|Tool|Introduce
2013 . The DDI Corpus : an annotated corpus with pharmacological substances and drug - drug interactions , submitted to BioInformatics more detailed description , the reader is directed to our annotation guidelines . _CITE_ For evaluation , a part of the DDI corpus consisting of 52 documents from DrugBank and 58 MedLine abstracts , is provided with the gold annotation hidden . The goal for participating systems is to recreate the gold annotation . Each participant system must output an ASCII list of reported entities , one per line , and formatted as : IdSentence | startOffset - endOffset | text | type Thus , for each recognized entity , each line must contain the id of the sentence where this entity appears , the position of the first character and the one of the last character of the entity in the sentence , the text of the entity , and its type .__label__Supplement|Document|Produce
Other systems can then build on top of these requests . One example for an architecture that includes CTS capabilities in a wider framework is CITE which is explained in the next section . More information can be found at http :// www . homermultitext . org / hmt - docs / specifications / cts / 2 . 3 CTS in the Context of the CITE Architecture The Collections , Indexes and Texts ( CITE ) architecture is a large framework for reference to the objects of study in Digital Humanities _CITE_ . The general design philosophy is to use URNs as a modern way of encoding citations . Besides providing a general framework for referencing objects and texts , with the latter task being implemented by CTS , CITE also defines a standard for encoding relations between references .__label__Supplement|Website|Introduce
or word vectors trained by the model of Collobert and Weston ( 2008 ) and provided by Turian et al . ( 2010 ). _CITE_ These vectors were trained on an unlabeled corpus of the English Wikipedia . Note that alternatives such as Brown clusters are not suitable since they do not capture sentiment information ( good and bad are usually in the same cluster ) and cannot be modified via backpropagation . The confessions section of the experience project website lets people anonymously write short personal stories or “ confessions ”.__label__Material|Data|Introduce
[ foot ] 9 [ foot ] http :// trac . loria . fr /- semconst More information about the requirements and installation procedure is available at http :// trac . loria . fr /- semtag . Note that this toolbox is made of two main components : the GenI [ foot ] 8 [ foot ] system and the SemConst [ foot ] 9 [ foot ] system , which respectively performs generation and parsing from common linguistic resources . The first is written in Haskell ( except the XMG part written in Oz ) and is multi - platform ( Linux , Windows , Mac OS ).__label__Supplement|Document|Produce
Semantic These features explore the polysemy of target and source words , i . e . the number of senses existing as entries in a WordNet for a given target word ti or a source word si . We employ the Universal WordNet , _CITE_ which provides access to WordNets of various languages . Pseudo - reference This binary feature explores the similarity between the target sentence and a translation for the source sentence produced by another MT system . The feature is 1 if the given word ti in position i of a target sentence 5 is also present in a pseudo - reference translation R . In our experiments , the pseudo - reference is produced by Moses systems trained over parallel corpora .__label__Method|Tool|Use
This ensures that a real relation will be preferred over NR if and only if its probability is greater than 0 . 5 , otherwise nothing will change . We conducted experiments to compare our cardpyramid parsing approach for joint entity and relation extraction to a pipelined approach . We used the dataset _CITE_ created by Roth & Yih ( 2004 ; 2007 ) that was also used by Giuliano et el . ( 2007 ). The sentences in this data were taken from the TREC corpus and annotated with entities and relations .__label__Material|Data|Use
Reliable blog classification is an important task in the blogosphere as it allows researchers , ping feeds ( used to broadcast blog updates ), trend analysis tools and many others to separate real blog content from blog - like content such as bulletin boards , newsgroups or trade markets . It is a task that so far has proved difficult as can be witnessed by checking any of the major blog update feeds such as weblogs . com or blo . gs . _CITE_ Both will at any given time list content that clearly is not a blog . In this paper we will explore blog classification using machine learning to improve blog detection and experiment with several methods to try and further improve the percentage of instances classified correctly . The main research question we address in this paper is exploratory in nature : - How hard is binary blog classification ?__label__Supplement|Website|Introduce
We also follow Wang et al . ( 2012 )’ s conventions and apply rulesets to preprocess the corpus ’ URLs , emoticons , “@ usernames ” and Hashtags as pre - segmented words , before input to CWS and IWR . For the CWS task , the first author manually labelled the same corpus following the segmentation guidelines published with the SIGHAN - 5 _CITE_ MSR dataset . We implemented several baseline systems to compare with proposed FCRF joint inference method . Existing Systems .__label__Material|Data|Use
In addition , main process contains two subprocesses : special process and general process . It performs CGED in the following steps : Preprocess in the system contains two modules : Chinese word segmentation and part - of - speech tagging . We uses “ Jieba ” Chinese text segmentation and NLPIR / ICTCLAS Chinese text segmentation _CITE_ to achieve the goal . A set of chunks with POS will be generated after this process . Main process contains two subprocess : special process and general process .__label__Method|Tool|Use
The model used with BART is a maximum entropy ranker trained on the ACE02 - npaper corpus ( LDC2003T11 ). In order to obtain a probability distribution over antecedent candidates rather than onebest predictions or coreference sets , we modified the ranking component with which BART resolves pronouns to normalise and output the scores assigned by the ranker to all candidates instead of picking the highest - scoring candidate . In order to create a simple , but reasonable baseline for our task , we trained a maximum entropy ( ME ) classifier with the MegaM software package _CITE_ using the features described in the previous section and the anaphora links found by BART . Results are shown in Table 2 . The baseline results show an overall higher accuracy for the TED data than for the newscommentary data .__label__Method|Tool|Use
While POS taggers such as TreeTagger are common , and there some supertaggers are available , notably that of Clark and Curran ( 2007 ) for CCG , no standard supertagger exists for HPSG . Consequently , we developed a Maximum Entropy model for supertagging using the OpenNLP implementation . _CITE_ Similarly to Zhang and Kordoni ( 2006 ), we took training data from the gold – standard lexical types in the treebank associated with ERG ( in our case , the July - 07 version ). For each token , we extracted features in two ways . One used features only from the input string itself : four characters from the beginning and end of the target word token , and two words of context ( where available ) either side of the target .__label__Method|Code|Use
observed this tendency in previous research as well ; in one experiment , reported in Owczarzak et al . ( 2006 ), where the rule - based system Logomedia was compared with Pharaoh , BLEU scored Pharaoh 0 . 0349 points higher , NIST scored Pharaoh 0 . 6219 points higher , but human judges scored Logomedia output 0 . 19 points higher ( on a 5 - point scale ). In order to check for the existence of a bias in the dependency - based metric , we created a set of 4 , 000 sentences drawn randomly from the SpanishEnglish subset of Europarl ( Koehn , 2005 ), and we produced two translations : one by a rule - based system Logomedia , and the other by the standard phrase - based statistical decoder Pharaoh , using alignments produced by GIZA ++ _CITE_ and the refined word alignment strategy of Och and Ney ( 2003 ). The translations were scored with a range of metrics : BLEU , NIST , GTM , TER , METEOR , and the dependency - based method . Besides the ability to allow syntactic variants as valid translations , a good metric should also be able to accept legitimate lexical variation .__label__Method|Tool|Use
The data we work with is a collection of financial news consolidated and distributed by Yahoo ! Fi See the DUC 2007 and 2008 update tracks . nance from various sources _CITE_ . Each story is labeled as being relevant for a company – i . e ., it appears in the company ’ s RSS feed – if the story mentions either the company itself or the sector the company belongs to . Altogether the corpus contains 88 , 974 news articles from a period of about 5 months ( 148 days ).__label__Material|Data|Use
In this paper , we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks . Our title already gave away what we discovered . Both count and predict models are extracted from a corpus of about 2 . 8 billion tokens constructed by concatenating ukWaC , the English Wikipedia _CITE_ and the British National Corpus . For both model types , we consider the top 300K most frequent words in the corpus both as target and context elements . We prepared the count models using the DISSECT toolkit .__label__Material|Data|Use
The graphical illustration in Figure 3 explains the situation . The observation is that most of the negative tags are coming from the middle - east and especially from the Islamic countries . We found a line in Wiki _CITE_ ( see in Religion Section ) that may give a good explanation : “ Blue in Islam : In verse 20 : 102 of the Qur ’ an , the word cjj � zurq ( plural of azraq ' blue ') is used metaphorically for evil doers whose eyes are glazed with fear ”. But other explanations may be there for this . This is an interesting observation that supports the effectiveness of PsychoSentiWordNet .__label__Supplement|Website|Use
The corpus was first developed and cleansed during pre - processing . Thereafter , various features were extracted and the resulting data was fed to machine learning algorithms . We have used Weka 3 . 7 . 7 _CITE_ for our classification experiments . Weka is an open source data mining tool . It presents collection of machine learning algorithms for data mining tasks .__label__Method|Tool|Use
We use LIBLINEAR ( Fan et al ., 2008 ) for classification , with penalty factors 3 and 1 for inanimate and animate classes , respectively , because the training data are unbalanced . We compare the following representations for animacy classification of markables . ( i ) Phrase embedding : Skip - bigram embeddings with skip distance k = 2 and 2 < k < 3 ; ( ii ) Word embedding : concatenation of the embeddings of the two enclosing words where the embeddings are either standard word2vec embeddings ( see Section 2 ) or the embeddings published by ( Collobert et al ., 2011 ); _CITE_ ( iii ) the one - hot vector representation of a SkipB : the concatentation of two one - hot vectors of dimensionality V where V is the size of the vocabulary . The first ( resp . second ) vector significantly lower than “ phrase embedding ”, k = 2 ; “†” means significantly lower than “ phrase embedding ”, 2 < k < 3 .__label__Material|Data|Use
Here , we evaluate a recently introduced adaptation of LG to the biomedical domain , BioLG ( Pyysalo et al ., 2006 ), incorporating the GENIA POS tagger ( Tsuruoka et al ., 2005 ) as well as a number of modifications to lexical processing and the grammar . To facilitate the comparison of results with those of Clegg and Shepherd , we use their modified subset of GENIA Treebank . _CITE_ As 600 of the 1100 BioInfer sentences have previously been used in the development of the BioLG parser , we only use the remaining 500 blind sentences of BioInfer in the evaluation . To evaluate the performance of the parsers , we determined the precision , recall and F - measure by comparing the parser output against the corpus gold standard dependencies . The matching criterion required that the correct words are connected and that the direction and type of the dependency are correct .__label__Material|Data|Use
To this aim , the author classifies the input into 3 - classes ( strongly - negative , ambivalent , and strongly - positive ), 4 classes ( strongly - negative , weakly - negative , weaklypositive and strongly - positive ) and 5 - classes ( strongly - negative , weakly - negative , ambivalent , weakly - positive and strongly - positive ). The results decrease considerably with the number of classes , from 62 % of accuracy for 3 - classes to 38 % of accuracy for 5 - classes . The evaluation of the system has been carried out using two corpora from two very distinct domains : the Sentence Polarity Movie Review Dataset _CITE_ and the one used in the SemEval 2007 Affective Text task . The first one consists of 10 . 662 sentences selected from different movie review websites . These sentences are labeled as positive or negative depending on whether they express a positive or negative opinion within the movie review .__label__Material|Data|Use
The website includes in the search the sentiment associated with each product . In the interface , the user can input a list of sentiment words , like “ excellent ”, “ cool ”, “ easy ” or “ powerful ” that the system will organize the results according the frequency of those words in reviews related to the products . The Stock Sonar _CITE_ has a timeline chart as the main interface . In this timeline , both positive and negative sentiments are displayed throughout time . The sentiments are retrieved from real - time news associated with a particular company .__label__Supplement|Website|Introduce
We used this model for searching in - vocabulary queries . To handle OOV queries , a combination of word and phonetic search was presented by Mamou ( Mamou et al ., 2007 ). In this paper , we explore fuzzy phonetic search extending Lucene _CITE_ , an Apache open source search library written in Java , for indexing and search . When searching for these OOVs in word - fragment indexes , they are represented phonetically ( and subsequently using wordfragments ) using letter - to - phoneme ( L2P ) rules . Each transcript is composed of basic units ( e . g ., word , word - fragment , phones ) associated with a begin time , duration and posterior probability .__label__Method|Code|Use
Then , each non - centroidal word is assigned to the cluster k if is a maximum among all clusters These two steps are repeated until convergence . call with low precision ( all words belong to one cluster ). signing word wi to cluster k , it start man Urdu websites on poetry _CITE_ , and The second dataset , SMS dataset , is obtained from chopaal , an based group SMS service . For evaluation , we use a manually annotated database of Roman Urdu variations ( Khan and Karim , 2012 ). Table 1 shows statistics of the datasets in comparison with the gold stan Here , ρk is zero if aik does not have a match in aj ∗ ( i . e ., in the context of word wj ); otherwise , ρk = 5 − max [ k , l ] − 1 where aik = ajl and l is the highest rank ( smallest integer ) at which a previous match had not occurred .__label__Material|Data|Use
Spectral shape includes centroid , band width , roll off and spectral flux . Spectral contrast features includes sub - band peak , sub - band velly , subband contrast . using jAudio _CITE_ toolkit . It is a music feature extraction toolkit developed in JAVA platform . The jAudio toolkit is publicly available for research purpose .__label__Method|Tool|Use
Although much work still needs to be done , the feedback from deaf associations was very positive . Extra details about this work can be found in ( Almeida , 2104 ) and ( Almeida et al ., 2015 ). The whole system is freely available _CITE_ . This paper is organised as follows : Section 2 describes the proposed architecture , and Section 3 its implementation . In Section 4 we present our prototype and , in Section 5 , a preliminary evaluation .__label__Method|Tool|Produce
As for spoken language , Caines & Buttery ( 2010 ) among others suggest that adaptation can also be made to the parser , such that it enters a ‘ speech - aware mode ’ in which the parser refers to additional and / or replacement rules adapted to the particular features of spoken language . They demonstrated this with the omission of auxiliary verbs in progressive aspect sentences (‘ you talking to me ?’, ’ how you doing ?’) and achieved a 30 % improvement in parsing success rate for this construction type . Our speech data consist of recordings from Business Language Testing Service ( BULATS ) speaking tests _CITE_ . In the test , learners are required to undertake five tasks ; we exclude the tasks involving brief question - answering (‘ can you tell me your full name ?’, ‘ where are you from ?’, etc ) and elicited imitation , leaving us with three free - form speech tasks . For this particular test the tasks were : [ a ] talk about some advice from a colleague ( monologue ), [ b ] talk about a series of charts from Business Today magazine ( monologue ), [ c ] give advice on starting a new retail business ( dialogue with examiner ).__label__Material|Data|Use
As for spoken language , Caines & Buttery ( 2010 ) among others suggest that adaptation can also be made to the parser , such that it enters a ‘ speech - aware mode ’ in which the parser refers to additional and / or replacement rules adapted to the particular features of spoken language . They demonstrated this with the omission of auxiliary verbs in progressive aspect sentences (‘ you talking to me ?’, ’ how you doing ?’) and achieved a 30 % improvement in parsing success rate for this construction type . Our speech data consist of recordings from Business Language Testing Service ( BULATS ) speaking tests _CITE_ . In the test , learners are required to undertake five tasks ; we exclude the tasks involving brief question - answering (‘ can you tell me your full name ?’, ‘ where are you from ?’, etc ) and elicited imitation , leaving us with three free - form speech tasks . For this particular test the tasks were : [ a ] talk about some advice from a colleague ( monologue ), [ b ] talk about a series of charts from Business Today magazine ( monologue ), [ c ] give advice on starting a new retail business ( dialogue with examiner ).__label__Supplement|Website|Use
The extraction process scans pairs of subsequent revisions of article pages and ignores any revision that was reverted due to vandalism . It parses the Wikitext and filters out markup , hyperlinks , tables and templates . The process analyzes the clean text of the two revisions _CITE_ and computes the difference between them . The process identifies the overlap between edit segments and sentence boundaries and extracts user edits . Features are calculated and user edits are stored and indexed .__label__Material|Data|Use
Kleene has been approved by SAP AG for release as free , open - source code under the Apache License , Version 2 . 0 , and will be available by August 2012 for downloading from http :// www . kleene - lang . org . The design , implementation , development status and future plans for the language are discussed . Kleene is a finite - state programming language in the tradition of the AT & T Lextools ( Roark and Sproat , 2007 ), _CITE_ the SFST - PL language ( Schmid , 2005 ), the Xerox / PARC finite - state toolkit ( Beesley and Karttunen , 2003 ) and FOMA ( Huld ´ en , 2009b ), all of which provide higher - level programming formalisms built on top of low - level finite - state libraries . Kleene itself is built on the OpenFst library ( Allauzen et al ., 2007 ), developed by Google Labs and NYU ’ s Courant Institute . The design and implementation of the language were motivated by three main principles , summarized as Syntax Matters , Licensing Matters and Open Source Matters .__label__Method|Tool|Introduce
There are 4 training sets ( train1a , train1b , train2 and train3 ) and 4 test sets ( test1 ... 4 ) provided , where all the data logs are transcribed and labelled , except train1b which is transcribed but not labelled ( and contains a much larger number of dialogues than others ). It is known in advance to participants that test1 was collected using the same dialogue system from Group A as train1 * and train2 , test2 was collected using a different version of Group A ’ s dialogue manager but is to a certain extent similar to the previous ones , train3 and test3 were collected using the same dialogue system from Group B ( but the training set for this scenario is relatively smaller than that for test1 ), and test4 was collected using Group C ’ s system totally different from any of the training sets . The evaluation is based on several different metrics _CITE_ , but considering the nature of our system , we will mainly focus on the hypothesis accuracy , i . e . stand for the ensemble , mixed - domain , in - domain and out - of - domain system groups , except for test4 where the last three groups are merged into the right - hand side column . percentage of turns in which the tracker ’ s 1 - best hypothesis is correct , but with the receiver operating characteristic ( ROC ) performance briefly discussed as well .__label__Method|Algorithm|Extent
Thus , the expected output was the syntactic aspect of subcategorization frames of verbs . We worked with the verbal sense as the unit . We compared the patterns associated to each verbal sense by IRASubcat with the subcategorization frames manually associated to the verbs at the a lexical data base of SenSem verbs _CITE_ . We manually inspected the results for the 20 most frequent verbal senses . Results can be seen at Table 1 .__label__Material|Data|Compare
Finally , on newstest2012 the results of the different system combination settings are compared . In this Section , all four different translation engines are presented . For the WMT 2013 evaluation , RWTH utilized a phrase - based decoder based on ( Wuebker et al ., 2012 ) which is part of RWTH ’ s open - source SMT toolkit Jane 2 . 1 _CITE_ . GIZA ++ ( Och and Ney , 2003 ) was employed to train a word alignment , language models have been created with the SRILM toolkit ( Stolcke , 2002 ). After phrase pair extraction from the wordaligned parallel corpus , the translation probabilities are estimated by relative frequencies .__label__Method|Tool|Use
Each of these datasets is a collection of posts under three categories from the 20 Newsgroups dataset . We use a 60 % training / 40 % testing split of this data that is available online . _CITE_ We preprocess the data by splitting each line on non - alphabet characters , converting the resulting tokens to lower - case , and filtering out any tokens that appear in a list of common English stop words . In addition , we remove the header of every file and filter every line that does not contain a non - trailing space ( which removes embedded ASCII - encoded attachments ). Finally , we shuffle the order of the documents .__label__Material|Data|Use
Such models are useful for teaching , presentations and exploratory research ( such as showing where a classification algorithm makes mistakes ). Ndaona includes embedding and graphics parameter estimation algorithms , and generates files in the format of Partiview ( Levy , 2001 ), an existing free open - source fast multidimensional data displayer that has traditionally been used in the planetarium community . Partiview _CITE_ supports a number of enhancements to regular scatterplots that allow it to display more than three dimensions ’ worth of information . Scatterplots are not the most efficient way of representing information ( Grinstein et al ., 2001 ). However , they are intuitive and stable ( Wong and Bergeron , 1997 ), and can be supplemented in several ways .__label__Method|Tool|Introduce
However , on tasks like machine translation with a very large number of input features , a Laplacian L1 regularization that also attempts to maximize the number of zero weights is highly desirable . A new L1 - regularized Maxent algorithms was proposed for density estimation ( Dudik et al ., 2004 ) and we adapted it to classification . We found this algorithm to converge faster than the current state - ofthe - art in Maxent training , which is L2 - regularized L - BFGS ( Malouf , 2002 ) _CITE_ . Moreover , the number of trained parameters is considerably smaller . We have performed experiments on the IWSLT06 Chinese - English training and development sets from 2005 and 2006 .__label__Method|Algorithm|Compare
The BL corpus was compiled by Lin et al . ( 2006 ) and is derived from a website that invites weekly discussions on a topic and publishes essays from two sets of authors each week . _CITE_ Two of the authors are guests , one from each perspective , and two essays are from the site ’ s regular contributors , also one from each perspective , for a total of four essays on each topic per week . We chose this corpus to allow us to directly compare our results with Greene and Resnik ’ s ( 2009 ) Observable Proxies for Underlying Semantics ( OPUS ) features and Lin et al .’ s Latent Sentence Perspective Model ( LSPM ). The classification goal for this corpus is to label each document with the perspective of its author , either Israeli or Palestinian .__label__Material|Data|Use
There are four types of slots for morphemes : ( 1 ) derivational prefixes ( four slots ), ( 2 ) the lexical part ( three slots – in the majority of cases only one is filled , the three slots are provided for verbal compounds of two roots and an interfix ), ( 3 ) derivational and conjugational suffixes ( three slots ), and ( 4 ) infinitive ending ( one slot ). The metadata in lexical entries indicate verbal aspect and types of reflexivity . _CITE_ The database enables queries across the full derivational span of a particular base form and provides extensive data about the distribution and frequency of affixes in the derivation of Croatian verbs . In the following section , the underlying analysis of affixal meanings is described . The majority of verbal prefixes in Croatian developed from prepositions , and the original locative component pervades in their meaning .__label__Material|Data|Introduce
The results from the service can be stored in a database , and can be later replayed . We also developed a Widget version of the annotator that can be embedded in other websites , and integrated in widget frameworks like Sefarad . The project is completely open source and can be downloaded from its Github repository _CITE_ . This section demonstrates how it would be possible to integrate sentiment analysis of different modes using SPARQL . In particular , it covers two scenarios : fusion of results from different modes , and detection of complex patterns using information from several modes .__label__Method|Code|Produce
The purpose is to examine the effect of the additional data , especially for out - of - domain ( ood ) data . Three machines ( with 2 . 5GHz Xeon CPU and 16G memory ) were used to train our models . During the peak time , Amazon ’ s EC2 ( Elastic Compute Cloud ) _CITE_ was used , too . Our system requires 15G memory at most and the longest training time is about 36 hours . During training the predicate classification ( PC ) and the semantic role labeling ( SRL ) models , golden syntactic dependency parsing results are used .__label__Method|Tool|Use
BRAT has been used throughout its development during 2011 in the annotation of six different corpora by four research groups in efforts that have in total involved the creation of well - over 50 , 000 annotations in thousands of documents comprising hundreds of thousands of words . These projects include structured event annotation for the domain of cancer biology , Japanese verb frame annotation , and genemutation - phenotype relation annotation . One prominent effort making use of BRAT is the BioNLP Shared Task 2011 , _CITE_ in which the tool was used in the annotation of the EPI and ID main task corpora ( Pyysalo et al ., 2012 ). These two information extraction tasks involved the annotation of entities , relations and events in the epigenetics and infectious diseases subdomains of biology . Figure 5 shows an illustration of shared task annotations .__label__Supplement|Website|Introduce
Unrelated languages often require a richer and more flexible deeper transfer architecture to tackle differing linguistic features . Examples are ( Gasser , 2012 ) and Matxin ( Mayor et al ., 2011 ). In this work we present an attempt to port the deep - transfer RBMT Matxin _CITE_ , designed to cope with dissimilar languages . The remaining work is organized as follows : Section 2 gives a brief overview of the architecture of the Matxin system . Section 3 describes the work done in each of the system ’ s modules .__label__Method|Tool|Use
More recent results of RELAXCOR on the same corpora are published in M ` arquez , Recasens , and Sapena ( 2012 ). 5 . 2 . 2 CoNLL - 2011 . The CoNLL - 2011 Shared Task was based on the English portion of the OntoNotes 4 . 0 data _CITE_ ( Pradhan et al . 2011 ). As is customary for CoNLL tasks , there was a closed and an open track .__label__Material|Data|Extent
( 2008 ), who explore the liki - Graph and score every category in order to assess its likelihood of belonging to the domain . Other tools are being developed to extract corpora from Wikipedia . Linguatools _CITE_ released a comparable corpus extracted from Wikipedias in 253 language pairs . Unfortunately , neither their tool nor the applied methodology description are available . CatScan2 is a tool that allows to explore and search categories recursively .__label__Method|Tool|Introduce
We chose GermanEnglish translation for the experiments in this paper . The following details the resources used . The LEO bilingual German - English dictionary _CITE_ is an ongoing volunteer effort . While it is still not finished , it already provides an outstanding resource with over 230 , 000 entries . Bilingual dictionaries may not be easily available for other language pairs , especially for lowdensity languages .__label__Material|Data|Introduce
This section describes the source coreference data set , the baselines , our implementation of StRip , and the results of our experiments . For evaluation we use the MPQA corpus ( Wiebe et al ., 2005 ). _CITE_ The corpus consists of 535 documents from the world press . All documents in the collection are manually annotated with phraselevel opinion information following the annotation scheme of Wiebe et al . ( 2005 ).__label__Material|Data|Use
In Brazil , PhD students have the possibility to take their complete PhD course abroad or , alternatively , only a part of it . In both cases , students may count on Brazilian funding agencies . The area is more strongly represented and promoted by Brazilian Computer Society ( SBC ) , particularly by its Special Interest Group on NLP ( CEPLN ) _CITE_ , created in 2007 . It is interesting that most researchers in Brazil ( independent from their background area ) do not differentiate CL from NLP , using both terms interchangeably . Research in Brazil is carried out mainly at public universities and at a few private universities and business companies .__label__Method|Tool|Introduce
This value is also measured on the tree corre sponding to the text , and the absolute difference between these two minimal distances is stored in order to compute final feature weights consisting in basic statistical values . The algorithm to obtain the distribution of distance differences is detailed in Figure 6 . end - for end - for The statistics generated from the resulting list of distances differences Dd are the following : In a similar way , differences in the depth level of nodes for aligned terms are also calculated . From the example exposed the following values were computed : The algorithms used as binary classifiers are two : Bayesian Logistic Regression ( BBR ) _CITE_ and TiMBL ( Daelemans et al ., 1998 ). Both algorithms have been trained with the devel data provided by the organization of the Pascal challange . As has been explained in previous sections , a model is generated via the supervised learning process .__label__Method|Tool|Use
Since we are extracting predicate - argument structure , syntactic variations such as passive constructions and relative clauses will be all ‘ normalized ’ into the same form . Consequently , ‘ the book which I read ’, ‘ I read the book ’, and ‘ the book was read by me ’ will form the exact same semantic tuple & lt ; I , read , the book , N / A , N / A >. The resulting tuple structures along with their associated text are stored in an Apache Solr / Lucene _CITE_ server which receives queries from the Searchbench user interface . The Searchbench user interface ( UI ) is a web application running in every modern , JavaScript - enabled web browser . As can be seen in Figure 4 , the UI is divided into three parts : ( 1 ) a sidebar on the left ( Filters View ), where different filters can be set that constrain the list of found documents ; ( 2 ) a list of found documents matching the currently set filters in the upper right part of the UI ( Results View ); ( 3 ) the Document View in the lower right part of the UI with different views of the current document .__label__Supplement|Website|Use
We assign each emoticon a label from the following set of labels : Extremely - positive , Extremely - negative , Positive , Negative , and Neutral . We compile an acronym dictionary from an online resource . _CITE_ The dictionary has translations for 5 , 184 acronyms . For example , lol is translated to laughing out loud . We pre - process all the tweets as follows : a ) replace all the emoticons with a their sentiment polarity by looking up the emoticon dictionary , b ) replace all URLs with a tag || U ||, c ) replace targets ( e . g .__label__Supplement|Website|Extent
Global keywordness We evaluated the keywordness using also the overall HAL keyword frequencies rather than only the training corpus . It had no impact on the results . Language Model deviation We experimented the usage of HMM deviation using LingPipe _CITE_ as alternative informativeness measure , resulting in − 3 . 7 % for the F - score at top 15 . Wikipedia term Relatedness Using Wikipedia Miner , we tried to apply as post - ranking a boosting of related terms , but saw no impact on the results . We think that automatic key term extraction can be highly valuable for assisting self - archiving of research papers by authors in scholarly repositories such as arXiv or HAL .__label__Method|Tool|Use
Our method , on the other hand , uses no parsing , and only the synset definitions ( and not the graph structure ) of WordNet . The non - reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing ( such as Twitter : ( Baldwin et al ., 2013 )), and the non - reliance on the graph structure of WordNet is significant in terms of portability to conventional “ flat ” sense inventories . While comparable results on a different dataset have been achieved with a proximity thesaurus ( McCarthy et al ., 2007 ) compared to a dependency one , _CITE_ it is not stated how McCarthy et al . ( 2004b ) obtained good results with definition overlap , but their implementation uses the relation structure alongside the definitions ( Banerjee and Pedersen , 2002 ). Iida et al .__label__Material|Data|Compare
We evaluate our mixture model on four different test sets . On the three most AAC - like test sets , we found substantial reductions in not only perplexity but also in potential keystroke savings when used in a predictive keyboard interface . Finally , to aid other AAC researchers , we have publicly released our crowdsourced AAC collection , word lists and best - performing language models _CITE_ . As we mentioned in the introduction , there are unfortunately no publicly available sources of genuine conversational AAC messages . We conjectured we could create surrogate data by asking workers on Amazon Mechanical Turk to imagine they were a user of an AAC device and having them invent things they might want to say .__label__Material|Data|Produce
We evaluate our mixture model on four different test sets . On the three most AAC - like test sets , we found substantial reductions in not only perplexity but also in potential keystroke savings when used in a predictive keyboard interface . Finally , to aid other AAC researchers , we have publicly released our crowdsourced AAC collection , word lists and best - performing language models _CITE_ . As we mentioned in the introduction , there are unfortunately no publicly available sources of genuine conversational AAC messages . We conjectured we could create surrogate data by asking workers on Amazon Mechanical Turk to imagine they were a user of an AAC device and having them invent things they might want to say .__label__Method|Algorithm|Produce
The second stage performs clustering using results of previous stage . The top k search results output by the previous stage are treated as centroids and the remaining search results are assigned to the centroid with the maximum similarity . The implemented system was tested on data given by SemEval - 2013 _CITE_ ( Roberto Navigli and Daniele Vannella , 2013 ). Data contains 100 queries , each with 64 search results . Each search result contains title , url and snippet .__label__Material|Data|Use
While very relevant for our research , their analysis is aggregated over all users participating in the study , and is not focusing on individuals ’ preferences . In this work we used the data provided for the MT Shared Task in the 2013 Workshop on Statistical Machine Translation ( WMT ) ( Bojar et al ., 2013 ). _CITE_ This data was of a particularly large scale , with crowdsourced human judges , either volunteer researchers or paid Amazon Turkers . For each source sentence , a judge was presented with the source sentence itself , a reference translation , and the outputs of five machine translation systems . The five systems were randomly selected from the pool of participating systems , and were anonymized and randomly - ordered when presented to the judge .__label__Material|Data|Use
We also discuss NLP - related applications that support the linguistic analysis of texts -- typically in the context of developing readability measures -- which continues to be a prominent area of research ; other research supports student tools allowing direct interaction with language forms ( Section 2 . 2 ). Language Demands on ELLs . The English Language Arts Common Core State Standards _CITE_ ( Standards ) ( NGA Center & CCSSO , 2010 ) has now been adopted by 46 states and is a trend - setter in U . S . education . The Standards emphasize the need for all learners ( including ELLs ) to read progressively more complex texts across multiple genres in the content areas , preparing learners for college and careers . To accomplish this , learners must have familiarity with numerous linguistic features related to vocabulary , English language structures , and a variety of text structures ( discourse ).__label__Supplement|Document|Introduce
In total , we end up with four domains ( TED , ITLSP1 , EM and ITLSP2 ), which allows us to evaluate the PAMTL algorithm in realistic conditions where the QE component is exposed to a continuous stream of heterogeneous data . Each domain is composed by 1 , 000 tuples formed by : i ) the English source sentence , ii ) its automatic translation in French , and iii ) a real - valued quality label obtained by computing the HTER between the translation and the post - edition with the TERCpp open source tool . _CITE_ Table 1 reports some macro - indicators ( number of tokens , vocabulary size , average sentence length ) that give an idea about the similarities and differences between domains . Although they contain data from different software manuals , similar vocabulary size and sentence lengths for the two IT domains seem to reflect some commonalities in their technical style and jargon . Larger values for TED and EM evidence a higher lexical variability in the topics that compose these domains and the expected stylistic differences featured by speech transcriptions and non - technical writing .__label__Method|Tool|Use
While distribution is broadly similar , i2b2 had a higher percentage of duration expressions while THYME had many prepostexp expressions , which in i2b2 were annotated as the date category . We implemented a variety of systems in an attempt to empirically evaluate the best way to model the time span classification task . For all systems , the temporal expression extractor is implemented within Apache cTAKES _CITE_ ( clinical Text Analysis and Knowledge Extraction System ) ( Savova et al ., 2011 ), making use of its components for feature generation as well as its interface to the source general - domain NLP system ClearTK ( Bethard et al ., 2014 ) which in turn interfaces with different machine learning libraries , including LibSVM ( Chang and Lin , 2011 ) and CRFSuite ( Okazaki , 2007 ). We developed three sequence - based models for this task , each with different perceived strengths . The first system is perhaps the simplest , a standard BIO ( Begin - Inside - Outside ) tagger using an off the shelf support vector machine ( SVM ) classifier ( Cortes and Vapnik , 1995 ).__label__Method|Tool|Use
To arrive at the point where information that is available in museum databases about paintings could be recorded using this model , we developed the painting ontology that integrates the CIDOC - CRM with more specific schemata . The Swedish Open Cultural Heritage ( SOCH ) is a web service used to search and fetch data from any organization that holds information related to the Swedish cultural heritage . _CITE_ The idea behind SOCH is to harvest any data format and structure that is used in the museum sector in Sweden and map it into SOCH ’ s categorization structure . The data model used by SOCH is an uniform data representation which is available in an RDF compatible form . The schema provided by SOCH helps to intermediate data between museums in Sweden and the Europeana portal .__label__Supplement|Website|Introduce
Methods proposed for Arabic task include : ( 1 ) two - level hierarchical classification method : classifying answers as irrelevant and not irrelevant in the first level and classifying not irrelevant answers as direct and related in the second level ; ( 2 ) ensemble learning method : training and choosing top N best classifiers and using the results of those classifiers to vote final result . For English task , CQA - QL corpus ( Màrquez et al ., 2015 ) was provided . This corpus was gotten from the Qatar Living Forum _CITE_ and was filtered and annotated manually . Questions in the corpus were labeled into GENERAL and YES_NO class in QTYPE dimension , and yes , no , unsure and Not Applicable class in QGOLD_YN dimension . Answers were labeled into Good , Potential , Bad , Dialogue , Not English and Other class in CGOLD dimension , and Yes , No , Unsure and Not Applicable class in CGOLD_YN dimension .__label__Material|Data|Use
This section describes how this testbed has been built . The testbed must have a certain number of features which , altogether , differentiate the task from current multi - document summarization evaluations : Complex information needs . Being Information Synthesis a step which immediately follows a document retrieval process , it seems natural to start with standard IR topics as used in evaluation conferences such as TREC _CITE_ , CLEF or NTCIR . The title / description / narrative topics commonly used in such evaluation exercises are specially well suited for an Information Synthesis task : they are complex and well defined , unlike , for instance , typical web queries . We have selected the Spanish CLEF 2001 - 2003 news collection testbed ( Peters et al ., 2002 ), because Spanish is the native language of the subjects recruited for the manual generation of reports .__label__Supplement|Website|Extent
Nevertheless , most of the work has focused on the task of factoid QA , where questions match short answers , usually in the form of named or numerical entities . Thanks to international evaluations organized by conferences such as the Text REtrieval Conference ( TREC ) and the Cross Language Evaluation Forum ( CLEF ) Workshop , annotated corpora of questions and answers have become available for several languages , which has facilitated the development of robust machine learning models for the task . _CITE_ The situation is different once one moves beyond the task of factoid QA . Comparatively little research has focused on QA models for non - factoid questions such as causation , manner , or reason questions . Because virtually no training data is available for this problem , most automated systems train either on small hand - annotated corpora built in - house ( Higashinaka and Isozaki 2008 ) or on question – answer pairs harvested from Frequently Asked Questions ( FAQ ) lists or similar resources ( Soricut and Brill 2006 ; Riezler et al .__label__Material|Data|Introduce
In this work we focus on these lexical units and propose how to automatically collect the missing sentences . Anyhow , the algorithm we propose is suitable also for expanding sentence sets already present in FrameNet . http :// framenet . icsi . berkeley . edu Wikipedia _CITE_ is one of the largest online repositories of encyclopedic knowledge , with millions of articles available for a large number of languages (& gt ; 2 , 800 , 000 for English ). The article ( or page ) is the basic entry in Wikipedia . Every article has an unique reference , i . e ., one or more words that identify the page and are present in its URL .__label__Supplement|Website|Introduce
Major limitation of this approach is that one cannot give richer context due to the problem of scarcity . To find whether the dependency label is correct or not , apart from node and its parent information , contextual features like sibling and child information is also helpful . Current state - of - the - art dependency parsers like MSTParser _CITE_ and MaltParser use these features for dependency labeling ( McDonald et al ., 2006 ; Nivre et al ., 2007 ; Kosaraju et al ., 2010 ). Finding similarity between patterns and merging similar patterns would not help when we wish to take a much richer context . In this paper , we propose a probability based statistical module ( PBSM ) to overcome this problem of FBSM .__label__Method|Tool|Introduce
We train MT systems using a significant portion of the training data and use these models as well as TM outputs to obtain a recommendation development data set . MT systems can be either in - house , e . g . a Moses - based system , or externally available systems , such as Microsoft Bing _CITE_ or Google Translate . For each sentence in the development data set , we have access to the reference as well as to the outputs for each of the MT and TM systems . We then select the best MT ( or TM ) output as the translation with the lowest TER score with respect to the reference and label the data accordingly .__label__Supplement|Website|Introduce
Since the 1980s , with the advent of computer technologies , digital equivalents of these camouflage techniques were invented to hide messages in digital cover media , such as images , video , and audio signals ( Fridrich 2009 ). For example , in 2010 , the United States Department of Justice documented that more than 100 text files were retrieved from images posted on publicly accessible Web sites . According to the Steganography Analysis and Research Centre , _CITE_ there have been over 1 , 100 digital steganography applications identified . Most of the digital steganography systems exploit the redundancy of the cover media and rely on the limitations of the human auditory or visual systems . For example , a standard image steganography system uses the least - significant - bit substitution technique .__label__Supplement|Website|Extent
It uses Javascript , angular , and jquery to visualize the information in a web browser . Most interactive IE systems focus on annotation of text , labeling of entities , and manual writing of rules . Some annotation and labeling tools are : MITRE ’ s Callisto , Knowtator , SAPIENT ( Liakata et al ., 2009 ), brat _CITE_ , Melita ( Ciravegna et al ., 2002 ), and XConc Suite ( Kim et al ., 2008 ). Akbik et al . ( 2013 ) interactively helps non - expert users to manually write patterns over dependency trees .__label__Method|Tool|Introduce
After trigger detection , a rule - based step attempts to extend predicted trigger spans forwards and backwards to cover the correct span . When extending the spans of BB training set gold entity head tokens , this step produced the correct span for 91 % ( 399 out of 440 ) of Bacterium - type entities . To aid in detecting Bacterium - entities a list of bacteria names from the List of Prokaryotic names with Standing in Nomenclature _CITE_ was used ( Euz ´ eby , 1997 ) as external features . To help in detecting the heterogeneous habitat - entities , synonyms and hypernyms from Wordnet were used ( Fellbaum , 1998 ). The development set lacked some event classes , so we moved some documents from the training set to the development set to include these .__label__Material|Data|Use
Further progress requires a robust formal ontology of structures , locations , functions and processes , linked together via relations such as is_part_of , is_located_at , is_realized_by , and so forth . As a step along this road , we provide a methodology for deriving and representing association rules between the entities present within the separate ontologies of the Gene Ontology . _CITE_ ( Gene Ontology Consortium , 2001 ). Such rules will be able to situate a biological process in relation to a cellular location to an agent . They will be able to relate lower - granularity molecular functions in relation to highergranularity biological processes , and establish other sorts of relations between entities in different parts of GO .__label__Material|Data|Use
As part of this module , we have students work through the exercises in the draft chapter on command line tools in Chris Brew and Marc Moens ’ Data - Intensive Linguistics course notes or Ken Church ’ s Unix for Poets tutorial . Grammar engineering with OpenCCG . The grammar engineering component of Computational Syntax in Spring 2006 used OpenCCG , _CITE_ a categorial grammar parsing system that Baldridge created with Gann Bierner and Michael White . The problem with using OpenCCG is that its native grammar specification format is XML designed for machines , not people . Students in the course persevered and managed to complete the assignments ; nonetheless , it became glaringly apparent that the non - intuitive XML specification language was a major stumbling block that held students back from more interesting aspects of grammar engineering .__label__Method|Tool|Use
DepPattern was used for several webbased IE applications , namely Open Information Extraction from Wikipedia ( Gamallo et al ., 2012 ), extraction of semantic relations with distant supervision ( Garcia and Gamallo , 2011 ), and extraction of bilingual terminologies from comparable corpora ( Gamallo and Pichel , 2008 ). It has also been integrated into commercial tools , e . g . Linguakit _CITE_ and Avalingua . The parsers were developed for five languages : Spanish , Portuguese , Galician , French , and English . However , we have just written two small , and not very different , grammars : The cost of writing these two grammars is quite low .__label__Method|Tool|Extent
However , this library pursues a slightly different strategy by providing Ruby accessor methods to a data collection internally represented in RDF . In contrary , POSEIdON provides a simple way of getting an additional representation ( in RDF ) from an already existing library or data source in a read - only fashion , without modifying the source code of existing classes . Such data interfaces are typically based on XML documents or relational databases which are accessed with standard libraries ( e . g ., Nokogiri _CITE_ for XML or ActiveRecord for SQL databases ). A modifi markup URIs ( line 3 ) or to express rules for the export of instance properties ( lines 4 - 8 ). — ( b ) The RDF resulting from these POSEIdON instructions .__label__Material|Data|Introduce
Banko and Brill ( 2001a , 2001b ) experiment with context - sensitive spelling correction , a task for which large amounts of data can be obtained straightforwardly , as no manual annotation is required . They demonstrate that the learning algorithms typically used for spelling correction benefit significantly from larger training sets , and that their performance shows no sign of reaching an asymptote as the size of the training set increases . Arguably , the largest data set that is available for NLP is the Web , _CITE_ which currently consists of at least 3 , 033 million pages . Data retrieved from the Web therefore provide enormous potential for training NLP algorithms , if Banko and Brill ’ s ( 2001a , 2001b ) findings for spelling corrections generalize ; potential applications include tasks that involve word n - grams and simple surface syntax . There is a small body of existing research that tries to harness the potential of the Web for NLP .__label__Material|Data|Introduce
ROUGE scores , based on n - gram overlap between human abstracts and automatic extracts , were also calculated for comparison [ 5 ]. ROUGE2 , based on bigram overlap , is considered the most stable as far as correlating with human judgments , and this was therefore our ROUGE metric of interest . ROUGE - SU4 , which evaluates bigrams with intervening material between the two elements of the bigram , has recently been shown in the context of the Document Understanding Conference ( DUC ) _CITE_ to bring no significant additional information as compared with ROUGE - 2 . Results from [ 4 ] and from DUC 2005 also show that ROUGE does not always correlate well with human judgments . It is therefore included in this research in the hope of further determining how reliable the ROUGE metric is for our domain of meeting summarization .__label__Material|Data|Introduce
ROUGE scores , based on n - gram overlap between human abstracts and automatic extracts , were also calculated for comparison [ 5 ]. ROUGE2 , based on bigram overlap , is considered the most stable as far as correlating with human judgments , and this was therefore our ROUGE metric of interest . ROUGE - SU4 , which evaluates bigrams with intervening material between the two elements of the bigram , has recently been shown in the context of the Document Understanding Conference ( DUC ) _CITE_ to bring no significant additional information as compared with ROUGE - 2 . Results from [ 4 ] and from DUC 2005 also show that ROUGE does not always correlate well with human judgments . It is therefore included in this research in the hope of further determining how reliable the ROUGE metric is for our domain of meeting summarization .__label__Supplement|Document|Introduce
We use maximum likelihood estimators ( MLE ) for estimating the parameters ( p , Oz ). The MLEs for Bernoulli and MVN parameters have analytical solutions . Dirichlet parameters were estimated using an estimation method proposed and implemented by Tom Minka _CITE_ . We experiment with three model setups : Supervised , semi - supervised , and unsupervised . In the supervised setup we use the training data described in Section 3 . 1 for parameter estimation and then use thus fitted models to classify the tuning and test dataset .__label__Method|Code|Use
We use maximum likelihood estimators ( MLE ) for estimating the parameters ( p , Oz ). The MLEs for Bernoulli and MVN parameters have analytical solutions . Dirichlet parameters were estimated using an estimation method proposed and implemented by Tom Minka _CITE_ . We experiment with three model setups : Supervised , semi - supervised , and unsupervised . In the supervised setup we use the training data described in Section 3 . 1 for parameter estimation and then use thus fitted models to classify the tuning and test dataset .__label__Method|Algorithm|Use
We took part in the constrained task . Unless explicitly stated otherwise , the translation model in our experiments was trained on the combined News - Commentary v8 and Europarl v7 corpora . _CITE_ Note that there is only News Commentary and no Europarl for Russian . We were also able to evaluate several combinations with large parallel corpora : the UN corpus ( English , French and Spanish ), the Giga French - English corpus and CzEng ( Czech - English ). We did not use any large corpus for Russian - English .__label__Material|Data|Use
The CMU pronouncing dictionary provides the phonemic representations of English pronunciations with a sequence of phoneme symbols . For instance , the English word KNOX is segmented and tagged as the phonemic representation < N AA K S & gt ;. Since the CMU pronouncing dictionary does not cover all the pronunciation information of the name entities in the training data , we also apply LOGIOS Lexicon Tool _CITE_ to generate the phonemic representations of all other name entities not in the CMU pronouncing dictionary . After obtaining the phonemic representation of all the English named entities in the training data , we formulate the sequence of phoneme symbols of the English name entities as a string and apply the substring alignment method mentioned earlier to get the mappings from English phoneme symbols to Korean letters . For the previous example , the phoneme symbols < N AA K S & gt ; from the English name entity KNOX are aligned to the letters of its corresponding Korean word “ nok sur ” as [ N → n , AA → o , K → k , S → sui ].__label__Method|Tool|Use
It includes support for training with usersupplied data . LangDetect implements a Naive Bayes classifier , using a character n - gram based representation without feature selection , with a set of normalization heuristics to improve accuracy . It is trained on data from Wikipedia , _CITE_ and can be trained with usersupplied data . CLD is a port of the embedded language identifier in Google ’ s Chromium browser , maintained by Mike McCandless . Not much is known about the internal design of the tool , and there is no support provided for re - training it .__label__Material|Data|Use
However , suppose that we express the same English meaning in the following way : Chief of Mali defense wants more weapons . Then BING produces a much better translation : Chef d ’ état - major de la défense du Mali veut plus d ’ armes . The fact that the formulation of the source can strongly influence the quality of the translation has long been known , and there have been studies indicating that adherence to so - called “ Controlled Language ” guidelines , such as Simplified Technical English _CITE_ can reduce the MT post - edition effort . However , as one such study ( O ’ Brien , 2006 ) notes , it is unfortunately not sufficient to just “ apply the rules [ i . e . guidelines ] and press Translate .__label__Supplement|Document|Introduce
Usually , this approach requires an additional process to disambiguate the sentiment polarities of all the morphological variants . To improve the sentiment classification for the target language , Banea , Mihalcea , and Wiebe ( 2010 ) translate the English sentiment lexicon into the target language using Google Translator . _CITE_ Similarly , Google Translator is used by Steinberger et al . ( 2011 ). They manually produce two high - level gold - standard sentiment lexicons for two languages ( e . g ., English and Spanish ) and then translate them into the third language ( e . g ., Italian ) via Google Translator .__label__Supplement|Website|Use
The idea that there is a basic vocabulary composed of a few hundred or at most a few thousand elements has a long history going back to the Renaissance – for a summary , see Eco ( 1995 ). The first modern efforts in this direction are Thorndike ’ s ( 1921 ) Word Book , based entirely on frequency counts ( combining TF and DF measures ), and Ogden ’ s ( 1944 ) Basic English , based primarily on considerations of definability . Both had lasting impact , with Thorndike ’ s approach forming the basis of much subsequent work on readability ( Klare 1974 , Kanungo and Orr 2009 ) and Ogden ’ s forming the basis of the Simple English Wikipedia _CITE_ . An important landmark is the Swadesh ( 1950 ) list , which puts special emphasis on cross - linguistic definability , as its primary goal is to support glottochronological studies . Until the advent of large MRDs , the frequencybased method was much easier to follow , and Thorndike himself has extended his original list of ten thousand words to twenty thousand ( Thorndike 1931 ) and thirty thousand ( Thorndike and Lorge 1944 ).__label__Supplement|Website|Extent
And finally , most experiments have been carried out on English paired with other European languages , and it is not clear whether the results translate across to other language pairs . In this research , we use the translations of MWEs and their components to estimate the relative degree of compositionality of a MWE . There are several resources available to translate words into various languages such as Babelnet ( Navigli and Ponzetto , 2010 ), _CITE_ Wiktionary , Panlex ( Baldwin et al ., 2010 ) and Google Translate . As we are ideally after broad coverage over multiple languages and MWEs / component words in a given language , we exclude Babelnet and Wiktionary from our current research . Babelnet covers only six languages at the time of writing this paper , and in Wiktionary , because it is constantly being updated , words and MWEs do not have translations into the same languages .__label__Method|Tool|Use
In order to allow the tree kernel to find subtree matches at the word level , we include an additional layer of dummy leaves as was done in ( Moschitti et al ., 2007 ); not shown in Figure 2 , for simplicity . In our experiments , we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English . _CITE_ This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation campaigns , both consisting of 3 , 003 sentences , for four different language pairs : Czech - English ( CSEN ), French - English ( FR - EN ), German - English ( DE - EN ), and Spanish - English ( ES - EN ); as well as a dataset with the English references . We measured the correlation of the metrics with the human judgments provided by the organizers . The judgments represent rankings of the output of five systems chosen at random , for a particular sentence , also chosen at random .__label__Material|Data|Use
To tag a training window w of a training term t with ATTW and ROUGE - W , we obtain a set Ct of definitions of t from encyclopedias . Stop - words , punctuation , and non - alphanumeric characters are removed from Ct and w , and a stemmer is applied ; the testing windows undergo the same preprocessing . _CITE_ For each definition d E Ct , we find the longest common word subsequence of w and d . If w is the word sequence ( A , B , F , C , D , E ) and d = ( A , B , E , C , G , D ), the longest common subsequence is ( A , B , C , D ). The longest common subsequence is divided into consecutive matches , producing in our example ( A , B | C | D ). We then compute the following score ( weighted longest common subsequence ), where m is the number of consecutive matches , ki is the length of the i - th consecutive match , and f is a weighting function .__label__Method|Algorithm|Use
We do not attempt , e . g ., to disambiguate the sense of individual word terms to tell whether the slang sense of a word is the one intended . Rather , we simply check to see if each word has a slang usage in Wiktionary . _CITE_ A continuous feature is set to the value of i for each article . Discrete features highi and lowi are set as : where ¯ i and σ are , respectively , the mean and standard deviation of i across all articles . Lexical approaches are clearly inadequate if we assume that good satirical news articles tend to emulate real news in tone , style , and content .__label__Supplement|Website|Use
The final filtered version of hrWaC contains 51M sentences and 1 . 2B tokens . The corpus is freely available for download , along with a more detailed description of the preprocessing steps . _CITE_ Tagging , lemmatization , and parsing . For morphosyntactic ( MSD ) tagging , lemmatization , and dependency parsing of hrWaC , we use freely available tools with models trained on the new SETimes Corpus of Croatian ( SETIMES . HR ), based on the Croatian part of the SETimes parallel corpus . SETIMES . HR and the derived tools are prototypes racy that are about to be released as parts of another work .__label__Material|Data|Produce
The final filtered version of hrWaC contains 51M sentences and 1 . 2B tokens . The corpus is freely available for download , along with a more detailed description of the preprocessing steps . _CITE_ Tagging , lemmatization , and parsing . For morphosyntactic ( MSD ) tagging , lemmatization , and dependency parsing of hrWaC , we use freely available tools with models trained on the new SETimes Corpus of Croatian ( SETIMES . HR ), based on the Croatian part of the SETimes parallel corpus . SETIMES . HR and the derived tools are prototypes racy that are about to be released as parts of another work .__label__Supplement|Document|Produce
This is due to both the informal nature of the medium and the requirement to limit content to at most 140 characters . Thus , before measuring the semantic similarity , we replace abbreviation and slang to the readable version . We collected about 685 popular abbreviations and slang terms from several Web resources _CITE_ and combined these with the provided twitter normalization lexicon developed by Han Bo and Timothy Baldwin ( 2011 ). After replacing abbreviations and slang terms , we remove all stop words to get our final desired processed tweets . Then we produce a set of twoskip trigrams for each tweet and name these sets as trigram sets .__label__Material|Data|Use
These approaches performed well but are limited due to requiring large annotated training data specific to OCR spell checking in languages that are very hard to obtain . Further , research in spell checking for Vietnamese language has been understudied . Hunspell − spellcheck − vn _CITE_ & Aspell are interactive spell checking tools that work based on pre - defined dictionaries . According to our best knowledge , there is no work in the literature reported the task of spell checking for Vietnamese OCR - scanned text documents . In this paper , we approach this task in terms of 1 ) fully automatic scheme ; 2 ) without using any annotated corpora ; 3 ) capable of solving both non - word & real - word spelling errors simultaneously .__label__Method|Tool|Introduce
The baseline StandardCNN means that we adopt the standard CNNS with fixed window size for summary prior representation . To explore the effects of the learned summary prior representations , we design a baseline system named Reg Manual which adopts manuallycompiled document - independent features such as NUMBER ( whether number exist ), NENTITY ( whether named entities exist ) and STOPRATIO ( the ratio of stopwords ). Then we combine these features with document - dependent features in Table 1 and tune the feature weights through LIBLINEAR _CITE_ support vector regression . From Table 2 , we can see that PriorSum can achieve a comparable performance to the stateof - the - art summarization systems R2N2 , ClusterCMRW and REGSUM . With respect to baselines , PriorSum significantly outperforms Reg Manual which uses manually compiled features and the graph - based summarization system LexRank .__label__Method|Tool|Use
Here , the difficulty arises when the verbal stem ( e . g ., k ¨ undigen ) is separated from its particle ( e . g ., an ) in German verb - initial and verb - second clause types . As a preprocessing step for target word identification , the text is split into individual sentences , tokenized , and lemmatized . For this purpose , the sentence detector and the tokenizer of the suite of Apache OpenNLP tools _CITE_ and the TreeTagger ( Schmid , 1994 ) are used . Further , compounds are split by using BananaSplit10 . Since the automatic lemmatization obtained by the tagger and the compound splitter are not 100 % accurate , target word identification also utilizes the full set of inflected forms for a target word whenever such information is available .__label__Method|Tool|Use
Base features represent the basic properties of event causality like nouns , templates , and their excitation polarities ( See Section E in the supplementary notes ). For B3 and B4 , 500 semantic classes were obtained from our web corpus using the method of Kazama and Torisawa ( 2008 ). Using the above features , a classifier _CITE_ classifies each event causality candidate into causality and non - causality . An event causality candidate is given a causality score CScore , which is the SVM score ( distance from the hyperplane ) that is normalized to [ 0 , 1 ] by the sigmoid function 1 1 + e − x . Each event causality candidate may be given multiple original sentences , since a phrase pair can appear in multiple sentences , in which case it is given more than one SVM score .__label__Method|Tool|Use
However , the characteristics of GP - written free text make accurate part of speech ( POS ) tagging and chunking difficult . Major problems are caused by unknown tokens and ambiguities due to omitted words or phrases . We evaluate two standard chunking tools , YamCha ( Kudo and Matsumoto , 2003 ) and CRF ++ _CITE_ , selected based on their support for trainable context features . The tools were applied to the Har different models ; the baseline is with no tagging . The IOB and BEISO columns compare the impact of two chunk representation strategies .__label__Method|Tool|Use
However , Hussain ( 2008 ) has done a good job in assimilating most of the resources available on the internet . The lexicon provided as a part of the EMILLE ( 2003 ) data set for Urdu has about 200 , 000 words . CRL _CITE_ has released a lexicon of 8000 words as a part of their Urdu data collection . They also provide an NE tagged data set mostly used for morphological analysis . The lexicon includes POS information as well .__label__Material|Data|Introduce
Segmentation is also performed within parentheticals ( marked by parentheses or hyphens ). The gold standard test set consists of 9 humanannotated texts . The 9 documents include 3 texts from the RST literature _CITE_ , 3 online product reviews from Epinions . com , and 3 Wall Street Journal articles taken from the Penn Treebank . The texts average 21 . 2 sentences , with the longest text having 43 sentences and the shortest having 6 sentences , for a total of 191 sentences and 340 discourse segments in the 9 gold - standard texts . The texts were segmented by one of the authors following guidelines that were established from the project ’ s beginning and was used as the gold standard .__label__Material|Data|Use
Training data were annotated with lemmas by means of the TreeTagger Toolkit . Next , word - alignment for all the sentences in the parallel training corpus is established and uses the same methodology as in phrase - based models ( symmetrized GIZA ++ alignments ) to create the phrase table . We also specified a language model using the IRST Language Modeling Toolkit _CITE_ to train a lemma based tri - gram model on the total size of the Europarl corpus ( 1 . 8M sentences ). Afterwards , we applied the above - described integration strategies . The features used in the BASELINE system include : ( 1 ) four translation probability features , ( 2 ) one language model and ( 3 ) word penalty .__label__Method|Tool|Use
First , our results may be idiosyncratic to the specifics of the particular domain of our experiment . We would point out , however , that the domain is more complex , and arguably more realistic , than the much - simplified experimental contexts that have served as intuitions for earlier work in the field ; we have in mind here in particular the experiments discussed in ( Ford and Olson , 1975 ), ( Sonnenschein , 1985 ) and ( Pechmann , 1989 ). In the belief that the data provides a good test set for the generation of referring expressions , we are making the data set publicly available _CITE_ , so others may try to develop algorithms covering the data . A second concern is that we have only explored the extent to which three specific algorithms are able to cover the human data . Many of the other algorithms in the literature take these as a base , and so are unlikely to deliver significantly different results .__label__Material|Data|Produce
Experiments conducted on a real world Q & A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models . Community - driven question answering services , such as Yahoo ! Answers and Live Search QnA _CITE_ , have been rapidly gaining popularity among Web users interested in sharing information online . By inducing users to collaboratively submit questions and answer questions posed by other users , large amounts of information have been collected in the form of question and answer ( Q & A ) pairs in recent years . This user - generated information is a valuable resource for many information seekers , because users can acquire information straightforwardly by searching through answered questions that satisfy their information need .__label__Supplement|Website|Introduce
A new style for exchanging and sharing information is social media . Social media refers to the means of interaction among people in which they create , share , and exchange information and ideas in virtual communities and networks ( like Twitter and Facebook ). According to CNN _CITE_ , more Americans get their news from the Internet than from newspapers or radio , and three - fourths say they hear of news via e - mail or updates on social media sites . Social media , in many cases , provide more up - to - date information than conventional sources like online news . To make use of this vast amount of information , it is required to extract structured information out of these heterogeneous unstructured information .__label__Method|Algorithm|Introduce
We therefore experiment with three types of machine learning algorithms : a standard classifier , an ordinal ( ranking ) classifier and a regressor . Each algorithm assumes different relations among the groups : the classifier assumes no relation , the ordinal classifier assumes that the groups are ordered , and the regressor assumes that the groups are continuous . As classifier we use the Support Vector Machines ( SVM ) implementation in the Weka _CITE_ toolkit ( SMO ). As ordinal classifier we use a meta classifier in Weka which takes SMO as the base classification algorithm and performs pairwise classifications ( OrdinalClassClassifier ). For regression we use the SVM regression implementation in Weka ( SMO - reg ).__label__Method|Code|Use
These forms are then submitted as literal queries , and the resulting hits are summed up ( e . g ., history changes expands to & quot ; history change & quot ;, & quot ; histories change & quot ;, & quot ; history changed & quot ;, etc .). John Carroll ’ s suite of morphological tools ( morpha , morphg , and ana ) was used to generate inflected forms of verbs and nouns . _CITE_ In certain cases ( detailed below ), determiners were inserted before nouns in order to make it possible to recognize simple NPs . This insertion was limited to a / an , the , and the empty determiner ( for bare plurals ). All queries ( other than the ones using the NEAR operator ) were performed as exact matches ( using quotation marks in Altavista ).__label__Method|Tool|Introduce
For our experiments , it is necessary to use a corpus that includes widely used biological terms and common English words . This dataset , therefore , will allow us to accurately extract the information of biology related affixes . As a proof - of - concept prototype , our experiments are conducted on two widely used corpora : Genia corpus ( v3 . 02 ) and Brown corpus _CITE_ . The Genia version 3 . 02 corpus is used as the biological corpus BC in our experiments . It contains 2 , 000 biological research paper abstracts . They were selected from the search results in the MEDLINE database , and each biological term has been annotated into different terminal classes based on the opinions of experts in biology .__label__Material|Data|Use
The results show that coverage is similar on these unseen testsets . Obviously , coverage only indicates how often the parser found a full parse , but it does not indicate whether that parse actually was the correct parse . For this reason , we also closely monitored the performance of the parser on the Alpino tree - bank _CITE_ ( van der Beek et al ., 2002a ), both in terms of parsing accuracy and in terms of average number of parses per sentence . The average number of parses increased , which is to be expected if the grammar and lexicon are extended . Accuracy has been steadily increasing on the Alpino tree - bank .__label__Material|Data|Use
The Stanford POS tagger is used for part - ofspeech tagging . We used the EVALB program to evaluate parsing performance . _CITE_ Every experiment reported here was performed on hardware Feature We borrow the feature templates from Sagae and Lavie ( 2006 ). However , we found the full feature templates make training and decoding of the structured perceptron much slower , and instead developed simplified templates by removing some , e . g ., that access to the child information on the second top node on the stack . Result Table 1 summarizes the results that indicate our assumption is true .__label__Method|Tool|Use
Considering that most of its similar mentions are mapped to the American country singer “ Luke Bryan ”, our model tends to link “ LukeBryanOnline ” to the same entity . We evaluate our method on the public available data set shared by Meij et al . ( 2012 ) _CITE_ . Experimental results show that our method outperforms two baselines , i . e ., Wikify ! ( Mihalcea and Csomai , 2007 ) and system proposed by Meij et al .__label__Material|Data|Use
Many of the commonly used tasks that otherwise require writing programs , can be performed with one or more queries . Overcoming the language barrier in the Indian sub - continent is a very challenging task . Sampark _CITE_ is an effort in this direction . Sampark has been developed as part of the consortium project called Indian Language to India Language Machine translation ( ILMT ) funded by TDIL program of Department of Information Technology , Government of India . Work on this project is contributed to by 11 major research centres across India working on Natural Language Processing .__label__Method|Tool|Introduce
Metrics : We compare the systems using three popular metrics for coreference — MUC ( Vilain et al ., 1995 ), BCUB ( Bagga and Baldwin , 1998 ), and Entity - based CEAF ( CEAF ,) ( Luo , 2005 ). Following , the CoNLL shared tasks ( Pradhan et al ., 2012 ), we use the average F1 scores of these three metrics as the main metric of comparison . Features : We build our system on the publicly available Illinois - Coref system _CITE_ primarily because it contains a rich set of features presented in Bengtson and Roth ( 2008 ) and Chang et al . ( 2012a ) ( the latter adds features for pronominal anaphora resolution ). We also compare with the Best - Left - Link approach described by Bengtson and Roth ( 2008 ).__label__Method|Tool|Use
After initial experimentation , the set of features we use in the matching function is φm ( ti ) = { general - pos , entity - type , relationargument }, and the similarity function examines the remaining features . In our experiments we tested the following five kernels : We also experimented with the function C ( vq , vr ), the compatibility function between two feature values . For example , we can increase the importance of two nodes having the same Wordnet hypernym _CITE_ . If vq , vr are hypernym features , then we can define When & gt ; 1 , we increase the similarity of nodes that share a hypernym . We tested a number of weighting schemes , but did not obtain a set of weights that produced consistent significant improvements .__label__Method|Tool|Use
In our previous work ( Ciobanu and Dinu , 2014a ) we applied this method on a Romanian dictionary , while here we extract cognates from Romanian corpora . We identify cognate pairs between Romanian and six other languages : Italian , French , Spanish , Portuguese , Turkish and English . We use electronic dictionaries _CITE_ to extract etymology - related information and Google Translate to translate Romanian words . We are restricted in our investigation by the available resources , but we plan to extend our method to other related languages as well . We selected these six languages for the following reason : the first four in our list are Romance languages , and our intuition is that there are numerous words in these languages which share a common ancestor with Romanian words .__label__Material|Data|Use
The first RTE challenge aimed to provide the NLP community with a new benchmark to test progress in recognizing textual entailment , and to compare the achievements of different groups . This goal proved to be of great interest , and the community & apos ; s response encouraged the gradual expansion of the scope of the original task . The Second RTE challenge _CITE_ built on the success of the first , with 23 groups from around the world ( as compared to 17 for the first challenge ) submitting the results of their systems . Representatives of participating groups presented their work at the PASCAL Challenges Workshop in April 2006 in Venice , Italy . The event was successful and the number of participants and their contributions to the discussion demonstrated that Textual Entailment is a quickly growing field of NLP research .__label__Method|Algorithm|Introduce
The output layer consists of all candidate synsets of the verb . The individual output weights Wc are candidate specific . To ensure better generalization and to deal with the sparseness of training corpora , Wc is defined as the following sum : where s ( c ) is the respective synset of c , Ps is the set of all hypernyms of s ( transitive closure ) and Es are the synsets entailed by s . We used ClearNLP _CITE_ ( Choi , 2012 ) for extracting SRLs . The results of our system are shown in Table 1 . Our approaches to the disambiguation of English nouns , named entities and verbs generally outperformed all other submissions across different domains as well as the strong baseline provided by the most - frequent - sense ( MFS ).__label__Method|Tool|Use
We show how a Wikipedia - specific Semantic Relatedness measure that leverages the link structure of Wikipedia ( Milne and Witten , 2008b ) allows 3W to be radically more precise at high levels of yield when compared to baseline Wikifiers that target general text . Our experiment shows that 3W can add on average seven new links per article at precision of 0 . 98 , adding approximately 28 million new links to 4 million articles across English Wikipedia . _CITE_ In this section , we define our link extraction task . A link l is a pair of a surface form sl and a concept tl . A surface form is a span of tokens in an article , and the concept is a Wikipedia article referred to by the surface form .__label__Method|Tool|Produce
In full evaluation mode , which generally provides more feedback , the user provides a full paper as an input . This includes semi - automatic import of the manuscript from certain standard document formats such as TeX , MS Office and OpenOffice , as well as semi - automatic structure detection of the manuscript . For the well - known Adobe ’ s portable document format ( PDF ) we use state - of - the - art freely available PdfBox extractor _CITE_ . Unfortunately , PDF format is originally designed for layout and printing and not for structured text interchange . Most of the time , simple copy & paste from a source document to the simple evaluation fields is sufficient .__label__Method|Tool|Use
The vocabulary ( W ) was selected based on word frequency : we used the 5000 most frequent words in the corpus , excluding stop words and strings containing non - alphabetic characters . During computation of the cooccurrence tensor , OOV words were ignored ( rather than deleted ), and the context window was allowed to span sentence boundaries . Models were evaluated using reference data extracted from DiCoEnviro _CITE_ , a specialized dictionary of the environment . This dictionary describes the meaning and behaviour of terms of the environment domain as well as the lexicosemantic relations between these terms . Of the various relations encoded in the dictionary , we focused on a subset of three paradigmatic relations : near - synonyms ( terms that have similar meanings ), antonyms ( opposite meanings ), and hyponyms ( kinds of ).__label__Material|Data|Use
Beside the common task of identifying POS and of reducing this set to NEs , they provide more and more disambiguation facility with URIs that describe web resources , leveraging on the web of real world objects . Moreover , these services classify such information using common ontologies ( e . g . DBpedia ontology or YAGO _CITE_ ) exploiting the large amount of knowledge available from the web of data . Tools such as AlchemyAPI , DBpedia Spotlight , Evri , Extractiv , Lupedia , OpenCalais , Saplo , Wikimeta10 , Yahoo ! Content Extraction11 and Zemanta12 represent a clear opportunity for the web community to increase the volume of interconnected data .__label__Material|Data|Introduce
Different automatic rule translation strategies are evaluated and discussed , providing a comprehensive overview of the challenge . In recent years , inspired by the success of MUC evaluations , a growing number of initiatives ( e . g . TREC , CLEF _CITE_ , CoNLL , Senseval ) have been developed to boost research towards the automatic understanding of textual data . Since 1999 , the Automatic Content Extraction ( ACE ) program has been contributing to broaden the varied scenario of evaluation campaigns by proposing three main tasks , namely the recognition of entities , relations , and events . In 2004 , the Timex2 Detection and Recognition task ( also known as TERN , for Time Expression Recognition and Normalization ) has been added to the ACE program , making the whole evaluation exercise more complete .__label__Material|Data|Introduce
The main difficulty encountered related to the distinction required between proper and common nouns , the morphological boundary between the two being unclear in those fields where common nouns are often reclassified as “ proper nouns ”, as is demonstrated by the presence of these names ( Tanabe et al ., 2005 ) notes that “ a more detailed definition of a gene / protein name , as well as additional annotation rules , could improve inter - annotator agreement and help solve some of the tagging inconsistencies ”. in nomenclatures ( small , acid - soluble spore protein A is an extreme case ) or acronymisation phenomena ( one finds for example across the outer membrane ( OM )). In those cases , annotators were instructed to refer to official lists , such as SwissProt _CITE_ , which requires a significant amount of time . Delimiting the boundaries of the elements to be annotated also raised many questions . One can thus choose to annotate nifh messenger RNA if it is considered that the mention of the state messenger RNA is part of the determination of the reference , or only nifh , if it is considered that the proper noun is enough to build the determination .__label__Supplement|Document|Use
We avoid the word segmentation problem by simply operating at the character level . The Chinese corpus we used in our experiments was also downloaded from the Internet . _CITE_ Eight of the most popular modern Chinese martial art novelists were included in this study , shown in Table 4 . One or two novels was selected from each author to be used as training data , and 20 additional novels were used as a test set . Note that a significant difference between Chinese and English or Greek is that the Chinese character vocabulary is much larger than the English or Greek character vocabularies .__label__Material|Data|Use
This model included a list of “ objective ” error types , graded by their severity and pre - assigned penalty points . The SAE J2450 standard , from the automotive service , also became popular . _CITE_ What became clear from these first efforts was that no one - fits - all evaluation scheme is possible for MT . Each player within the translation workflow , from developers to vendors and clients , has its own needs and the information they expect from the evaluations is different . After LISA ceased operations , two major efforts emerged : TAUS presented its Dynamic Quality Framework ( DQF ) and the QTLaunchPad project developed the Multidimensional Quality Metrics ( MQM ).__label__Supplement|Document|Introduce
The computations for training and testing were done on a MacBook . Required computational resources were minimal . Data resources are as follows : A . CMU pronouncing dictionary : “ an opensource machine - readable pronunciation dictio nary for North American English that contains over 134 , 000 words and their pronunciations ” _CITE_ B . count 1w . txt from Peter Norvig : “ The 1 / 3 million most frequent words , all lowercase , with counts .” 4 C . 10 million tweets collected by Steven Bedrick , of Oregon Health and Sciences University . Table 1 shows the results for the unconstrained task . The project described in the present work is named “ bekli ”.__label__Material|Data|Use
For further details and results , the reader is referred to the key papers cited herein . The first step is to crawl data from a variety of forums and mailing lists , for which we have developed open - source scraping software in the form of SITESCRAPER . _CITE_ SITESCRAPER is designed such that the user simply copies relevant content from a browserrendered version of a given set of pages , which it interprets as a structured record , and translates into a generalised XPATH query . Next , we perform named entity recognition ( NER ) over each thread to identify entities such as package and distribution names , version numbers and snippets of code ; as part of this , we perform version anchoring , in identifying what entity each version number relates to . To generate thread - level metadata , we classify each thread for the following three features , based on an ordinal scale of 1 – 5 ( Baldwin et al ., 2007 ): Complete : Is the problem description complete ?__label__Method|Tool|Produce
However they are located in different places , have been developed using different standards ( if any ) and in many cases are not well documented . High fragmentation and a lack of unified access to language resources are the key obstacles to European innovation potential in language technology ( LT ) development and research . To address these issues the European Commission has dedicated specific activities in its FP7 R & D and ICT - PSP programmes _CITE_ . The overall objective is to ease and speed up the provision of online services centred around computerbased translation and cross - lingual information access and delivery . The focus is on assembling , linking across languages , and making widely available the basic language resources used by developers , professionals and researchers to build specific products and applications .__label__Supplement|Document|Introduce
If the system finds a subsumption of H in the T graph without polarity markings , but no subsumption with polarity markings , then the pair is classified as CONTRADICTION . Polarity marking allows us to use the same structures for both entailment and contradiction testing . Our polarity marking approach is parallell to how negation is represented in AMR _CITE_ . The ERG parser can output a ranked list of candidate analyses for a sentence . We extended our system with n - best matching to facilitate entailment recognition when the top - ranked analysis does not correspond to the perceived meaning of the sentence , i . e ., to reduce the impact of errors in parse ranking .__label__Method|Tool|Use
Currently the system can work with two speech recognition engines , CMU PocketSphinx and Google Chrome ASR . But for our experiments we also considered Apple Dictation . _CITE_ One major decision when selecting a speech recognizer is whether it allows for training domain - specific language models ( LMs ) or not . Purely domain - specific LMs cannot recognize outof - domain words or utterances . On the other hand , general - purpose LMs do not perform well with domain - specific words or utterances .__label__Method|Tool|Use
We have selected 200 sentences from fresh newspaper articles of the biggest Czech on - line daily newspapers . Several headline news were selected in order to avoid author bias although the the domain remained daily news . We have selected articles from ” iDnes ” , ” Lidovky ” and ” Novinky ” _CITE_ . The test set was created from randomly selected articles on the dates between 14 . 7 . 2014 and 18 . 7 . 2014 . All the test - data is publicly available at the language technologies server of the University of Primorska .__label__Material|Data|Use
However , these small snippets of text have several liguistic peculiarities that can be employed to improve the sentiment classification performance . We describe these peculiarities below : However , users tweet in more than 80 languages . The information it contains can be useful to obtain information and updates about , for example , crisis events _CITE_ , in real time . In order to benefit from this , however , a system processing these texts has to be easily adaptable to other languages and it has to work in near real time . Bearing this in mind , the main contributions we bring in this paper are : 1 .__label__Supplement|Document|Introduce
This is lower than Bergsma ’ s top score of 92 . 2 % ( Figure 3 ), but again , Bergsma ’ s top system relies on Google search queries for each new word , while ours are all pre - stored in a table for fast access . We are pleased to be able to share our gender and number data with the NLP community . _CITE_ In Section 6 , we show the benefit of this data as a probabilistic feature in our pronoun resolution system . Probabilistic data is useful because it allows us to rapidly prototype resolution systems without incurring the overhead of large - scale lexical databases such as WordNet ( Miller et al ., 1990 ). Researchers since Dagan and Itai ( 1990 ) have variously argued for and against the utility of collocation statistics between nouns and parents for improving the performance of pronoun resolution .__label__Material|Data|Produce
To summarize , preferences are an effective way to “ define ” the event structure to the learner , which is essential in an unsupervised setting , which may not be easy to do with other forms of constraints . Preferences are naturally decomposable , which allows us to extend their impact without significantly effecting the complexity of inference . In this section , we present our experimental results on the standard ACE05 _CITE_ dataset ( newswire portion ). We choose to perform our evaluations on 4 events ( namely , “ Attack ”, “ Meet ”, “ Die ” and “ Transport ”), which are the only events in this dataset that have more than 50 instances . For each event , we randomly split the instances into two portions , where 70 % are used for learning , and the remaining 30 % for evaluation .__label__Material|Data|Use
Our solution to this problem , partially inspired by ( Lapata and Lascarides , 2003 ), is to collect statistics from an enormous amount of data in order to statistically filter out these coercions . The English Gigaword Forth Edition corpus contains over 8 . 5 million documents of newswire text collected over a 15 year period . We processed these documents with the SENNA _CITE_ ( Collobert and Weston , 2009 ) suite of natural language tools , which includes a part - of - speech tagger , phrase chunker , named entity recognizer , and PropBank semantic role labeler . We chose SENNA due to its speed , yet it still performs comparably with many state - of - the - art systems . Of the 8 . 5 million documents in English Gigaword , 8 million were successfully processed .__label__Method|Tool|Use
The parameters were estimated using maximum likelihood on the training set ; we also implemented a simple absolute discounting smoothing method ( Zhai and Lafferty , 2001 ) that improves the results for both tasks . Table 2 shows the results ( F - measures ) for the problem of finding the most likely sequence of roles given the features observed . In this case , the relation is hidden and we marginalize over it _CITE_ . We experimented with different values for the smoothing factor ranging from a minimum of 0 . 0000005 to a maximum of 10 ; the results shown fix the smoothing factor at its minimum value . We found that for the dynamic models , for a wide range of smoothing factors , we achieved almost identical results ; nevertheless , in future work , we plan to implement cross - validation to find the optimal smoothing factor .__label__Method|Code|Produce
A human evaluation over 202 dialogue acts does not show any difference in naturalness and informativeness between BAGEL ’ s outputs and human utterances . Additionally , we find that the data collection process can be optimised using active learning , resulting in a significant increase in performance when training data is limited , according to ratings from 18 human judges . _CITE_ These results suggest that the proposed framework can largely reduce the development time of NLG systems . While this paper only evaluates the most likely realisation given a dialogue act , we believe that BAGEL ’ s probabilistic nature and generalisation capabilities are well suited to model the linguistic variation resulting from the diversity of annotators . Our first objective is thus to evaluate the quality of BAGEL ’ s n - best outputs , and test whether sampling from the output distribution can improve naturalness and user satisfaction within a dialogue .__label__Material|Data|Use
2004 ), ( b ) its recurring elements ( semantic primitives ( Schank , 1975 ; Wierbicka , 1996 ) or ( c ) its role in discourse : words are grouped by domain , ( see Roget ' s Thesaurus , Roget , 1852 ). Unlike linguists , psychologists are more interested in word relations . Gathering typically related terms ( x evoking y ) they ' ve built association lists ( Deese , 1965 ; Schvaneveldt , 1989 ). Such lists are nowadays freely available in different languages : Dutch , English ( , ), French , German , Japanese , _CITE_ and Russian . The Edinburgh Associative Thesaurus is particularly interesting , in as it shows not only the words evoked (' red ', ' flower ', etc .) in response to a given stimulus (' rose '), but also the causes ( primes ) of this input .__label__Supplement|Document|Introduce
We opted for generalized iterative scaling as it is commonly used for other NLP tasks and off - the - shelf implementations exist . Here we used YASMET . _CITE_ We used a development set of 240 news articles to train YASMET . As YASMET is a supervised optimizer , we had to generate annotated data on which it was to be trained . For each document in the development set , we labeled each sentence as to whether a story highlight was generated from it .__label__Method|Tool|Use
TER is an error metric and it gives an edit ratio ( often referred to as edit rate or error rate ) in terms of how much editing is required to convert a sentence into another with respect to the length of the first sentence . Allowable edit operations include insert , delete , substitute and shift . We use the TER metric ( using tercom - 7 . 251 _CITE_ ) to find the edit rate between a test sentence and the TM reference sentences . Simard and Fujita ( 2012 ) first proposed the use of MT evaluation metrics as similarity functions in implementing TM functionality . They experimented with several MT evaluation metrics , viz .__label__Method|Algorithm|Use
For example , generic ( that is , not query - specific ) summaries , which are often indicative , providing just the gist of a document , are only useful if they happen to address the underlying need of the user . In a push to make summaries more responsive to user needs , the field of summarisation has explored the overlap with complex question - answering & apos ; Information and Communication Technologies Centre research to produce query - focused summaries . Such work includes the recent DUC challenges on queryfocused summarisation , _CITE_ in which the user needs are represented by short paragraphs of text written by human judges . These are then used as input to the summarisation process . However , modelling user needs is a difficult task .__label__Supplement|Website|Introduce
( 2008 ) attempted to create extremely huge training data from the Web using a seed set of entities and relations . In generating training data automatically , this study used context - based tagging . They reported that quite a few good resources ( e . g ., Wikipedia _CITE_ ) listed entities for obtaining training data automatically . This paper described an approach to the acquisition of huge amounts of training data for highperformance Bio NER automatically from a lexical database and unlabeled text . The results demonstrated that the proposed method outperformed dictionary - based NER .__label__Supplement|Website|Introduce
( 2002 ), who define MWEs as : different but related phenomena [ which ] can be described as a sequence of words that acts as a single unit at some level of linguistic analysis . This generic and intentionally vague definition can be narrowed down according to the application needs . For example , for the statistical machine translation ( MT ) system _CITE_ used in the examples shown in Table 1 , an MWE is any sequence of words which , when not translated as a unit , generates errors : ungrammatical or unnatural verbal constructions ( sentence 1 ), awkward literal translations of idioms ( sentence 2 ) and problems of lexical choice and word order in specialised texts ( sentence 3 ). These examples illustrate the importance of correctly dealing with MWEs in MT applications and , more generally , MWEs can speed up and help remove ambiguities in many current NLP applications , for example : Despite the importance of MWEs in several applications , they are often neglected in the design and construction of real - life systems . In 1993 , Smadja pointed out that “... although disambiguation was originally considered as a performance task , the collocations retrieved have not been used for any specific computational task .” Most of the recent and current research in the MWE community still focuses on MWE acquisition instead of integration of automatically acquired or manually compiled resources into applications .__label__Method|Tool|Introduce
In task 2 all entities are provided and only relations are detected , in task 3 also the entities must be predicted . The BB task was the only 2013 task in which we used ( limited ) task specific resources , as TEES 2 . 0 resources developed for the 2011 BB task were directly applicable to the 2013 tasks . A dictionary of bacteria name tokens , derived from the List of Prokaryotic names with Standing in Nomenclature _CITE_ ( Euz ´ eby , 1997 ) was used to improve entity detection performance . Unlike the 2011 task , WordNet features were not used . TEES 2 . 1 achieved F - scores of 42 % and 14 % for tasks 2 and 3 respectively , reaching first place in both tasks .__label__Material|Data|Use
The CBOW and skip - gram embeddings were induced using the word2vec tool , while we used our own implementation of the structured skipgram . The default values in word2vec were employed for most of the parameters , but we set the negative sampling rate to 25 words ( Goldberg and Levy , 2014 ). For the GloVe model , we used the available implementation _CITE_ with the default parameters . In all the models , words occurring less than 100 times in the corpus were discarded , resulting in a vocabulary of around 210 , 000 tokens . Finally , embeddings of different sizes were built , with 50 , 200 , 400 and 600 dimensions .__label__Method|Code|Use
Given six datasets in Catalan , Dutch , English , German , Italian , and Spanish , the task we present involved automatically detecting full coreference chains — composed of named entities ( NEs ), pronouns , and full noun phrases — in four different scenarios . For more information , the reader is referred to the task website . _CITE_ The rest of the paper is organized as follows . Section 2 presents the corpora from which the task datasets were extracted , and the automatic tools used to preprocess them . In Section 3 , we describe the task by providing information about the data format , evaluation settings , and evaluation metrics .__label__Supplement|Website|Produce
Given six datasets in Catalan , Dutch , English , German , Italian , and Spanish , the task we present involved automatically detecting full coreference chains — composed of named entities ( NEs ), pronouns , and full noun phrases — in four different scenarios . For more information , the reader is referred to the task website . _CITE_ The rest of the paper is organized as follows . Section 2 presents the corpora from which the task datasets were extracted , and the automatic tools used to preprocess them . In Section 3 , we describe the task by providing information about the data format , evaluation settings , and evaluation metrics .__label__Supplement|Document|Produce
We verify that the empirical time complexity of inference in SBTDM increases sub - linearly in the number of topics , and show that for large topic spaces SBTDM is more than an order of magnitude more efficient than the hierarchical Pachinko Allocation Model ( Mimno et al ., 2007 ) and nHDP . Lastly , we release an implementation of SBTDM as open - source software . _CITE_ The intuition in SBTDM that topics are naturally arranged in hierarchies also underlies several other models from previous work . Paisley et al . ( 2015 ) introduce the nested Hierarchical Dirichlet Process ( nHDP ), which is a tree - structured generative model of text that generalizes the nested Chinese Restaurant Process ( nCRP ) ( Blei et al ., 2010 ).__label__Method|Code|Produce
Furthermore , they have been shown to model quite well the semantic composition of short phrases via simple vector addition ( Mikolov et al ., 2013b ). To build a vector for a sentence , we simply sum the distributed vectors of the individual words . _CITE_ For both representations , we remove the stopwords before building the vectors . To compute the similarity between two sentences , we compute the cosine similarity between their corresponding vectors . Semantic textual similarity ( STS ): Following on the work of Boltuˇzi ´ c and ˇSnajder ( 2014 ), we use an off - the - shelf STS system developed by ˇSari ´ c et al .__label__Method|Algorithm|Use
By this definition , all 6 strings in TABLE 2 are often thought as a word , but they are not MSUs in our view . Their corresponding MSU forms are shown in TABLE 2 . We collect all the MSUs from the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff _CITE_ . We choose the Peking University ( PKU ) data because it is more fine - grained than all other corpora . Suppose we represent the segmented data as L ( In our case L is the PKU word segmentation data ), the MSU selecting algorithm is shown in TABLE 3 .__label__Material|Data|Use
By making the BioInfer corpus available in the Stanford scheme , we also increase the value of the corpus for biomedical IE . The transformation has the further benefit of allowing Link Grammar output to be normalized into a more applicationoriented form . Finally , to assess the practical value of the conversion method and of the BioInfer syntactic annotation in the Stanford scheme , we compare the Charniak - Lease constituency parser _CITE_ ( Charniak and Lease , 2005 ) and BioLG , an adaptation of LG ( Pyysalo et al ., 2006 ), on the newly unified dataset combining the constituency - annotated GENIA Treebank with the dependency - annotated BioInfer corpus . The transformation rules and software as well as the Stanford annotation of the BioInfer corpus , the main practical results of this work , are freely available at http :// www . it . utu . fi / BioInfer . To support the development of IE systems , it is important for a corpus to provide three key types of annotation capturing the named entities , their relationships and the syntax .__label__Method|Tool|Compare
To compute the domain specificity of a term , we needed its frequency in the general language corpus . For the English language , we used a subpart of the New York Times corpus . For Russian , we used a frequency list computed from The Russian National Corpus _CITE_ . Concerning the dictionaries , we exploited the monolingual parts of the following bilingual general language dictionaries : FR - EN from the ELRA catalogue for the English part , RU - EN from EnglishRussian full dictionary for the Russian part . To obtain the lists of neoclassical roots and prefixes , we firstly took the English and Russian equivalents of neoclassical elements enumerated in ( B ´ echade , 1992 ).__label__Material|Data|Use
This corpus contains English transcriptions and multilingual , sentencealigned translations of talks from the TED conference . While the corpus is aimed at machine translation tasks , we use the keywords associated with each talk to build a subsidiary corpus for multilingual document classification as follows . _CITE_ The development sections provided with the IWSLT 2013 corpus were again reserved for development . We removed approximately 10 percent of the training data in each language to create a test corpus ( all talks with id ≥ 1 , 400 ). The new training corpus consists of a total of 12 , 078 parallel documents distributed across 12 language pairs .__label__Material|Data|Produce
The rest contained negation , but were misclassified either because an incorrect MRS analysis was chosen by the parser or because synonymous words within the scope of the negation were not recognized . EDITS We used a backoff - system for the pairs when the rule - based system fails to produce re sults . Our choice was EDITS _CITE_ as it provides a strong baseline system for recognizing textual entailment ( Kouylekov et al ., 2011 ). EDITS ( Kouylekov and Negri , 2010 ) is an open source package which offers a modular , flexible , and adaptable working environment for experimenting with the RTE task over different datasets . The package allows to : i ) create an entailment engine by defining its basic components ; ii ) train this entailment engine over an annotated RTE corpus to learn a model and iii ) use the entailment engine and the model to assign an entailment judgment and a confidence score to each pair of an unannotated test corpus .__label__Method|Tool|Use
The indomain data is divided into a training set ( for SMT The recurrent states are unrolled for several time - steps , then stochastic gradient descent is applied . pipeline and neural LM training ), a tuning set ( for MERT ), a validation set ( for choosing the optimal threshold in data selection ), and finally a testset of 1616 sentences . _CITE_ Table 1 lists data statistics . For each language pair , we built a baseline indata SMT system trained only on in - domain data , and an alldata system using combined in - domain and general - domain data . We then built 3 systems from augmented data selected by different LMs : All systems are built using standard settings in the Moses toolkit ( GIZA ++ alignment , grow - diagfinal - and , lexical reordering models , and SRILM ).__label__Material|Data|Use
This corpus is close to the experimental conditions , but it contains only 11272 sentences couples . This is quite low for the LSI approach . Therefore , we use also the EnglishSpanish part of the Europarl ( Koehn , 2005 ) corpus composed of 2M sentences couples _CITE_ . Each training corpus leads to one LSI model . To synthesize , we extract a feature from a ( s , t ) LSI is a function which projects a sentence into the numeric LSI space .__label__Material|Data|Use
So we map the words ranging from − 5 to − 1 in SentiStrength to negative in our grading system , and the words ranging from + 1 to + 5 to positive . The rest are mapped to neutral . We do the same for the other two lexicon resources : OpinionFinder ( Wiebe et al ., 2005 ) and SentiWordNet _CITE_ ( Esuli and Sebastiani , 2006 ; Baccianella and Sebastiani , 2010 ). We further filter out the low - frequency features which have been observed less than 3 times in the training data . Because these features are not stable indicators of sentiment .__label__Material|Data|Use
For our analysis , we have chosen the topic of gene regulatory events in E . coli , which is a domain of very active research and grand challenges . Currently the gold standard of the existing body of knowledge of such events is represented by the fact database REGULONDB . _CITE_ Its content has been man The field of gene regulation is one of the most prominent topics of research and often mentioned as one of the core fields of future research in molecular biology ( cf , e . g ., the Grand Challenge I - 2 described by Collins et al . ( 2003 )). ually gathered from the scientific literature and describes the curated computational model of mechanisms of transcriptional regulation in E . coli .__label__Material|Data|Use
Furthermore , we hypothesize that it is better not to exclude stop words . Quotation mining can thus in our view be thought of as an application that is similar to sentence classification in that famous quotes are relatively small , and similar to authorship attribution in that style is an important predictor of whether a sentence is a famous quote . We obtain the database of famous quotes from a popular on - line collection of quotes _CITE_ and use philosophical and literary text sampled from the Gutenberg corpus as negative data . In particular we use the portion of Gutenberg documents that is distributed in the corpora collection at NLTK . This gives us a total of 44 , 385 positive data points ( famous quotes ) and 247 , 115 negative data points ( ordinary sentences ).__label__Material|Data|Use
The detected names are then mapped onto unique identifiers across a range of taxonomic databases such as uBio Name Bank , Encyclopaedia of Life ( EoL ) , and Catalogue of Life ( CoL ) . Taxonomic names that cannot be found in online databases will be validated manually . Potential unknown taxonomic names are presented for validation or correction to the research community via the Scratchpads social network _CITE_ ( e . g ., professional taxonomists , experienced citizen scientists and other biodiversity specialists ) in a community - driven verification process . The newly verified taxonomic name , along with additional metadata recording the user who verified the name , its context and bibliographic details is published as a semantic web service layer ( currently a Scratchpads portal ). Automatic identification of taxonomic names from biodiversity text has attracted increasing research interest over the past few years , but is difficult because of the problems of erroneous transcription and synonymy .__label__Supplement|Website|Use
The platform also supports visualization of individual Text - Hypothesis pairs , showing the alignments that were created by the system as well as the features computed on the basis of the alignments . The visualization was built on the basis of the BRAT library . _CITE_ Figure 2 shows an example for the Text The judges made an assessment of Peter ’ s credibility and the Hypothesis The judges assessed if Peter was credible . The top line shows the final prediction , Entailment , and the confidence ( 75 %). The main part shows the Text and the Hypothesis below each other , connected by alignment links that are labeled with their source and their score .__label__Method|Code|Extent
This section describes how this testbed has been built . The testbed must have a certain number of features which , altogether , differentiate the task from current multi - document summarization evaluations : Complex information needs . Being Information Synthesis a step which immediately follows a document retrieval process , it seems natural to start with standard IR topics as used in evaluation conferences such as TREC , CLEF or NTCIR _CITE_ . The title / description / narrative topics commonly used in such evaluation exercises are specially well suited for an Information Synthesis task : they are complex and well defined , unlike , for instance , typical web queries . We have selected the Spanish CLEF 2001 - 2003 news collection testbed ( Peters et al ., 2002 ), because Spanish is the native language of the subjects recruited for the manual generation of reports .__label__Supplement|Website|Extent
In some cases , the test taker ’ s spoken response was nearly identical to an identified source ; in other cases , several sentences or phrases were clearly drawn from a particular source , although some modifications were apparent . Table 1 presents a sample source that was identified for several of the 239 responses in the data set . _CITE_ Many of the plagiarized responses contained extended sequences of words that directly match idiosyncratic features of this source , such as the phrases “ how romantic it can ever be ” and “ just relax yourself on the beach .” In total , 49 different source materials were identified for all of the potentially plagiarized responses in the corpus . In addition to the source materials and the plagiarized responses , a set of non - plagiarized control responses was also obtained in order to conduct classification experiments between plagiarized and non - plagiarized responses . Since the plagiarized responses were collected over the course of more than one year , they were drawn from many different TOEFL iBT test forms ; in total , the 239 plagiarized responses comprise 103 distinct Independent test questions .__label__Material|Data|Use
Only selected positions in a sentence ( determined by rules ) undergo punctuation correction . The spelling correction candidates are given by a spell checker . We used GNU Aspell _CITE_ in our work . As described in Section 3 . 2 , the weight of each variable is a linear combination of the language model score , three classifier confidence scores , and three classifier disagreement scores . We use the Web 1T 5 - gram corpus ( Brants and Franz , 2006 ) to compute the language model score for a sentence .__label__Method|Tool|Use
The CALBC SSC - I has been harmonised from the annotations of the systems provided by the four project partners . Three of them are dictionary based systems while the other is an ML based system . The systems utilized different types of resources such as GENIA corpus ( Kim et al ., 2003 ), Entrez Genes , Uniprot _CITE_ , etc . The CALBC SSCII corpus has been harmonised from the annotations done by the 11 participants of the first CALBC challenge and the project partners . Some of the participants have used the CALBC SSC - I versions for training while others used various gene databases or benchmark GSCs such as the BioCreAtIvE II GM corpus .__label__Material|Data|Use
Graphs have long been used to describe linguistic annotations , most familiarly in the form of trees ( a graph in which each node has a single parent ) for syntactic annotation . Annotation Graphs ( Bird and Liberman , 2001 ) have been widely used to represent layers of annotation , each associated with primary data , although the concept was not extended to allow for annotations linked to other annotations and thus to consider multiple annotations as a single graph . More recently , the Penn Discourse TreeBank released its annotations of the Penn TreeBank as a graph , accompanied by an API that provides a set of standard graphhandling functions for query and access _CITE_ . The graph model therefore seems to be gaining ground as a natural and flexible model for linguistic annotations which , as we demonstrate below , can repre LAF provides a general framework for representing annotations that has been described elsewhere in detail ( Ide and Romary , 2004 , 2006 ). Its development has built on common practice and convergence of approach in linguistic annotation over the past 15 - 20 years .__label__Material|Data|Introduce
Graphs have long been used to describe linguistic annotations , most familiarly in the form of trees ( a graph in which each node has a single parent ) for syntactic annotation . Annotation Graphs ( Bird and Liberman , 2001 ) have been widely used to represent layers of annotation , each associated with primary data , although the concept was not extended to allow for annotations linked to other annotations and thus to consider multiple annotations as a single graph . More recently , the Penn Discourse TreeBank released its annotations of the Penn TreeBank as a graph , accompanied by an API that provides a set of standard graphhandling functions for query and access _CITE_ . The graph model therefore seems to be gaining ground as a natural and flexible model for linguistic annotations which , as we demonstrate below , can repre LAF provides a general framework for representing annotations that has been described elsewhere in detail ( Ide and Romary , 2004 , 2006 ). Its development has built on common practice and convergence of approach in linguistic annotation over the past 15 - 20 years .__label__Supplement|Website|Introduce
We add a full stop ourselves to avoid them being connected with the first sentence . For Chinese , we split sentences according to ending punctuation marks , while for other nine languages , the full stop “.” could have other functions . We adopt machine learning method _CITE_ . After some experiments , we choose Support Vector Machine model for English and French , Naïve Bayes model for other 7 languages . We add ICTCLAS word segmentation to Chinese to make all languages have the same word separator .__label__Method|Algorithm|Use
Using two different sets of morpho - syntactic features results in more effective models , as they create a kind of agreement for a given word in case of match . Concerning the PCFG model , grammars , tree binarization and the different tree representations are created with our own scripts , while entity tree parsing is performed with the chart parsing algorithm described in ( Johnson , 1998 ). _CITE_ All results are expressed in terms of Slot Error Rate ( SER ) ( Makhoul et al ., 1999 ) which has a similar definition of word error rate for ASR systems , with the difference that substitution errors are split in three types : i ) correct entity type with wrong segmentation ; ii ) wrong entity type with correct segmentation ; iii ) wrong entity type with wrong segmentation ; here , i ) and ii ) are given half points , while iii ), as well as insertion and deletion errors , are given full points . Moreover , results are given using the well known F1 measure , defined as a function of precision and recall . In this section we provide evaluations of the models described in this work , based on combination of CRF and PCFG and using different tree representations of named entity trees .__label__Method|Algorithm|Use
acquired web pages that have almost the same content ) are reviewed and compared in Theobald et al . ( 2008 ). Efficient focused web crawlers can be built by adapting existing open - source frameworks like Heritrix _CITE_ , Nutch and Bixo . For instance , Combine is an open - source focused crawler that is based on a combination of a general web crawler and a text classifier . Other approaches make use of search engines APIs to identify in - domain web pages ( Hong et al ., 2010 ) or multilingual web sites ( Resnik and Smith , 2003 ).__label__Method|Code|Use
SemCor The SENSEVAL - 2 English tasks have decided to use the WordNet 1 . 7 sense inventory , and therefore we had to deal with the task of mapping SemCor senses , which were assigned using an earlier version of WordNet , to the corresponding senses in WordNet 1 . 7 . When a word sense from WordNet 1 . 6 is missing we assign a default sense of 0 . _CITE_ WordNet The main idea in generating a sense tagged corpus out of WordNet is very simple . It is based on the underlying assumption that each example pertains to a word belonging to the current synset , thereby allowing us to assign the correct sense to at least one word in each example . For instance , the example given for mother # 4 is " necessity is the mother of invention ", and the word mother can be tagged with its appropriate sense .__label__Supplement|Website|Introduce
About the parser , the accuracy of the 47 - type POS tagging is 94 . 22 %. According to these evaluation results , it is believed that our traditional Chinese parser is sophisticated enough . Four text corpora , the LDC Chinese Giga - byte _CITE_ , Sinica Balanced Corpus , CIRB030 ( Chinese Information Retrieval Benchmark , version 3 . 03 ), the Taiwan Panorama Magazine and context of Wikipedia ( zh_tw version ) were used to construct a 100k tri - gram LM . There are in total 440 million words in the corpora . They were first parsed and post - processed ( text normalization , word variation replacement , numbers into short - word conversion , etc .).__label__Material|Data|Use
In the following experiments , we used a fixed context size for SOUL of m = 10 , and used k = 300 . All the parallel data allowed in the constrained task are pooled together to create a single parallel corpus . This corpus is word - aligned using MGIZA ++ _CITE_ with default settings . For the English monolingual training data , we used the same setup as last year and thus the same target language model as detailed in ( Allauzen et al ., 2011 ). For English , we also took advantage of our inhouse text processing tools for the tokenization and detokenization steps ( Dchelotte et al ., 2008 ) and our system is built in “ true - case ”.__label__Method|Tool|Use
In this paper , we bring these two parallel views together , grounding the linguistic realization of these relations in the semantics of planning operations . The challenge and opportunity of this fusion comes from the mismatch between the abstractions of human language and the granularity of planning primitives . Consider , for example , text describing a virtual world such as Minecraft _CITE_ and a formal description of that world using planning primitives . Due to the mismatch in granularity , even the simple relations between wood , pickaxe and stone described in the sentence in Figure 1a results in dozens of lowlevel planning actions in the world , as can be seen in Figure 1b . While the text provides a high - level description of world dynamics , it does not provide sufficient details for successful plan execution .__label__Supplement|Media|Introduce
In the rest of this section we first provide an introduction to the LAP , then we move to the EC and finally describe the configuration files . The Linguistic Analysis Pipeline is a collection of annotation components for Natural Language Processing ( NLP ) based on the Apache UIMA framework . _CITE_ Annotations range from tokenization to part of speech tagging , chunking , Named Entity Recognition and parsing . The adoption of UIMA enables interoperability among components ( e . g ., substitution of one parser by another one ) while ensuring language independence . Input and output of the components are represented in an extended version of the DKPro type system based on UIMA The Entailment Core performs the actual entailment recognition based on the preprocessed text made by the Linguistic Analysis Pipeline .__label__Method|Tool|Extent
Clearly , monolingual ( Cebuano ) texts were necessary in order to have clean data to work on . Various websites were found containing news texts , though these had mostly to be downloaded daily because there were no archives . In particular we found two good sources : Superbalita _CITE_ and iliganon . com ( local news from Ili gan City and the surrounding area ). Other sites found bilingual text resources online , such as the Bible , but these were not particularly helpful to us since they did not contain the kinds of entities we were interested in . Had we found any such texts , it could have been very useful as a method of mining the English texts for gazetteer entries and grammar rules .__label__Material|Data|Use
T 34 = ( 2 / 3 x 1 + 1 / 3 x 1 )/ 1 = 1 . We apply hierarchical clustering ( Voorhees , 1986 ) to the similarity matrix above to obtain the final global clustering results . We recorded 10 games from 3J3F _CITE_ , one of the most popular Chinese online killer game websites . A screenshot of the game system interface is shown in Figure 5 . There are 16 participating players per game : 4 detectives , 4 killers and 8 citizens .__label__Supplement|Website|Use
With our WaW model - based reordering pruning technique , we aim at solving both issues . We generate the WaW training data from the first 30K sentences of the News - commentary2010 parallel corpus , using a sampling window of width 5 = 10 . This results in 8 million training samples , which are fed to the binary classifier implementation of the MegaM Toolkit _CITE_ . Features with less than 20 occurrences are ignored and the maximum number of training iterations is set to 100 . Evaluated intrinsically on test08 , the model achieves the following classification accuracy : 67 . 0 % precision , 50 . 0 % recall , 57 . 2 % F - score .__label__Method|Tool|Use
The English Translation Treebank , ETT ( Bies , 2007 ), is the translation of broadcast news in Arabic . For literature , we use the BROWN corpus ( Francis and Kuˇcera , 1979 ) and the same division as ( Gildea , 2001 ; Bacchiani et al ., 2006 ; McClosky et al ., 2006b ). We also use raw sentences which we downloaded from Project Gutenberg _CITE_ as a self - trained corpus . The Switchboard corpus ( SWBD ) consists of transcribed telephone conversations . While the original trees include disfluency information , we assume our speech corpora have had speech repairs excised ( e . g .__label__Material|Data|Use
An implementation of the proposed multi - sentence compression approach is available for download . We constructed an evaluation dataset made of 40 sets of related sentences along with reference compressions composed by humans . This dataset is freely available for download _CITE_ . We performed both manual and automatic evaluations and showed that our method significantly improves the informativity of the generated compressions . We also investigated the correlation between manual and automatic evaluation metrics and found that ROUGE and BLEU have a medium correlation with manual ratings .__label__Material|Data|Produce
PropBank ( Hwang et al ., 2010 ), and WordNet ( Vincze et al ., 2012 ). In this paper , we propose a formal representation of LVCs in the valency lexicon of Czech verbs , VALLEX . _CITE_ The VALLEX lexicon is a collection of rich linguistically annotated data resulting from an attempt at a formal description of the valency behavior of Czech verbs . It provides the information on the valency structure of Czech verbs in the form of valency frames , each valency frame corresponding to a single verbal lexical unit . For the description of valency , the valency theory formulated within the Functional Generative Description ( FGD ) – a dependency based framework – has been adopted ( Sgall et al ., 1986 ).__label__Method|Algorithm|Produce
The lexical variations of a lexical entry usually have high phonetic , string - based , and contextual similarity . We integrate a phonetic - based encoding scheme , UrduPhone , a feature - based similarity function , and a clustering algorithm , Lex - C . Several sound - based encoding schemes for words have been proposed in literature such as Soundex ( Knuth , 1973 ; Hall and Dowling , 1980 ), NYSIIS ( Taft , 1970 ), Metaphone ( Philips , 1990 ), Caverphone ( Wang , 2009 ) and Double Metaphone . _CITE_ These schemes encode words based on their sound Spell correction is also considered as a variant of text normalization ( Damerau , 1964 ; Tahira , 2004 ; Fossati and Di Eugenio , 2007 ). Here , we limit ourselves to the previous work on short text normalization . which in turn serves as grouping words of similar sounds ( lexical variations ) to one code .__label__Method|Tool|Extent
Depending on the generality of the knowledge domains they cover , several types of ontologies are distinguished . These are upper - level ontologies , domain ontologies and application ontologies . Upper - level ontologies , or foundational ontologies , describe very general concepts that can be used across multiple domains ; examples include DOLCE , SUMO _CITE_ , and PROTON . Domain ontologies cover the conceptualization of given subject domains . They describe concepts and relationships representative for the subject domain like biology , vehicle sales , product types , etc .__label__Material|Data|Introduce
For each relation , we randomly extracted the same number of positive and negative instances as training data , while all instances in sections 21 and 22 are used as our test set . The statistics of various data sets is listed in Table 1 . We tokenized PDTB corpus using Stanford NLP Tool _CITE_ . For all experiments , we empirically set and test ( Test ) sets . COMP .= COMPARISON , CONT .= CONTINGENCY , EXP .= EXPANSION and TEMP .= TEMPORAL d = 128 and A = 1e − .__label__Method|Tool|Use
We train an ML - based classifier using the corpus . Previous studies have found that , among several ML - based approaches , the SVM classifier generally performs well in many subjectivity analysis tasks ( Pang et al ., 2002 ; Banea et al ., 2008 ). We use SVMLzght with its default configurations , _CITE_ inputted with a sentence represented as a feature vector of word unigrams and their counts in the sentence . An SVM score ( a margin or the distance from a learned decision boundary ) with a positive value predicts the input as being subjective , and negative value as objective . Lexicon - based ( S - LB ): OpinionFinder contains a list of English subjectivity clue words with intensity labels ( Wilson et al ., 2005 ).__label__Method|Tool|Use
The dimension of bilingual embeddings d is set to 50 , and destruction fraction ν is set to 0 . 2 . Effects of Bilingual Embedding Learning Methods We first compare our unsupervised bilingual embedding learning method with the parallel corpora based method . The parallel corpora based method uses the paired documents in the parallel corpus _CITE_ to learn bilingual embeddings , while our method only uses the English training documents and their Chinese translations ( Sentiment polarity labels of documents are not employed here ). The Boolean feature weight calculation method is adopted to represent documents for bilingual embedding learning and BDR is employed to represent training data and test data for sentiment classification . To represent the paired documents in the parallel corpus , 27 , 597 English words and 31 , 786 Chinese words are extracted for bilingual embedding learning .__label__Material|Data|Use
Drug name recognition was not necessary because the evaluation focused only on a set of four drugs : carbamazepine , olanzapine , http :// www . hc - sc . gc . ca / dhp - mps / medeff / indexeng . php An extension of this system was accomplished by Nikfarjam and Gonzalez ( 2011 ). The authors applied association rule mining to extract frequent patterns describing opinions about drugs . The rules were generated using the Apriori tool _CITE_ , an implementation of the Apriori algorithm ( Agrawal and Srikant , 1994 ) for association rule mining . The system was evaluated using the same corpus created for their previous work ( Leaman et al ., 2010 ), and which has been described above . The system achieved a precision of 70 . 01 % and a recall of 66 . 32 %.__label__Method|Tool|Use
Thus , the sentences often lack correct syntactic structure . The domain of this task is appointment scheduling and travel arrangements . The POS information for the English part of the corpus was generated using the Brill tagger _CITE_ . As Table 4 shows , the splicing operation increases the cardinality of the English vocabulary as well as the number of singletons significantly . Nevertheless , they are still below those numbers for Spanish and Catalan .__label__Method|Tool|Use
The types were unknown ( response indicates participant is unsure what to do next ), ask for more ( ask for more details ), propose alternative ( propose alternative object ), ask for help ( ask operator to physically manipulate environment ), and off topic . We recruited 30 participants . All participants completed the web form through the Amazon Mechanical Turk ( MTurk ) web portal _CITE_ , all were located in the United States and had a task approval rate ≥ 95 %. The group included 29 self - reported native English speakers born in the United States ; 1 self - reported as a native Bangla speaker born in Bangladesh . The gender distribution was 15 male to 15 female .__label__Supplement|Website|Use
For example , functionality specification before implementation was described for CGAL and is typical of large projects but would have been cumbersome for Moses . Secondly , the aims of Moses and these projects are different . The goal of the CGAL project is to ‘ make ... computational geometry available for industrial application ’ _CITE_ . Both CGAL and DCMTK are used extensively in commercial applications . Therefore , issues such robustness , cross - platform compatibility and easeof - use are predominant for these projects .__label__Supplement|Document|Introduce
dry - PRF - PL DEF - cloth - PL - SUBJ - NOM ‘ The colthes dried .’ In example ( 1 ) the causative / incoative alternation is realized through an overt morphological change on the head of the sentence ( reduplication of the second root consonant in ( 1a )), in such a way that the verb changes to a new entry , which according to the hierarchical organisation of the class and especially to the inheritance relation between its subparts , cannot longer be kept into the original class . Transporting the new verb entry into a new class risks to loose its connection to the original class , which is an undesired effect , since it does not necessarily reflect the natural organisation of the lexicon of Arabic . Arabic VerbNet _CITE_ is a large coverage verb lexicon exploiting Levin ’ s classes ( Levin , 1993 ) and the basic development procedure of Kipper Schuler ( 2005 ). The current version has 202 classes populating 4707 verbs and 834 frames . Every class is a hierarchical structure providing syntactic and semantic information about verbs and percolating them to subclasses .__label__Material|Data|Introduce
Evaluation results using different alignment methods based on the same data sets are given in Tables 5 and 7 . The system built based on GIZA ++/ Moses pipeline as a baseline system is given in Table 5 . We also show the evaluation results obtained by the WAT 2015 automatic evaluation _CITE_ in Table 6 and 8 . The results in Table 7 and 8 show that there are no significant differences among the evaluation results based on different versions of Moses , different Anymalign timeouts or different versions of Cutnalign . However , the training times changed considerably depending on the timeouts for Anymalign .__label__Method|Algorithm|Use
Then , Wikipedia titles a page for a proboxer to “ FA '" 3L — Oa ” instead of his proper name “ FA吉丈 — Oa ”, and then guides us to the page , even if we , on top of Wikipedia , search a page with the proper name “ FA吉丈 — Oa ”. We must take care of extended kanji characters with surrogate pairs in data resources . N - Triples _CITE_ is a line - based , plain text format for encoding an RDF graph , but the character encoding in string is designated to 7 - bit US - ASCII . So , nonASCII characters must be made available by \ escape sequences , such as ‘\ u3042 ’ for Japanese hiragana ‘ あ ’ ( U + 3042 ). 10 RDF / XML syntax11 designates %- encoding for disallowable characters that do not correspond to permitted US - ASCII in URI encoding , in spite that the UNICODE string as UTF - 8 is designated to the RDF / XML representation . Therefore , the disallowed URL http :// ja . dbpedia . org / page / K � 1 t — M must be escaped as http :// ja . dbpedia .__label__Method|Algorithm|Introduce
Answers : Arts , Education , Health , Science , and Sports . We randomly chose 200 questions from each category to create a raw dataset with 1 , 000 questions total . Then , we labeled the dataset with annotators from the Amazon ’ s Mechanical Turk service _CITE_ . For annotation , each question was judged by 5 Mechanical Turk workers who passed a qualification test of 10 questions ( labeled by ourselves ) with at least 9 of them correctly marked . The qualification test was required to ensure that the raters were sufficiently competent to make reasonable judgments .__label__Supplement|Website|Use
To recognize the fact that H is “ entailed ” by T , we often need to use some background commonsense knowledge . For instance , in the example above it is essential that financing is an activity . The approach to recognizing textual entailment employed in Bos and Markert ( 2005 ) and implemented in the system Nutcracker _CITE_ can be summarized as follows : Related work is described in Akhmatova ( 2005 ); Fowler et al . ( 2005 ). The approach to the problem proposed in Baral et al .__label__Method|Tool|Introduce
This is similar to a named entity recognition or to a chunking task where only one type of entity / chunk is detected . For comparison we compare the performance of the approach with a rule based method for detecting appositive post - modifiers . The corpus was used to build two supervised tagging models based on Conditional Random Fields ( Lafferty et al ., 2001 ): CRF ++ _CITE_ and crfsuite . Four feature sets were used . Model A contains standard features used in chunking , such as word form , lemma and part of speech ( POS ) tag .__label__Method|Tool|Use
A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems . In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a ) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first - order parsing ; b ) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second - order parsing ; c ) we empirically demonstrate that our approach with up to third - order and global features outperforms the state - of - the - art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non - projective CoNLL datasets . _CITE_ Dependency parsing is typically guided by parameterized scoring functions that involve rich features exerting refined control over the choice of parse trees . As a consequence , finding the highest scoring parse tree is a provably hard combinatorial inference problem ( McDonald and Pereira , 2006 ). Much of the recent work on parsing has focused on solving these problems using powerful optimization techniques .__label__Material|Data|Use
Thus , physicians practicing EBM should be constantly aware of the new ideas and the best methodologies available based on the most recent literature . But the amount of clinical documents available is increasing everyday . For example , Pubmed , a service of the US National Library of Medicine contains more than 21 million citations for biomedical literature from MEDLINE , life science journals , and online books ( last updated on December 7 , 2011 ) _CITE_ . The abundance of digital information makes difficult the task of evaluating the quality of results presented and the significance of the conclusions drawn . Thus , it has become an important task to grade the quality of evidence so that the most significant evidence is incorporated into the clinical practices .__label__Material|Data|Introduce
Their inputs are often statement sentences which include subject and object entities for a given predicate , whereas NLquestions lack either a subject or object entity that is the potential answer . Hence , we can only use the information of a subject or object entity , which leads to a different training instance generation procedure and a different training criterion . Recently , researchers have developed open domain systems based on large scale KBs such as FREEBASE _CITE_ ( Cai and Yates , 2013 ; Fader et al ., 2013 ; Berant et al ., 2013 ; Kwiatkowski et al ., 2013 ; Bao et al ., 2014 ; Berant and Liang , 2014 ; Yao and Van Durme , 2014 ). Their semantic parsers for Open QA are unified formal and scalable : they enable the NL - question to be mapped into the appropriate logical form . Our method obtains similar logical forms , but using only lowdimensional embeddings of n - grams , entity types , and predicates learned from texts and KB .__label__Material|Data|Extent
These sentences constitute the impact - based summary of the paper . Despite the use of citation contexts and anchor text in many information retrieval and natural language processing tasks , to our knowledge , we are the first to propose the incorporation of citation context information available in citation networks in a co - training framework for topic classification of research papers . The dataset used in our experiments is a subset sampled from the CiteSeer " digital library _CITE_ and labeled by Dr . Lise Getoor ’ s research group at the University of Maryland . This subset was previously used in several studies including ( Lu and Getoor , 2003 ) and ( Kataria et al ., 2010 ). The dataset consists of 3186 labeled papers , with each paper being categorized into one of six classes : Agents , Artificial Intelligence ( AI ), Information Retrieval ( IR ), Machine Learning ( ML ), HumanComputer Interaction ( HCI ) and Databases ( DB ).__label__Material|Data|Use
It has been shown that using syntactic information can improve the accuracy of sentiment models ( Joshi and Ros ´ e , 2009 ). Thus , instead of representing documents as a bag of words , we will experiment with using features returned by a dependency parser . For this , we used the Stanford parser _CITE_ , which returns dependency tuples of the form rel ( a , b ) where rel is some dependency relation and a and b are tokens of a sentence . We can use these specific tuples as features , referred here as the full - tuple representation . One problem with this representation is that we are using very specific information and it is harder for learning algorithms to find patterns due to the lack of redundancy .__label__Method|Tool|Use
The datasets that were used in the task were extracted from the above - mentioned corpora . Table 1 summarizes the number of documents ( docs ), sentences ( sents ), and tokens in the training , development and test sets . Catalan , Spanish , English Predicted lemmas and PoS were generated using FreeLing _CITE_ for Catalan / Spanish and SVMTagger for English . Dependency information and predicate semantic roles were generated with JointParser , a syntacticsemantic parser . Dutch Lemmas , PoS and NEs were automatically provided by the memory - based shallow parser for Dutch ( Daelemans et al ., 1999 ), and dependency information by the Alpino parser ( van Noord et al ., 2006 ).__label__Method|Tool|Use
We use the ESA similarity measure ( Gabrilovich and Markovitch , 2007 ) as implemented in the DKPro similarity software pack age ( B ¨ ar et al ., 2013 ), calculated on English Wik - Features with the highest Information Gain tionary and WordNet as two separate concept were the ones based on Liuaugmented . Adding the spaces . The ESA vectors are freely available _CITE_ . weighted intensifiers of Brooke to the sentiment This way we obtain in total six features : sim ( orig - lexicons did not outperform the simple lexicon inal tweet word list , positive word list ), sim ( orig - lookup . They were followed by features derived inal tweet word list , negative word list ), differ - from the lexicons of Steinberger , which includes ence between the two , sim ( expanded tweet word invertors , intensifiers and four polarity levels of list , positive word list ), sim ( expanded tweet word words .__label__Material|Data|Use
We report experiments on Finnish and English corpora . The new category - learning algorithm is compared to two other algorithms , namely the baseline segmentation algorithm presented in ( Creutz , 2003 ), which was also utilized for initializing the segmentation in the category - learning algorithm , and the Linguistica algorithm ( Goldsmith , 2001 ). The Finnish corpus consists of mainly news texts from the CSC ( The Finnish IT Center for Science ) _CITE_ and the Finnish News Agency . The corpus consists of 32 million words and it was divided into a development set and a test set , each containing 16 million words . For experiments on English we have used the Brown corpus .__label__Material|Data|Use
The experiments were carried out in a Brazilian Portuguese ( pt ) and English ( en ) parallel corpus containing 108 pairs of syntactic trees . These trees were generated by syntactic parser for pt [ Bick 2000 ] and en [ Collins 1999 ], separately . These trees represent sentences derived from articles of the Pesquisa FAPESP _CITE_ Brazilian magazine . From this test corpus , a gold standard manually aligned by a human expert was produced to serve as reference in the automatic comparison . Table 1 shows the precision , recall and F scores for the five models .__label__Material|Data|Use
The method is described in the following steps . Frequency List : Corpus Factory method requires a frequency list of the language of interest to start corpus collection . The frequency list of the language is built from its Wikipedia dump _CITE_ . The dump is processed to remove all the Wiki and HTML markup to extract raw corpus , the Wiki corpus . The frequency list is then built from the tokenised Wiki corpus .__label__Material|Data|Use
Furthermore , the sentences could be declarative , imperative , or interrogative , as long as they conformed to the requirements stated above . The full test suite of 236 sentences is made freely available to the community . Two popular SMT systems , namely Google Translate and Bing Translator , _CITE_ were utilised to perform German to English translation on the test suite . The translation results were then manually evaluated under the following criteria : the translation of the base verb was judged to be either correct or incorrect in order to show if the particle of an incorrectly translated VPC was ignored , or if the verb was translated incorrectly for any other reason . For VPCs , this was judged to be correct if either the VPC , or at least the base verb was translated correctly .__label__Method|Tool|Use
), which can be instantiated in different ways depending on the annotator ❑ s approach and goals . We have implemented both the abstract model and various instantiations using XML schemas ( Thompson , et al ., 2000 ), the Resource Definition Framework ( RDF ) ( Lassila and Swick , 2000 ) and RDF schemas ( Brickley and Guha , 2000 ), which enable description and definition of abstract data models together with means to interpret , via the model , information encoded according to different conventions . The results have been incorporated into XCES ( Ide , et al ., 2000a ), part of the EAGLES Guidelines developed by the Expert Advisory Group on Language Engineering Standards ( EAGLES ) _CITE_ . The XCES provides a ready - made , standard encoding format together with a data architecture designed specifically for linguistically annotated corpora . In this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation .__label__Supplement|Website|Introduce
), which can be instantiated in different ways depending on the annotator ❑ s approach and goals . We have implemented both the abstract model and various instantiations using XML schemas ( Thompson , et al ., 2000 ), the Resource Definition Framework ( RDF ) ( Lassila and Swick , 2000 ) and RDF schemas ( Brickley and Guha , 2000 ), which enable description and definition of abstract data models together with means to interpret , via the model , information encoded according to different conventions . The results have been incorporated into XCES ( Ide , et al ., 2000a ), part of the EAGLES Guidelines developed by the Expert Advisory Group on Language Engineering Standards ( EAGLES ) _CITE_ . The XCES provides a ready - made , standard encoding format together with a data architecture designed specifically for linguistically annotated corpora . In this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation .__label__Supplement|Document|Introduce
For annotation , task - specific tools are being used , e . g . EXMARaLDA , annotate , RSTTool , and MMAX . _CITE_ Data is then converted into a standoff data interchange format , which is fed into the linguistic database ANNIS . ANNIS aims at providing functionalities for exploring and querying the data , offering suitable means for both visualization and export . Central requirements evolving out of the scenario sketched above and , as we believe , for multilevel annotation in general are Data heterogeneity , Data reuse , and Accessibility ( cf .__label__Method|Tool|Use
E . g . by substituting “ happen ” by “ occur ” ( example 3 ), one character is saved without affecting the sentence meaning . The data used in our experiments was collected in the context of the MUSA ( Multilingual Subtitling of Multimedia Content ) project ( Piperidis et al ., 2004 ) _CITE_ and was kindly provided for the current study . The data was provided by the BBC in the form of Horizon documentary transcripts with the corresponding audio and video . The data for two documentaries was used to create a dataset consisting of sentences from the transcripts and the corresponding substitution examples in which selected words are substituted by a shorter Wordnet synonym .__label__Method|Tool|Use
E . g . by substituting “ happen ” by “ occur ” ( example 3 ), one character is saved without affecting the sentence meaning . The data used in our experiments was collected in the context of the MUSA ( Multilingual Subtitling of Multimedia Content ) project ( Piperidis et al ., 2004 ) _CITE_ and was kindly provided for the current study . The data was provided by the BBC in the form of Horizon documentary transcripts with the corresponding audio and video . The data for two documentaries was used to create a dataset consisting of sentences from the transcripts and the corresponding substitution examples in which selected words are substituted by a shorter Wordnet synonym .__label__Material|Data|Use
The results are parsed , and piped through to the language filtering , lexical normalisation , and geolocation modules , and finally stored in a flat file , which the GUI interacts with . For language identification , we use langid . py , a language identification toolkit developed at The University of Melbourne ( Lui and Baldwin , 2011 ). _CITE_ langid . py combines a naive Bayes classifier with cross - domain feature selection to provide domain - independent language identification . It is available under a FOSS license as a stand - alone module pre - trained over 97 languages . langid . py has been developed specifically to be able to keep pace with the speed of messages through the Twitter “ garden hose ” feed on a single - CPU machine , making it particularly attractive for this project .__label__Method|Tool|Use
This is a simplistic view of an utterance ; however it is in line with our compression task , which also operates over sentences . We determine which entities are invoked by a sentence using two methods . First , we perform named entity identification and coreference resolution on each document using LingPipe , _CITE_ a publicly available system . Named entities are not the only type of entity to occur in our data , thus to ensure a high entity recall we add named entities and all remaining nouns to the Cf list . Entity matching between sentences is required to determine the Cb of a sentence .__label__Method|Tool|Use
Researchers were incorporated via special programmes from both the Faculty and the Argentinean Government to increase the number of doctors in Computer Science in the scientific system in Argentina . Most of our efforts in the first years went to raise awareness about the area and provide foundational and advanced courses . This policy lead to a significant number of graduation theses _CITE_ and to the incorporation of various PhD students to our group . We taught several undergraduate and graduate courses on various NLP topics at our own University , at the University of Rio Cuarto , at the University of Buenos Aires and at the Universidad de la Rep ´ ublica ( Uruguay ), as well as crash courses at the Society for Operative Investigations ( SADIO ) and at the Conferencia Latinoamericana de Inform ´ atica ( CLEI 2008 ). We also gave several talks at various universities in the country , and participated in local events , like JALIMI ’ 05 ( Jornadas Argentinas de Ling ¨ u ´ ıstica Inform ´ atica : Modelizaci ´ on e Ingenier ´ ıa ) or the Argentinean Symposium on Artificial Intelligence .__label__Supplement|Paper|Produce
All parsers are trained with 5 iterations of split , merge , smooth . To produce PCTB - style analysis , we train the Berkeley parse with PCTB 5 . 0 data that contains 18804 sentences and 508764 words . For the evaluation of development experiments , we used the EVALB tool _CITE_ for evaluation , and used labeled recall ( LR ), labeled precision ( LP ) and F1 score ( which is the harmonic mean of LR and LP ) to measure accuracy . For the head classification , we use SVMhmm , an implementation of structural SVMs for sequence tagging . The main setting of learning parameter is C that trades off margin size and training error .__label__Method|Tool|Use
This rule seems to be validated by the fact that in the data sets with which we experiment , we note that the predicted number of clusters according to this rule and the classes identified in the gold data are very close as illustrated in Table 2 . On average we note that the gold data has the number of classes per thread to be roughly 2 - 5 . We use data from two online forums - Create Debate [ CD ] _CITE_ and discussions from Wikipedia [ WIKI ] . There is a significant difference in the kind of discussions in these two sources . Our WIKI data comprises 117 threads crawled from Wikipedia .__label__Material|Data|Use
Constructing and maintaining an alignment between an ontology and an UW lexicon is a challenging task ( Rouquet and Nguyen , 2009b ). Basically , any lexical resource can be represented in an ontology language as a graph . We propose to use an OWL version of the UW volume available on Kaiko website _CITE_ . It allows us to benefit of classical ontology matching techniques and tools ( Euzenat and Shvaiko , 2007 ) to represent , compute and manipulate the alignment . We implemented two string based matching techniques on top of the alignment API ( Euzenat , 2004 ).__label__Supplement|Website|Use
The TP3 ( 2000s ) writers should have well - maintained blogs as well . Therefore this study will examine C - FWAA effectiveness in three genres : novel , essay , and blog . All electronic copies of the selected works were downloaded from online literature repositories such as YiFan Public Library and TianYa Book _CITE_ . The first experiment was to test the effectiveness of the EM algorithm for FWAA . The famous Federalist Papers data set was used as the test case .__label__Supplement|Website|Use
Given the diversity of topics , the in - domain data alone cannot ensure sufficient coverage to an SMT system . The addition of background data can certainly improve the n - gram coverage and thus the fluency of our translations , but it may also move our system towards an unsuitable language style , such as that of written news . In our study , we focus on the subproblem of target language modeling and consider two English text collections , namely the in - domain TED and the out - of - domain NEWS _CITE_ , summarized in Table 2 . Because of its larger size – two orders of magnitude – the NEWS corpus can provide a better LM coverage than the TED on the test data . This is reflected both on perplexity and on the average length of the context ( or history h ) actually used by these two LMs to score the test ’ s reference translations .__label__Material|Data|Use
As a summary measure , we report one minus area under the ROC curve ( 1 − AUC ) as a percentage with confidence intervals , which is the TREC ’ s official evaluation measure . Note that lower 1 − AUC (%) value means better performance . We used the TREC Spam Filter Evaluation Toolkit _CITE_ in order to perform the ROC analysis . All experiments were performed using 10 - fold cross validation . Statistical significance of differences between results were computed with a twotailed paired t - test .__label__Method|Tool|Use
Table 4 summarizes the results of the techniques for injecting external knowledge . It is important to note that , although the world class model was learned on the superset of CoNLL03 data , and although the Wikipedia gazetteers were constructed based on CoNLL03 annotation guidelines , these features proved extremely good on all datasets . Word class models discussed in Section 6 . 1 are computed offline , are available online _CITE_ , and provide an alternative to traditional semi - supervised learning . It is important to note that the word class models and the gazetteers and independednt and accumulative . Furthermore , despite the number and the gigantic size of the extracted gazetteers , the gazeteers alone are not sufficient for adequate performance .__label__Method|Algorithm|Produce
In other words , labels should be used together with answers ’ content to account for stronger and more effective dependencies . We use the CQA - QL corpus , which was used for Subtask A of SemEval - 2015 Task 3 on Answer Selection in cQA . The corpus contains data from the Qatar Living forum , _CITE_ and is publicly available on the task ’ s website . The dataset consists of questions and a list of the answers for each question , i . e ., the question - answer thread . Each question , and also each answer , consists of a short title and a more detailed description .__label__Material|Data|Use
Using weighted Support Vector Machines , to address the issue of class imbalance , our system obtains positive class F - scores of 0 . 701 and 0 . 656 , and negative class F - scores of 0 . 515 and 0 . 478 over the training and test sets , respectively . recent years . Twitter , for example , has over 645 , 750 , 000 users and grows by an estimated 135 , 000 users every day , generating 9 , 100 tweets per second _CITE_ ). Users often express their views and emotions regarding a range of topics on social media platforms . As such , social media has become a crucial resource for obtaining information directly from end - users , and data from social media has been utilized for a variety of tasks ranging from personalized marketing to public health monitoring .__label__Supplement|Website|Introduce
The details of the system are presented below . Sections 2 . 2 , 2 . 3 , and 2 . 4 describe the annotation client , the web - accessible data repository , and the portal to the TeraGrid , respectively , as shown in Figure 1 below . The SIDGrid client provides an interactive multimodal annotation interface , building on the opensource ELAN annotation tool from the Max Planck Institute _CITE_ . A screenshot appears in Figure 2 . ELAN supports display and synchronized playback of multiple video files , audio files , and arbitrarily many annotation ” tiers ” in its ” music - score ”- style graphical interface .__label__Method|Tool|Use
For example , selection of negative examples based on cohyponyms was found useful for Cr classifiers in R10 , while random examples were used in the rest of the cases . We used SVMperf ( Joachims , 2006 ) with a linear kernel and binary feature weighting . For querying the corpus we used the Lucene search engine _CITE_ in its default setting . Up to 150 positive examples were retrieved for each classifier , with 5 examples set as the required minimum . This resulted in generating 100 % of the hypothesis classifiers for both datasets and 95 % and 70 % of the rule classifiers for R10 and 20NG , respectively .__label__Method|Tool|Use
Unfortunately , most of the Web dictionaries and glossaries available online comprise just a few hundred definitions , and they therefore provide only a partial view of a domain . This is also the case with manually compiled glossaries created by means of collaborative efforts , such as Wikipedia . _CITE_ The coverage issue is addressed by online aggregation services such as Google Define , which bring together definitions from several online dictionaries . However , these services do not classify textual definitions by domain : they just present the collected definitions for all the possible meanings of a given term . In order to automatically obtain large domain glossaries , in recent years computational approaches have been developed which extract textual definitions from corpora ( Navigli and Velardi , 2010 ; Reiplinger et al ., 2012 ) or the Web ( Velardi et al ., 2008 ; Fujii and Ishikawa , 2000 ).__label__Supplement|Website|Introduce
The majority of the English descriptions were collected from the Metropolitan Museum . The majority of the Hebrew descriptions were taken from Artchive . _CITE_ Table 1 gives an overview of the three text collections . In addition , we extracted 40 parallel texts that are available under the sub - domain Painting from Wikipedia . All sentences in the reference material were tokenised , part - of - speech tagged , lemmatized , and parsed using open - source software .__label__Supplement|Website|Use
Considering that most coverage - based systems explore event information , we opted for not including them in this comparative analysis . To assess the informativeness of the summaries generated by our methods , we used ROUGE - 1 and ROUGE - 2 ( Lin , 2004 ) on DUC 2007 and TAC 2009 datasets . The main summarization task in DUC 2007 _CITE_ is the generation of 250 - word summaries of 45 clusters of 25 newswire documents ( from the AQUAINT corpus ) and 4 human reference summaries . The TAC 2009 Summarization task has 44 topic clusters . Each topic has 2 sets of 10 news documents obtained from the AQUAINT 2 corpus . There are 4 human 100 - word reference summaries for each set , where the reference summaries for the first set are query - oriented , and for the second set are update summaries .__label__Supplement|Website|Introduce
Given such a hypergraph , we use the Algorithm 3 described in ( Huang and Chiang , 2005 ) to extract its k - best ( k = 500 in our experiments ) derivations . Since different derivations may lead to the same target language string , we further adopt Algorithm 3 ’ s modification , i . e ., keep a hash - table to maintain the unique target sentences ( Huang et al ., 2006 ), to efficiently generate the unique k - best translations . The JST Japanese - English paper abstract corpus _CITE_ , which consists of one million parallel sentences , was used for training and testing . This corpus was constructed from a Japanese - English paper abstract corpus by using the method of Utiyama and Isahara ( 2007 ). Table 3 shows the statistics of this corpus .__label__Material|Data|Introduce
The corpus contains a set of 1 , 235 sentence pairs that are manually word aligned . The corpora were processed using a standard procedure for machine translation . The English texts were tokenized with the tokenization script released with Europarl corpus ( Koehn , 2005 ) and converted to lowercase ; the Chinese texts were segmented into words using the Stanford Word Segmenter ( Xue et al ., 2002 ) ; the Japanese texts We found the memory of our server is large enough , so we did not implement it were segmented into words using the Kyoto Text Analysis Toolkit ( KyTea _CITE_ ). Sentences longer than 100 words or those with foreign / English word length ratios between larger than 9 were filtered out . GIZA ++ was run with the default Moses settings ( Koehn et al ., 2007 ).__label__Method|Tool|Use
The DFKI Citation Corpus has been used for classifying citation function ( Dong and Sch ¨ afer , 2011 ), but the dataset also includes polarity annotation . The dataset has 1768 citation sentences with polarity annotation : 190 are labeled as positive , 57 as negative , and the vast majority , 1521 , are left neutral . The second citation corpus , the IMS Citation Corpus _CITE_ contains 2008 annotated citations : 1836 are labeled positive and 172 are labeled negative . Jochim and Sch ¨ utze ( 2012 ) use annotation labels from Moravcsik and Murugesan ( 1975 ) where positive instances are labeled confirmative , negative instances are labeled negational , and there is no neutral class . Because each of the citation corpora is of modest size we combine them to form one citation dataset , which we will refer to as CITD .__label__Material|Data|Introduce
Here , models a probability that word sequence is transformed into phone sequence , and models a probability that is linguistically acceptable . These factors are usually called acoustic and language models , respectively . For the speech recognition module , we use the Japanese dictation toolkit ( Kawahara et al ., 2000 ) _CITE_ , which includes the “ Julius ” recognition engine and acoustic / language models . The acoustic model was produced by way of the ASJ speech database ( ASJJNAS ) ( Itou et al ., 1998 ; Itou et al ., 1999 ), which contains approximately 20 , 000 sentences uttered by 132 speakers including the both gender groups . This toolkit also includes development softwares so that acoustic and language models can be produced and replaced depending on the application .__label__Method|Tool|Use
Political debates are an interesting example : political scientists carefully analyze what gets said in debates to explore how candidates shape the debate ’ s agenda and frame issues or how answers subtly ( or not so subtly ) shift the conversation by dodging the question that was asked ( Rogers and Norton , 2011 ). Computational methods can contribute to the analysis of topical dynamics , for example through topic segmentation , dividing a conversation into smaller , topically coherent segments ( Purver , 2011 ); or through identifying and summarizing the topics under discussion ( Blei et al ., 2003 ; Blei , 2012 ). However , the topics uncovered by such methods can be difficult for people to interpret ( Chang et al ., 2009 ), and previous visualization frameworks for topic models — e . g ., ParallelTopics ( Dou et al ., 2011 ), TopicViz ( Eisenstein et al ., 2012 ), the Topical Guide , _CITE_ or topic model visualization ( Chaney and Blei , 2012 )— are not particularly well suited for linearly structured conversations . This paper describes Argviz , an integrated , interactive system for analyzing the topical dynamics of multi - party conversations . We bring together previous work on Interactive Topic Modeling ( ITM ) ( Hu et al ., 2011 ), which allows users to efficiently inject their needs , expertise , and insights into model building via iterative topic refinement , with Speaker Identity for Topic Segmentation ( SITS ) ( Nguyen et al ., 2012 ), a state - of - the - art model for topic segmentation and discovery of topic shifts in conversations .__label__Method|Tool|Introduce
For the classification of the messages into the positive , negative and neutral classes we use three linear models , which were trained in an one - vs .- all manner . At prediction time we simply choose the label with the highest score . All training was done using the open - source machine learning toolkit scikitlearn , _CITE_ which provides a consistent Python API to fast implementations of various machine learning algorithms ( Pedregosa et al ., 2011 ). The linear models were trained using stochastic gradient descent ( SGD ), which is a gradient descent optimization method that minimizes a given loss function . The term “ stochastic ” refers to the fact that the weights of the model are updated for each training example , which is an approximation of batch gradient descent , in which all training examples are considered to make a single step .__label__Method|Tool|Use
Fortunately , machine translation techniques have been well developed in the NLP field , though the translation performance is far from satisfactory . A few commercial machine translation services can be publicly accessed , e . g . Google Translate , Yahoo Babel Fish and Windows Live Translate _CITE_ . In this study , we adopt Google Translate for both English - to - Chinese Translation and Chinese - toEnglish Translation , because it is one of the state - of - the - art commercial machine translation systems used today . Google Translate applies statistical learning techniques to build a translation model based on both monolingual text in the target language and aligned text consisting of examples of human translations between the languages .__label__Supplement|Website|Introduce
Table 1 shows an example of the bilingual sense labels for two test occurrences of the English noun bank in our parallel corpus which will be further described in Section 2 . 1 . Table 2 presents the multilingual sense labels for the same sentences . ... giving fish to people living on the [ bank ] of the river ... giving fish to people living on the [ bank ] of the river The document collection which serves as the basis for the gold standard construction and system evaluation is the Europarl parallel corpus _CITE_ , which is extracted from the proceedings of the European Parliament ( Koehn , 2005 ). We selected 6 languages from the 11 European languages represented in the corpus : English ( our target language ), Dutch , French , German , Italian and Spanish . All sentences are aligned using a tool based on the Gale and Church ( 1991 ) algorithm .__label__Material|Data|Use
For annotation , task - specific tools are being used , e . g . EXMARaLDA , annotate , RSTTool , and MMAX . _CITE_ Data is then converted into a standoff data interchange format , which is fed into the linguistic database ANNIS . ANNIS aims at providing functionalities for exploring and querying the data , offering suitable means for both visualization and export . Central requirements evolving out of the scenario sketched above and , as we believe , for multilevel annotation in general are Data heterogeneity , Data reuse , and Accessibility ( cf .__label__Method|Tool|Use
Electronic career guidance is , thus , a supplement to career guidance by human experts , helping young people to decide which profession to choose . The goal is to automatically compute a ranked list of professions according to the user ’ s interests . A current system employed by the German Federal Labour Office ( GFLO ) in their automatic career guidance front - end _CITE_ is based on vocational trainings , manually annotated using a tagset of 41 keywords . The user must select appropriate keywords according to her interests . In reply , the system consults a knowledge base with professions manually annotated with the keywords by career guidance experts .__label__Supplement|Document|Introduce
Our aim is to compile and distribute resources that will be available to all interested researches . Corpora include smaller manually annotated samples and large automatically annotated corpora ( Ljubeˇsi ´ c and Klubiˇcka , 2014 ); annotation will be improved and enriched in the course of the project . Free existing morphological dictionaries _CITE_ are extended inside the project in a semiautomated fashion ( Ljubeˇsi ´ c et al ., in press ). Tools currently include a state - of - the - art part - of - speech tagger and lemmatiser reaching a new best performance for both languages (∼ 91 % for full morphosyntactic annotation and ∼ 97 % for lemmatisation ). Development of tools for other kinds of analysis ( dependency syntax , semantic role labelling ) is planned for the remainder of the project .__label__Material|Data|Extent
The current WN . Br database presents the following figures : 11 , 000 verb forms ( 4 , 000 synsets ), 17 , 000 noun forms ( 8 , 000 synsets ), 15 , 000 adjective forms ( 6 , 000 synsets ), and 1 , 000 adverb forms ( 500 synsets ), amounting to 44 , 000 word forms and 18 , 500 synsets ( Dias - da - Silva et al , 2008 ). project started in September 2009 and shall be finished finish in August 2011 . It has been developed in the laboratory of the Research Group of Terminology _CITE_ ( GETerm ) in Federal University of São Carlos ( UFSCar ) with the collaboration of the Interinstitutional Center for Research and Development in Computational Linguistics ( NILC / University of São Paulo ) researchers . The TermiNet project has two main objectives . The first is to instantiate the generic NLP methodology , proposed by Dias - da - Silva ( 2006 ), for developing terminological databases according to the WN . Pr model .__label__Supplement|Website|Introduce
Finally , we also use the number of words between the nominals as a feature because relations such as Product - Producer and Entity - Origin often have no intervening tokens ( e . g ., organ builder or Coconut oil ). Syntactic and semantic parses capture long distance relationships between phrases in a sentence . Instead of a traditional syntactic parser , we chose the Stanford dependency parser _CITE_ for the simpler syntactic structure it produces . Our dependency features are based on paths in the dependency tree of length 1 and length 2 . The paths encode the dependencies and words those dependencies attach to .__label__Method|Tool|Use
Each version of our translation system was trained on the same bilingual training data . The bilingual parallel corpus that we used was distributed as part of the 2008 NIST Open Machine Translation Evaluation Workshop . _CITE_ The training set contained 88 , 108 Urdu – English sentence pairs , and a bilingual dictionary with 113 , 911 entries . For our development and test sets , we split the NIST MT - 08 test set into two portions ( with each document going into either test or dev , and preserving the genre split ). Our test set contained 883 Urdu sentences , each with four translations into English , and our dev set contained 981 Urdu sentences , each with four reference translations .__label__Supplement|Website|Introduce
During the calculation we reversed the sentiment orientation of the term if a negation occurs before it . We manually built a negative list : { no , nor , not , neither , none , nobody , nothing , hardly , seldom }. Eight sentiment lexicons are used : Bing Liu opinion lexicon , General Inquirer lexicons , IMDB , MPQA , SentiWordNet _CITE_ , NRC emotion lexicon , NRC Hashtag Sentiment Lexicon10 and NRC Sentiment140 Lexicon11 . With regard to the synonym selection of SentiWordNet , we selected the first term in the synset as our lexicon . If the eight words surrounding the aspect term do not exist in the eight corresponding sentiment lexicons , we set their three sentiment scores as 0 .__label__Material|Data|Use
In other words , we intersected the terms in sentence 1 with all the conceptual term lists in sentence 2 . After computing all the co - occurrences , we used these values to calculate the Jaccard ’ ( Jaccard , 1901 ), Lin ’ ( Lin , 1998 ) and PMI ’ ( Turney , 2001 ) scores . This feature takes advantage of the Align , Disambiguate and Walk ( ADW ) _CITE_ library ( Pilehvar et al ., 2013 ), a WordNet - based approach for measuring semantic similarity of arbitrary pairs of lexical items . It is important to mention that this feature is the only one that only works for English , which explains why we have a translation model ( see section 2 . 1 . 3 ). In other words , when we are dealing with Spanish text , we use the trained model to translate from Spanish to English .__label__Method|Code|Use
The Japanese word dictionary is IPADic 2 . 7 . 0 . We also use a dictionary of Japanese verb conjugations , because verbs in learners ’ sentence are followed by conjugational endings but they are separated in our word dictionary . The conjugation dictionary is made of all the occurrences of verbs and their conjugations extracted from Mainichi newspaper of 1991 , with a Japanese dependency parser CaboCha 0 . 53 _CITE_ to find bunsetsu ( phrase ) containing at least one verb . The number of extracted unique conjugations is 243 , 663 . Words which are not matched in either the English or the Japanese dictionary in the language identification step are corrected by the following method .__label__Method|Tool|Use
The availability of sensors such as Microsoft Kinect and ( almost ) affordable eye trackers bring new methods of naturalistic human - computer interaction within reach . Studying the possibilities of such methods requires building infrastructure for recording and analysing such data ( Kousidis et al ., 2012a ). We present such an infrastructure — the mint . tools collection ( see also ( Kousidis et al ., 2012b )) _CITE_ — and present results of a study we performed on whether speaker gaze and speaker arm movements can be turned into an information source for an interactive system . The mint . tools collection comprises tools ( and adaptations to existing tools ) for recording and analysis of multimodal data . The recording architecture ( Figure 1 ) is highly modular : each information source ( sensor ) runs on its own dedicated workstation and transmits its data via the local area network .__label__Method|Tool|Use
For study # 1 , our base system ( Spitkovsky et al ., 2010b ) is an instance of the popular ( unlexicalized ) Dependency Model with Valence ( Klein and Manning , 2004 ). This model was trained using hard EM on WSJ45 ( WSJ sentences up to length 45 ) until successive changes in per - token cross - entropy fell below 2 − 20 bits ( Spitkovsky et al ., 2010b ; 2010a , § 4 ). _CITE_ We confirmed that the base model had indeed converged , by running 10 steps of hard EM on WSJ45 and verifying that its objective did not change much . Next , we applied a single alternation of simple lateen EM : first running soft EM ( this took 101 steps , using the same termination criterion ), followed by hard EM ( again to convergence — another 23 iterations ). The result was a decrease in hard EM ’ s cross - entropy , from 3 . 69 to 3 . 59 bits per token ( bpt ), accompanied by a 2 . 4 % jump in accuracy , from 50 . 4 to 52 . 8 %, on Section 23 of WSJ ( see Table 1 ).__label__Method|Algorithm|Introduce
We manually checked all the cases of majority agreement , correcting only 7 . 92 % of the majority adjudications , and manually adjudicated all the snippets for which there was no agreement . We observed during adjudication that in many cases the disagreement was due to the existence of subtle sense distinctions , like between MORTAL KOMBAT ( VIDEO GAME ) and MORTAL KOMBAT ( 2011 VIDEO GAME ), or between THE DA VINCI CODE and INACCURACIES IN THE DA VINCI CODE . The average number of senses associated with the search results of each query was 7 . 69 ( higher than in previous datasets , such as AMBIENT + MORESQUE _CITE_ , which associates 5 . 07 senses per query on average ). Following Di Marco and Navigli ( 2013 ), we evaluated the systems ’ outputs in terms of the snippet clustering quality ( Section 3 . 1 ) and the snippet diversification quality ( Section 3 . 2 ). Given a query q ∈ Q and the corresponding set of 64 snippet results , let C be the clustering output by a given system and let G be the gold - standard clustering for those results .__label__Material|Data|Compare
We used Techmeme , a news aggregator for technology news , to construct a comparable corpus . Its website lists the major tech stories , each with links to several articles from different sources . We used the Readability API _CITE_ to download and extract the article text for each document . We scraped two years worth of data from Techmeme and only took stories containing at least 5 documents . Our corpus contains approximately 160 million words , 25k stories , and 375k documents .__label__Method|Tool|Use
These technique have been used successfully in patent prior art search by Article One , Inc . which puts out a call to researchers to find art on a particular patent . If the client selects that art to support their case , the contributor is paid . The patent office itself attempted something similar in the Peer to Patent program _CITE_ , which depended on people ’ s desire to improve the quality of patents by letting them contribute art . It was moderately successful , but without the kind of specific reward the Article One provides , they did not get nearly as much material as they would have liked . MedLingMap , SpeechMap and other efforts of its kind have the same problem : no one has enough time .__label__Method|Tool|Extent
3 . For each of the German , Spanish , and French terms obtained , we used the title term , the meta keywords , and the emphasized concepts obtained from the same English wikipedia page as its potential translations . For example , consider an English page titled as “ World War II ” _CITE_ . The title term , the meta keywords , the emphasized concepts in English , and the hyperlinks ( to German , Spanish , and French ) associated are shown in Figure 1 . We first extract the basenames “ Zweiter Weltkrieg ” ( in German ), “ Segunda Guerra Mundial ” ( in Spanish ), and “ Seconde Guerre mondiale ” ( in French ) using the hyperlink feature .__label__Supplement|Website|Introduce
Generally , these similarity measures are represented as numerical values and combined using regression model . Generally , we perform text preprocessing before we compute each text similarity measurement . Firstly , Stanford parser _CITE_ is used for sentence tokenization and parsing . Specifically , the tokens n ’ t and ’ m are replaced with not and am . Secondly , Stanford POS Tagger is used for POS tagging .__label__Method|Tool|Use
We focus our attention on a subpart of the legal domain , namely law reports , for three main reasons : ( a ) the existence of manual summaries means that we have evaluation material for the final summarisation system ; ( b ) the existence of differing target audiences allows us to explore the issue of tailored summaries ; and ( c ) the texts have much in common with the scientific articles papers that T & M worked with , while remaining challengingly different in many respects . In this paper we describe the corpus of legal texts that we have gathered and annotated . The texts in our corpus are judgments of the House of Lords _CITE_ , which we refer to as HOLJ . These texts contain a header providing structured information , followed by a sequence of Law Lord ’ s judgments consisting of free - running text . The structured part of the document contains information such as the respondent , appellant and the date of the hearing .__label__Material|Data|Use
In this approach , we assigned one of seven tags { B , I , O , B2 , I2 , B3 , I3 } to each word . Thus , the disorder identification problem was converted into a classification problem to assign one of the seven labels to each word . The algorithms machine learning and feature set offered by Stanford Named Entity Recognizer _CITE_ was used . The Stanford CoreNLP was used for splitting sentences and tokenizers from the training and test data . Also , some simple rules were used for labeling disorder words , i . e .__label__Method|Tool|Use
. Learning mechanisms Originally , we started with two learning algorithms , Random Forests ( RF ) and Support Vector Machines ( SVM ), running them in the R system . _CITE_ The Random forests algorithm joins randomness with classification decision trees . They iterate the process of two random selections and training a decision tree k - times on a subset of m features . Each of them classifies a new input instance x and the class with the most votes becomes the output class of x .__label__Method|Tool|Use
This is the preferred method of using langid . py from other programming environments , as most languages include libraries for interacting with web services over HTTP . It also allows the language identification service to be run as a network / internet service . Finally , langid . py is WSGI - compliant , _CITE_ so it can be deployed in a WSGIcompliant web server . This provides an easy way to achieve parallelism by leveraging existing technologies to manage load balancing and utilize multiple processors in the handling of multiple concurrent requests for a service . LDfeatureselect . py implements the GD feature selection .__label__Method|Tool|Introduce
For instance , one can search for verbs which have their subject in the partitive case , unless that subject has a numeral modifier , and unless the verb is governed by the clausal complement relation . In addition to the constraints on the syntactic structure , any combination of normal and negated constraints on the morphology of the words is possible . The full description of the query system capabilities is , however , out of scope of this paper , and we refer the interested reader to the online documentation _CITE_ . In addition to a scriptable , command - line utility meant for gathering data for further processing , the query system also has an online interface which allows the results to be visualized and inspected in real time ( Figure 3 ). In Section 5 we will demonstrate several real use - cases where this query system was used to obtain material for linguistic research from the parsebank .__label__Supplement|Document|Produce
The extraction can be restricted to the top N words to generate a summary of specified length . In our study each document contains a list of topscoring responses from the dataset , i . e ., each topscoring response would constitute a sentence . For our study we use MEAD _CITE_ to extract summaries of length that match the lengths of the summaries generated by the graph - based cohesion technique . The choice of cluster to which a response is added depends on the cluster ’ s similarity , i . e ., a response is added to the cluster with higher similarity . If both responses ( in the pair ) have same cluster similarities , then the larger cluster is chosen as the target .__label__Method|Tool|Use
We first removed the XML tags from data and then transformed the abbreviations to their normal form s , i . e ., “ don ’ t ” to “ do not ”. We used Stanford Parser tools for tokenization , POS tagging and parsing . Finally , the WordNet - based Lemmatizer implemented in NLTK _CITE_ was adopted to lemmatize words to their base forms with the aid of their POS tags . In this work , we used three types of features : sentiment lexicon features , linguistic features and domain - specific features . All features were extracted from pending words as described above .__label__Method|Code|Use
The above mentioned parts of the dictionary used as the corpus comprised 24308 characters in the “ negative ” part and 7898 characters in the “ positive ” part . A corpus of E - Bay customers ’ reviews of products and services was used as a test corpus . The total number of reviews is 128 , of which 37 are negative ( average length 64 characters ) and 91 are positive ( average length 18 characters ), all of the reviews were tagged as ‘ positive ’ or ‘ negative ’ by the reviewers _CITE_ . We computed two scores for each item ( a review ): one for positive sentiment , another for negative sentiment . The decision about an item ’ s sentiment polarity was made every time by finding the biggest score of the two .__label__Material|Data|Use
( 2005 ) do to evaluate our system PAIR and recruit 9 students who have different backgrounds to participate in our experiment . We use query topics from TREC 2005 and 2004 Hard Track , TREC 2004 Terabyte track for English information retrieval , and use query topics from HTRDP 2005 Evaluation for Chinese information retrieval . _CITE_ The reason why we utilize multiple TREC tasks rather than using a single one is that more queries are more likely to cover the most interesting topics for each participant . Initially , each participant would freely choose some topics ( typically 5 TREC topics and 5 HTRDP topics ). Each query of TREC topics will be submitted to three systems : UCAIR ( Xuehua Shen et al ., 2005 ), “ PAIR No QE ” ( PAIR system of which the query expansion function is blocked ) and PAIR .__label__Material|Data|Use
Dataset details are given in Appendices A ( Chinese ) and B ( German ). Baseline We use the Moses MT system ( Koehn et al ., 2007 ) as a baseline and closely follow the example training procedure given for the WMT - 07 and WMT - 08 shared tasks . _CITE_ In particular , we perform word alignment in each direction using GIZA ++ ( Och and Ney , 2003 ), apply the “ grow - diag - finaland ” heuristic for symmetrization and use a maximum phrase length of 7 . In addition to the two phrase translation conditionals p ( e | f ) and p ( f | e ), we use lexical translation probabilities in each direction , a word penalty , a phrase penalty , a lengthbased reordering model , a lexicalized reordering model , and an n - gram language model , SRILM implementation ( Stolcke , 2002 ) with modified KneserNey smoothing ( Chen and Goodman , 1998 ). Minimum error - rate ( MER ) training ( Och , 2003 ) was applied to obtain weights ( am in Equation 2 ) for these features .__label__Material|Data|Use
Relevant domain data . Furthermore , we were interested if simply adding more unlabelled data , not necessarily from the relevant domain , produced the same increase in accuracy . We obtained the plaintext claims and description parts of 13 , 600 patents freely available in the Global IP Database which is based on the Espacenet _CITE_ , creating a corpus with 42 . 5 mln tokens , i . e . which was similar in size to the reviews and weblogs sections of the SANCL unlabelled dataset . Table 8 compares results achieved when building clusters from the patents corpus and when using the reviews and weblogs texts from the SANCL unlabelled dataset .__label__Material|Data|Extent
Various networks built in BEL were mainly focusing on disease mechanisms ( Schlage et al ., 2011 ) and are used for causal reasoning ( Chindelevitch et al ., 2012 , Huang et al ., 2012 and Selventa 2012 ). Since 2012 , BEL is also available in the public domain through the OpenBEL consortium . The OpenBel portal _CITE_ defines the BEL language standard and provides formatted content and compatible tools for research . The necessary information to develop a BEL knowledge base is currently harvested mainly by manual translation of literature into BEL statements . To support automated extraction of statements by text mining techniques , additional efforts and adaptations of existing text mining platforms are necessary .__label__Supplement|Website|Introduce
In ( Passonneau et al ., 2008 ), a similar lack of correlation between interannotator agreement and machine learning performance is found in an empirical investigation . The Manually Annotated Sub - Corpus ( MASC ) project ( Ide et al ., 2010 ) is creating a small , representative corpus of American English written and spoken texts drawn from the Open American National Corpus ( OANC ). _CITE_ The MASC corpus includes hand - validated or manual annotations for a variety of linguistic phenomena . The first MASC release , available as of May 2010 , consists of 82K words . One of the goals of MASC is to support efforts to harmonize WordNet ( Miller et al ., 1993 ) and FrameNet ( Ruppenhofer et al ., 2006 ), in order to bring the sense distinctions each makes into better alignment .__label__Material|Data|Introduce
Figure 2 gives one view of the type of information extracted by the collection of learned category and relation classifiers . Note the initial seed examples provided to CBL did not include information about either company or any of these relation instances . _CITE_ To estimate the capacity of our algorithm to contribute additional facts to publicly available semantic resources , we compared the complete lists of instances promoted during the Full 15 iteration run for certain categories to corresponding lists in the Freebase database ( Metaweb Technologies , 2009 ). Excluding the categories that did not have a directly corresponding Freebase list , we computed for each category : Precision x | CBLInstances |− | Matches |, where Precision is the estimated precision from our random sample of 30 instances , | CBLInstances | is the total number of instances promoted for that category , and | Matches | is the number of promoted instances that had an exact match in Freebase . While exact matches may underestimate the number of matches , it should be noted that rather than make definitive claims , our intent here is simply to give rough estimates , which are shown in Table 3 .__label__Supplement|Document|Introduce
NTCIR provided only one reference sentence for each sentence . When we use only the provided reference sentences , we call it “ single ref ”. We apply a popular Japanese dependency parser CaboCha _CITE_ to the reference sentences , and manually corrected its output just like Isozaki et al . ( 2014 ). 40 % of NTCIR - 7 dependency trees and 50 % of NTCIR - 9 dependency trees were corrected .__label__Method|Tool|Use
As with its sister project LibreOffice , it is the successor of the OpenOffice . org project , whose first release dates back to 2002 . OpenOffice . org did not possess grammarchecking functionality , which made it less competitive compared to other non - open source alternatives . This motivated some NLP researchers to create CoGrOO _CITE_ , a Brazilian Portuguese grammar checker , a project initially sponsored by FINEP ( a Research and Projects Funding agency ). This research on CoGrOO began in 2004 , and since its first release in 2006 it has been adopted by important companies like Petrobras - the biggest company in Brazil and the 8th biggest in the world in market value - and Celepar - the Paran ´ a State information technology company , responsible for deploying software for government offices and public schools . CoGrOO accumulated over a hundred thousand downloads from its official website .__label__Method|Tool|Introduce
We have used the following tools : Extracting the main text : This module extracts the context of the target word from the source xml document removing the unnecessary tags and makes the context ready for further processing . Sentence Splitting , Text Stemming and Chunking : This module splits the context into sentences , then stems out the words and chunks those . We used OAK systems _CITE_ ( Sekine , 2002 ) for this purpose . Candidate Words Extraction : This module extracts the candidate words ( for task 7 : noun , verb , adjective and adverb ) from the chunked text . Each candidate word is expanded to all of its senses .__label__Method|Tool|Use
Sentences have been manually aligned during the human translation process , and words have been then aligned automatically using GIZA ++ ( Och and Ney , 2003 ). We have used valency frame annotation ( and other features ) of the PCEDT 2 . 0 in our previous work ; however , billingual alignment information has not been used before . PDT - Vallex _CITE_ ( Hajiˇc et al ., 2003 ; Urešová , 2011 ) is a valency lexicon of Czech verbs ( and nouns ), manually created during the annotation of the PDT / PCEDT 2 . 0 . Each entry in the lexicon contains a headword ( lemma ), according to which the valency frames ( i . e ., senses ) are grouped . Each valency frame includes the valency frame members and the following information for each of them ( see Fig .__label__Material|Data|Introduce
POS The presence or absence of part - of - speech tags are used as binary features . We use the CMU ARK Twitter Part - of - Speech Tagger ( Owoputi et al ., 2013 ) in our implementation . Lexicons The NRC Hashtag Sentiment Lexicon http :// www . ark . cs . cmu . edu / TweetNLP / clusters / 50mpaths2 and Sentiment140 Lexicon _CITE_ are used . These two lexicons are automatically generated by calculating pointwise mutual information ( PMI ) scores between the words and positive or negative labels ( Kiritchenko et al ., 2014 ). The hashtags and emoticons are used to assign noisy polarity labels for tweets .__label__Material|Data|Use
Also , the viewpoint presented here is that of interlingua users who experience R & D for a given NL , and not of its authors . By ' high quality ' we mean ' at least allowing for readability and understandability by any user '. The UNL Project _CITE_ has been launched by the United Nations University to foster and ease international web communication by means of NLP systems . Its main strength lies on the development of the UNL , as a unique semantic ( or meaning ) representation that can be interchanged with the various languages to be integrated in the ICBMT system . In the UNL Project , plug - in software to encode NL texts onto UNL ones ( NL - UNL encoders ) and to decode UNL into NL texts ( UNL - NL decoders ) have been developed by R & D groups in their own native languages .__label__Method|Tool|Introduce
To better address the agglutinative nature of Basque , the word alignments were obtained over the lemmas , and were then projected to the original word forms to complete the training process . The system was trained on translation memory ( TM ) data containing academic books , software manuals and user interface strings (- 12 million words ), and web - crawled data (- 1 . 5 million words ) made available by Elhuyar . _CITE_ For the language model , the Basque text of the parallel data and the Basque text of Spanish - Basque TMs of administrative text made available by Elhuyar (- 7 . 4 million sentences ) was used . Again , a set of 1 , 000 indomain interactions were used for tuning after manually translating the original text into Basque . The system was evaluated on a second test - set of 1 , 000 in - domain interactions , obtaining a BLEU score of 20 . 24 .__label__Material|Data|Use
More recently , Opinion Mining and Sentiment Analysis on large social media datasets have received an increasing amount of attention outside academia , where a growing number of businesses and public institutions seek to gain insight into public opinion . For example , companies are primarily interested in what is being said about their brand and products , while public organisations are more concerned with analysing reactions to recent events , or with capturing the general political and societal Zeitgeist . The social network Twitter has been a popular target for such analyses as the vast majority of tweets are publicly available , and easily obtainable via the Twitter API _CITE_ , which conveniently enables the harnessing of a large number of realtime responses to any user - defined keyword query . In this paper we are concerned with what we call agile social media analysis , which is best illustrated with an example . Imagine that a political scientist wants to investigate reactions on Twitter to a speech given by British Prime Minister David Cameron the previous night .__label__Supplement|Website|Introduce
paper we used a single heuristic : the possessor of a nominal event ’ s predicate is marked as its Arg0 , e . g ., Logan is the Arg0 to run in Logan ’ s run . We extracted named entity labels using the named entity recognizer from the Stanford CoreNLP suite . The training and test data sets were derived from the EventCorefBank ( ECB ) corpus _CITE_ created by Bejan and Harabagiu ( 2010 ) to study event coreference since standard corpora such as OntoNotes ( Pradhan et al ., 2007 ) contain a small number of annotated event clusters . The ECB corpus consists of 482 documents from Google News clustered into 43 topics , where a topic is described as a seminal event . The reason for including comparable documents was to increase the number of cross - document coreference relations .__label__Material|Data|Use
The model is trained using backpropagation . Training such a purely lexical model will induce representations with syntactic and semantic properties . We use the RNNLM toolkit _CITE_ to induce these word representations . In the RNN model (§ 6 . 1 ) most of the complexity is caused by the non - linear hidden layer . This is avoided in the new model proposed in Mikolov et al .__label__Method|Tool|Use
We also have a large unlabelled training set for each language . Table 1 gives statistics . The data sets for English , Finnish , Turkish and German are from the Morpho Challenge 2010 competition _CITE_ ( MC2010 ). We use the MC2010 training set of 1000 annotated word types as our labelled data , and for our dev sets we collate together the development data from all years of the MC competition . Final evaluation is done on the official MC2010 test sets , which are not public , so we rely on the MC organizers to perform the evaluation .__label__Material|Data|Use
Using a restricted lexicon , a set of clauses covering the possible syntactic patterns of the four verb classes and regular expressions describing sentence - semantics pairs , we develop a script generating ( sentence , semantics ) pairs where sentences contain one or more clauses . After having manually verified the correctness of the generated pairs , we used them to construct textual entailment testsuite items that is , pairs of sentences annotated with TRUE or FALSE dependending on whether the two sentences are related by entailment ( TRUE ) or not ( FALSE ). The resulting testsuite _CITE_ contains 4 976 items of which 2 335 are entailments between a sentence and a clause ( 1V + TE , example 2 ), 1 019 between two complex sentences ( 2V + TE , example 3 ) and 1 622 are non - entailments ( V - TE , example 4 ). Checking for entailment . For each testsuite item , we then checked for entailment by translating LSDs into FOL formulae and checking entailment between the first five LSDs derived from the parser output for the sentences contained in the testsuite item .__label__Material|Data|Use
Document summarization is an active subject of research and development . The ACM Digital Library has about 806 reports on the subject published since 1993 , with over half of them appearing in the last five years . While the impetus for much of this research is the annual Text Analysis Conference ( TAC ) workshop on document summarization , there is a growing demand in the consumer market for news summarization applications being met by tablet and smart - phone applications such as Clipped , Summoner , TLDR _CITE_ , and Yahoo News . Yahoo and Google even acquired two companies developing such applications , Summly ( Stelter , 2013 ) and Wavii ( Tsotsis , 2013 ) respectively , earlier this year . While summarization technology for news sources is coming to fruition , the performance of such technology on non - English documents outside the news domain has not been throughly assessed and may need further research .__label__Supplement|Website|Introduce
Unfortunately , the syntactic annotation scheme of the Negra treebank ( Skut et al ., 1997 ), which omits all projections that are not strictly necessary to determine the constituent structure of a sentence , is not very well suited for automatic extraction tasks . So far , we have only been able to extract adjective + noun pairs . We plan to use the TIGERSearch tool _CITE_ in combination with stylesheets to obtain reference data for PP + verb and noun + verb pairs . In addition to the hand - corrected part - of - speech tags in the Negra corpus , we used the IMS TreeTagger ( Schmid , 1994 ) for automatic tagging . With its standard training corpus , a tagging accuracy of 94 . 82 % was achieved .__label__Method|Tool|Use
Second , though the authors act independently , they tend to produce surprisingly similar text , making the same sorts of jokes , or referring to words in the same sorts of ways . Thirdly , the authors often try to be non - obvious : obvious jokes are often not funny , and obvious crossword clues make a puzzle less challenging . The New Yorker magazine holds a weekly contest _CITE_ in which they publish a cartoon without a caption and solicit caption suggestions from their readers . The three funniest captions are selected by the editor and published in the following weeks . Figure 1 shows an example of such a cartoon , while Table 1 shows examples of captions , including its winning captions .__label__Supplement|Website|Introduce
Our technique differs in that we use no k - best approximations , have fewer parameters to learn ( one consensus weight vector rather than one for each collaborating decoder ) and produce only one output , avoiding an additional system combination step at the end . We report results on the constrained data track of the NIST 2008 Arabic - to - English ( ar - en ) and Chineseto - English ( zh - en ) translation tasks . _CITE_ We train on all parallel and monolingual data allowed in the track . We use the NIST 2004 eval set ( dev ) for optimizing parameters in model combination and test on the NIST 2008 evaluation set . We report results using the IBM implementation of the BLEU score which computes the brevity penalty using the closest reference translation for each segment ( Papineni et al ., 2002 ).__label__Supplement|Website|Use
The main reference model of the painting ontology is the OWL 2 imple mentation of the CRM . The additional models that are correctly integrated in the ontology are : SOCH , Time Ontology , SUMO and Mid - LevelOntology . _CITE_ The painting ontology was constructed manually using the Prot ´ eg ´ e editing tool . Integration of the ontology concepts are accomplished by using the OWL construct : intersectionOf as specified below : The schemata that are stated in the above example are denoted with the following prefixes : painting ontology (& painting ), SOCH (& ksamsok ), Mid - Level - Ontology (& milo ) and CIDOCCRM ontology (& core ). In this example , the class Painting is defined in the painting ontology as a subclass of E22 Man - Made Object class from the CIDOC - CRM ontology and is an intersection of two classes , i . e .__label__Method|Algorithm|Introduce
The translation quality of state - of - the - art , phrase - based statistical machine translation ( SMT ) approaches heavily depends on the amount of bilingual language resources available to train the statistical models . For frequently used language pairs like French - English or ChineseEnglish , large - sized text data sets are readily available . There exist several data collection initiatives like the Linguistic Data Consortium , the European Language Resource Association _CITE_ , or the GSK , amassing and distributing large amounts of textual data . However , for less frequently used language pairs , e . g ., most of the Asian languages , only a limited amount of bilingual resources are available , if at all . In order to overcome such language resource limitations , recent research on multilingual SMT focuses on the usage of pivot languages .__label__Supplement|Website|Introduce
We have used YamCha toolkit ( http :// chasenorg /— taku / software / yamcha ), an SVM based tool for detecting classes in documents and formulating the NER task as a sequential labeling problem . Here , the pairwise multi - class decision method and polynomial kernel function have been used . The TinySVM - 0 . 0 _CITE_ classifier has been used for classification . The C ++ based CRF ++ package ( http :// crfpp . sourceforge . net ) and the C ++ based ME package have been used for NER . Performance of the supervised NER models is limited in part by the amount of labeled training data available .__label__Method|Tool|Use
This is an extension of the similarity task for compositional models developed by Mitchell and Lapata ( 2008 ), and constructed according to the same guidelines . The dataset contains 2500 similarity judgements , provided by 25 participants , and is publicly available . _CITE_ The data consists of transitive verbs , each paired with both a subject and an object noun – thus forming a small transitive sentence . Additionally , a ‘ landmark ’ verb is provided . The idea is to compose both the target verb and the landmark verb with subject and noun , in order to form two small compositional phrases .__label__Material|Data|Introduce
The objective of TrgCmb is to maximize Ot defined in equation ( 6 ). And the constraints of TrgCmb are defined by equations ( 7 - 9 ). In this paper , we employ lpsolve _CITE_ to solve all ILP models . In our experiments , we use the Xinhua News portion of Chinese and English data in LDC OntoNotes Release 3 . 0 . This data is a Chinese - English parallel proposition bank described in ( Palmer et al ., 2005 ).__label__Method|Tool|Use
We propose a Name - aware Machine Translation ( MT ) approach which can tightly integrate name processing into MT model , by jointly annotating parallel corpora , extracting name - aware translation grammar and rules , adding name phrase table and name translation driven decoding . Additionally , we also propose a new MT metric to appropriately evaluate the translation quality of informative words , by assigning different weights to different words according to their importance values in a document . Experiments on Chinese - English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation , name translation and word alignment over a high - quality MT baseline _CITE_ . A shrinking fraction of the world ’ s Web pages are written in English , therefore the ability to access pages across a range of languages is becoming increasingly important . This need can be addressed in part by cross - lingual information access tasks such as entity linking ( McNamee et al ., 2011 ; Cassidy et al ., 2012 ), event extraction ( Hakkani - Tur et al ., 2007 ), slot filling ( Snover et al ., 2011 ) and question answering ( Parton et al ., 2009 ; Parton and McKeown , 2010 ).__label__Method|Algorithm|Compare
We present some examples of the free combination liburua irakurri (“ to read a book ”) We count the number of variations for each bigram , for all NV bigrams , and for each combination of the type bigram component + POS of the other component ( e . g , for liburua irakurri , the variations of all the combinations liburua + V and N + irakurri ). To calculate flexibility , we experiment with all the measures described in section 2 . 3 : Fazly ’ s KL - div , Wulff ’ s NSSD and Hrel ( relative entropy ), and Bannard ’ s CPMI . In order to test the substitutability of the components of bigrams , we use two resources : ( i ) ELH : Sinonimoen Kutxa , a Basque dictionary of synonyms , published by the Elhuyar Foundation ( for nouns and verbs , 40 , 146 word - synomyn pairs ); ( ii ) WN : the Basque version of WordNet _CITE_ ( 68 , 217 wordsynomyn pairs ). First , we experimented with both resources on their own , but the results show that in many cases there either was no substitute candidate , or the corpus lacked combinations containing a substitute . In order to ensure a broader coverage , we combined both resources ( ELHWN ), and we expanded the set of substitutes including the siblings retrieved from Basque WordNet ( ELHWNexpand ).__label__Material|Data|Use
If one cannot build a lattice because no matching word can be found in the lexicon , unknown word processing is invoked . Here , candidate tokens are built using character types , such as hiragana , katakana , Chinese characters , alphabets , and numbers . Japanese part - of - speech ( POS ) tagsets used in the two major Japanese morphological analyzers ChaSen _CITE_ and JUMAN take the form of a hierarchical structure . For example , IPA tagset used in ChaSen consists of three categories : part - ofspeech , conjugation form ( cform ), and conjugate type ( ctype ). The cform and ctype are assigned only to words that conjugate , such as verbs and adjectives .__label__Method|Tool|Introduce
The reliable processing of temporal information is an important step in many NLP applications , such as information extraction , question answering , and document summarisation . Consequently , the tasks of identifying and assigning values to temporal expressions have recently received significant attention , resulting in the creation of mature corpus annotation guidelines ( e . g . TIMEX2 and TimeML _CITE_ ), publicly However , existing corpora have their limitations . In particular , the documents in these corpora tend to be limited in length and , in consequence , discourse structure . This impacts on the number , range and variety of temporal expressions they contain .__label__Supplement|Document|Introduce
Since the data set in the SemEval Affective Text task consists of news headlines , a relevant word space should be produced from topically and stylistically similar texts , such as newswire documents . For this reason , we trained our model on a corpus of English - language newsprint which is available for experimentation for participants in the Cross Language Evaluation Fo Proceedings of the 4th International Workshop on Semantic Evaluations ( SemEval - 2007 ), pages 296 – 299 , Prague , June 2007 . c � 2007 Association for Computational Linguistics rum ( CLEF ). _CITE_ The corpus consists of some 100 000 newswire documents from Los Angeles Times for the year 1994 . We presume any similarly sized collection of newsprint would produce similar results . We lemmatized the data using tools from Connexor , and removed stop words , leaving some 28 million words with a vocabulary of approximately 300 000 words .__label__Material|Data|Use
Moreover , SVM have been used suc The authors did not report any per class comparison between SVM and CRF . cessfully in many NLP areas of research in general ( Diab et al ., 2007 ), and for the NER task in particular ( Tran et al ., 2007 ). We use a sequence model Yamcha toolkit , _CITE_ which is defined over SVM . CRF are a generalization of Hidden Markov Models oriented toward segmenting and labeling sequence data ( Lafferty et al ., 2001 ). CRF are undirected graphical models .__label__Method|Tool|Use
The algorithm described in Section 2 was applied to Swedish data : we started with lemma embeddings computed from a corpus , and then created sense embeddings by using the SALDO semantic network ( Borin et al ., 2013 ). We created a corpus of 1 billion words downloaded from Spr ˚ akbanken , the Swedish language bank . _CITE_ The corpora are distributed in a format where the text has been tokenized , part - of - speechtagged and lemmatized . Compounds have been segmented automatically and when a lemma was not listed in SALDO , we used the parts of the compounds instead . The input to the software computing the lemma embedding consisted of lemma forms with concatenated part - of - speech tags , e . g .__label__Material|Data|Use
However , in larger sets , e . g . “ autism ”, it took 46 . 9 and 31 . 3 seconds for LDA and PAV - EM , respectively . We also ran another implementation _CITE_ of LDA , which was 30 times slower than Mallet . While PAV - EM and LDA can be implemented in parallel computation , this indicates that PAV - EM may be more efficient to obtain themes for a larger set of PubMed documents . The PAV - EM algorithm automatically learns themes from unlabeled PubMed documents , hence the performance measures that are used in supervised learning cannot be applied to our setup .__label__Method|Code|Use
To give their network a better initialization , they learn word embeddings using a nonprobabilistic language model , which was trained on English Wikipedia for about 2 months . They released their 50 - dimensional word embeddings ( vocabulary size 130K ) under the name SENNA . _CITE_ Mikolov et al . ( 2013a ) propose two log - linear models for computing word embeddings from large corpora efficiently : ( i ) a bag - of - words model CBOW that predicts the current word based on the context words , and ( ii ) a skip - gram model that predicts surrounding words given the current word . They released their pre - trained 300 - dimensional word embeddings ( vocabulary size 3M ) trained by the skip - gram model on part of Google news dataset containing about 100 billion words .__label__Material|Data|Introduce
We adopt an open vocabulary approach where each unique predicate defines a unary relation . In order to extract propositions that are likely to be political in nature and exhibit variability according to ideology , we collect data from a politically volatile source : comments on partisan blogs . We draw data from NPR , Mother Jones and Politico , all listed by Pew Research ( Mitchell et al ., 2014 ) as news sources most trusted by those with consistently liberal views ; Breitbart , _CITE_ most trusted by those with consistently conservative views ; and the Daily Caller , Young Conservatives and the Independent Journal Review , all popular among conservatives ( Kaufman , 2014 ). All data comes from articles published between 2012 – 2015 and is centered on the US political landscape . We gather comments using the Disqus API ; as a comment hosting service , Disqus allows users to post to different blogs using a single identity .__label__Material|Data|Use
Second , our lexical database and the FST morphology require features ( such as humanness for nouns and transitivity for verbs ) that are not provided by SAMA , and we want to automatically induce these features . To address the first problem , we use a datadriven filtering method that combines open web search engines and our pre - annotated corpus . Using frequency statistics from three web search engines ( Al - Jazeera , Arabic Wikipedia , _CITE_ and the Arabic BBC website ), we find that 7 , 095 lemmas in SAMA have zero hits . Frequency statistics from our corpus show that 3 , 604 lemmas are not used in the corpus at all , and 4 , 471 lemmas occur less than 10 times . Combining frequency statistics from the web and the corpus , we find that there are 29 , 627 lemmas that returned at least one hit in the web queries and occurred at least 10 times in the corpus .__label__Supplement|Website|Use
The human - labelled dataset was used as a sanity check to make sure the dataset labelled using the emoticons classifier was not too noisy and that the human and emoticon labels matched for a majority of tweets . We collected a total of 18 million , geo - tagged , English - language tweets over three years , from January 1st , 2012 to January 1st , 2015 , evenly divided across all 36 months , using Historical PowerTrack for Twitter provided by GNIP . We created geolocation bounding boxes _CITE_ for each of the 50 states which were used to collect our dataset . All 18 million tweets originated from one of the 50 states and are tagged as such . Moreover , all tweets contained one of the six emoticons in Table 1 and were labelled as either positive or negative based on the emoticon .__label__Material|Data|Produce
As the form of the subordinated verb depends heavily on the conjunction in the subordinated Spanish clause and the semantics of the main verb , we extracted this information from two treebanks and trained different classifiers on this data . We tested the best classifier on a set of 4 texts , increasing the correct subordinated verb forms from 80 % to 89 %. As part of our research project SQUOIA , _CITE_ we have developed several tools and resources for Cuzco Quechua . These include a treebank , currently consisting of around 500 sentences , and a rule - based MT system Spanish - Cuzco Quechua . The treebank is currently being enhanced with more annotated text and should reach about 4000 sentences upon project completion .__label__Method|Algorithm|Produce
It should be remarked that this font would not be useful to make readable a Linear A corpus that is non - translittered and encoded in Unicode . On June 16th 2014 , Version 7 . 0 of Unicode standard was released , adding 2 , 834 new characters and including , finally , the Linear A character set . Linear A block has been set in the range 10600 – 1077F and the order mainly follows GORILA ’ s one _CITE_ , as seen in Table 4 . This Unicode Set covers simple signs , vase shapes , complex signs , complex signs with vase shapes , fractions and compound fractions . This is a resource that opens , for the first time , the possibility to develop a Linear A digital corpus not consisting of a transliteration or alternative transcription .__label__Method|Algorithm|Extent
& gt ;& gt ;& gt ; relation = wn . relations [’ hypernym ’] & gt ;& gt ;& gt ; relation [’ name ’] u ’ hypernym ’ & gt ;& gt ;& gt ; relation [’ rname ’] u ’ hyponym ’ & gt ;& gt ;& gt ; synset = wn . get ("< literal & gt ; word (’ game ’)")[ 0 ] & gt ;& gt ;& gt ; print relation . neighbours ( synset )[ 0 ]. to_string () en - n : activity : 2 {} The example demonstrated the method neighbours , which returns the immediate neighbours of the given linguistic object . The API is used in many products of DCL like the DCL Search Engine , Bulgarian WordNet – web access ( RESTful webservice ) etc . The GUI classes were used for the open source corpora annotation tool Chooser _CITE_ but their use is beyond the scope of this paper . This paper was prepared within the project Integrating New Practices and Knowledge in Un dergraduate and Graduate Courses in Computational Linguistics ( BG051PO001 - 3 . 3 . 06 - 0022 ) implemented with the financial support of the Human Resources Development OP 2007 - 2013 cofinanced by the European Social Fund of the EU . The author takes full responsibility for the content and under no conditions can the conclusions be considered a position of the European Union or the Ministry of Education , Youth and Science of the Republic of Bulgaria .__label__Method|Tool|Introduce
Thus an average ROUGE score is assigned to each sentence in the document . We choose the top N sentences based on ROUGE scores to have the label + 1 ( summary sentences ) and the rest to have the label − 1 ( non - summary sentences ). Basic Element ( BE ) Overlap Measure We extract BEs , the “ head - modifier - relation ” triples for the sentences in the document collection using BE package 1 . 0 distributed by ISI _CITE_ . The ranked list of BEs sorted according to their Likelihood Ratio ( LR ) scores contains important BEs at the top which may or may not be relevant to the abstract summary sentences . We filter those BEs by checking possible matches with an abstract sentence word or a related word .__label__Method|Code|Use
Dutch verbal inflections and English verbal and nominal inflections are from the CELEX database ( Baayen et al ., 1995 ). French verbal inflections are from Verbiste , an online French conjugation dictionary . _CITE_ Whereas Mikolov et al . create analogies from various inflectional forms , we require the analogies to always include the base dictionary form : the infinitive for verbs , and the nominative singular for nouns . In other words , all analogies are limited to comparisons between the base form and an inflected form .__label__Material|Data|Use
The “ frequent terms ”- lists for these attributes were generated using a manually verified list gathered from a general purpose dataset used for earlier experiments . A “ host ”- attribute is also used , which we binarised into a large number of binary host name attributes as most machine learning algorithms cannot cope with string attributes . For this purpose we took the 30 most common hosts in our dataset , which included Livejournal , _CITE_ Xanga , 20six , etc ., but also a number of hosts that are obviously not blog sites ( but host many pages that resemble blogs ). Negative indicators on common hosts that don ’ t serve blogs are just as valuable to the machine learner as the positive indicators of common blog hosts . Last but not least a binary attribute was added that acts as a class label for the instance .__label__Material|Data|Use
Specifically , the data set consists of adjective - noun , subject - verb and object - verb pairs in English and German . The organizers also provided the Wacky corpora for English and German with lowercased lemmas . _CITE_ In addition , we also experimented with wordnets and using Europarl corpora for the two languages ( Koehn , 2005 ), but none of the features based on these resources were used in the final submission . Semantic compositionality is an ambiguous term in the linguistics litterature . It may refer to the position that the meaning of sentences is built from the meaning of its parts through very general principles of application , as for example in type - logical grammars .__label__Material|Data|Use
It is estimated instead using the discount mass created by the normalisation procedure . All three strategies were evaluated . All KSC sets were subsets of the British National Corpus ( BNC ) _CITE_ . A number of sets were prepared as follows . For those newspapers or periodicals for which the BNC contained over 300 , 000 running words of text , word frequency lists were generated and similarity and homogeneity were calculated ( using x ), We then selected pairs of text types which were moderately distinct , but not too distinct , to use to generate KSC sets , ( In initial experiments , more highly distinct text types had been used , but then both Spearman and x had scored 100 %, so ' harder ' tests involving more similar text types were selected .)__label__Material|Data|Use
If the number indicated no , the wizard would retrieve a passage from a database with correct question / answer pairs . Note that in our experiments we used specific task scenarios ( described later ), so it was possible to anticipate user information needs and create this database . If the number indicated that a problematic situation should be introduced , then the Lemur retrieval engine _CITE_ was used on the AQUAINT collection to retrieve the answer . Our assumption is that AQUAINT data are not likely to provide an exact answer given our specific scenarios , but they can provide a passage that is most related to the question . The use of the random number generator was to control the ratio between the occurrence of problematic situations and error - free situations .__label__Method|Tool|Use
Due to unavailability of such a system for French , we adopted a hybrid syllabification method . For words included in Lexique ( New et al ., 2004 ), we used the gold syllabification included in the dictionary . For all other words , we generated API phonetic representations with espeak _CITE_ , and then applied the syllabification tool used for Lexique3 ( Pallier , 1999 ). The accuracy of this process exceeded 98 %. For the comparison with an AI model , we extracted the same 46 features ( see Table 2 for the complete list ) used in Franc ¸ ois ’ model and trained a SVM model .__label__Method|Tool|Use
the United Nations is referred to as FN ). The major Danish political parties were also added to this gazetteer . For person names , we build lists of both notable people , _CITE_ and also populated GATE ’ s first and last name lists with common choices in Denmark . We include temporal annotation for Danish in this pipeline , making DKIE the first temporal annotation tool for Danish . We follow the TimeML temporal annotation standard ( Pustejovsky et al ., 2004 ), completing just the TIMEX3 part .__label__Supplement|Document|Produce
We extract relations from a KB in the form of tuples , ( e , r , v ), where e is an entity , v is a value , and r is a relation that holds between them ; for example ( J . R . R . Tolkien , occupation , author ). Our KB is Freebase _CITE_ , an online database of structured information , and our corpus is from the TAC KBP task ( McNamee and Dang , 2009 ) . For each tuple , we find sentences in a corpus that contain both an exact mention of the entity e and of the value v . Of course , such sentences may not attest to the relation r , so the process produces many incorrect examples . A Human Intelligence Task ( HIT ) is a short paid task on MTurk .__label__Material|Data|Use
In the context of creating a common European research infrastructure network , our wordnet is licensed through META - SHARE , being freely available for scientific purposes . The development of the Romanian wordnet ( RoWN henceforth ) started within BalkaNet project . Afterwards , it has been developed and maintained within several projects by the Natural Language Processing ( NLP ) group of the Romanian Academy Research Institute for Artificial Intelligence ( RACAI ): ROTEL , STAR _CITE_ , SIR RESDEC , ACCURAT , METANET4U , the Romanian Academy research plan . Within BalkaNet a core of 18000 synsets was created . They were aligned to the Princeton WordNet ( PWN ) versions available throughout time , respectively version 2 . 0 at the end of the project .__label__Supplement|Website|Introduce
The following features were used to characterize such relations : The above models described the translation from TempEval tasks to classification problems and classifier features . For BIO token - chunking problems , Mallet conditional random fields and LIBLINEAR support vector machines and logistic regression were applied . For the other problems , LIBLINEAR , Mallet MaxEnt and OpenNLP MaxEnt _CITE_ were applied . All classifiers have hyper - parameters that must be tuned during training – LIBLINEAR has the classifier type and the cost parameter , Mallet CRF has the iteration count and the Gaussian prior variance , etc . The best classifier for each training data set was selected via a grid search over classifiers and parameter settings .__label__Method|Tool|Use
We used Amazon Mechanical Turk for the crowdsourcing experiments . Two separate Human Intelligence Tasks were designed for flat and nested segmentation . The concept of flat and nested segmentation was introduced to the Turkers with the help of two short videos _CITE_ . When in doubt regarding the meaning of a query , the Turkers were advised to issue the query on a search engine of their choice and find out its possible interpretation ( s ). Only Turkers who had completed more than 100 tasks at an acceptance rate of > 60 % were allowed to participate in the task and were paid $ 0 . 02 for a flat and $ 0 . 06 for a nested segmentation .__label__Supplement|Media|Produce
The negative - sampling parameter is set to 15 in all the training processes . All embeddings are trained on a free Chinese news archive that contains about 170 millions sentences and 3 . 4 billions words . We segment and parse these sentences using the MVT implementation of ZPar 0 . 7 _CITE_ ( Zhang and Clark , 2011 ), which is trained on a large - scale annotated corpus and achieves state - of - the - art analyzing accuracy on contemporary Chinese ( Qiu et al ., 2014 ) . Targets and contexts for word and dependency embeddings were filtered with a minimum frequency of 100 and 10 , respectively , and all the four types of embeddings are trained with 200 dimensions . Three datasets are used for evaluating Chinese embeddings .__label__Method|Code|Use
We can notice that all the comparable corpora have a high degree of comparability with a better comparability of the breast cancer corpora as opposed to the diabetes corpora . In the remainder of this article , [ breast cancer corpus i ] for instance stands for the breast cancer comparable corpus composed of the unique French part and the English part i ( i E [ 1 , 14 ]). The bilingual dictionary used in our experiments is the French / English dictionary ELRA - M0033 available from the ELRA catalogue _CITE_ . This resource is a general language dictionary which contains only a few terms related to the medical domain . To evaluate the quality of terminology extraction , we built a bilingual terminology reference list for each comparable corpus .__label__Material|Data|Use
In this study , we aim to construct such a system and to demonstrate that it outperforms strict string matching approaches . We refer to our system as SimSem , as in “ Similarity ” and “ Semantic ”. SimString _CITE_ is a software library utilising the CPMerge algorithm ( Okazaki and Tsujii , 2010 ) to enable fast approximate string matching . The software makes it possible to find matches in a collection with over ten million entries using cosine similarity and a similarity threshold of 0 . 7 in approximately 1 millisecond with modest modern hardware . This makes it useful for querying a large collection of strings to find entries which may differ from the query string only superficially and may still be members of the same semantic category .__label__Method|Code|Produce
For the knowledge - based measures , we use the WordNet - based implementation of the word - toword similarity metrics , as available in the WordNet :: Similarity package ( Patwardhan et al ., 2003 ). For latent semantic analysis , we use the InfoMap package . _CITE_ For ESA , we use our own implementation of the ESA algorithm as described in ( Gabrilovich and Markovitch , 2006 ). Note that all the word similarity measures are normalized so that they fall within a 0 – 1 range . The normalization is done by dividing the similarity score provided by a given measure with the maximum possible score for that measure .__label__Method|Code|Use
We call clusters that satisfy this property singleton clusters . To obtain Brown clusters for the source and target languages , we used code from Liang ( 2005 ). _CITE_ We used the data from the news commentary corpus along with the first 500K sentences of the additional monolingual newswire data also provided for the WMT shared tasks . We used 300 clusters , ignoring words that appeared only once in this corpus . We did not use the hierarchical information from the clusters but merely converted each cluster name into a unique integer , using one additional integer for unknown words .__label__Method|Code|Use
The ACL Anthology is a comprehensive electronic collection of scientific papers in our own field ( Bird et al ., 2008 ). It is updated regularly with new publications , but also older papers have been scanned and are made available electronically . We have implemented the ACL Anthology Searchbench _CITE_ for two reasons : Our first aim is to provide a more targeted search facility in this collection than standard web search on the anthology website . In this sense , the Searchbench is meant to become a service to our own community . Our second motivation is to use the developed system as a showcase for the progress that has been made over the last years in precision - oriented deep linguistic parsing in terms of both efficiency and coverage , specifically in the context of the DELPHIN community .__label__Method|Code|Produce
This paper describes the task . A full write up of the results is published separately ( Rosé & Siemens , 2014 ). Research on Massively Open Online Courses ( MOOCs ) _CITE_ is an emerging area for real world impact of technology for analysis of social media at a large scale ( Breslow et al ., 2013 ). Modeling user experience in MOOCs supports research towards understanding user needs better so that experiences that are more conducive to learning can be offered . Beyond that , automated analyses enable adaptive technology to tailor the experience of users in real time ( Rosé et al ., 2014a ).__label__Supplement|Website|Introduce
Especially when using bag - ofwords term weighting , such as in our evaluation , information on what is not relevant to the query only introduces noise . Thus , we select the most noisy field of the query to test whether the application of our hypotheses indeed results in the reduction of noise . During indexing , we remove stopwords , and stem the collections and the queries , using Porter ’ s _CITE_ stemming algorithm . We use the Terrier IR platform , and apply five different weighting schemes to match query terms to document descriptors . In IR , term weighting schemes estimate the relevance of a document for a query , as : , where is a term in , is the query term weight , and is the weight of document for term .__label__Method|Algorithm|Use
At the start of the session the relative positions of the MEG and EEG sensors were determined using a Polyhemus 3 - D digitisation system . Data preprocessing was conducted using the MNE , FieldTrip and EEGLAB packages . _CITE_ The data was band - pass filtered at 1 - 50Hz to remove slow drifts in the signal and high - frequency noise , and then down - sampled to 125Hz . Eye and muscle artefacts were not removed , but these lie outside the range of frequencies that were considered in the analysis described below . The analysis method first applies a time / frequency filter to select an information - rich band and interval for the distinction of interest ; a supervised decomposition to extract components of whole - scalp synchronous activity that are sensitive to this class distinction ( Common Spatial Patterns , or CSP – see Parra et al ., 2005 ; Model and Zibulevsky , 2006 ; Philiastides et al ., 2006 for examples of other applications to cognitive neuroscience ); and a general purpose machine learning algorithm ( SupportVector Machine or SVM ) that uses the resulting measures of signal power to predict the semantic class of each trial .__label__Method|Code|Use
Additionally , we experimented with a set of features ( 9 ) that exploit the co - occurrence statistics of the Proceedings of the 5th International Workshop on Semantic Evaluation , ACL 2010 , pages 210 – 213 , Uppsala , Sweden , 15 - 16 July 2010 . c � 2010 Association for Computational Linguistics nominals and a set of clue words chosen manually , examining the relation definitions and examples provided by the organizers . The clues characterize the relations addressed in the task ( e . g . cargo , goods , content , box , bottle characterize the Content - Container relation ) _CITE_ . Each feature type was distinguished from the others using a prefix . All but the semantic relatedness features we used were binary , denoting whether a specific word , lemma , POS tag , etc .__label__Method|Algorithm|Introduce
The version of TectoMT submitted to WMT12 builds upon the WMT11 version . Several rule - based components were slightly refined . However , most of the effort was devoted to creating a better and bigger parallel treebank — CzEng 1 . 0 _CITE_ ( Bojar et al ., 2012b ), and re - training the statistical components on this resource . Apart from bigger size and improved filtering , one of the main differences between CzEng 0 . 9 ( Bojar and Žabokrtský , 2009 ) ( used in WMT11 ) and CzEng 1 . 0 ( used in WMT12 ) is the revised annotation of formemes . There are two distinct structural layers used in the TectoMT system : The analytical layer can be obtained using different dependency parsers ( Popel et al ., 2011 ); the tectogrammatical representation is then created by rulebased modules from the analytical trees .__label__Material|Data|Introduce
The corpus , de scribed in details in ( Fraser et al ., 2013 ), contains a training set of 40472 sentences , a development and a test set of both 5000 sentences . We consider the two tagging tasks , with first a coarse tagset ( 54 tags ), and then a morpho - syntactical rich tagset ( 619 items observed on the the training set ). All the models are implemented _CITE_ with the Theano library ( Bergstra et al ., 2010 ). For optimization , we use Adagrad ( Duchi et al ., 2011 ), with a learning rate of 0 . 1 . The other hyperparameters are : the window sizes , d , and dw , respectively set to 5 and 9 , the dimension of character embeddings , word embeddings and of the hidden layer , n ,, nf and nh , that are respectively of 100 , 200 and 200 .__label__Method|Code|Produce
Our database thus stores rich information about the locations visited , acquired from a variety of sources summarised below : Habitats : Land cover maps are used to associate different habitat types ( e . g ., coniferous woodland , moorland , improved grassland , etc .) to locational fixes . Terrain features : Ordnance Survey Vector Map data _CITE_ are used to identify features ( e . g ., lochs , rivers , roads , etc .) in the vicinity of the fixes . Names : Ordnance Survey Gazetteer data is used to obtain place and feature names .__label__Material|Data|Use
Traditional QA : In this scenario answers are dynamically constructed from larger documents ( Pasca , 2001 ). We use this setup to answer questions from a biology textbook , where each section is indexed as a standalone document , and each paragraph in a given document is considered as a candidate answer . We implemented the document indexing and retrieval stage using Lucene _CITE_ . The candidate answers are scored using a linear interpolation of two cosine similarity scores : one between the entire parent document and question ( to model global context ), and a second between the answer candidate and question ( for local context ). Because the number of answer candidates is typically large ( e . g ., equal to the number of paragraphs in the textbook ), we return the N top candidates with the highest scores .__label__Method|Tool|Use
It is apparent from the comparison of the “ Total ” rows in this table and Table 3 that the first five argument labels cover more that their syntactic counterparts . For example , the arguments A0 - A4 account for all but 3 % of all arguments labels , whereas Spanish and Catalan have much more rich set of argument labels , with a high entropy of the most - frequent - label distribution . The Catalan and Spanish datasets ( Taul ´ e et al ., 2008 ) were generated from the AnCora corpora _CITE_ through an automatic conversion process from a constituentbased formalism to dependencies ( Civit et al ., 2006 ). AnCora corpora contain about half million words for Catalan and Spanish annotated with syntactic and semantic information . Text sources for the Catalan corpus are EFE news agency (- 75Kw ), ACN Catalan news agency (- 225Kw ), and ‘ El Peri ´ odico ’ newspaper (- 200Kw ).__label__Material|Data|Introduce
for all concepts ci of t , P = 0 otherwise . To create a grammar - based proximity matrix , we extracted the concept - token pairs from the parser output on the reference transcriptions in both corpora . In order to create a wordnet - based proximity matrix , we retrieve the hypernyms for the corresponding from Wordnet using the Wordnet 3 . 0 database _CITE_ . For the freebase concept - token pairs , we query tokens for a list of types with the help of the MQL query interface to the freebase . To retrieve beliefs from NELL we downloaded a tsv formatted database called every - belief - in - theKB and then queried for facts using unix grep command .__label__Material|Data|Use
We use the Web 1T 5 - gram corpus ( Brants and Franz , 2006 ) to compute the language model score for a sentence . Each of the three classifiers ( article , preposition , and noun number ) is trained with the multi - class confidence weighted algorithm ( Crammer et al ., 2009 ). The training data consists of all non - OCR papers in the ACL Anthology _CITE_ , minus the documents that overlap with the HOO 2011 data set . The features used for the classifiers follow those in ( Dahlmeier and Ng , 2012a ), which include lexical and part - of - speech n - grams , lexical head words , web - scale n - gram counts , dependency heads and children , etc . Over 5 million training examples are extracted from the ACL Anthology for use as training data for the article and noun number classifiers , and over 1 million training examples for the preposition classifier .__label__Material|Data|Use
We can see that the medical history of patient can become more precise and detailed thanks to such contextual information . In this way , factual information related to the stomach aches of patient may receive these additional descriptions which make each occurrence different and nonredundant . Notice that the previous I2B2 contests _CITE_ addressed the information extraction tasks related to different kinds of contextual information . Temporality has become an important research field in the NLP topics and several challenges addressed this taks : ACE ( ACE challenge , 2004 ), SemEval ( Verhagen et al ., 2007 ; Verhagen et al ., 2010 ; UzZaman et al ., 2013 ), I2B2 2012 ( Sun et al ., 2013 ). We propose to continue working on the extraction of temporal information related to medical events .__label__Supplement|Website|Introduce
Finally , we would like to mention that Equation 2 represents our ranking function for our full model which accounts for predicate similarity between our target argument ( data or backing ) and original claim . Our baseline model does not include predicate similarity between the targeted argument and original claim . Given the five topic motion phrases animal testing , death penalty , cosmetic surgery , smoking in public places , and junk food from schools that were randomly selected from the iDebate , a popular , wellstructured online debate platform , Top 100 Debate list _CITE_ , we construct 5 Toulmin instantations for the topic motion ban ( House , Y )), where Y is a topic motion phrase . Similarly , we construct 5 Toulmin instantations for the topic motion not ban ( House , Y )), which serves as a counterclaim . For each topic motion , we use WordNet [ 12 ] to collect the full hyponyms and lemmas of the topic motion keyword .__label__Material|Data|Use
The corresponding sequence of different degrees of sentiment is : “ very good : 5 . 0 ” & gt ; “ good : 4 . 5 ” & gt ; “ not very good : 2 . 0 ” & gt ; “ bad : 1 . 5 ” & gt ; “ very bad : 1 . 0 ”. In this section we present a systematic evaluation of the proposed approaches conducted on real data . We crawled a data collection of 137 , 569 reviews on 24 , 043 restaurants in 9 cities in the U . S . from an online restaurant evaluation website _CITE_ . Most of the reviews have both pros / cons and free - style text . For the purpose of evaluation , we take those reviews containing pros / cons as the experimental set , which is 72 . 7 % ( 99 , 147 reviews ) of the original set .__label__Material|Data|Use
The corresponding sequence of different degrees of sentiment is : “ very good : 5 . 0 ” & gt ; “ good : 4 . 5 ” & gt ; “ not very good : 2 . 0 ” & gt ; “ bad : 1 . 5 ” & gt ; “ very bad : 1 . 0 ”. In this section we present a systematic evaluation of the proposed approaches conducted on real data . We crawled a data collection of 137 , 569 reviews on 24 , 043 restaurants in 9 cities in the U . S . from an online restaurant evaluation website _CITE_ . Most of the reviews have both pros / cons and free - style text . For the purpose of evaluation , we take those reviews containing pros / cons as the experimental set , which is 72 . 7 % ( 99 , 147 reviews ) of the original set .__label__Supplement|Website|Use
Either we allow NNs as heads of VPs ( not very elegant but which is what we did ) or we have a VP without a head . The first solution might also introduce errors elsewhere ... As Ramshaw and Marcus ( 1995 ) already noted : " While this automatic derivation process introduced a small percentage of errors on its own , it was the only practical way both to provide the amount of training data required and to allow for fully - automatic testing ." For the CoNLL shared task , we have chosen to work with the same sections of the Penn Treebank as the widely used data set for base noun phrase recognition ( Ramshaw and Marcus , 1995 ): WSJ sections 15 - 18 of the Penn Treebank as training material and section 20 as test materia1 _CITE_ . The chunks in the data were selected to match the descriptions in the previous section . An overview of the chunk types in the training data can be found in table 1 .__label__Material|Data|Use
So , for example , if the word glasses is used in the English Wikipedia article on Harry Potter , and the English Wikipedia article on Google , and the word Brille occurs in the corresponding German ones , the two words are likely to get similar representations . In our experiments , we use the common subset of available German , English , French , Spanish , and Swedish Wikipedia dumps . _CITE_ We leave out words occurring in more than 5000 documents and perform dimensionality reduction using stochastic , two - pass , rank - reduced SVD - specifically , the latent semantic indexing implementation in Gensim using default parameters . We use the word embedding models of Klementiev et al . ( 2012 ) ( KLEMENTIEV ), and Chandar et al .__label__Material|Data|Use
As shown in Figure ( 1 . a ), we select the initial ML ( Mainland China ) vocabulary ( about 50 , 000 words ) and HK ( Hong Kong ) ML or TW ( Taiwan )- ML parallel news website as our data source . The preprocessing phase illustrated in Figure 1 includes sentence boundary detection , word segmentation , part - of - speech and name entity recognition ( the name of people , or the name of locations , or the name of organizations ). In specific , we firstly adopt the jsoup _CITE_ utility to iteratively crawl the parallel texts written in simplified script for Chinese Mainland and traditional script for Hong Kong and Taiwan from the Wikipedia . Secondly , we take punctuations of & quot ;.& quot ; or & quot ;!& quot ; or & quot ;?& quot ; or & quot ;;& quot ; as the sentence boundary , and employ ICTCLAS and Ikanalyzer to generate word segmentatio and part - of - speech and name entity identification for the sentence . Then , we generate parallel sentence pairs written in simplified script for Chinese Mainland and sentences written in traditional script for Hong Kong and Taiwan , respectively .__label__Method|Tool|Use
Our parser consistently outperforms the Turbo and MST parsers across 14 different languages . We also obtain the best published UAS results on 5 languages . _CITE_ Finding an expressive representation of input sentences is crucial for accurate parsing . Syntactic relations manifest themselves in a broad range of surface indicators , ranging from morphological to lexical , including positional and part - of - speech ( POS ) tagging features . Traditionally , parsing research has focused on modeling the direct connection between the features and the predicted syntactic relations such as head - modifier ( arc ) relations in dependency parsing .__label__Supplement|Document|Produce
The HOG system ( Callmeier et al ., 2004 ) for the integration of shallow and deep linguistic processors ( using a pipeline making use of XML plus XSLT transformations to pass data between processors ) was developed during the Deep Thought project , as was a standard for the integration of semantic analyses produced by diverse components : RMRS ( Copestake , 2003 ) allows underspecification of semantic analyses in such a way that the analysis produced by a shallow component may be considered an underspecification of a fuller semantic analysis produced by a deeper component . Other work ( Waldron et al ., 2006 ) has provided a representation of partial analyses at the level of tokenization / morphology – using a modification of MAF ( Clement and de la Clergerie , 2005 ). Current work within the SciBorg project _CITE_ is investigating more fine - grained integration of shallow and deep processors . Our standoff annotation framework borrows heavily from the MAF proposal . The key components of our framework are ( i ) grounding in primary linguistic data via flexible standoff pointers , ( ii ) dec oration of individual annotations with structured content , ( iii ) representation of structural ambiguity via a lattice of annotations and ( iv ) a structure of intra - annotation dependencies .__label__Method|Tool|Introduce
Sentence - level simplification ( Specia , 2010 ) has proposed to model text simplification as a Statistical Machine Translation ( SMT ) task where the goal is to translate sentences to their simplified version in the same language . In this approach , a simplification model is learnt from a parallel corpus of texts and their simplified versions . Apply ing this method , we train an SMT model from English to Simple English , based on the PWKP parallel corpus generated from Wikipedia ( Zhu et al ., 2010 ); _CITE_ we use only alignments involving a single sentence on each side . This results in a phrase table containing many entries where source and target phrases are identical , but also phrase - pairs that are mapping complex phrases to their simplified counterparts , such as the following : Also , the language model is trained with Simple English sentences to encourage the generation of simpler texts . Given a source text , it is translated to its simpler version , and its n - best translations are assessed by the confidence estimation component .__label__Material|Data|Use
Object - relational adjectives are those that require a second argument , such as ‘ known ’, which can only be understood as being ‘ known ’ to some person , in comparison to ‘ famous ’. Thus , the modelling of the relational adjective known is quite similar to the semantics of the corresponding verb know . It can be modelled for instance via the frame ‘ X is known to Y ’ and reference foaf : knows as : In this section we empirically analyze the adequacy of the modelling proposed in this paper with respect to the QALD - 4 _CITE_ dataset , a shared dataset for Question Answering over Linked Data . The 250 training and test questions of the QALD - 4 benchmark contain 76 adjectives in total ( not counting adjectives in names such as ‘ Mean Hamster Software ’). 18 of the occurring adjectives do not have a semantic contribution w . r . t .__label__Material|Data|Use
This triggered the development of the lemon model ( McCrae et al ., 2012 ) that allowed to optimally relate , in a machine - readable way , the content of these annotation properties with the objects they describe . While LOD enables connecting and querying databases from different sources , the recently emerging Linguistic Linked Open Data ( LLOD ) facilitates connecting and querying also in terms of linguistic constructs . Based on the activities of the Working Group on Open Data in Linguistics and of projects such as the European FP7 Support Action “ LIDER ” _CITE_ , the linked data cloud of linguistic resources is expanding . Our goal in the current study is to develop and promote the modeling of linguistic and semantic phenomena related to hashtags , adopting the On toLex model . This model , a result of the W3C Ontology - Lexicon community group , lies at the core of the publication of language data and linguistic information in the LLOD cloud .__label__Method|Tool|Extent
The data set includes a list of 11 , 340 notable individuals with the link to their Wikipedia page in multiple languages , plus a number of additional information such as date and place of birth , category and language editions , which we do not consider for our study . Only the persons whose Wikipedia page is translated in at least 25 languages are included in Pantheon , as a proxy of prominent world personalities . For each person in the list , we download the corresponding Wikipedia page in English and preprocess it using TheWikiMachine library _CITE_ . Overall , we collect 11 , 075 pages , while 265 pages could not be retrieved because of problems with the links ( mainly redirection links ). We randomly select 100 pages as development set , 500 pages for test and the remaining 10 , 475 for building the training set .__label__Method|Code|Use
Using the contents and structure of T *, an appropriate sentence is then generated that describes the image . In the following sections , we first introduce the image dataset used for testing followed by details of how these components are derived . We use the UIUC Pascal Sentence dataset , first introduced in [ Farhadi et al ., 2010 ] and available online _CITE_ . It contains 1000 images taken from a subset of the Pascal - VOC 2008 challenge image dataset and are hand annotated with sentences that describe the image by paid human annotators using Amazon Mechanical Turk . Fig .__label__Material|Data|Use
Klein - Braley ( 1984 ) performs a linear regression analysis with only two difficulty indicators – average sentence length and type - token ratio – obtaining good results for her target group . Eckes ( 2011 ) intend to calibrate C - test difficulty using a Rasch model in order to compare different C - tests and build a test pool . _CITE_ Kamimoto ( 1993 ) was the first to perform classical item analysis on the gap level . He created a tailored C - test that only contains selected gaps in order to better discriminate between the students . However , the gap selection is based on previous test results instead of specific gap features and thus cannot be applied on new tests .__label__Method|Algorithm|Use
Its place is Aberdeen ’). Such ‘ utility ’ classes are used across domains . In PolicyGrid we have created a utility ontology that contains classes such as ‘ Person ’, ‘ Address ’ and ‘ Date ’ _CITE_ . Instances of these classes are generated to a special surface form . In order to get the best realisation from the WYSIWYM tool , domain ontologies should use the classes from this utility ontology .__label__Material|Data|Produce
to improve the performance of DM . We use the concrete noun dataset of the ESSLLI 2008 Distributional Semantics shared task , including 44 concrete nouns to be clustered into cognitively justified categories of increasing generality : 6 - way ( birds , ground animals , fruits , greens , tools and vehicles ), 3 - way ( animals , plants and artifacts ) and 2 - way ( natural and artificial entities ). Following the task guidelines , we clustered the target row vectors in the CxLX matrix with CLUTO , _CITE_ using its default settings , and evaluated the resulting clusters in terms of cluster - sizeweighted averages of purity and entropy ( see the CLUTO documentation ). An ideal solution would have 100 % purity and 0 % entropy . Table 3 provides percentage results for our models as well as for the ESSLLI systems that reported all the relevant performance measures , indexed by first author .__label__Method|Tool|Use
We obtain state - of - the - art results on most language pairs . In this section , we describe our three base systems : DIRECTL +, SEQUITUR , and SMT . DIRECTL + is a publicly - available _CITE_ discriminative string transduction tool , which was initially developed for grapheme - to - phoneme conversion ( Jiampojamarn et al ., 2008 ). DIRECTL + was successfully applied to transliteration in the previous NEWS shared tasks by our team ( Jiampojamarn et al ., 2009 ; Jiampojamarn et al ., 2010b ; Bhargava et al ., 2011 ; Kondrak et al ., 2012 ), as well as by other teams ( Okuno , 2012 ; Wu et al ., 2012 ). We make use of all features described by Jiampojamarn et al .__label__Method|Tool|Extent
Second , we derive a polarity lexicon for Italian , organised by senses , also using a fully automatic strategy which can replicated to obtain such a resource for other languages ( Section 3 . 1 ). Third , we use the lexicon to automatically assign polarity to two subsets of the tweets in our corpus , and evaluate results against manually annotated data ( Sections 3 . 2 – 3 . 4 ). We collected one year worth of tweets , from February 2012 to February 2013 , using the Twitter filter API _CITE_ and a language recognition strategy which we describe below . The collection , named TWITA , consists of about 100 million tweets in Italian enriched with several kinds of meta - information , such as the time - stamp , geographic coordinates ( whenever present ), and the username of the twitter . Additionally , we used off - the - shelf language processing tools to tokenise all tweets and tag them with partof - speech information .__label__Supplement|Website|Use
Each extracted rule is assigned a count fC . In this section we will explore variations of this rule extraction procedure involving alternative definitions of the ranking and counting functions , fR and fC , based on probabilities over alignment models . Common practice ( Koehn et al ., 2003 ) takes a set of word alignment links L and defines the alignment constraints CA so that there is a consistency between the links in the ( fj _CITE_ as the set of alignment links is generally obtained after applying a symmetrization heuristic to sourceto - target and target - to - source Viterbi alignments . In the following section we depart from this approach and apply novel functions to rank and count target - side translations according to their quality in the context of each parallel sentence , as defined by the word alignment models . We also depart from common practice in that we do not use a set of links as alignment constraints .__label__Material|Data|Introduce
Furthermore , for examples of compound functional expressions listed in Table 2 ( a ), Table 2 ( b ) gives their example sentences as well as the description of their usages . This section describes summaries of formalizing the chunking task using SVMs ( Tsuchiya et al ., 2006 ). In this paper , we use an SVMs - based chunking tool YamCha _CITE_ ( Kudo and Matsumoto , 2001 ). In the SVMs - based chunking framework , SVMs are used as classifiers for assigning labels for representing chunks to each token . In our task of chunking Japanese compound functional expressions , each sentence is represented as a sequence of morphemes , where a morpheme is regarded as a token .__label__Method|Tool|Use
Conversely , if sim ( ejl , ej , l ,) G 0 . 5 , both excerpts may be selected . Solving the ILP Solving an integer linear program is NP - hard ( Cormen et al ., 1992 ); however , in practice there exist several strategies for solving certain ILPs efficiently . In our study , we employed lp solve , _CITE_ an efficient mixed integer programming solver which implements the Branch - and - Bound algorithm . On a larger scale , there are several alternatives to approximate the ILP results , such as a dynamic programming approximation to the knapsack problem ( McDonald , 2007 ). * Defined as the first unigram in the excerpt .__label__Method|Tool|Use
lp solve is used for our joint inference . This paper focuses on automatically labeling the full argument spans of discourse connectives . For a fair comparison with start - of - theart systems , we use the NUS PDTB - style endto - end discourse parser _CITE_ to perform other subtasks of discourse parsing except argument labeling , which includes connective identification , non explicit discourse relation identification and classification . Finally , we evaluate our system on two aspects : ( 1 ) the dependence on the parse trees ( GS / Auto , using gold standard or automatic parse trees and sentence boundaries ); and ( 2 ) the impact of errors propagated from previous components ( noEP / EP , using gold annotation or automatic results from previous components ). In combination , we have four different settings : GS + noEP , GS + EP , Auto + noEP and Auto + EP .__label__Method|Tool|Use
Thus , in a dependency tree in HyDT , each node is a chunk and the edge represents the relations between the connected nodes labeled with the karaka or other relations . All the modifier - modified relations between the heads of the chunks ( inter - chunk relations ) are marked in this manner . The annotation is done using Sanchay _CITE_ mark up tool in Shakti Standard Format ( SSF ) ( Bharati et al ., 2005 ). For the work in this paper , to get the complete dependency tree , we used an automatic rule based intrachunk relation identifier . The rules mark these intra - chunk relations with an accuracy of 99 . 5 %, when evaluated on a test set .__label__Method|Tool|Use
Of the 1000 test instances , we provided a single “ best ” candidate 630 times , two candidates 253 times , three candidates 70 times , four candidates 30 times , and six candidates 17 times . ( We never returned five candidates ). The SWAT - S system used both Google ’ s and Yahoo ’ s _CITE_ online translation tools , the Spanish section of the Web1T European 5 - gram corpus , Roget ’ s online thesaurus , TreeTagger ( Schmid , 1994 ) for morphological analysis and both Google ’ s and Yahoo ’ s English - Spanish dictionaries . We formed a single Spanish - English dictionary by combining the translations found in both dictionaries . To find the cross - lingual lexical substitutes for a target word in an English sentence , we first translate the sentence into Spanish and then use the syntagmatic coherence criterion on the translated Spanish sentence .__label__Method|Tool|Use
In ( 8 ), both a literal ( triggered by “ arriving in ”) and a place - for - people reading ( triggered by “ leading critic ”) are invoked . We introduced the category mixed to deal with these cases . Using Gsearch ( Corley et al ., 2001 ), we randomly extracted 1000 occurrences of country names from the BNC , allowing any country name and its variants listed in the CIA factbook _CITE_ or WordNet ( Fellbaum , 1998 ) to occur . Each country name is surrounded by three sentences of context . The 1000 examples of our corpus have been independently annotated by two computational linguists , who are the authors of this paper .__label__Supplement|Document|Use
Semantic textual similarity relates to textual entailment ( Dagan et al ., 2005 ), lexical substitution ( McCarthy and Navigli , 2009 ) and paraphrasing ( Hirst , 2003 ). The key issue for semantic textual similarity is that the task is to determine similarity , where similarity is cast as meaning equivalence . _CITE_ In textual entailment the relation under question is the more specific relation of entailment , where the meaning of one sentence is entailed by another and a system needs to determine the direction of the entailment . Lexical substitution relates to semantic textual similarity though the task involves a lemma in the context of a sentence , candidate substitutes are not provided , and the relation at question in the task is one of substitutability . Paraphrase recognition is a highly related task , for example using comparable corpora ( Barzilay and Elhadad , 2003 ), and it is likely that semantic textual similarity measures might be useful for ranking candidates in paraphrase acquisition .__label__Supplement|Document|Introduce
We use the Universal Dependency Treebank v1 ( McDonald et al ., 2013 ) for annotation projection , parser training and evaluation . It is a collection of data sets with consistent syntactic annotation for six languages : English , French , German , Korean , Spanish , and Swedish . _CITE_ The annotation is based on Stanford Typed Dependencies for English ( De Marneffe et al ., 2006 ) but has been adapted and harmonized to allow adequate annotation of typologically different languages . This is the first collection of data sets that allows reliable evaluation of labeled dependency parsing accuracy across multiple languages ( McDonald et al ., 2013 ). We use the dedicated training and test sets from the treebank distribution in all our experiments .__label__Material|Data|Use
On the other hand , in the sentence ( B ), the expression simply corresponds to a literal concatenation of the usages of the constituents : the post - positional particle “ に ( ni )” and the verb “ ついて ( tsuite )”, and has a content word meaning “ follow ”. Therefore , when considering machine translation of those Japanese sentences into English , it is necessary to precisely judge the usage of the compound expression “ に ( ni ) ついて ( tsuite )”, as shown in the English translation of the two sentences in Table 1 . There exist widely - used Japanese text processing tools , i . e ., pairs of a morphological analysis tool and a subsequent parsing tool , such as JUMAN + KNP and ChaSen + CaboCha _CITE_ . However , they process those compound expressions only partially , in that their morphological analysis dictionaries list only limited number of compound expressions . Furthermore , even if certain expressions are listed in a morphological analysis Grammatical Function Type # of major # of Example expressions variants post - positional subsequent to predicate 36 67 となると particle / modifying predicate ( to - naru - to ) type subsequent to nominal 45 121 にかけては / modifying predicate ( ni - kakete - ha ) subsequent to predicate , nominal 2 3 という / modifying nominal ( to - iu ) auxiliary verb type 42 146 ていい ( te - ii ) total 125 337 — dictionary , those existing tools often fail in resolving the ambiguities of their usages , such as those in Table 1 .__label__Method|Tool|Introduce
A few observations we made while working on blog dataset were : Much of the information present in telegraph . co . uk the blog ( s ) were factual , most of the opinions expressed were either in comparison format or negatively orientated . In this subsection , we explain the method used for evaluating our approach . We hired three human annotators for this task and calculation of their mutual agreement is done using Cohen ’ s Kappa measurement _CITE_ . Validation task was divided into three basic steps 2 . Modifier Identification : After step 1 , they were asked to mark and decide the orientation ( positive or negative ) for all the modifier words ( adjectives , adverbs and verbs ) from the text .__label__Method|Algorithm|Use
Count of elements in an intersection of two synsets indicates the commonality of the two sets and its absolute value stands for their commonality measure . Considering the common elements as the dimensions of the vector space , similarity based techniques are applied to measure the semantic affection of the two components present in a bigram . The system uses a large number of Bengali articles written by the noted Indian Nobel laureate Rabindranath Tagore _CITE_ . We are primarily interested in single document term affinity rather than document information and document length normalization . Merging all of the articles , a medium size raw corpus consisting of 393 , 985 tokens and 283 , 533 types has been prepared .__label__Material|Data|Use
The first step involves extraction of semantic and temporal features for the annotated medical concepts , as described in Section 4 from both corpora . The semantic relatedness scores are computed using the kDLS ( Xiang et al ., 2011 ) method to calculate the relationship between concepts in the UMLS with value of - y set to 7 . The type of relation between medical concepts is derived by matching word stems in each medical concept using the Lucene _CITE_ implementation of the Porter stemming algorithm . We query the latest release ( UMLS 2011AB ) of the UMLS Metathesaurus for finding a match between medical concept and the UMLS definition or UMLS atoms . The WordNet similarity score is computed using Java API for WordNet Searching ( JAWS ). 10 Explicit temporal expressions annotated in the corpora are included in our temporal feature set .__label__Method|Code|Use
Second , unlike other image databases , ImageNet consists of millions of images , and it is a growing resource with more images added on a regular basis . This aligns with our long - term goal of building a large - scale joint semantic space of images and words . Finally , third , although we can search for relevant images using keywords in ImageNet , _CITE_ there is currently no method to query it in the reverse direction . Given a test image , we must search through millions of images in the database to find the most similar image and its corresponding synset . A joint semantic model can hopefully augment this shortcoming by allowing queries to be made in both directions .__label__Material|Data|Use
( 2012 ) extended the EventCorefBank corpus with entity coreference information and additional annotations of event coreference . One important step in the creation process of the ECB corpus consists of finding sets of related documents that describe the same seminal event such that the annotation of coreferential event mentions across documents is possible . In this regard , we searched the Google News archive _CITE_ for various topics whose description contains keywords such as commercial transaction , attack , death , sports , announcement , terrorist act , election , arrest , natural disaster , and so on , and manually selected sets of Web documents describing the same seminal event for each of these topics . In a subsequent step , for every Web document , we automatically tokenized and split the textual content into sentences , and saved the preprocessed data in a uniquely identified text file . Next , we manually annotated a limited set of events in each text file in accordance with the TimeML specification ( Pustejovsky et al .__label__Supplement|Website|Use
We work with a Chinese – English parallel training corpus of 3 . 0 M sentence pairs ( 77 . 5 M Chinese / 81 . 0 M English running words ). To train the German -+ French baseline system , we use 2 . 0 M sentence pairs ( 53 . 1 M French / 45 . 8 M German running words ) that are partly taken from the Europarl corpus ( Koehn , 2005 ) and have partly been collected within the Quaero project . _CITE_ Word alignments are created by aligning the data in both directions with GIZA ++ and symmetrizing the two trained alignments ( Och and Ney , 2003 ). When extracting phrases , we apply several restrictions , in particular a maximum length of ten on source and target side for lexical phrases , a length limit of five on source and ten on target side for hierarchical phrases ( including non - terminal symbols ), and no more than two non - terminals per phrase . A standard set of models is used in the baselines , comprising phrase translation probabilities and lexical translation probabilities in both directions , word and phrase penalty , binary features marking hierarchical rules , glue rule , and rules with non - terminals at the boundaries , three simple count - based binary features , phrase length ratios , and a language model .__label__Material|Data|Use
In the following sections we describe in more detail the dataset used for training and testing , the system developed , the evaluation methodology , as well as ablation experiments aimed at studying the contribution of different feature types to the AA task . We show experimentally that discriminative models with appropriate feature types can achieve performance close to the upper bound , as defined by the agreement between human examiners on the same test corpus . The Cambridge Learner Corpus ( CLC ), developed as a collaborative project between Cambridge University Press and Cambridge Assessment , is a large collection of texts produced by English language learners from around the world , sitting Cambridge Assessment ’ s English as a Second or Other Language ( ESOL ) examinations _CITE_ . For the purpose of this work , we extracted scripts produced by learners taking the First Certificate in English ( FCE ) exam , which assesses English at an upper - intermediate level . The scripts , which are anonymised , are annotated using XML and linked to meta - data about the question prompts , the candidate ’ s grades , native language and age .__label__Supplement|Website|Introduce
In the following sections we describe in more detail the dataset used for training and testing , the system developed , the evaluation methodology , as well as ablation experiments aimed at studying the contribution of different feature types to the AA task . We show experimentally that discriminative models with appropriate feature types can achieve performance close to the upper bound , as defined by the agreement between human examiners on the same test corpus . The Cambridge Learner Corpus ( CLC ), developed as a collaborative project between Cambridge University Press and Cambridge Assessment , is a large collection of texts produced by English language learners from around the world , sitting Cambridge Assessment ’ s English as a Second or Other Language ( ESOL ) examinations _CITE_ . For the purpose of this work , we extracted scripts produced by learners taking the First Certificate in English ( FCE ) exam , which assesses English at an upper - intermediate level . The scripts , which are anonymised , are annotated using XML and linked to meta - data about the question prompts , the candidate ’ s grades , native language and age .__label__Supplement|Website|Other
The majority of existing applications for Russian IE , and Natural Language Processing ( NLP ) in general , are commercially based , and are either published in Russian only , or not at all . One major player in Russian text mining is Yandex , the leading Russian search engine . Yandex uses IE to support its main search service , e . g ., to underline addresses and persons in search results , and in a service called “ Press Portraits ,” _CITE_ which builds profiles for various personalities found in the news . A profile may include the profession , biographical facts , news that s / he is involved in , and related people — using information automatically extracted from on - line Russian media . Yandex also recently unveiled an open - source toolkit Tomita , for developing IE systems based on context - free grammars .__label__Supplement|Website|Introduce
One of the advantages of corpusbased approaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually . This naturally led the way for international resource standardisation , and indeed there is a long standing precedent in the West for it . The Human Language Technology ( HLT ) society in Europe has been particularly zealous in this regard , propelling the creation of resource interoperability through a series of initiatives , namely EAGLES ( Sanfilippo et al ., 1999 ), PAROLE / SIMPLE ( Lenci et al ., 2000 ), ISLE / MILE ( Ide et al ., 2003 ), and LIRICS _CITE_ . These continuous efforts have matured into activities in ISO - TC37 / SC4 , which aims at making an international standard for language resources . However , due to the great diversity of languages themselves and the differing degree of technological development for each , Asian languages , have received less attention for creating resources than their Western counterparts .__label__Material|Data|Introduce
Recent interest in Mathematical information retrieval ( MIR ) has prompted the construction of the NTCIR Math IR test collection ( Aizawa et al ., 2013 ). Like many general - purpose , domainspecific IR test collections , the NTCIR collection is composed of broad queries intended to test systems over a wide spectrum of query complexity . In this paper we present a test collection composed of real - life , research - level mathematical topics and associated relevance judgements procured from the online collaboration web - site MathOverflow _CITE_ . The resulting test collection con tains 160 atomic questions - material derived from 120 MathOverflow discussion threads . Topics in our test collection capture specialised information needs that are complex to resolve and often demand collective effort from multiple domain experts .__label__Supplement|Website|Use
In ( Widdows et al ., 2002 ) tools are presented that visualize meanings of nouns as vector space representation , using LSA ( Deerwester et al ., 1990 ) and graph models using co - occurrences . There is also a range of text - based tools , without any quantitative statistics , e . g . Textpresso ( M ¨ uller et al ., 2004 ), PhraseNet and Walden _CITE_ . For searching words in context , Luhn ( 1960 ) introduced KWIC ( Key Word in Context ) which allows us to search for concordances and is also used in several corpus linguistic tools e . g . ( Culy and Lyding , 2011 ), BNCWeb , Sketch Engine ( Kilgarriff et al ., 2004 ), Corpus Workbench and MonoConc ( Barlow , 1999 ).__label__Method|Tool|Introduce
A word graph is here described as a directed acyclic graph G = ( V , E ) with one root node n0 E V . Edges are labeled with tokens ( words or translation units ) and optionally with accumulated scores . We will use ( ns ( ne ′′ t ′′ s )), to denote an edge starting at node ns and ending at node ne , with token t and score s . The file format of word graphs coincides with the graph file format recognized by the CARMEL _CITE_ finite state automata toolkit . We can mainly find two applications for which word graphs are used as input of an SMT system : the recognition output of an automatic speech recognition ( ASR ) system ; and a reordering graph , consisting of a subset of the whole word permutations of a given input sentence . In our case we are using the input graph as a reordering graph .__label__Method|Tool|Use
Roughly one half of this course was devoted to the treebanking project . In the first week , part - of - speech ( POS ) tagging was introduced , with English as the example language . During the practicum , students reviewed POS concepts with exercises and Stanford ’ s online tagger _CITE_ . In the second , dependency trees were introduced , again using examples in English . Lectures in the third and fourth weeks turned the attention to Chinese POS and dependency trees , using respectively the schemes defined at the Penn Chinese Treebank ( Xue et al ., 2005 ) and Stanford ( Chang et al ., 2009 ).__label__Method|Tool|Use
A first step towards making this possible is gathering a large amount of bibliographic data , extract mentions of papers , authors , venues , and institutions , and perform massive - scale cross document entity resolution ( coreference ) and relation extraction to identify the real - world entities . To this end , we implement a prototype “ epistemological ” knowledge base for bibliographic data . Currently , we have supplemented DBLP _CITE_ with extra mentions from BibTeX files to create a database with over ten million mentions ( 6 million authors , 2 . 3 million papers , 2 . 2 million venues , and 500k institutions ). We perform joint coreference between authors , venues , papers , and institutions at this scale . We describe our coreference model next .__label__Supplement|Website|Extent
For each task the system must replace the UNKNOWN values with one of the six allowed values listed above . The EVENT and TIMEX3 annotations were taken verbatim from TimeBank version 1 . 2 . _CITE_ The annota tion procedure for TLINK tags involved dual annotation by seven annotators using a web - based annotation interface . After this phase , three experienced annotators looked at all occurrences where two annotators differed as to what relation type to select and decided on the best option . For task C , there was an extra annotation phase where the main events were marked up .__label__Method|Tool|Use
On the other hand , O ’ Connor et al . ( 2014 ) investigated the normalisation of medical terms in Twitter messages . In particular , they proposed to use the Lucene retrieval engine _CITE_ to retrieve medical concepts that could be potentially mapped to a given Twitter phrase , when mapping between Twitter phrases and medical concepts . In contrast , we argue that the medical text normalisation task can be achieved by using wellestablished phrase - based MT techniques , where we translate a text written in a social media language ( e . g . ‘ No way I ’ m gettin any sleep 2nite ’) to a text written in a formal medical language ( e . g .__label__Method|Tool|Use
This machine is then used to evaluate the systems , which makes the experiments directly reproducible in the future . System submissions are currently becoming increasingly popular in shared tasks . For example , the CoNLL 2015 shared task on shallow discourse parsing _CITE_ applies this technology . We plan to use the same system as the CoNLL task , TIRA ( Gollub et al ., 2012 ), which is already successfully applied in the PAN workshops on plagiarism detection . We propose a shared task for mining the argumentative structure in newspaper editorials .__label__Method|Algorithm|Introduce
We extracted distributional semantic vectors using as source corpus the concatenation of ukWaC , Wikipedia ( 2009 dump ) and BNC , 2 . 8 billion tokens in total . We use a bag - of - words approach and we count co - occurrences within sentences and with a limit of maximally 50 words surrounding the target word . By tuning on the MEN lexical relatedness dataset , _CITE_ we decided to use the top 10K most frequent content lemmas as context features ( vs . top 10K inflected forms ), and we experimented with positive Pointwise and Local Mutual Information ( Evert , 2005 ) as association measures ( vs . raw counts , log transform and a probability ratio measure ) and dimensionality reduction by Non - negative Matrix Factorization ( NMF , Lee and Seung ( 2000 )) and Singular Value Decomposition ( SVD , Golub and Van Loan ( 1996 )) ( both outperforming full dimensionality vectors on MEN ). For both reduction techniques , we varied the number of dimensions to be preserved from 50 to 300 in 50 - unit intervals . As Local Mutual Information performed very poorly across composition experiments and other parameter choices , we dropped it .__label__Material|Data|Use
It involves several important NLP research areas : automatic speech recognition ( ASR ), statistical machine translation ( SMT ) and speech synthesis , also known as text - to - speech ( TTS ). In recent years significant advance have also been made in relevant technological devices : the size of powerful computers has decreased to fit in a mobile phone and fast WiFi and 3G networks have spread widely to connect them to even more powerful computation servers . Several hand - held S2ST applications and devices have already become available , for example by IBM , Google or Jibbigo _CITE_ , but there are still serious limitations in vocabulary and language selection and performance . When an S2ST device is used in practical human interaction across a language barrier , one feature that is often missed is the personalization of the output voice . Whoever speaks to the device in what ever manner , the output voice always sounds the same .__label__Supplement|Website|Introduce
In order to build compact translation models , we have preprocessed the parallel corpus using different word elimination strategies so that unimportant words would be removed from parallel strings . We have also used a stoplist consisting of 429 words to remove stopwords . The out - of - the - box GIZA ++ _CITE_ ( Och and Ney , 2004 ) has been used to learn translation models using the pre - processed parallel corpus for our retrieval experiments . We have also trained initial translation models , using a parallel corpus from which only the stopwords are removed , to compare with the compact translation models . Eventually , the number of parameters needed for modeling translations would be minimized if unimportant words are eliminated with different ap proaches .__label__Method|Tool|Use
It is often the case that interoperability can be problematic to achieve , especially for resources that have different developers or creators . Reasons for this include the following : Having to deal with such issues can be both time - consuming and a source of frustration for the developer , often requiring program code to be rewritten or extra code to be produced in order to ensure that data can pass freely and correctly between the different resources used in the application . One way to overcome some of the problems of interoperability is to adopt the use of the Architecture ( UIMA ) _CITE_ ( Ferrucci et al ., 2006 ), which aims to facilitate the seamless combination of LRs into workflows that can carry out different natural language processing ( NLP ) tasks . U - Compare ( Kano et al ., 2009 ; Kano et al ., 2011 ), which is built on top of UIMA , provides additional means for ensuring more universal interoperability between resources , as well as providing special facilities that allow the rapid construction and evaluation of natural language - processing / text - mining applications using interoperable UIMAcompliant resources , without the need for any additional programming . METANET4U is one of a set of projects ( together with META - NORD and CESAR ), which are preparing LRs that operate on a wide range of different European languages for inclusion within META - SHARE .__label__Method|Tool|Use
We now detail these stages . Stage 1 immediately discards webpages that do not meet three criteria from subsequent automatic processing . For a page to be automatically processed by subsequent recognition and parsing phases , FireCite requires that the webpage : The included domains include web pages from academic institutions , digital libraries such as CiteseerX _CITE_ and ACM Portal , and online encyclopedias such as Wikipedia 5 – basically , web pages where reference strings are likely to be found . The keywords serve to further filter away pages unlikely to contain lists of reference strings , by requiring words that are likely to appear in the headings of such lists . Stage 1 runs very quickly and filters most nonscholarly web pages away from the subsequent , more expensive processing .__label__Method|Tool|Use
In this paper , we use 3 . 6M parallel patent sentences with the highest scores of sentence alignment . As a toolkit of a phrase - based SMT model , we use Moses ( Koehn et al ., 2007 ) and apply it to the whole 3 . 6M parallel patent sentences . Before applying Moses , Japanese sentences are segmented into a sequence of morphemes by the Japanese morphological analyzer MeCab _CITE_ with the morpheme lexicon IPAdic . For Chinese sentences , we examine two types of segmentation , i . e ., segmentation by characters and segmentation by morphemes . As the result of applying Moses , we have a phrase table in the direction of Japanese to Chinese translation , and another one in the opposite direction of Chinese to Japanese translation .__label__Method|Tool|Use
We replaced these ‘ correct answers ’ with their explicit names . We also removed zeros in quoted sentences because they are quite different from other sentences . In addition , we decided to use the output of ChaSen 2 . 2 . 9 and CaboCha 0 . 34 _CITE_ instead of the morphological information and the dependency information provided by the Kyoto Corpus since classification of the joshi ( particles ) in the Corpus was not satisfactory for our purpose . Since CaboCha was trained by Kyoto Corpus 3 . 0 , CaboCha ’ s dependency output is very similar to that of the Corpus . In this paper , we combine heuristic ranking rules and machine learning .__label__Method|Algorithm|Use
This context is possible through the use of multiple ontologies , which are specialized in different components , such as the annotation layer , the domain concepts , and the linguistic rules specification . Despite the higher computational cost that this approach can present when compared with some other options , the results , as described in the result analysis section , presents a good precision and are not dependent of a large volume of documents to generate basic and reference models . The computational phase of the methodology suggested is implemented in the SAURON system , developed in Java Language and the OWL Api _CITE_ support , integrating the Pellet reasoner10 . This system is inspired in the unifying logic layer of the standard technology stack for semantic web11 , since one of the objectives of this system is to unify the use of several semantic technologies applied . The system provides the necessary support to the tasks involving Natural Language Processing , such as the text preprocessing , the syntactic parser access and some format conversions tasks .__label__Supplement|Website|Use
Consequently , we set r = 10 in POS tagging . For sentiment analysis , we used all features in the source domain labeled reviews as distributional features , weighted by their scores given by Equation 4 , taking the inverse - rank . In both tasks , we parallelised similarity computations using BLAS _CITE_ level - 3 routines to speed up the computations . The source code of our implementation is publicly available . To evaluate DA for POS tagging , following Blitzer et al .__label__Method|Tool|Use
On the other hand , a lexicon - based approach using natural language processing techniques , developed for a generic sentiment analysis task with no adaptation to the provided training corpus . Results , though far from the best runs , prove that the generic model is more robust as it achieves a more balanced evaluation for message polarity along the different test sets . SemEval _CITE_ is an international competitive evaluation workshop on semantic related tasks . Among the ten different tasks that have been proposed in 2014 , Task 9 at SemEval - 2014 focuses on sentiment analysis in Twitter . Sentiment analysis could be described as the application of natural language processing and text analytics to identify and extract subjective information from texts .__label__Supplement|Website|Introduce
We used descriptions of research talks collected from the university ’ s website . We used a web - based interface for data collection ; the interface presented the prompt material and recorded the subject ’ s voice response . Testvox was used to setup the experiments and Wami _CITE_ for audio recording . We recruited 40 researchers ( graduate students ) from the School of Computer Science , at Carnegie Mellon , representative of the user population for the EVENTSPEAK dialog system . Each subject responded to prompts from the QUERYDRIVEN , PERSONAL and SHOW & ASK strategies .__label__Method|Tool|Use
Our results possibly indicate the opposite , the “ polarizing ” model suggested by ( Franklin and Kosaki , 1989 ) and ( Johnson and Martin , 1998 ), where more negative opinions are observed after the decision ( in Figure 4 ), at least for a short period . By learning and visualize political sentiments , we could crystalize the nature of the decision that influences the degree to which the Supreme Court can move opinion in the direction of its decisions . Figure 5 shows a website _CITE_ that visualizes political sentiments over time . The website shows several popular U . S . Supreme Court cases , such as “ gay marriage ”, “ voting right act ”, “ tax cases ”, etc ., and general topics , such as “ Supreme Court ” and “ Justices ”. Each of the topics is represented by a list of keywords developed by political science experts .__label__Supplement|Website|Introduce
Wikipedia ), constructing dictionaries ( e . g . Wiktionary ), or hosting online communities ( e . g . ACLWiki _CITE_ ). However , as wikis do not enforce their users to structure pages or add complementary metadata , wikis often end up as a mass of unmanageable pages with meaningless page titles and no usable link structure ( Buffa , 2006 ). To solve this issue , we present the Wikulu system which uses natural language processing to support wiki users with their typical tasks of adding , Portmanteau of the Hawaiian terms wiki (“ fast ”) and kukulu (“ to organize ”) organizing , and finding content .__label__Supplement|Website|Introduce
The system behavior can be controlled by passing arguments through the command line interface . For example , the user can specify which clustering algorithm should be used . To facilitate using the system for research purposes , the system comes with a clustering evaluation component that uses the ClusterEvaluator package . _CITE_ . If the input to the system contains subgroup labels , it can be run in the evaluation mode in which case the system will output the scores of several different clustering evaluation metrics such as purity , entropy , f - measure , Jaccard , and RandIndex . The system also has a Java API that can be used by researchers to develop other systems using our code .__label__Method|Code|Use
- Constrained PMI : To build constrained system , we generated two domain - specific sentiment lexicons from the given training data respectively ( i . e ., laptop and restaurant ). Given a term w , this PMI - based score is calculated from labeled reviews as below : where PMI stands for pointwise mutual information . - Bing Liu opinion lexicon _CITE_ : This sentiment lexicon contains two annotated words lists : positive ( about 2 , 000 ) and negative ( about 4 , 800 ). - General Inquirer lexicon : The General Inquirer lexicon tries to classify English words along several dimensions , including sentiment polarity and we selected about 1 , 500 positive words and 2 , 000 negative words . - IMDB : This lexicon is generated from a large data set from IMDB which contains 25 , 000 positive and 25 , 000 negative movie reviews and the PMI - based sentiment score of each word is calculated as above .__label__Material|Data|Introduce
The enormous potential of the web as a source of material for linguistic research in a wide range of areas is well established ( Kilgarriff and Grefenstette , 2003 ), with many new opportunities created by web - scale resources ranging from simple N - grams ( Brants and Franz , 2006 ) to syntactically analyzed text ( Goldberg and Orwant , 2013 ). Yet , while the use of multilingual web data to support linguistic research is well recognized ( Way and Gough , 2003 ), cross - linguistic efforts involving syntax have so far been hampered by the lack of consistent annotation schemata and difficulties relating to coincidental differences in the syntactic analyses produced by parsers for different languages ( Nivre , 2015 ). The Universal Dependencies ( UD ) project _CITE_ seeks to define annotation schemata and guidelines that apply consistently across languages , standardizing e . g . part - of - speech tags , morphological feature sets , dependency relation types , and structural aspects of dependency graphs . The project further aims to create dependency treebanks following these guidelines for many languages .__label__Method|Tool|Introduce
The optimal value of 0 was manually tuned on the development set . The training data for our MT system consists of 1 . 76 million sentences of German - English parallel data . Parallel TED talks _CITE_ are used as in - domain data and our translation models are adapted to the domain . Before training , we apply preprocessing such as text normalization , tokenization , and smartcasing . Additionally , German compound words are split .__label__Material|Data|Use
The interest in the research community for the extraction of the sentiment in Twitter posts is reflected in the organization of several workshops with the aim of promoting the research in this task . Two are the most relevant , the first is the task Sentiment Analysis in Twitter celebrated within the SemEval workshop whose first edition was in 2013 ( Nakov et al ., 2013 ). The second is the workshop TASS _CITE_ , which is a workshop for promoting the research in sentiment analysis in Spanish in Twitter . The first edition of the workshop took place in 2012 ( Villena - Rom ´ an et al ., 2013 ). The 2014 edition of the task Sentiment Analysis in Twitter proposes a first subtask , which has as challenge the sentiment classification at entity level , and a second subtask that consists of the polarity classification at document or tweet level .__label__Supplement|Website|Introduce
The latter is particularly interesting , because it is well published , it includes both an alternative , centroid - based technique to automatically tag training examples and a soft - matching classifier , We also experimented with other similarity measures ( e . g ., edit distance ) and ROUGE variants , but we obtained the best results with ROUGE - W . We use Stanford ’ s classifier ; see http :// nlp . stanford . edu /. and it is publicly available . _CITE_ We show that ATTW outperforms Cui et al .’ s centroid - based technique , and that our overall system is also clearly better than Cui et al .’ s in the task we address . Section 2 discusses ATTW with ROUGE - W , Cui et al .’ s centroid - based method to tag training examples , and experiments showing that ATTW is better . Section 3 describes our new overall system , the system of Cui et al ., and the baselines .__label__Method|Tool|Use
to facilitate system comparison later . We evaluate C & W word embeddings with 25 , 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al . ( 2010 ) and can be downloaded here _CITE_ . The fact that we utilize the large , general and unbiased resources generated from the previous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single - system DA . We use the ACE 2005 corpus for DA experiments ( as in Plank and Moschitti ( 2013 )).__label__Material|Data|Use
They incorporated their character - based transliteration model learnt from Urdu – Hindi parallel word list into Models M1 and M2 . They reported very low BLUE scores of 19 . 35 and 18 . 34 for models M1 and M2 respectively . We have developed statistical models for solving the problem of Urdu  Hindi transliteration ( bidirectional ) using an Urdu – Hindi parallel word list resource , extracted from the data of a dictionary of Urdu , classical Hindi and English ( Platts 1884 ) _CITE_ , digitized under the project “ Digital South Asian Library ” at University of Chicago , USA and Center for Research Libraries . In this paper , we have discussed our training data , the Urdu – Hindi parallel lexicon in Section 2 . Various components of Statistical Transliteration ( ST ) systems like data alignment techniques , transliteration models and target language models are discussed in Section 3 .__label__Material|Data|Use
Recognition of named entities in natural language text is an important subtask of information extraction and thus bears importance for modern text mining and information retrieval applications . The need to identify named entities such as persons , locations , organizations and places , arises both in applications where the entities are first class objects of interest , such as in Wikification of documents ( Ratinov et al ., 2011 ), and in applications where knowledge of named entities is helpful in boosting performance , e . g ., machine translation ( Babych and Hartley , 2003 ) and question answering ( Leidner et al ., 2003 ). The advent of massive machine readable factual databases , such as Freebase and the proposed Wikidata _CITE_ , will likely push the need for automatic extraction tools further . While these databases store information about entity types and the relationships between those types , the named entity recognition ( NER ) task concerns finding occurrences of named entities in context . This view originated with the Message Understanding Conferences ( MUC ) ( Grishman and Sundheim , 1996 ).__label__Supplement|Website|Introduce
By 1992 the creators of WordNet had demonstrated the power and utility of a searchable and open database of English words , organized around core semantic relations such as synonymy , meronymy and holonymy , hypernymy and hyponymy , and so on . Although WordNet was an inspiration to us , its purposes and structure are somewhat different from those of FrameNet . The goal of the FrameNet project _CITE_ ( Fillmore , Johnson , and Petruck 2003 ) was to create a database , to be used by humans and computers , that would include a list of all of the Frames that we could possibly have time to describe . Frames are the cognitive schemata that underlie the meanings of the words associated with that Frame . The example of the frame Compliance is given in Figure 4 .__label__Method|Tool|Introduce
Thus , this feature serves to constrain the Arg1 search space for intra - sentential argument span extraction . The value of the feature is either ARG2 suffixed for whether a token is Inside ( I ), Begin ( B ), or End ( E ) of the span , or ‘ O ’ if it does not belong to the Arg2 span . These features are expanded during training with n - grams ( feature of CRF ++ _CITE_ ): tokens with 2 - grams in the window of + 1 tokens , and the rest of the features with 2 & 3 - grams in the window of The in - domain performance of argument span extraction models is provided in the following section , after the description of the evaluation methodology . In this Section we first describe the evaluation methodology and then the experiments on crossdomain evaluation of argument position classification and argument span extraction models . The experimental settings for PDTB are the following : Sections 02 - 22 are used for training and Sections 23 - 24 for testing .__label__Method|Tool|Use
A lexical resource is usually based on semantic judgements about lexical elements ( a human judgement performed by a lexicographer , or a machine - based judgement in the case of automatically built resources ). Often , two independently built resources that describe the same linguistic reality only show a weak agreement even when based on human judgements under the same protocol ( Murray and Green , 2004 ). Many of such resources , such as WordNet ( Fellbaum , 1998 ) or Wiktionary _CITE_ ( Zesch et al ., 2008 ; Sajous et al ., 2010 ) can be modelled as graphs . A graph encodes a binary relation on a set V of vertices . A graph G = ( V , E ) is therefore defined by a finite , non empty set of n = JV J vertices and by a set E C_ V x V of m = JEJ couples of vertices ( edges ).__label__Material|Data|Introduce
Comme ce travail est tres connu , nous ne le d6crirons pas plus en d6tail ici ( Miller , 1990 ). Si WN est une ressource lexicale , il peut 6galement etre vu comme un corpus . Ceci peut s ' av6rer tres utile , si l ' on veut le comparer avec d ' autres corpus — comme , par exemple , Wikipedia _CITE_ qui est une encyclop6die multilingue , collaborative et libre — ou si l ' on veut faire usage d ' une partie sp6cifique de la base , par exemple , les gloses . Puisque les gloses correspondent sch6matiquement ˆ la signification d ' un mot ( d6finition ), leurs 6l6ments ( sac de mots ) peuvent etre utilis6s pour acc6der au mot dont ils d6finissent le sens ( entr6e lexicale , lemme ). WN a eu un grand impact dans la communaut6 TAL od il est fortement utilis6 .__label__Supplement|Website|Introduce
Essentially , our approach will extract linguistic patterns ( hopefully “ objective ” for newspaper news items and “ subjective ” for parliamentary speeches and blog posts ) by comparing frequencies against a reference corpus . Our method is relevant for hybrid approaches as it combines linguistic and statistic information . Our reference corpus , the Reference Corpus of Contemporary Portuguese ( CRPC ) _CITE_ , is an electronically based linguistic corpus of around 310 million tokens , taken by sampling from several types of written texts ( literature , newspapers , science , economics , law , parliamentary debates , technical and didactic documents ), pertaining to national and regional varieties of Portuguese . A random selection of 10 , 000 texts from the entire CRPC will be used for our experiment . The experiment flow - chart is shown in Figure 1 .__label__Material|Data|Use
When a numerical expression is accompanied by a modifier such as over , about , or more than , we updated the value and modifier fields appropriately . Internally , all values are represented by ranges ( e . g ., 75 is represented by the range [ 75 , 75 ]). We developed an extractor and a normalizer for Japanese numerical expressions _CITE_ . We will outline the algorithm used in the normalizer with an example sentence : “ Roughly three thousand kilograms of meats have been provided every day .” We used a dictionary to perform procedures 2 and 3 ( Table 3 ). If the words that precede or follow an extracted number match an entry in the dictionary , we change the semantic representation as described in the operation .__label__Method|Tool|Produce
For example , in the British National Corpus ( 2007 ), “ however ” seems more negative than “ but ”. Also , compared with “ but ”, “ however ” appears more frequently at the beginning of a sentence . With this in mind , we proposed GLANCE _CITE_ , a text visualization tool , which presents corpus data using charts and graphs to help language learners understand the lexical phenomena of a word quickly and intuitively . In this paper , we focused on five types of lexical phenomena : polarity , position , POS , form and discipline , which will be detailed in the Section 3 . Given a single query word , the GLANCE system shows graphical representations of its lexical phenomena sequentially within a single web page .__label__Method|Tool|Produce
This is especially important in situations where multiple pairwise comparisons are conducted , and small result differences are expected . The experimental setup we employed to compare evaluation measures and significance tests is a discriminative reranking experiment on 1000 - best lists of a phrase - based SMT system . Our system is a re - implementation of the phrase - based system described in Koehn ( 2003 ), and uses publicly available components for word alignment ( Och and Ney , 2003 ) _CITE_ , decoding ( Koehn , 2004a ) , language modeling ( Stolcke , 2002 ) and finite - state processing ( Knight and Al - Onaizan , 1999 ) . Training and test data are taken from the Europarl parallel corpus ( Koehn , 2002 ) . Phrase - extraction follows Och et al .__label__Method|Tool|Use
More than “ topics ”, these topics represent symbolic meanings . She concludes that , in order to understand them , a closed reading of the poems is necessary . We have run LDA Topic Modeling over our corpus of sonnets _CITE_ . Using different configurations ( 10 , 20 , 50 , 100 and 1000 topics ), we are developing several analysis . In the next sections I will present these analysis together with some preliminary results and comments .__label__Material|Data|Use
We perform query expansion , which consists in creating additional queries using question word synonyms in the purpose of increasing the recall of the search engine . Synonyms are obtained via the WordNet 2 . 0 lexical database . Document retrieval We retrieve the top 20 documents returned by Google _CITE_ for each query produced via query expansion . These are processed in the following steps , which progressively narrow the part of the text containing relevant information . Keyphrase extraction Once the documents are retrieved , we perform keyphrase extraction to determine their three most relevant topics using Kea ( Witten et al ., 1999 ), an extractor based on Naïve Bayes classification .__label__Material|Data|Use
Tools . The subjects performed their annotations on Viglen Genie workstations with LG Flatron monitors running Windows XP , using the MMAX 2 annotation tool ( M ¨ uller and Strube , 2003 ). _CITE_ Subjects . Eighteen paid subjects participated in the experiment , all students at the University of Essex , mostly undergraduates from the Departments of Psychology and Language and Linguistics . Procedure .__label__Method|Tool|Use
2011 ], based on multilayer perceptron neural networks and vector space models , and that achieves state - of - the - art performance in English . Our resulting tagger is also available online . Besides our model , we performed experiments with the OpenNLP POS tagger _CITE_ for comparison . We chose to use the Mac - Morpho corpus because it is the biggest one available with POS tags in Portuguese . Mac - Morpho is composed of 109 files with texts from the Brazilian newspaper Folha de S ˜ ao Paulo , and is divided in 10 sections , each having a given topic ( such as agriculture , politics , sports , etc .).__label__Method|Tool|Use
Discussion Forum Treebank The treebank is an extension of that described in Foster ( 2010 ). It contains 481 sentences taken from two threads on the BBC Sport 606 discussion forum in November 2009 . _CITE_ The discussion forum posts were split into sentences by hand . The sentences were first parsed automatically using an implementation of the Collins Model 2 generative statistical parser ( Bikel , 2004 ). They were then corrected by hand using as a reference the Penn Treebank ( PTB ) bracketing guidelines ( Bies et al ., 1995 ) and the PTB trees themselves ( Marcus et al ., 1994 ).__label__Material|Data|Introduce
The second one is the Chinese thesaurus Tongyicicilin ( Cilin ) ( Che et al ., 2010 ), which groups 74 , 000 Chinese words into five - layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models ( Qiu et al ., 2011 ; Jin et al ., 2012 ). The third level of Cilin , which contains 1428 classes , is used to evaluate whether two words are semantically similar . For comparison between Chinese and English , we also use an English analogy question dataset , the Google dataset _CITE_ ( Mikolov et al ., 2013a ), to evaluate the English word embeddings of Levy and Goldberg ( 2014a ) on analogy detection . On both the CAQS and the Google datasets , the 3COSMUL method ( Levy and Goldberg , 2014b ) is used to to answer analogy questions based on given embeddings . The results on the CWS dataset are evaluated using the two standard metrics for the task , namely Spearman ’ s ρ and Kendall ’ s τ rank correlation coefficients .__label__Material|Data|Use
Table 1 gives some details about this training data . We use ROUGE as a metric to maximize because it is also used in DUC and TAC . _CITE_ However , it should be noted that any automatic metric could be used instead of ROUGE . In particular we use ROUGE 1 ( R - 1 ), ROUGE 2 ( R - 2 ) and ROUGE SU4 ( R - SU4 ). R - 1 and R - 2 compute the number give detail about the number of documents ( descriptions ) for each place , number of sentences for each place and document ( description ) and the lengths of the sentences .__label__Method|Algorithm|Use
for and against , policies ultimately adopted by the government , and the impact of those policies . The set of datasets made available is listed in Table 1 . Several additional datasets were suggested on the website , _CITE_ but were not part of the official data . Forty teams initially registered to participate in the unshared task ; ten submitted papers . The teams came from a variety of institutions spread across six countries .__label__Material|Data|Produce
for and against , policies ultimately adopted by the government , and the impact of those policies . The set of datasets made available is listed in Table 1 . Several additional datasets were suggested on the website , _CITE_ but were not part of the official data . Forty teams initially registered to participate in the unshared task ; ten submitted papers . The teams came from a variety of institutions spread across six countries .__label__Supplement|Website|Produce
Terminological Processing The aim of the 3stage terminological processing phase is to identify and correctly classify instances of the term classes described above in section 3 . 1 . 1 . During the morphological analysis stage individual tokens are analysed to see if they contain interesting biochemical affixes such as - ase or - in that indicate candidate protein names respectively . During the lexical lookup stage the previously tokenised terms are matched against terminological lexicons which have been compiled from biological databases such as CATHS and SCOP _CITE_ and have been augmented with terms produced by corpus processing techniques ( Demetriou and Gaizauskas , 2000 ). Additional subcategorisation information is provided for multi - token terms by splitting the terms into their constituents and placing the constituents into subclasses whose combination is determined by grammar rules . Finally , in a terminology parsing stage , a rulebased parser is used to analyse the tokenisation information and the morphological and lexical properties of component terms and to combine them into a single multi - token unit .__label__Material|Data|Use
The system obtains accuracies of 0 . 70 and 0 . 73 for the restaurant and laptop domain respectively , and performs second best in the out - of - domain hotel , achieving an accuracy of 0 . 80 . Nowadays Sentiment Analysis is proving very useful for tasks such as decision making and market analysis . The ever increasing interest is also shown in the number of related shared tasks organized : TASS ( Villena - Rom ´ an et al ., 2012 ; Villena - Rom ´ an et al ., 2014 ), SemEval ( Nakov et al ., 2013 ; Pontiki et al ., 2014 ; Rosenthal et al ., 2014 ), or the SemSA Challenge at ESWC2014 _CITE_ . Research has also been evolving towards specific opinion elements such as entities or properties of a certain opinion target , which is also known as ABSA . The Semeval 2015 ABSA shared task aims at covering the most common problems in an ABSA task : detecting the specific topics an opinion refers to ( slot1 ); extracting the opinion targets ( slot2 ), combining the topic and target identification ( slot1 & 2 ) and , finally , computing the polarity of the identified word / targets ( slot3 ).__label__Supplement|Website|Introduce
We also allowed for “ Not Applicable ” option to capture ratings where the Turkers did not have sufficient knowledge about the statement or if the statement was not really a claim . Figure 6 shows the set of instructions provided to the Turkers , and Figure 5 illustrates the annotation interface . _CITE_ We excluded tweets for which three or more Turkers gave a rating of “ Not Applicable ,” leaving us with a dataset of 1170 tweets . Within this set , the average variance per tweet ( excluding “ Not Applicable ” ratings ) was 0 . 585 . Having obtained a corpus of factuality ratings , we now model the factors that drive these ratings .__label__Supplement|Website|Introduce
RDQAs present some characteristics that prevent us from a direct use of ODQA systems . The most important differences are : More information about RDQA systems can be found in the ACL 2004 Workshop on QA in Restricted Domains and the AAAI 2005 Worshop on Question Answering in Restricted Domains ( Molla and Vicedo , 2005 ) . GeoTALP - QA has been developed within the framework of ALIADO _CITE_ project . The system architecture uses a common schema with three phases that are performed sequentially without feedback : Question Processing ( QP ), Passage Retrieval ( PR ) and Answer Extraction ( AE ). More details about this architecture can be found in ( Ferr ´ es et al ., 2005 ) and ( Ferr ´ es et al ., 2004 ).__label__Method|Tool|Use
The number of documents in the collection is n . Similarly to the first criterion , we consider only patterns positively correlated with the corresponding category : The x statistic was previously reported to be the best feature selection strategy for text categorization ( Yang and Pedersen , 1997 ). Criterion 4 : Mutual Information ( MI ) Mutual information is a well known information theory criterion that measures the independence of two variables , in our case a pattern p and a category y ( Yang and Pedersen , 1997 ). Using the same contingency table introduced above , the MI criterion is estimated as : For all experiments reported in this paper we used the following three document collections : ( a ) the AP collection is the Associated Press ( year 1999 ) subset of the AQUAINT collection ( LDC catalog number LDC2002T31 ); ( b ) the LATIMES collection is the Los Angeles Times subset of the TREC5 collection _CITE_ ; and ( c ) the REUTERS collection is the by now classic Reuters - 21578 text categorization collection . Similarly to previous work , for the REUTERS collection we used the ModApte split and selected the ten most frequent categories ( Nigam et al ., 2000 ). Due to memory limitations on our test machines , we reduced the size of the AP and LATIMES collections to their first 5 , 000 documents ( the complete collections contain over 100 , 000 documents ).__label__Material|Data|Introduce
In this illustration , for computational reasons , the data is sub - sampled to 100 data - points . Figure 1 shows visualisations of the data using both PCA and our GPLVM algorithm which required 766 iterations of SCG . The positions for the GPLVM model were initialised using PCA ( see _CITE_ for the MATLAB code used ). The gradient based optimisation of the RBF based GPLVM ’ s latent space shows results which are clearly superior ( in terms of greater separation between the different flow domains ) to those achieved by the linear PCA model . Additionally the use of a Gaussian process to perform our ‘ mapping ’ means that there is uncertainty in the positions of the points in the data space .__label__Method|Code|Use
Second , we compared the performance of GP - Vol against standard econometric models GARCH , EGARCH and GJRGARCH on fifty real financial time series . Finally , we compared RAPCF with the batch MCMC method PGAS in terms of accuracy and execution time . The code for RAPCF in GP - Vol is publicly available at _CITE___label__Method|Code|Produce
Thus we will focus on making comparison to the PGBN with a single layer , with its layer width set to be large to approximate the performance of the gamma - negative binomial process PFA . We evaluate the PGBNs ’ performance by examining both how well they unsupervisedly extract low - dimensional features for document classification , and how well they predict heldout word tokens . Matlab code will be available in _CITE_ We use Algorithm 1 to learn , in a layer - wise manner , from the training data the weight matrices ( D ( 1 ),...,( D ( Tmax ) and the top - layer hidden units ’ gamma shape parameters r : to add layer T to a previously trained network with T − 1 layers , we use BT iterations to jointly train ( D ( T ) and r together with {( D ( t )} 1 , T − 1 , prune the inactive factors of layer T , and continue the joint training with another CT iterations . We set the hyper - parameters as a0 = b0 = 0 . 01 and e0 = f0 = 1 .__label__Method|Code|Produce
School Data . This data set comes from the Inner London Education Authority ( ILEA ) and has been used to study the effectiveness of schools . It is publicly available under the name of “ school effectiveness ” at _CITE_ It consists of examination records from 139 secondary schools in years 1985 , 1986 and 1987 . It is a random 50 % sample with 15362 students .__label__Material|Data|Use
This quantity is an estimate of type - I error under H0 , and corresponds to test power when H1 is true . We set α = 0 . 01 in all the experiments . All the code and preprocessed data are available at _CITE_ Optimization The parameter tuning objective ˆλtrn / 2 ( θ ) is a function of θ consisting of one real - valued σ and J test locations each of d dimensions . The parameters θ can thus be regarded as a Jd + 1 Euclidean vector .__label__Method|Code|Produce
This quantity is an estimate of type - I error under H0 , and corresponds to test power when H1 is true . We set α = 0 . 01 in all the experiments . All the code and preprocessed data are available at _CITE_ Optimization The parameter tuning objective ˆλtrn / 2 ( θ ) is a function of θ consisting of one real - valued σ and J test locations each of d dimensions . The parameters θ can thus be regarded as a Jd + 1 Euclidean vector .__label__Material|Data|Produce
This demonstration shows that the system is capable of using prior knowledge to recognize new objects in the scene and learn about new object categories in an open - ended fashion . A video of this demonstration is available at : _CITE_ Another demonstration has been performed using Washington RGB - D Scenes Dataset v2 . This dataset consists of 14 scenes containing a subset of the objects in the RGB - D Object Dataset , including bowls , caps , mugs , and soda cans and cereal boxes .__label__Supplement|Media|Produce
The probability of each cluster being in each state depends on the sum of the biases involved . Figure 1 shows that the mixing rate of the sampling process is improved by using Swendsen - Wang allowing us to find accurate marginals for a single position in a couple of seconds . 2URL : _CITE_ Loopy Belief Propagation In order to perform very rapid ( approximate ) inference we used the loopy belief propagation ( BP ) algorithm [ 9 ] and the results are examined in Section 4 . This algorithm is similar to an influence function [ 10 ], as often used by Go programmers to segment the board into Black and White territory and for this reason is laid out below . For each board vertex j E N , create a data structure called a node containing :__label__Method|Algorithm|Use
For each pair , corresponding sequences were chosen from the respective pools at random . The volunteers were only told that the sequences were either real or artificial , and were asked to either select the real video or to indicate that they could not decide . The test is kept available on - line for validation at _CITE_ The results are shown in Table 1 . The first row , e . g ., shows that when comparing Brand ’ s model with the DPDS , people thought that the sequence generated with the former model was real in 5 cases , could not make up their mind in 7 cases , and thought the sequence generated with DPDS was real in 54 instances .__label__Method|Algorithm|Produce
We have performed some preliminary evaluations of semantic precision using unsupervised AQBC , and we have found it to work very well for retrieving semantic neighbors for extremely high - dimensional sparse data ( like the 20 Newsgroups dataset ), while ITQ currently works better for lower - dimensional , denser data . In the future , we plan to investigate how to improve the semantic precision of AQBC using either unsupervised or supervised learning . Additional resources and code are available at _CITE_ Acknowledgments . We thank Henry A . Rowley and Ruiqi Guo for helpful discussions , and the reviewers for helpful suggestions . Gong and Lazebnik were supported in part by NSF grants IIS 0916829 and IIS 1228082 , and the DARPA Computer Science Study Group ( D12AP00305 ).__label__Method|Code|Produce
We have performed some preliminary evaluations of semantic precision using unsupervised AQBC , and we have found it to work very well for retrieving semantic neighbors for extremely high - dimensional sparse data ( like the 20 Newsgroups dataset ), while ITQ currently works better for lower - dimensional , denser data . In the future , we plan to investigate how to improve the semantic precision of AQBC using either unsupervised or supervised learning . Additional resources and code are available at _CITE_ Acknowledgments . We thank Henry A . Rowley and Ruiqi Guo for helpful discussions , and the reviewers for helpful suggestions . Gong and Lazebnik were supported in part by NSF grants IIS 0916829 and IIS 1228082 , and the DARPA Computer Science Study Group ( D12AP00305 ).__label__Supplement|Document|Produce
The results obtained for MDS , Isomap , spectral clustering and LLE are shown in figure 1 for different values of m . Experiments are done over a database of 698 synthetic face images described by 4096 components that is available at _CITE_ Qualitatively similar results have been obtained over other databases such as Ionosphere ( http :// www . ics . uci . edu /- mlearn / MLSummary . html ) and swissroll ( http :// www . cs . toronto . edu /- roweis / lle /). Each algorithm generates a twodimensional embedding of the images , following the experiments reported for Isomap .__label__Material|Data|Use
We tested the algorithm using fMRI data collected from 10 subjects viewing a movie split into 2 sessions separated by a short break . The data was preprocessed following [ 5 ]. For each subject , a structural scan was acquired before each session , from which the cortical surface model was derived (§ 2 ) and then anatomically aligned to a template using FreeSurfer ( Fischl , _CITE_ ). Similar to [ 5 ], we find that anatomical alignment based on cortical curvature serves as a superior starting point for functional alignment over Talairach alignment . First , functional connectivity was found for each subject and session : Ck , i , k = 1 , ... , Ns , i = 1 , 2 .__label__Method|Tool|Use
Experimental evaluation of the new algorithm was performed on the modified KDD Cup 1998 data set . The original data set is available under _CITE_ The following modifications were made to obtain a pure regression problem :__label__Material|Data|Produce
Each active filter has response 1 . We give an example of learning S in a convolutional setting . We use the centered faces from the faces in the wild dataset , available at _CITE_ From each of the 13233 images we subsample by a factor of two and pick a random 48 x 48 patch . The 48 x 48 image x is then contrast normalized to x − b * x , where b is a 5 x 5 averaging box filter ; the images are collected into the 48 x 48 x 13233 data set X .__label__Material|Data|Use
rithms across the four image sets . The four sets of images are available at _CITE___label__Material|Data|Use
In the tested cases the appearance of the found motifs did not change drastically while varying the Bl penalty within one order of magnitude , so fine - tuning it is not necessary . Instead of specifying the penalty a on the B0 norm of the activations directly , we chose to stop the matching pursuit algorithm when adding an additional assembly appearance increases the reconstruction error or when the difference of reconstruction errors from two consecutive steps falls below a small threshold . All code for the proposed method is available at : _CITE_ Sparse - convolutional - coding - for - neuronal - assembly - detection__label__Method|Code|Produce
One intuitive metric of performance can be obtained by having human annotators judge the visual quality of samples [ 2 ]. We automate this process using Amazon Mechanical Turk ( MTurk ), using the web interface in figure Fig . 2 ( live at _CITE_ ), which we use to ask annotators to distinguish between generated data and real data . The resulting quality assessments of our models are described in Section 6 .__label__Supplement|Website|Use
Consider , for instance , the problem of generating recommendations within ResearchIndex ( a . k . a ., CiteSeer ), 1 an online digital library of computer science papers , receiving thousands of user accesses per hour . The site automatically locates computer science papers found on the Web , indexes their full text , allows browsing via the literature citation graph , and isolates the text around citations , among other services [ 8 ]. The archive contains over 470 , 000 l_CITE_ documents including the full text of each document , citation links between documents , and a wealth of user access data . With so many documents , and only seven accesses per user on average , the user - document data matrix is exceedingly sparse and thus challenging to model . In this paper , we work with the ResearchIndex data , since it is an interesting application domain , and is typical of many recommendation application areas [ 14 ].__label__Material|Data|Use
( 2016 ), and the aligned and cropped version of CelebA was scaled from 218 x 178 pixels to 78 x 64 pixels and center - cropped at 64 x 64 pixels ( Liu et al ., 2015 ). We used the Adam optimizer ( Kingma and Ba , 2014 ) and the Theano framework ( Al - Rfou et al ., 2016 ). More details are in Appendix and code for training and generation is at _CITE___label__Method|Code|Produce
We refer to this problem as selective labeling , in contrast to conventional random labeling . To achieve the goal of selective labeling , it is crucial to consider the out - of - sample error of a specific learner . We choose Laplacian Regularized Least Squares ( LapRLS ) as the learner [ 4 ] because it is a l_CITE_ state - the - art semi - supervised learning method , and takes many linear regression methods as special cases ( e . g ., ridge regression [ 15 ]). We derive a deterministic out - of - sample error bound for LapRLS trained on subsampled data , which suggests to select the data points to label by minimizing this upper bound . The resulting selective labeling method is a combinatorial optimization problem .__label__Method|Algorithm|Use
The results obtained for MDS , Isomap , spectral clustering and LLE are shown in figure 1 for different values of m . Experiments are done over a database of 698 synthetic face images described by 4096 components that is available at http :// isomap . stanford . edu . Qualitatively similar results have been obtained over other databases such as Ionosphere ( _CITE_ ) and swissroll ( http :// www . cs . toronto . edu /- roweis / lle /). Each algorithm generates a twodimensional embedding of the images , following the experiments reported for Isomap . The number of neighbors is 10 for Isomap and LLE , and a Gaussian kernel with a standard deviation of 0 . 01 is used for spectral clustering / Laplacian eigenmaps .__label__Material|Data|Use
This section shows empirical examples to demonstrate some consequences of our theoretical analysis . We use the MNIST data set ( _CITE_ ), consisting of hand - written digit images ( representing 10 classes , from digit “ 0 ” to digit “ 9 ”). In the following experiments , we randomly draw m = 2000 samples . We regard n = 100 of them as labeled data , and the remaining m − n = 1900 as unlabeled test data .__label__Material|Data|Use
In this section , we compare our proposed Bayesian DA algorithm with the commonly used DA technique [ 19 ] ( denoted as PMDA ) on several image classification tasks ( code available at : _CITE_ ). This comparison is based on experiments using the following three datasets : MNIST [ 20 ] ( containing 60 , 000 training and 10 , 000 testing images of 10 handwritten digits ), CIFAR - 10 [ 18 ] ( consisting of 50 , 000 training and 10 , 000 testing images of 10 visual classes like car , dog , cat , etc . ), and CIFAR - 100 [ 18 ] ( containing the same amount of training and testing samples as CIFAR - 10 , but with 100 visual classes ).__label__Method|Code|Compare
For the NOC - SVM method , we used the implementation provided by the authors 6 . The LibSVM package [ 1 ] was used to implement the HMVE and I - OCSVM methods . An implementation of our q - OCSVM estimator is available from : _CITE_ All experiments were carried out with a Gaussian kernel ( γ = 1 2 . 5__label__Method|Code|Produce
Many biological networks are conjectured to be scale - free , and additionally ERG modelling techniques are known to produce good results on biological networks [ 16 ]. So we consider micro - array datasets a natural test - bed for our method . We ran our method and the L1 reconstruction method on the first 500 genes from the GDS1429 dataset ( _CITE_ ), which contains 69 samples for 8565 genes . The parameters for both methods were tuned to produce a network with near to 50 edges for visualization purposes . The major connected component for each is shown in Figure 2 .__label__Material|Data|Use
prediction appearing in computational biology , where our method obtains less than a half of the error rate of the best competing HMM - based method . Our predictions are available at Wormbase : _CITE_ Additional data and results are available at the project ’ s website http :// www . fml . mpg . de / raetsch / projects / msplicer . Acknowledgments We thank K .- R . M ¨ uller , B . Sch ¨ olkopf , E . Georgii , A . Zien , G . Schweikert and G . Zeller for inspiring discussions .__label__Supplement|Website|Produce
prediction appearing in computational biology , where our method obtains less than a half of the error rate of the best competing HMM - based method . Our predictions are available at Wormbase : _CITE_ Additional data and results are available at the project ’ s website http :// www . fml . mpg . de / raetsch / projects / msplicer . Acknowledgments We thank K .- R . M ¨ uller , B . Sch ¨ olkopf , E . Georgii , A . Zien , G . Schweikert and G . Zeller for inspiring discussions .__label__Supplement|Document|Produce
We use a Riemannian trust - region ( RTR ) method [ ABG07 ] to minimize ( 10 ), via the freely available Matlab package GenRTR ( version 0 . 3 . 0 ) with its default parameter values . The package is available at this address : _CITE_ At the current iterate OZI = col ( U ), the RTR method uses the retraction RU ( 8 ) to build a quadratic model mU : TUg ( m , r ) → ] IR of the lifted objective function f o RU ( lift ). It then classically minimizes the model inside a trust region on this vector space ( solve ), and retracts the resulting tangent vector H to a candidate U + = RU ( H ) on the Grassmannian ( retract ).__label__Method|Code|Use
Legislation documents and metadata ( bill sponsorship , party affiliation of voters , etc .) are available for sessions 101 – 110 ( 19892008 ). For the legislation , stop words were removed using a common stopword list ( the 514 stop words are posted at _CITE_ , and the corpus was stemmed using a Porter stemmer ). These data are available from www . govtrack . us and from the Library of Congress thomas . loc . gov ( votes , text and metadata ), while the votes dating from 1789 are at voteview . com . A binary matrix is manifested by mapping all “ affirmative ” vote codes ( e . g ., “ Yea ”, “ Yes ”, “ Present ”) to one , and “ negative ” codes ( e . g ., “ Nay ”,“ No ”,“ Not Present ”) to zero .__label__Material|Data|Use
In our experiments on both synthetic functions and tuning practical machine learning algorithms , q - KG consistently finds better function values than other parallel BO algorithms , such as parallel EI [ 2 , 19 , 25 ], batch UCB [ 5 ] and parallel UCB with exploration [ 3 ]. q - KG provides especially large value when function evaluations are noisy . The code in this paper is available at _CITE_ The rest of the paper is organized as follows . Section 2 reviews related work .__label__Method|Code|Produce
Afterwards , all objects are classified correctly in all 12 remaining scenes ( Fig . 4 ( c )). This evaluation illustrates the process of acquiring categories in an open - ended fashion . A video of this demonstration is online at : _CITE___label__Supplement|Media|Produce
We now present empirical results on byte - prediction tasks and partially - observable RL . Our code and instructions for its use is publicly available at : _CITE_ Byte Prediction We compare the performance of D2 - CTW against CTW on the 18 - file variant of the Calgary Corpus [ 3 ], a benchmark of text and binary data files . For each file , we ask the algorithms to predict the next byte given the preceding data , such that | E |= 256 across all files .__label__Method|Code|Produce
We now present empirical results on byte - prediction tasks and partially - observable RL . Our code and instructions for its use is publicly available at : _CITE_ Byte Prediction We compare the performance of D2 - CTW against CTW on the 18 - file variant of the Calgary Corpus [ 3 ], a benchmark of text and binary data files . For each file , we ask the algorithms to predict the next byte given the preceding data , such that | E |= 256 across all files .__label__Supplement|Document|Produce
Principal component analysis ( PCA ) [ 1 , 5 ] seeks the best ( in an ` 2 - sense ) such low - rank representation of the given data matrix . It enjoys a number of optimality properties when the data are only mildly corrupted by small noise , and can be stably and efficiently computed via the singular value decomposition . ∗ For more information , see _CITE_ This work was partially supported by NSF IIS 08 - 49292 , NSF ECCS 07 - 01676 , and ONR N00014 - 09 - 1 - 0230 . One major shortcoming of classical PCA is its brittleness with respect to grossly corrupted or outlying observations [ 5 ].__label__Supplement|Document|Produce
This material is based upon work supported by the National Science Foundation under Grant No . 1125228 . The code as an R package is available at : _CITE___label__Method|Code|Produce
In our case we showed this through the influence of spatial frequency on speed estimation . We have thus provided just one example of how the optimized motion stimulus and accompanying theoretical work might serve to improve our understanding of inference behind perception . The code associated to this work is available at _CITE___label__Method|Code|Produce
Like previous studies [ 1 – 7 , 9 ], this paper employs the v - SVM algorithms [ 16 ] for generating the classification model . Indeed , we use the binary v - SVM for datasets with just two categories of stimuli and multi - label v - SVM [ 3 , 16 ] as the multi - class approach . All datasets are separately preprocessed by FSL 5 . 0 . 9 ( _CITE_ ), i . e . slice timing , anatomical alignment , normalization , smoothing . Regions of Interests ( ROI ) are also denoted by employing the main reference of each dataset .__label__Method|Tool|Use
Then , we provide empirical evidence that our algorithm is more robust to artifacts and outliers than three competing CSC methods [ 6 , 7 , 12 ]. Finally , we consider LFP data , where we illustrate that our algorithm can reveal interesting properties in electrophysiological signals without supervision , even in the presence of severe artifacts . The source code is publicly available at _CITE_ Synthetic simulation setup : In our synthetic data experiments , we simulate N trials of length T by first generating K zero mean and unit norm atoms of length L . The activation instants are integers drawn from a uniform distribution in Q0 , T − L �. The amplitude of the activations are drawn from a uniform distribution in [ 0 , 1 ].__label__Method|Code|Produce
We want to design our experiment — choose p ( x ) — optimally in some sense . One natural idea would be to choose p ( x ) in such a way that we learn as much as possible about the underlying model , on average . Information theory thus suggests we choose p ( x ) to optimize the * A longer version of this paper , including proofs , has been submitted and is available at _CITE_ following objective function :__label__Supplement|Document|Produce
In the experiments , we compared our ODC with four different clustering algorithms , i . e ., the conventional K - means [ 1 ], normalized cut ( NC ) [ 9 ], DisCluster [ 3 ] and DisKmeans [ 13 ]. It is worth noting that two discriminative clustering algorithms : DisCluster [ 3 ] and DisKmeans [ 13 ], are very closely related to our ODC , because they are derived from the discriminant analysis criteria in essence ( also see the analysis in Section 3 . 3 ). In addition , the implementation code for NC is available at _CITE_ For the sake of simplicity , the parameter Q2 in ODC is sought from the range Q2 E { 10 − 3 , 10 − 2 . 5 , 10 − 2 , 10 − 1 . 5 , 10 − 1 , 10 − 0 . 5 , 100 , 100 . 5 , 101 , 101 . 5 , 102 , 102 . 5 , 103 }. Similarly , the parameters in other clustering algorithms compared here are also searched in a wide range .__label__Method|Code|Use
We implemented misoKG ’ s statistical model and acquisition function in Python 2 . 7 and C ++ leveraging functionality from the Metrics Optimization Engine [ 23 ]. We used a gradient - based optimizer [ 28 ] that first finds an optimizer via multiple restarts for each IS B separately and then picks ( B ( n + 1 ), x ( n + 1 )) with maximum misoKG factor among these . An implementation of our method is available at _CITE_ We compare to misoEI of Lam et al . [ 18 ] and to MTBO +, an improved version of Multi - Task Bayesian Optimization proposed by Swersky et al .__label__Method|Code|Produce
However , the proposed methods can be used for general point processes , and can be applied to other areas . Although we have proved consistency of the proposed measures , further statistical analysis such as small sample power analysis , rate of convergence , and asymptotic properties would be interesting to address . A MATLAB implementation is freely available on the web ( _CITE_ ) with BSD - license .__label__Method|Code|Produce
Reliable inference from the observed number of spikes about the underlying firing rate of a neuronal response , however , requires a sufficiently long time interval , while integration times of neurons in vivo [ 3 ] as well as reaction times of humans or animals when performing classification tasks [ 4 , 5 ] are known to be rather short . Therefore , it is important to understand , how neural rate coding is affected by a limited time window available for decoding . While rate codes are usually characterized by tuning functions relating the intensity of the * _CITE_ neuronal response to a particular stimulus parameter , the question , how relevant the idea of analog coding actually is does not depend on the particular entity represented by a neuron . Instead it suffices to determine the shape of the gain function , which displays the mean firing rate as a function of the actual analog signal to be sent to subsequent neurons . Here we seek for optimal gain functions that minimize the minimum average squared reconstruction error for a uniform source signal transmitted through a Poisson channel as a function of the maximum mean number of spikes .__label__Supplement|Document|Introduce
SVMs have since been successfully applied on many tasks but primarily in the areas of data mining and pattern classification . With the present study we explore the feasibility and usefulness of one - class SVM classification [ 5 ] for tasks faced by AIBO robots within the legged league environment of RoboCup [ 6 ]. We focus on two particularly critical issues : detection of objects based on ∗ _CITE_ correct colour classification and detection of robot - to - robot collisions . Both issues seemed not to be sufficiently solved and implemented by the teams of RoboCup2002 and caused significant deterioration in the quality of play even in the world - best teams of that league . The article has five more sections addressing the environment and tasks , the methods , followed by the experiments and applications for colour classification and collision detection , respectively .__label__Method|Algorithm|Use
In January 2000 the Nomad robot was deployed to the Elephant moraine in Antarctica for robotic meteorite searching trials . Nomad searched areas known to contain meteorites , autonomously acquiring color images and reflection spectra of both native terrestrial rocks and meteorites , and classifying them . On January 22 , 2000 Nomad successfully identified a meteorite amongst terrestrial rocks on the ice sheet ( _CITE_ ), Overall performance ( using spectra only , due to a problem that developed with camera zoom control ) is indicated by the ROC performance curves in Figure 6 . These were generated from a test set of rocks and meteorites ( 40 and 4 samples respectively , with multiple readings of each ) in a particular area of the moraine . Figure 6 ( i ) is using the a priori classifier built from the lab data ( used to generate Figure 5 ), acquired prior to arrival in Antarctica .__label__Method|Tool|Introduce
In this section , we discuss our experiments on a synthetic dataset and three real text corpora . The TDM and DDM implementations are available at _CITE_ For both models we initialized the hyperparameters to be αd , t = 1 and βt , w = √ 1V for all d , t , and w . The reason that βt , w was not initialized to 1 was to encourage the algorithm to find topics with more concentrated word distributions .__label__Method|Code|Produce
We define loss as the cross entropy between predicted and demonstrated action sequences and use RMSProp [ 35 ] for training . See Appendix C . 7 for details . Our implementation in Tensorflow [ 1 ] is available online at _CITE___label__Method|Code|Produce
The analyses were performed in Python . We used nilearn to handle the large quantities of neuroimaging data [ 1 ] and Theano for automatic , numerically stable differentiation of symbolic computation graphs [ 5 , 7 ]. All Python scripts that generated the results are accessible online for reproducibility and reuse ( _CITE_ ).__label__Method|Code|Produce
We chose the hyperparameter settings that produced the highest quality images . We note that we found no correlation between the activation of a neuron and the recognizability of its visualization . Our code and parameters are available at _CITE___label__Method|Code|Produce
We chose the hyperparameter settings that produced the highest quality images . We note that we found no correlation between the activation of a neuron and the recognizability of its visualization . Our code and parameters are available at _CITE___label__Supplement|Document|Produce
This tool has applications in conditional and discriminative learning for latent variable models . For further results , extensions , etc . see : _CITE_ Hebaraibounds .__label__Supplement|Document|Produce
Training can take days but once trained predictions can be carried on a proteomic or protein engineering scale . Several improvements are currently in progress including ( a ) developing a classifier to discriminate protein chains that do not contain any disulphide bridges , using kernel methods ; ( b ) assessing the effect on prediction of additional input information , such as secondary structure and solvent accessibility ; ( c ) leveraging the predicted cysteine contacts in 3D protein structure prediction ; and ( d ) curating a new larger training set . The current version of our disulphide prediction server DIpro ( which includes step ( a )) is available through : _CITE___label__Supplement|Website|Produce
Our goal was to segment the scenes into two classespuppet and background . We use five of the scenes for our training data , three for validation and three for testing . Sample scans from the training and test set can be seen at _CITE_ We computed spin images of size 10 x 5 bins at two different resolutions , then scaled the values and performed PCA to obtain 45 principal components , which comprised our node features . We used the surface links output by the scanner as edges between points and for each edge only used a__label__Material|Data|Produce
that no new runs start after TL elapsed seconds . The starred datasets are those used in Bachem et al . ( 2016 ), the remainder are available at _CITE_ Rather than run each method a fixed number of times , we therefore run each method as many times as possible in a given time limit , ‘ TL ’. This dataset dependent time limit , given by columns TL in Table 3 , is taken as 80 × the time of a single run of km +++ lloyd .__label__Material|Data|Use
For FSSD tests , we use J = 5 ( see Section A for an investigation of test power as J varies ). All tests with optimization use 20 % of the sample size n for parameter tuning . Code is available at _CITE_ Figure 2 shows the rejection rates of the six tests for the two problems , where each problem is repeated for 200 trials , resampling n points from q every time . In Figure 2a ( Gaussian vs . Laplace ), high performance of FSSD - opt indicates that the test performs well when there are local differences between p and q .__label__Method|Code|Produce
Simplifying assumptions about the joint measurement statistics are often made in order to yield tractable analytic forms . For example Hershey and Movellan have shown that correlations between video data and audio can be used to highlight regions of the image which are the & quot ; cause & quot ; of the audio signal . While such pragmatic choices may lead to * _CITE_ simple statistical measures , they do so at the cost of modeling capacity . Furthermore , these assumptions may not be appropriate for fusing modalities such as video and audio . The joint statistics for these and many other mixed modal signals are not well understood and are not well - modeled by simple densities such as multi - variate exponential distributions .__label__Method|Algorithm|Introduce
For AlexNet , the Caffe reimplementation is used which is slightly different from the original architecture ( pooling and normalization layers are swapped ). We use a fork of MatConvNet framework for all experiments , except for fine - tuning of AlexNet and VGG - 16 , for which we use a fork of Caffe . The source code is available at _CITE_ , https :// github . com / mfigurnov / perforated - cnn - caffe . We begin our experiments by comparing the proposed perforation masks in a common benchmark setting : acceleration of a single AlexNet layer . Then , we compare whole - network acceleration with the best - performing masks to baselines such as decrease of input images size and an increase of strides .__label__Method|Code|Produce
The problem of tagging images in personal albums with names of people present in them , is a problem of high practical relevance [ 19 ]. The spectral kernels were used solve this problem . Images from publicly available sources like _CITE_ 1 were used for experimentation . Five personal albums having 20 - 55 images each were downloaded and many images had upto 6 people . Face detector from openCV library was used to automatically detect faces in images .__label__Material|Data|Use
Additionally , the proposed method for constructing splits based on fitting sub - clusters is , to our knowledge , the first parallelizable split algorithm for mixture models . Results on both synthetic and real data demonstrate that the speed of the sampler is orders of magnitude faster than other exact MCMC methods . Publicly available source code used in this work can be downloaded at _CITE___label__Method|Code|Use
Our approach provides a first step towards surpassing this limitation , by not just anticipating but certifying the reliability of a defender , thus implicitly considering an infinite number of attacks before they occur . Reproducibility . The code and data for replicating our experiments is available on GitHub ( _CITE_ ) and Codalab Worksheets ( http :// bit . ly / cl - datapois ). Acknowledgments . JS was supported by a Fannie & John Hertz Foundation Fellowship and an NSF Graduate Research Fellowship .__label__Method|Code|Produce
Our approach provides a first step towards surpassing this limitation , by not just anticipating but certifying the reliability of a defender , thus implicitly considering an infinite number of attacks before they occur . Reproducibility . The code and data for replicating our experiments is available on GitHub ( _CITE_ ) and Codalab Worksheets ( http :// bit . ly / cl - datapois ). Acknowledgments . JS was supported by a Fannie & John Hertz Foundation Fellowship and an NSF Graduate Research Fellowship .__label__Material|Data|Produce
Note that , while a naive implementation of the arg max in Algorithm 3 requires evaluating the objective for each item in U , here we can exploit the fact that DPPs are closed under conditioning to compute all necessary values with only two matrix inversions [ 5 ]. We report baseline runtimes using this optimized greedy algorithm , which is about 10 times faster than the naive version at N = 200 . The code and data for all experiments can be downloaded from _CITE___label__Method|Code|Produce
Note that , while a naive implementation of the arg max in Algorithm 3 requires evaluating the objective for each item in U , here we can exploit the fact that DPPs are closed under conditioning to compute all necessary values with only two matrix inversions [ 5 ]. We report baseline runtimes using this optimized greedy algorithm , which is about 10 times faster than the naive version at N = 200 . The code and data for all experiments can be downloaded from _CITE___label__Material|Data|Produce
Such a case is shown in Figure 4 ( f ), where n = 10000 data points were generated ( but only a small subset of these was actually plotted ). Here the polygonal line algorithm approximates the generating curve with much better accuracy than the HS algorithm . The Java implementation of the algorithm is available at the WWW site _CITE___label__Method|Code|Produce
We sorted the magnitudes of the signal coefficients , normalized them by their corresponding value of R . We then plotted the results on a log - log scale in Fig . 1 . At _CITE_ , we provide a MATLAB routine ( randcs . m ) so that it is easy to repeat the same experiment for the rest of the distributions in Table 4 .__label__Method|Code|Produce
Similar ideas have seen application in a wide and somewhat scattered literature ; for a partial bibliography , see the longer draft of this paper at _CITE_ Somewhat surprisingly , we have not seen any applications of the information - theoretic objective function ( 1 ) to the design of neurophysiological experiments ( although see the abstract by [ 7 ], who seem to have independently implemented the same idea in a simulation study ). The primary goal of this paper is to elucidate the asymptotic behavior of the a posteriori density pN () when we choose x according to the recipe outlined above ; in particular , we want to compare the adaptive strategy to the more usual case , in which the stimuli are drawn i . i . d .__label__Supplement|Document|Produce
proposed extended DMD [ 35 ], which works on pre - determined basis functions instead of the monomials of observables . Although in extended DMD the Koopman mode is defined as the eigenvector of the corresponding operator of coefficients on basis functions , the resulting procedure is similar to the robust - version of our algorithm . & apos ; The Matlab code is available at _CITE_ In system control , subspace identification [ 23 , 14 ], or called the eigensystem realization method , has been a popular approach to modeling of dynamical systems . This method basically identifies low - dimensional ( hidden ) states as canonical vectors determined by canonical correlation analysis , and estimates parameters in the governing system using the state estimates . This type of method is known as a spectral method for dynamical systems in the machine learning community and has recently been applied to several types of systems such as variants of hidden Markov models [ 31 , 19 ], nonlinear dynamical systems [ 15 ], and predictive state - representation [ 17 ].__label__Method|Code|Produce
nitive science and visual neuroscience topics . The interested reader is invited to visit _CITE_ to interactively explore this model , including the topics , their connections , and the articles that exhibit them . We compared the CTM to LDA by fitting a smaller collection of articles to models of varying numbers of topics . This collection contains the 1 , 452 documents from 1960 ; we used a vocabulary of 5 , 612 words after pruning common function words and terms that occur once in the collection .__label__Method|Algorithm|Introduce
4 ( b ). Refer to supplementary for exhaustive details on experiments and additional results . Open - source code and details can be found here : _CITE___label__Method|Code|Produce
4 ( b ). Refer to supplementary for exhaustive details on experiments and additional results . Open - source code and details can be found here : _CITE___label__Supplement|Document|Produce
The posterior distribution of all model parameters has been computed using Gibbs sampling ; the detailed update equations are provided as supplemental material at _CITE_ The first 1000 Gibbs iterations were discarded as burn - in followed by 500 collection iterations . The truncation levels on the model are T = 20 , M = 10 , K = 30 , and the number of words in the vocabulary is V = 5249 . Hyperparameters were set as a = b = e = f = 10 − 6 , c = d = 1 , g = 103 , and h = 10 − 3 .__label__Supplement|Document|Produce
Table 1 reports the results obtained with PEWA on each individual image for different values of standard deviation of noise . Table 2 compares the average PSNR values on these 25 images obtained by PEWA ( after 1 and 2 iterations ) and two state - of - the - art denoising methods [ 6 , 12 ]. We used the implementations provided by the authors : BM3D ( _CITE_ ) and NL - Bayes ( www . ipol . im ). The best PSNR values are in bold and the results are quantitatively quite comparable except for very high levels of noise . We compared PEWA to the baseline NL - means [ 2 ] and DCT [ 26 ] ( using the implementation of www . ipol . im ) since they form the core of PEWA .__label__Method|Code|Use
It has long been recognized in the image processing community that wavelet transforms form an excellent basis for representation of images . Within the class of linear transforms , it represents a compromise between many conflicting but desirable properties of image representation such as multi - scale and multi - orientation representation , locality both in space and frequency , and orthogonality resulting in decorrelation . A particularly suitable wavelet transform which forms the basis of the best denoising algorithms today is the over - complete steerable wavelet pyramid [ 4 ] freely downloadable from _CITE_ In our experiments we have confirmed that the best results were obtained using this wavelet pyramid . In the following we will describe a model for the statistical dependencies between wavelet coefficients .__label__Method|Algorithm|Introduce
This condition provably holds in a finite number of iterations and still guarantees that || fk + 1 - fk || 2 -+ 0 . The concrete decay estimate provided by SD algorithm therefore allows us to give precise meaning to “ sufficiently lowers the energy .” We investigate these aspects of the algorithm and prove convergence for this practical implementation in future work . Reproducible research : The code is available at _CITE_ Acknowledgements : This work supported by AFOSR MURI grant FA9550 - 10 - 1 - 0569 and Hong Kong GRF grant # 110311 .__label__Method|Code|Produce
In this section , we show the denoising results of Neural DUDE for the synthetic binary data , real binary images , and real Oxford Nanopore MinION DNA sequence data . All of our experiments were done with Python 2 . 7 and Keras package ( _CITE_ ) with Theano [ 17 ] backend .__label__Method|Code|Use
where ✶ P ,,,,,, denotes Kronecker ’ s delta , and where we have used independence of δP , δ ,,,,,, and g . We refer the reader to _CITE_ for an illustration of the model . This generative model draws model discrepancies δ ` independently across IS . This is appropriate when IS are different in kind and share no relationship except that they model a common objective .__label__Supplement|Document|Produce
The clustering procedure described in [ 2 ] is applied to microarray data in order to identify 18 co - occurrences arising from different environmental stresses or growth factors ( path source ) and terminating in the production of SAPK / JNK or NFκB proteins . The reconstructed network ( combined SAPK / JNK and NFκB signal transduction pathways ) is depicted in Figure 2 . This structure agrees with the signalling pathways identified using traditional experimental techniques which test individually for each possible edge ( e . g ., “ MAPK ” and “ NF - κB Signaling ” on _CITE_ ).__label__Method|Tool|Use
The Reuters - 21578 corpus contains 21 , 578 Reuters news articles in 1987 . For this dataset , we divided the data into training and test sets according to the LEWISSPLIT attribute that is available as part of the distribution at _CITE_ The text was passed through a stemmer , and stopwords and words appearing in five or fewer documents were removed . This resulted in a total of 1 , 307 , 468 tokens and a vocabulary of 7 , 720 distinct words .__label__Material|Data|Use
In our experiments , we considered a dataset consisting of GPS data collected from taxicab routes in San Francisco . 8 We acquired public map data from _CITE_ , i . e ., the undirected graph representing the streets ( edges ) and intersections ( nodes ) of San Francisco . We projected the GPS data onto the San Francisco graph using the map - matching API of the graphhopper package . 9 For more on map - matching , see , e . g ., [ Froehlich and Krumm , 2008 ].__label__Material|Data|Use
is run 20 times , and the best solution kept . The reconstruction performance on a ( large ) hold - out set is reported as a function of k . The results for four different training set cardinalities are reported : for small number of points , the reconstruction error decreases sharply for small k and then increases , while it is simply decreasing for larger data sets . A similar experiment , yielding similar results , is performed on subsets of the MNIST ( _CITE_ ) database ( right ). In this case the data might be thought to be concentrated around a low dimensional manifold . For example [ 22 ] report an average intrinsic dimension d for each digit to be between 10 and 13 .__label__Material|Data|Use
4 for the Wikipedia dataset . We set A = 1 in the first two experiments which yields roughly 20 , 000 non - zeros and varied A for the third experiment . Each of the components is trivially parallelized using OpenMP ( _CITE_ ). All timing experiments were conducted on the TACC Maverick system with Intel Xeon E5 - 2680 v2 Ivy Bridge CPUs ( 2 . 80 GHz ), 20 CPUs per node , and 12 . 8 GB memory per CPU ( https :// www . tacc . utexas . edu /). The scaling is generally linear in the parameters except for fitting topic matrices which is O ( k2 ).__label__Method|Tool|Use
Once a suitable 0 is found , the correction step is taken and ( wc , zc ) becomes the new ( w , z ). The method is guaranteed to converge linearly to a solution w *, z * [ 11 , 9 ]. See the longer version of this paper at _CITE_ for details . By comparison , Exponentiated Gradient [ 4 ] has sublinear convergence rate guarantees , while Structured SMO [ 18 ] has none . The key step influencing the efficiency of the algorithm is the Euclidean projection onto the feasible sets W and Zi .__label__Supplement|Document|Produce
The next example illustrates that the HSOM can provide more information about an unknown text than just it ’ s category . For this experiment we have taken movie reviews from the rec . art . movies . reviews newsgroup . Since all the reviews describe a certain movie , we retrieved their associated genres from the Internet Movie Database ( _CITE_ ) to build a set of category labels for each document . The training set contained 8923 ran__label__Material|Data|Use
The improved run time for the linear algorithm also reflects the benefits of some improvements in our culling technique and a cleaner implementation permitted by the linear time algorithm . The quadratic code is simply too slow to run on the Wean Hall data . Log files for these runs are available from the DP - SLAM web page : _CITE_ The results show a significant practical advantage for the linear code , and vast improvement , both in terms of time and number of particles , for the hierarchical implementation .__label__Supplement|Website|Use
The improved run time for the linear algorithm also reflects the benefits of some improvements in our culling technique and a cleaner implementation permitted by the linear time algorithm . The quadratic code is simply too slow to run on the Wean Hall data . Log files for these runs are available from the DP - SLAM web page : _CITE_ The results show a significant practical advantage for the linear code , and vast improvement , both in terms of time and number of particles , for the hierarchical implementation .__label__Supplement|Document|Use
We presume no more objectivity in answering this question than we would have in judging the merits of our other children . However , we believe that the level of musicality attained by our system is truly surprising , while the reliability is sufficient for live demonstration . We hope that the interested reader will form an independent opinion , even if different from ours , and to this end we have made musical examples demonstrating our progress available on the web page : _CITE___label__Material|Data|Produce
We presume no more objectivity in answering this question than we would have in judging the merits of our other children . However , we believe that the level of musicality attained by our system is truly surprising , while the reliability is sufficient for live demonstration . We hope that the interested reader will form an independent opinion , even if different from ours , and to this end we have made musical examples demonstrating our progress available on the web page : _CITE___label__Supplement|Website|Produce
The quality of sound in the estimated signal is an important factor in determining the effectiveness of the algorithm . To demonstrate improvement in the perceptual quality of sound we have placed audio files on the web ; for demonstrations please check , _CITE_ Our algorithm consistently outperformed the algorithm proposed in [ 1 ] both in terms of sound quality and in matching the observed spectrogram . Fig .__label__Supplement|Media|Produce
: 0 nMol at the absorbing wall ( see Figure 1 ); the concentration profile approaches this new steady state with time constant of approximately 1 . 25 msec . Sampling boxes centered along the planes x = f13 . 5µm measured the local concentration , allowing us to validate the expected model behavior . Figure 1 : Gradient sensing simulations performed with MCell ( a Monte Carlo simulator of cellular microphysiology , _CITE_ ) and rendered with DReAMM ( Design , Render , and Animate MCell Models , http :// www . mcell . psc . edu /). The model cell comprised a sphere triangulated with 980 tiles with one cAMP receptor per tile . Cell radius R = 7 . 5µm ; cube side L = 30µm .__label__Method|Tool|Use
In this section , we demonstrate Algorithm 1 on an exploration task . We consider the setting in [ 14 ], the exploration of the surface of Mars with a rover . The code for the experiments is available at _CITE_ For space exploration , communication delays between the rover and the operator on Earth can be prohibitive . Thus , it is important that the robot can act autonomously and explore the environment without risking unsafe behavior .__label__Method|Code|Produce
This observation asks for further investigation . Moreover , we plan to apply the algorithms introduced in this paper to the problem of rollout allocation for classification - based policy iteration in reinforcement learning [ 9 , 6 ], where the goal is to identify the greedy action ( arm ) in each of the states ( bandit ) in a training set . Acknowledgments Experiments presented in this paper were carried out using the Grid ’ 5000 experimental testbed ( _CITE_ ). This work was supported by Ministry of Higher Education and Research , Nord - Pas de Calais Regional Council and FEDER through the “ contrat de projets ´ etat region 2007 – 2013 ”, French National Research Agency ( ANR ) under project LAMPADA n ° ANR - 09 - EMER - 007 , European Community ’ s Seventh Framework Programme ( FP7 / 2007 - 2013 ) under grant agreement n ° 231495 , and PASCAL2 European Network of Excellence .__label__Method|Tool|Use
In this section , we briefly outline the PF algorithm for generating samples from p ( dyi , t101 : t ). ( For details , please refer to our extended technical report at _CITE_ Assume that at time t - 1 we have N particles { y11_1 } 1 ' 11 distributed according to P ( dyi , t_1101 : t — i ) from which one can get the following empirical distribution approximation__label__Supplement|Document|Produce
We take the Theano implementation of DRAW provided at _CITE_ draw and use it to model the MNIST data set of handwritten digits . We then make a single modification to the model : we apply weight normalization to all weight vectors . As can be seen in figure 4 , this significantly speeds up convergence of the optimization procedure , even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization .__label__Method|Code|Use
As methods based on GPs require the inversion of n x n matrices , where n is the number of training examples , we are looking into methods such as query selection for large dataset problems . Other future research directions include the investigation of different covariance functions and improvements on the approximations employed . We hope to make our MATLAB code available from _CITE___label__Method|Code|Produce
SSL is a generic regularization to adaptively adjust multiple structures in DNN , including structures of filters , channels , filter shapes within each layer , and structure of depth beyond the layers . SSL combines structure regularization ( on DNN for classification accuracy ) with locality optimization ( on memory access for computation efficiency ), offering not only well - regularized big models with improved accuracy but greatly accelerated computation ( e . g ., 5 . 1x on CPU and 3 . 1x on GPU for AlexNet ). Our source code can be found at _CITE___label__Method|Code|Produce
FLDA , in contrast , naturally uses labeled data in constructing a low - dimensional embedding . It seeks a a linear projection of the objects ’ coordinates in a high - dimensional ambient space that maximizes between - class variance and minimizes within - class variance . The set of objects comprised 5500 human - classified web pages : 500 pages sampled from each of 11 top level classes in Japanese directories of Open Directory ( _CITE_ ). Pages with less than 50 words , or which occurred under multiple categories , were eliminated . A Naive Bayes ( NB ) classifier was trained on the full data ( represented as word frequency vectors ).__label__Material|Data|Use
Here , we use the L2 - distance estimated by LSDD and the difference of KDEs for this distribution matching . Note that , when LSDD is used to estimate the L2 - distance , separate estimation of ptrain ( x | y = f1 ) is not involved , but the difference between ptest ( x ) and qtest ( x ; 7r ) is directly estimated . We use four UCI benchmark datasets ( _CITE_ ), where we randomly choose 10 labeled training samples from each class and 50 unlabeled test samples following true class - prior 7r ∗ = 0 . 1 , 0 . 2 , ... , 0 . 9 . Figure 6 plots the mean and standard error of the squared difference between true and estimated class - balances 7r and the misclassification error by a weighted 22 - regularized least - squares classifier [ 17 ] with weighted cross - validation [ 18 ] over 1000 runs . The results show that LSDD tends to provide better class - balance estimates than the KDEi - based , the KDEj - based , and the EM - based methods [ 5 ], which are translated into lower classification errors .__label__Material|Data|Use
Similar comments also apply to the regularisation of the process variances for the NCNM . However , these preliminary results appear encouraging for the NCNM . Code for recreating all our experiments is available at _CITE___label__Method|Code|Produce
PSVM distributedly loads training data on parallel machines , reducing memory requirement through approximate factorization on the kernel matrix . PSVM solves IPM in parallel by cleverly arranging computation order . We have made PSVM open source at _CITE___label__Method|Code|Produce
The measure III ( S ; R ; C ) thus translates all the conceptual features of intersection information into a well - defined analytical tool : Eq . 3 defines how III ( S ; R ; C ) can be computed numerically from real data once the distribution p ( s , r , c ) is estimated empirically . In practice , the estimated p ( s , r , c ) defines the space Ap where the problem defined in Eq . 2 should be solved . We developed a gradient - descent optimization algorithm to solve these problems numerically with a Matlab package that is freely available for download and reuse through Zenodo and Github _CITE_ ( see Supp . Info Sec . 2 ).__label__Method|Code|Use
Until now research has focused on how to optimize both step 1 and step 2 . For example , it is important to optimize the prediction horizon of the model [ 2 ] and to reduce complexity as much as possible . This way it was possible to learn the attractor of the benchmark laser time series data from the 1991 Santa Fe sDelftChemTech , Chemical Reactor Engineering Lab , Itilianalaan 136 , 2628 BL , Delft , The Netherlands ; _CITE___label__Material|Data|Use
We thank Alexander Ecker and the lab of Andreas Tolias for sharing their data with us [ 5 ] ( see _CITE_ ), and for allowing us to use it in this publication , as well as Maneesh Sahani and Alexander Ecker for valuable comments . This work was funded by the Gatsby Charitable Foundation ( MP and GB ) and the German Federal Ministry of Education and Research ( MP and JHM ) through BMBF ; FKZ : 01GQ1002 ( Bernstein Center T ¨ ubingen ). Code available athttp :// www . mackelab . org / code .__label__Material|Data|Use
This may result in lower test risks given some Gr0p slightly larger than Grp as shown in Figure 3 . This is also why under - misspecified Gr0 p hurt more than over - misspecified Gr0p . All the experiments were done with Chainer [ 45 ], and our implementation based on it is available at _CITE___label__Method|Tool|Use
We set A = 1 in the first two experiments which yields roughly 20 , 000 non - zeros and varied A for the third experiment . Each of the components is trivially parallelized using OpenMP ( http :// openmp . org /). All timing experiments were conducted on the TACC Maverick system with Intel Xeon E5 - 2680 v2 Ivy Bridge CPUs ( 2 . 80 GHz ), 20 CPUs per node , and 12 . 8 GB memory per CPU ( _CITE_ ). The scaling is generally linear in the parameters except for fitting topic matrices which is O ( k2 ). For the AIS sampling , the scaling is linear in the number of non - zeros in 4 ) irrespective of p . Overall , we believe our implementations provide both good scaling and practical performance ( code available online ).__label__Method|Tool|Use
Some other directions currently under investigation include ( i ) the use of Gaussian processes for classification problems by softmaxing the outputs of k regression surfaces ( for a k - class classification problem ), ( ii ) using non - stationary covariance functions , so that C ( x , x ') # C ( 1 x x ' I ) and ( iii ) using a covariance function containing a sum of two or more terms of the form given in line 1 of equation 3 . We hope to make our code for Gaussian process prediction publically available in the near future . Check _CITE_ for details .__label__Supplement|Document|Produce
In our third experiment , we tested the models on a subset of the Yelp Academic Dataset ( _CITE_ ). We took the 129 , 524 reviews in the dataset that were given to businesses in the Food category . The reviews were randomly split so that 70 % were used for training and 30 % for testing .__label__Material|Data|Use
It can be made less restrictive , but this goes beyond the scope of this paper . Due to space limitations we have to omit the proof . It is found in a technical report , which can be downloaded from _CITE___label__Supplement|Document|Use
Our first experiment aims at detecting transcription start sites ( TSS ) of RNA Polymerase II binding genes in genomic DNA sequences . We experiment on the TSS data set , which we downloaded from _CITE_ This data set , which is a subset of the data used in the larger study of [ 23 ], comes with 5 kernels , capturing various complementary aspects : a weighted - degree kernel representing the TSS signal TSS , two spectrum kernels around the promoter region ( Promo ) and the 1st exon ( 1st Ex ), respectively , and two linear kernels based on twisting angles ( Angle ) and stacking energies ( Energ ), respectively . The SVM based on the uniform combination of these 5 kernels was found to have the highest overall performance among 19 promoter prediction programs [ 24 ], it therefore constitutes a strong baseline .__label__Material|Data|Use
The method was implemented on GPflow [ 20 ] and compared against GPflow ’ s version of the following baselines : exact GP ( GP ), sparse GP using the collapsed bound ( SGP ), and stochastic variational inference using the uncollapsed bound ( SVI ). In all the experiments , the RBF kernel with ARD lengthscales is used , but this is not a limitation required by the new methods . An implementation of the proposed method can be found at _CITE_ Full experimental results and additional discussion points are included in the appendix .__label__Method|Code|Produce
Availability . A web interface for AUTOBAYES is currently under development . More information is available at _CITE___label__Supplement|Document|Produce
There are 2040 instances of flowers for training and 6149 for testing , mainly acquired from the web , with varying scales , resolutions , etc ., which are labeled into 102 categories . In [ 21 ], four relevant features are identified : Color , histogram of gradient orientations and the scale invariant feature transform , sampled on both the foreground region and its boundary . More information is available at _CITE_ For this type of dataset , state of the art performance has been achieved using a weighted linear combination of kernels ( one per feature ) in a support vector machine ( SVM ) classifier . A different set of weights is learned for each class .__label__Supplement|Document|Produce
In this section we present the results of an extensive empirical evaluation of the dynamical ensemble pruning method described in the previous section . The experiments are performed in a series of benchmark classification problems from the UCI Repository [ 1 ] and synthetic data [ 4 ] using Random Forests [ 5 ]. The code is available at : _CITE_ The protocol for the experiments is as follows : for each problem , 100 partitions are created by 10 x 10 - fold cross - validation for real datasets and by random sampling in the synthetic datasets . All the classification tasks considered are binary , except for New - thyroid , Waveform and Wine , which have three classes .__label__Method|Code|Produce
Inference on a gHMM is a relatively straighforward business of dynamic programming . We have used unigram , bigram and trigram models , with each transition model fitted using an electronic version of Caesar ’ s Gallic Wars , obtained from _CITE_ We do not believe that the choice of author should significantly affect the fitted transition model — which is at the level of characters — but have not experimented with this point . The important matter is the emission model .__label__Method|Tool|Use
( for adaptation to Huber hinge loss ) is provided as a supplementary material , and is also available on _CITE_ Whenever possible , we used warmstart approach , i . e ., when we trained a new solution , we used the closest solutions trained so far ( either approximate or optimal ones ) as the initial starting point of the optimizer . All the computations were conducted by using a single core of an HP workstation Z800 ( Xeon ( R ) CPU X5675 ( 3 . 07GHz ), 48GB MEM ).__label__Supplement|Document|Produce
The two corpora we use are 1 .) a collection of 8447 articles from the New York Times from the years 1987 to 2007 with a vocabulary size of 8269 unique types and around one million tokens and 2 .) a sample of 10000 articles from Wikipedia ( _CITE_ ) with a vocabulary size of 15273 unique types and three million tokens .__label__Material|Data|Use
Finally , the dictionary is constructed by clustering the features using the k - means algorithm . The centers of the V extracted clusters are used as visual words , wt ( 1 & lt ; t & lt ; V ). A video of the robot exploring an environment1 is available at : _CITE___label__Supplement|Media|Produce
All convolutional highway networks utilize the rectified linear activation function [ 16 ] to compute the block state H . To provide a better estimate of the variability of classification results due to random initialization , we report our results in the format Best ( mean ± std . dev .) based on 5 runs wherever available . Experiments were conducted using Caffe [ 33 ] and Brainstorm ( _CITE_ ) frameworks . Source code , hyperparameter search results and related scripts are publicly available at http :// people . idsia . ch /˜ rupesh / very_deep_learning /.__label__Method|Tool|Use
For all the experiments , the 1 - Nearest - Neighbor ( 1NN ) algorithm is applied for classification and ten - fold cross validation is used for computing the classification accuracy . Datasets : We use three face datasets in our study : PIX , ORL , and PIE , which are publicly available . PIX ( available at _CITE_ ), contains 300 face images of 30 persons . The image size is 512 x 512 . We subsample the images down to a size of 100 x 100 = 10000 .__label__Material|Data|Use
Image intensity or RGB values are normalized to [ 0 1 ]. We extracted all low level features with 16x16 image patches over dense regular grids with spacing of 8 pixels . We used publicly available dense SIFT code at _CITE_ lazebnik [ 13 ], which includes spatial binning , soft binning and truncation ( nonlinear cutoff at 0 . 2 ), and has been demonstrated to obtain high accuracy for object recognition . For our gradient kernel descriptors we use the same gradient computation as used for SIFT descriptors . We also evaluate the performance of the combination of the three kernel descriptors ( KDES - A ) by simply concatenating the image - level features vectors .__label__Method|Code|Use
We have assessed the ability of the ADIOS model to deal with novel inputs by training it on the CHILDES collection and then subjecting it to a grammaticality judgment test , in the form of multiple choice questions used in English as Second Language ( ESL ) classes . The particular test ( _CITE_ ) has been administered to more than 10 , 000 people in the G ¨ oteborg ( Sweden ) education system as a diagnostic tool when assessing students on upper secondary levels ( that is , children who typically had 9 years of school , but only 6 - 7 years of English ; a test designed for assessing proficiency of younger subjects in their native language would be more suitable , but is not available ). The test consists of 100 three - choice questions ; a score lower than 50 % is considered pre - intermediate , 50 %− 70 % intermediate , and a score greater than 70 % – advanced , with 65 % being the average score for the population mentioned . For each of the three choices in a given question , our algorithm provided a grammaticality score .__label__Method|Algorithm|Use
The various inference algorithms were tested on set of 325 X - ray crystal structures with resolution better than or equal to 2A , R factor below 20 % and length up to 300 residues . One representative structure was selected from each cluster of homologous structures ( 50 % homology or more ). Protein structures were acquired from Protein Data Bank site ( _CITE_ ). Many proteins contain Cysteine residues which tend to form strong disulfide bonds with each other . A standard technique in side - chain prediction ( used e . g .__label__Material|Data|Use
We solved the convex optimization problems ( 2 ) using the CVX solver _CITE_ As already noted , the AATP problem can be solved efficiently using a distributed computing environment . The convex optimization problem of the INFINITEPUSH algorithm ( see ( 3 . 9 ) of [ 1 ]) can also be solved using CVX .__label__Method|Tool|Use
They lead to improved semi - supervised learning peformance and improved sample generation . We hope that some of them may form the basis for future work , providing formal guarantees of convergence . All code and hyperparameters may be found at _CITE___label__Method|Code|Produce
They lead to improved semi - supervised learning peformance and improved sample generation . We hope that some of them may form the basis for future work , providing formal guarantees of convergence . All code and hyperparameters may be found at _CITE___label__Supplement|Document|Produce
Such “ recursive ” training has previously been applied to neural networks for boundary detection [ 8 , 15 , 16 ], but not to ConvNets . ZNN for 3D deep learning Very deep ConvNets with 3D filters are computationally expensive , so an efficient software implementation is critical . We trained our networks with ZNN ( _CITE_ , [ 17 ]), which uses multicore CPU parallelism for speed . ZNN is one of the few deep learning implementations that is well - optimized for 3D .__label__Method|Tool|Use
The guide policy chooses steps using qφ ( zt | ct − 1 , x ), We train the primary / guide policy components ωθ , pθ , and qφ simultaneously on the objective : minimize E E [ τ ∼ qφ ( E - log q ( xu | cT )] + KL ( q ( τ | xu , xk )|| p ( τ | xk )) ( 17 ) θ , φ x ∼ DX m ∼ DM τ | Xu � k ) where q ( xu | cuT ) , p ( xu | cuT ). We train our models using Monte - Carlo roll - outs of q , and stochastic backpropagation as in [ 6 , 14 ]. Full implementations and test code are available from _CITE___label__Method|Code|Produce
For each of these samples , we then approximate the corresponding entropy function H [ p ( y | Dn , x , x ( i ) ? )] using expectation propagation [ 18 ]. The code for all these operations is publicly available at _CITE___label__Method|Code|Produce
In contrast , abstraction to higher - level features is provided by preceding layers in our architecture . We used mean squared error loss , dropout 0 . 2 after input layer , 0 . 5 after each hidden layer , one pixel stride , no pooling . The network is trained in Lasagne ( _CITE_ ) using the Adam algorithm [ 16 ] with learning rate 0 . 0001 for 100 epochs .__label__Method|Tool|Use
EM - LDS was able to correct some of the deformation errors of EM - Gaussian . The average Z error for EM - LDS on the shark sequence after 100 EM iterations is 1 . 24 %. Videos of the shark reconstructions and the Matlab software used for these experiments are available from _CITE_ In highly - constrained cases — low - rank motion , no image noise , and no missing data ILSQ achieved reasonably good results . However , EM - Gaussian gave better results in nearly every case , and dramatically better results in underconstrained cases .__label__Method|Tool|Use
EM - LDS was able to correct some of the deformation errors of EM - Gaussian . The average Z error for EM - LDS on the shark sequence after 100 EM iterations is 1 . 24 %. Videos of the shark reconstructions and the Matlab software used for these experiments are available from _CITE_ In highly - constrained cases — low - rank motion , no image noise , and no missing data ILSQ achieved reasonably good results . However , EM - Gaussian gave better results in nearly every case , and dramatically better results in underconstrained cases .__label__Supplement|Document|Use
By contrast , the KBP - FA model yields a good music segmentation , while also capturing subtle differences in the music over time ( e . g ., in voices ). Note that the use of the DP to allow repeated use of dictionary elements as a function of time ( covariates ) is important here , due to the repetition of structure in the piece . One may listen to the music and observe the segmentation at _CITE___label__Supplement|Media|Produce
For a = 1 ( right plot ), if C is too small , convergence is very slow ( and not at the rate n − 1 ), as already observed ( see , e . g ., [ 8 , 6 ]). Medium - scale experiments with linear logistic regression . We consider two situations where W = Rp : ( a ) the “ alpha ” dataset from the Pascal large scale learning challenge ( _CITE_ ), for which p = 500 and n = 50000 , and ( b ) a synthetic example where p = 100 , n = 100000 ; we generate the input data i . i . d . from a multivariate Gaussian distribution with mean zero and a covariance matrix sampled from a Wishart distribution with p degrees of freedom ( thus with potentially bad condition number ), and the output is obtained through a classification by a random hyperplane . For different values of a , we choose C in an adaptive way where we consider the lowest test error after n / 10 iterations , and report results in Figure 3 .__label__Material|Data|Use
For all MLPs and CNNs , we universally use SGD with learning rate 10 − 3 , momentum 0 . 9 , L2 weight decay 10 − 3 and batch size 128 to reduce the grid search complexity by focusing on architectural hyperparameters . All networks are trained for 100 epochs on MNIST and CIFAR10 , and 20 epochs on SVHN2 , without data augmentation . The source code and scripts for reproducing our experiments are available at _CITE_ Table 1 summarizes our experimental results , including both one - pass ( i . e . first - epoch ) and asymptotic ( i . e .__label__Method|Code|Produce
Interestingly , we show Wasserstein GAN [ 8 ] is the special case of the proposed MMD GAN under certain conditions . The unified view shows more connections between moment matching and GAN , which can potentially inspire new algorithms based on well - developed tools in statistics [ 15 ]. Our experiment code is available at _CITE___label__Method|Code|Produce
The images are rather inhomogeneous , since they show different persons with different facial expressions . Some sample images are depicted in figure 6 . For a complete overview over the whole image set , we refer the reader to our supplementary web page _CITE_ , where all images can be viewed in higher quality .__label__Material|Data|Produce
The images are rather inhomogeneous , since they show different persons with different facial expressions . Some sample images are depicted in figure 6 . For a complete overview over the whole image set , we refer the reader to our supplementary web page _CITE_ , where all images can be viewed in higher quality .__label__Supplement|Website|Produce
Comparison with RAPPOR [ 10 ]. Here we compare our implementation with the only publicly available code for locally private frequency estimation . We took the snapshot of the RAPPOR code base ( _CITE_ ) on May 9th , 2017 . To perform a fair comparison , we tested our algorithm against one of the demo experiments available for RAPPOR ( Demo3 using the demo . sh script ) with the same privacy parameter e = ln ( 3 ), the number of data samples n = 1 million , and the data set to be the same data set generated by the demo . sh script . In Figure 2 we observe that for higher frequencies both RAPPOR and our algorithm perform similarly , with ours being slightly better .__label__Method|Code|Use
2 . The Nematode Biology data set contains 2500 ( randomly sampled ) research abstracts ( _CITE_ ). It has 2944 unique terms , around 179K observed words and an average of 52 unique terms per document . 3 .__label__Material|Data|Introduce
We thank T . W . Chen , K . Svoboda and the GENIE project at Janelia Research Campus for sharing their published GCaMP6 data , available at _CITE_ We also thank T . Deneux for sharing his results for comparison and comments on the manuscript and D . Greenberg , L . Paninski and A . Mnih for discussions . This work was supported by SFB 1089 of the German Research Foundation ( DFG ) to J . H . Macke .__label__Material|Data|Use
The code to fit the models and reproduce the figures is available online at : _CITE___label__Method|Code|Produce
In order to investigate the empirical behavior of FRFs as models of large - scale networks , we design two different groups of experiments ( in link prediction and graph generation respectively ), using collaboration networks drawn from the arXiv e - print repository ( _CITE_ ), where nodes represent scientists and edges represent paper coauthorships . Some basic network statistics are reported in Table 1 . Link prediction .__label__Material|Data|Use
To date , most meteorites recovered throughout history have been done so in Antarctica in the last 20 years . Furthermore , they are less likely to be contaminated by terrestrial compounds . _CITE_ Meteorites are of interest to space scientists because , with the exception of the Apollo lunar samples , they are the sole source of extra - terrestrial material and a window on the early evolution of the solar system . The identification of Martian and lunar meteorite samples , and the ( controversial ) evidence of fossil bacteria in the former underscores the importance of systematically retrieving as many samples as possible . Currently , Antarctic meteorite samples are collected by human searchers , either on foot , or on snowmobiles , who systematically search an area and retrieve samples according to strict protocols .__label__Supplement|Website|Introduce
The datasets can be identified from their names . They were converted to binary classification problems . Each categorical input attribute was converted into n binary attributes by a 1 - of - n encoding ( where n is the attribute ’ s arity ). The post - processed versions of these datasets are at _CITE___label__Material|Data|Use
We show a set of toy experiments to illustrate the general behavior of v - Arc . As base hypothesis class G we use the RBF networks of [ 11 ], and as data a two - class problem generated from several 2D Gauss blobs ( cf . Banana shape dataset from _CITE_ ). We obtain the following results :__label__Material|Data|Use
Details about these algorithms are in Section 6 . 2 of [ 14 ]. The high - level differences among these algorithms are best explained in the context of the disagreement region : OAC does importanceweighted querying of labels with an optimized query probability in the disagreement region , while using predicted labels outside ; IWAL0 and IWAL1 maintain a non - zero minimum query probability everywhere ; ORA - OAC , ORA - IWAL0 and ORA - IWAL1 query labels in their respective disagreement regions with probability 1 , using predicted labels otherwise . We implemented these algorithms in Vowpal Wabbit ( _CITE_ ), a fast learning system based on online convex optimization , using logistic regression as the ERM oracle . We performed experiments on 22 binary classification datasets with varying sizes ( 103 to 106 ) and diverse feature characteristics . Details about the datasets are in Appendix G . 1 of [ 14 ].__label__Method|Tool|Use
In this section , we experimentally demonstrate the usefulness of LSDD . A MATLAB ® implementation of LSDD used for experiments is available from “ _CITE_ Illustration : Let N ( x ; µ , Σ ) be the multi - dimensional normal density with mean vector µ and variance - covariance matrix Σ with respect to x , and let__label__Method|Code|Use
The retraction mechanism is implemented by 3 resonators ( , Hz ) which connect the collision sensors ( CS ) to the neurons ( speed ) and ( steering angle ) with fixed weights ( reflex ). Each range finder ( RF ) is fed into a filter bank of 10 resonators with Hz where its output converges with variable weights on both the and - neuron . A more detailed technical description together with a set of movies can be found at : _CITE_ – movie 1 . ( b , d ) Parts of the motion trajectory for one trial in an arena of with three obstacles ( shaded ). Circles denote collisions .__label__Material|Data|Produce
The retraction mechanism is implemented by 3 resonators ( , Hz ) which connect the collision sensors ( CS ) to the neurons ( speed ) and ( steering angle ) with fixed weights ( reflex ). Each range finder ( RF ) is fed into a filter bank of 10 resonators with Hz where its output converges with variable weights on both the and - neuron . A more detailed technical description together with a set of movies can be found at : _CITE_ – movie 1 . ( b , d ) Parts of the motion trajectory for one trial in an arena of with three obstacles ( shaded ). Circles denote collisions .__label__Supplement|Document|Produce
Following [ 12 ], we used the first 8 examples per task as the training data and the last 4 examples per task as the test data . We measured the root mean square error of the predicted from the actual ratings for the test data , averaged across people . The second data set is the school data set from the Inner London Education Authority ( see _CITE_ ). It consists of examination scores of 15362 students from 139 secondary schools in London . Thus , there are 139 tasks , corresponding to predicting student performance in each school .__label__Material|Data|Use
Following [ 12 ], we used the first 8 examples per task as the training data and the last 4 examples per task as the test data . We measured the root mean square error of the predicted from the actual ratings for the test data , averaged across people . The second data set is the school data set from the Inner London Education Authority ( see _CITE_ ). It consists of examination scores of 15362 students from 139 secondary schools in London . Thus , there are 139 tasks , corresponding to predicting student performance in each school .__label__Supplement|Website|Use
As an application of the two - level influence model , we investigate the influence of participants in meetings . Status , dominance , and influence are important concepts in social psychology for which our model could be particularly suitable in a ( dynamic ) conversational setting [ 8 ]. We used a public meeting corpus ( available at _CITE_ ), which consists of 30 five - minute four - participant meetings collected in a room equipped with synchronized multi - channel audio and video recorders [ 12 ]. A snapshot of the meeting is shown in Fig . 2 ( b ).__label__Material|Data|Use
We randomly augment the training data in the following ways : ( 1 ) Scaling between [ 1 , 2 ], ( 2 ) Rotating within [− 17 °, 17 °], ( 3 ) Adding Gaussian noise with a sigma of 0 . 1 , ( 4 ) Using color jitter with respect to brightness , contrast and saturation uniformly sampled from [ 0 , 0 . 04 ]. We then crop images to 384 × 384 patches and normalize by the mean and standard deviation computed from the ImageNet dataset [ 13 ]. The source code is publicly available on _CITE_ semiFlowGAN .__label__Method|Code|Produce
The FLIC - full dataset contains 20928 training images , however many of these training set images contain samples from the 1016 test set scenes and so would allow unfair overtraining on the FLIC test set . Therefore , we propose a new dataset - called FLIC - plus ( _CITE_ plus . htm ) - which is a 17380 image subset from the FLIC - plus dataset . To create this dataset , we produced unique scene labels for both the FLIC test set and FLICplus training sets using Amazon Mechanical Turk . We then removed all images from the FLIC - plus training set that shared a scene with the test set .__label__Material|Data|Produce
This work is supported by ONR grant N00014 - 96 - 1 - 0418 and a Kxasnow Foundation Postdoctoral fellowship . Thanks to Gerry Tesauro for providing PUBEVAL and subsequent means to calibrate it , Jack Laurence and Pablo Funes for development of the WWW front end to our evolved player . Interested players can challenge our evolved network using a web browser through our home page at : _CITE___label__Supplement|Website|Produce
Our real - world data experiments use an image data set ( MNIST ) and a text data set ( RCV1 ). The MNIST data set , downloadable from _CITE_ , consists of hand - written digit image data ( representing 10 classes , from digit “ 0 ” to “ 9 ”). For our experiments , we randomly choose 2000 images ( i . e ., m = 2000 ). Reuters Corpus Version 1 ( RCV1 ) consists of news articles labeled with topics .__label__Material|Data|Use
We evaluated the scalability of CO - Linear and CO - Quad by generating social networks of varying sizes , constructing CCMRFs with them , and measuring the running time required to find a MPE . We compared our approach to the previous state - of - the - art approach for finding MPEs in CCMRFs , which uses an interior point method implemented in MOSEK , a commercial optimization package ( _CITE_ ). Next we describe the social - network and CCMRF generation procedure , the implementations and setup , and then present the results .__label__Method|Code|Use
The 20 amino acid types are represented by the letters A , C , D , E , F , G , H , I , K , L , M , N , P , Q , R , S , T , V , W , Y . Besides , a “ gap ” ( represented as “–”) is used as a 21st character to account for insertions and deletions . For the purpose of this work , all the input alignments have been generated with jackhmmer , part of HMMER package ( version 3 . 1b2 , _CITE_ ) run against the UniParc database released in summer 2015 . The alignment has been constructed with the E - value inclusion threshold of 1 , allowing for inclusion of distant homologs , at a risk of contaminating the alignment with potentially evolutionarily unrelated sequences . The resultant multiple sequence alignments have not been modified in any way , except for removal of inserts ( positions that were not present in the protein sequence of interest ).__label__Method|Code|Use
Leave - one - out ( LOO ) is a standard procedure in predicting the generalization power of a trained classifier , both from a theoretical and empirical perspective [ 12 ], It is naturally implemented by decremental unlearning , adiabatic reversal of incremental learning , on each of the training data from the full trained solution . Similar ( but different ) bookkeeping of elements migrating across 5 , E and R applies as in the incremental case . & apos ; Matlab code and data are available at _CITE___label__Method|Code|Produce
Leave - one - out ( LOO ) is a standard procedure in predicting the generalization power of a trained classifier , both from a theoretical and empirical perspective [ 12 ], It is naturally implemented by decremental unlearning , adiabatic reversal of incremental learning , on each of the training data from the full trained solution . Similar ( but different ) bookkeeping of elements migrating across 5 , E and R applies as in the incremental case . & apos ; Matlab code and data are available at _CITE___label__Material|Data|Produce
We then present two applications on real - world data , where we assess differences in the persistent homology of functions on 3D surfaces of lateral ventricles and corpora callosa with respect to different group assignments ( i . e ., age , demented / non - demented ). In all experiments , filtrations and the persistence diagrams are obtained using DrPaA2 , which can directly handle our types of input data . Source code to reproduce the experiments is available at _CITE___label__Method|Code|Produce
Real datasets . In this experiment we applied VDP and Fast - VDP for clustering image data . We used the MNIST dataset ( _CITE_ ) which consists of 60 , 000 images of the digits 0 – 9 in 784 dimensions ( 28 by 28 pixels ). We first applied PCA to reduce the dimensionality of the data to 50 . Fast - VDP found 96 clusters in 3 , 379 seconds with free energy F = 1 . 759 × 107 , while VDP found 88 clusters in 72 , 037 seconds with free energy 1 . 684 × 107 .__label__Material|Data|Use
Evaluated in two different 3D biomedical image segmentation applications , our proposed approach can achieve the state - of - the - art performance and outperform known DL schemes utilizing 3D contexts . Our framework provides a new paradigm to migrate the superior performance of 2D deep architectures to exploit 3D contexts . Following this new paradigm , we will explore BDC - LSTMs in different deep architectures to achieve further improvement and conduct more extensive evaluations on different datasets , such as BraTS ( _CITE_ ) and MRBrainS ( http :// mrbrains13 . isi . uu . nl ).__label__Material|Data|Use
In order to evaluate the clustering results , we use three commonly used clinical biomarkers of colon and rectal cancer ( The Cancer Genome Atlas Network , 2012 ): ( i ) micro - satellite instability ( i . e ., a hypermutable phenotype caused by the loss of DNA mismatch repair activity ) ( ii ) hypermutation ( defined as having mutations in more than or equal to 300 genes ), and ( iii ) mutation in BRAF gene . Note that these three biomarkers are not directly identifiable from the input data sources used . The preprocessed genomic characterizations of the patients can be downloaded from a public repository at _CITE_ , where DNA copy number , mRNA gene expression , DNA methylation , and mutation data consist of 20313 , 20530 , 24980 , and 14581 features , respectively . The micro - satellite instability data can be downloaded from https :// tcga - data . nci . nih . gov / tcga / dataAccessMatrix . htm . In the resulting data set , there are 204 patients with available genomic and clinical biomarker data .__label__Material|Data|Use
We now empirically explore our method ’ s behavior . All of our code , data , and experiments may be found on the CodaLab worksheet for this paper at _CITE_ , which also contains more detailed plots beyond those shown here . We would like to answer the following questions :__label__Method|Code|Produce
We now empirically explore our method ’ s behavior . All of our code , data , and experiments may be found on the CodaLab worksheet for this paper at _CITE_ , which also contains more detailed plots beyond those shown here . We would like to answer the following questions :__label__Material|Data|Produce
We now empirically explore our method ’ s behavior . All of our code , data , and experiments may be found on the CodaLab worksheet for this paper at _CITE_ , which also contains more detailed plots beyond those shown here . We would like to answer the following questions :__label__Supplement|Website|Produce
We now empirically explore our method ’ s behavior . All of our code , data , and experiments may be found on the CodaLab worksheet for this paper at _CITE_ , which also contains more detailed plots beyond those shown here . We would like to answer the following questions :__label__Method|Algorithm|Produce
We now empirically explore our method ’ s behavior . All of our code , data , and experiments may be found on the CodaLab worksheet for this paper at _CITE_ , which also contains more detailed plots beyond those shown here . We would like to answer the following questions :__label__Supplement|Document|Produce
The last building block is a method for sampling from convex bodies . We suggest the hit and run [ 9 ] random walk for this purpose in section 4 . A Matlab implementation of KQBC is available at _CITE_ The empirical part of this work is presented in section 5 . We demonstrate how KQBC works on two binary classification tasks .__label__Method|Code|Produce
In the third experiment we demonstrate the ability of our method to infer the spectrum of airline passenger data , to perform long - range extrapolations on real data , and to demonstrate the utility of accounting for uncertainty in the kernel . In the final experiment we demonstrate the scalability of our method through training the model on a 100 , 000 data point sound waveform . Code is available at _CITE_ f ( X )__label__Method|Code|Produce
We augment the train data by randomly positioning the original image on a 32x32 pixel canvas . CIFAR10 . The CIFAR - 10 dataset ( _CITE_ ) consists of 50k 32x32 training images and 10k testing images in 10 classes . We hold out 5k training images as validation set , and use the remaining 45k as the training set . To augment the data , we follow common practice , see e . g .__label__Material|Data|Use
The resulting sampled trajectories roughly fit the 2D observations , but were rather unrealistic — for instance , the subject ’ s feet often floated above the floor . We ran the embedded HMM using five pool states for 300 iterations , taking 1 . 7 hours of compute time . The resulting sampled trajectories were more realistic ' Data from the graphics lab of Jessica Hodgins , at _CITE_ We chose markers 167 , 72 , 62 , 63 , 31 , 38 , downsampled to 30 frames per second . The experiments reported here use frames 400 - 520 of trial 20 for subject 14 .__label__Material|Data|Use
Accuracy on the CMU - MIT dataset ( a standard , public data set for benchmarking frontal face detection systems ) is comparable to [ 13 ]. Because the strong classifiers early in the sequence need very few features to achieve good performance ( the first stage can reject of the nonfaces using only features , using only 20 simple operations , or about 60 microprocessor instructions ), the average number of features that need to be evaluated for each window is very small , making the overall system very fast . The source code for the face detector is freely available at _CITE___label__Method|Code|Produce
It is clear for the plot that only a small number of obstacles would not have been avoided by the robot . The best performance measure is a set of actual runs through representative testing grounds . Videos of typical test runs are available at _CITE_ Figure 2 shows a snapshot of the trained system in action . The network was presented with a scene that was not present in the training set .__label__Supplement|Media|Produce
This work is supported in part by the National Hi - Tech Project of China under grant 2008AA01Z150 and the Natural Science Foundation of China under grant 60745002 and 61175057 . Feng Wu is supported in part by the ORCHID project ( _CITE_ ). We are grateful to the anonymous reviewers for their constructive comments and suggestions .__label__Method|Tool|Introduce
Implementation . All experiments were implemented in PyTorch3 , using DIPHA4 and Perseus [ 23 ]. Source code is publicly - available at _CITE___label__Method|Code|Produce
This database was created with funding from NSF EIA - 0196217 . The second data set used in this project was obtained fromhttp :// people . csail . mit . edu / ehsu / work / sig05stf /. ForMatlab playback of motion and generation of videos , we have used Neil Lawrence ’ s motion capture toolbox ( _CITE_ ).__label__Method|Tool|Use
Only our method could acquire a successful strategy to locate both the training and test holes , although RWR was eventually able to insert the peg into one of the four holes in 2D . This task illustrates one of the advantages of learning expressive neural network policies , since no single trajectory - based policy can represent such a search strategy . Videos of the learned policies can be viewed at _CITE___label__Supplement|Media|Produce
For continuous data , we use the hierarchical component analysis for modeling handwritten digits ( _CITE_ ). This dataset contains 3823 handwritten digits as a training set and__label__Method|Algorithm|Use
Constructing a Markov chain that adapts to the target distribution while still drawing samples from the correct stationary distribution is challenging , although much research over the last 15 years has resulted in a variety of approaches and theoretical results . Adaptive MCMC for example , allows for global adaptation based on the partial or full history of a chain ; this breaks its Markov property , although it has been shown that subject to some technical conditions [ 2 , 3 ] the resulting chain will still converge to the desired stationary distribution . Most recently , advances in Riemannian Manifold MCMC allow locally changing , position specific proposals to be made based on the underlying ∗ _CITE_ geometry of the target distribution [ 1 ]. This directly takes into account the changing sensitivities of the model for different parameter values and enables very efficient inference over a number of popular statistical models . It is useful for inference over large numbers of strongly covarying parameters , however this methodology is still not suitable for all statistical models ; in its current form it is only applicable to models that admit an analytic expression for the metric tensor .__label__Supplement|Document|Extent
Similar results were obtained for both larger and smaller training sample sizes . For the UCI experiments , the results are very similar to the synthetic networks , showing good results again for the convex EM relaxation . Finally , we conducted additional experiments on three real world Bayesian networks : Alarm , Cancer and Asian ( downloaded from _CITE_ ). We picked one well connected node from each model to serve as the hidden variable , and generated data by sampling from the models . Table 1 shows the experimental results for these three Bayesian networks .__label__Method|Algorithm|Use
The first data set used in this project was obtained from mocap . cs . cmu . edu . This database was created with funding from NSF EIA - 0196217 . The second data set used in this project was obtained from_CITE_ ForMatlab playback of motion and generation of videos , we have used Neil Lawrence ’ s motion capture toolbox ( http :// www . dcs . shef . ac . uk /∼ neil / mocap /).__label__Material|Data|Use
We compared performance of RETAIN to RNNs and traditional machine learning methods . Given space constraints , we only report the results on the learning to diagnose ( L2D ) task and summarize the disease progression modeling ( DPM ) in Appendix C . The RETAIN source code is publicly available at_CITE___label__Method|Code|Produce
In this paper , we will try to shed more light on this topic . The motivation is twofold . First , the policy oscillation phenomenon is intimately connected to some aspects of the learning dynamics at the very heart of approximate dynamic An extended version of this paper is available at _CITE_ programming ; the lack of understanding in the former implies a lack of understanding in the latter . In the long run , this state might well be holding back important theoretical developments in the field .__label__Supplement|Document|Produce
The backgrounds made trajectory - trails difficult to see , so we masked the background ( only for rendering purposes ). Trajectory trails are shown for rollouts between 40 and 60 time steps , depending on the dataset . We encourage the reader to view the videos at ( _CITE_ ). Those include the CIFAR backgrounds and show very long rollouts of up to 200 timesteps , which demonstrate the VIN ’ s extremely realistic predictions . We find no reason to doubt that the predictions would continue to be visually realistic ( if not exactly tracking the ground - truth simulator ) ad infinitum .__label__Supplement|Media|Produce
The Inception score is used for assessing generations from GANs and is more appropriate for our scenario that traditional metrics such as PSNR or SSIM ( see appendix B for further discussion ). The curves show the mean scores of our generations decaying more gracefully than MCNet [ 33 ]. Further examples and generated movies may be viewed in appendix A and also at _CITE_ A natural concern with high capacity models is that they might be memorizing the training examples . We probe this in Fig .__label__Supplement|Document|Produce
The Inception score is used for assessing generations from GANs and is more appropriate for our scenario that traditional metrics such as PSNR or SSIM ( see appendix B for further discussion ). The curves show the mean scores of our generations decaying more gracefully than MCNet [ 33 ]. Further examples and generated movies may be viewed in appendix A and also at _CITE_ A natural concern with high capacity models is that they might be memorizing the training examples . We probe this in Fig .__label__Supplement|Media|Produce
The tasks described in Section 3 were offered on Amazon Mechanical Turk ( _CITE_ ), which allows workers ( our pool of prospective subjects ) to perform small jobs for a fee through a Web interface . No specialized training or knowledge is typically expected of the workers . Amazon Mechanical Turk has been successfully used in the past to develop gold - standard data for natural language processing [ 22 ] and to label images [ 23 ].__label__Supplement|Website|Introduce
A learning algorithm could potentially learn a non - efficient code , for instance , but nonetheless describe the establishment of receptive fields seen in adult animals . Details of the algorithms , parameters , and fitting methods can be found in the supplementary information . Results from our grid searches are available at _CITE_ .__label__Supplement|Document|Produce
ORL ( available at http :// www . uk . research . att . com / facedatabase . html ), contains 400 face images of 40 persons . The image size is 92 x 112 . PIE is a subset of the CMU – PIE face image dataset ( available at _CITE_ 418 . html ). It contains 6615 face images of 63 persons . The image size is 640 x 480 .__label__Material|Data|Use
Algorithms for solving this problem are nearly as efficient as those for solving regular min - cost flow problems . In case of word alignment , the running time scales with the cube of the sentence length . We use publicly - available code for solving this problem [ 8 ] ( see _CITE_ ).__label__Method|Code|Use
In particular , we achieve a 25 % higher recall for 2K proposals than the state - of - the - art RGB - D method MCG - D [ 14 ]. Combined with CNN scoring , our method outperforms all published results on object detection for Car , Cyclist and Pedestrian on KITTI [ 11 ]. Our code and data are online : _CITE___label__Method|Code|Produce
In particular , we achieve a 25 % higher recall for 2K proposals than the state - of - the - art RGB - D method MCG - D [ 14 ]. Combined with CNN scoring , our method outperforms all published results on object detection for Car , Cyclist and Pedestrian on KITTI [ 11 ]. Our code and data are online : _CITE___label__Material|Data|Produce
For the large - scale real - world experiments , we use gene expression datasets that are available at the Gene Expression Omnibus ( _CITE_ ). We use several of the__label__Material|Data|Use
For the large - scale real - world experiments , we use gene expression datasets that are available at the Gene Expression Omnibus ( _CITE_ ). We use several of the__label__Supplement|Website|Use
The top graphs in Figure 7 ( a ) display the original timeseries ( true change points were manually annotated ) and change scores obtained by KLIEP and LSDD . The graphs show that the LSDD - based change score indicates the existence of change points more clearly than the KLIEP - based change score . Next , we use a dataset taken from the Human Activity Sensing Consortium ( HASC ) challenge 2011 ( _CITE_ ), which provides human activity information collected by portable three - axis accelerometers . Because the orientation of the accelerometers is not necessarily fixed , we take the 22 - norm of the 3 - dimensional data . The HASC dataset is relatively simple , so we artificially added zero - mean Gaussian noise with standard deviation 5 at each time point with probability 0 . 005 .__label__Material|Data|Use
Each results figure plots the progression of minxn f ( xn ) over the number of function evaluations or time , averaged over multiple runs of each algorithm . If not specified otherwise , xeext = argmaxx a ( x ) is computed using gradientbased search with multiple restarts ( see supplementary material for details ). The code used is made publicly available at _CITE___label__Method|Code|Use
The model structure means that the most likely harmonisation leaves these states unornamented . Nevertheless , where ornamentation has been added it fits with its context and enhances the harmonisations . The authors will publish further example harmonisations , including MIDI files , online at _CITE___label__Supplement|Media|Produce
Lower panels show scanpaths of all 7 subjects . The trend of visiting the faces first - typically within the 1st or 2nd fixation - is evident . All images are available at _CITE___label__Supplement|Media|Produce
The optimal firing rate response curve ( dotted line ) is asymptotically proportional to the cumulative probability distribution of inputs . The inset illustrates the typical timecourse of the dendritic voltage in the trained model . relationship at the soma , can be predicted from the theory of dynamical systems ( see _CITE_ for details ). The voltage and the conductances are nonlinearly coupled : the conductances affect the voltage , which , in turn , sets the conductances . Since the mutual information is a global property of the stimulus set , the learning rule for any one conductance would depend on the values of all other conductances , were it not for the nonlinear feedback loop between voltages and conductances .__label__Supplement|Document|Produce
that all random variables are normalized to have expectation 0 and variance 1 . The objective R2z , X is monotone increasing , but not necessarily submodular [ 6 ]. We use two data sets from _CITE_ The budget k is set to { 10 , 12 , ... , 20 }. For estimating R2 in the optimization process , we use a random sample of 1000 instances .__label__Material|Data|Use
In high - dimensional complex domain , the ADMM algorithm demonstrates superior performance in our simulated examples and real images . Finally , the paper also provides practical guidelines to practitioners at large working on other similar nonsmooth SDP applications . To aid peer evaluation , the source code of all the algorithms have been made available at : _CITE___label__Method|Code|Produce
We have presented a method to learn discriminative feature transforms using Maximum Mutual Information as the criterion . Formulating MI using Renyi entropy , and Gaussian ' Example video clips can be viewed at _CITE_ where the input space GMM parameters are and with the equalities and .__label__Supplement|Media|Produce
Our GLM graphical models will be important for understanding genomic networks learned from other high - throughput technologies that do not produce approximately Gaussian data . Here , we demonstrate the versatility of our model by learning two cancer genomic networks , a genomic copy number aberration network ( from aCGH data ) for Glioblastoma learned by multinomial graphical models and a meta - miRNA inhibitory network ( from next generation sequencing data ) for breast cancer learned by Poisson graphical models . Level III data , breast cancer miRNA expression ( next generation sequencing ) [ 13 ] and copy number variation ( aCGH ) Glioblastoma data [ 14 ], was obtained from the the Cancer Genome Atlas ( TCGA ) data portal ( _CITE_ ), and processed according to standard techniques . Data descriptions and processing details are given in the supplemental materials . A Poisson graphical model and a multinomial graphical model were fit to the processed miRNA data and aberration data respectively by performing neighborhood selection with the sparsity of the graph determined by stability selection [ 15 ].__label__Material|Data|Use
All models are implemented in Theano [ 16 ], based on the implementation of restricted - capacity uRNNs by [ 10 ], available from _CITE_ All code to replicate our results is available from https :// github . com / stwisdom / urnn . All models use RMSprop [ 15 ] for optimization , except that full - capacity uRNNs optimize their recurrence matrices with a fixed learning rate using the update step ( 6 ) and optional RMSprop - style gradient normalization .__label__Method|Code|Extent
The results are compared using the DICE overlap ( DC ), the modified Hausdorff distance ( MD ), and the absolute volume difference ( AVD ) [ 13 ]. MR brain image segmentation results are evaluated by the ISBI NEATBrain15 organisers [ 13 ] who provided the extensive comparison to other approaches on _CITE_ Table 2 compares our results to those of the top five teams . The organisers compute nine measures in total and rank all teams for each of them separately .__label__Method|Algorithm|Compare
We conclude with a brief experiment exemplifying some of the ideas discussed so far . The statistics division of the United Nations makes available extensive data sets detailing the amounts of trade between major sovereign nations ( see _CITE_ ). We used a data set indicating , for each pair of nations , the total amount of trade in U . S . dollars between that pair in the year 2002 . For our purposes , we would like to extract a discrete network structure from this numerical data .__label__Material|Data|Introduce
Each image is first segmented into 800 “ superpixels ”, which are local , coherent and preserve most of the structure necessary for segmentation at the scale of interest [ 19 ]. The software used for over - segmentation is discussed in [ 17 ] and is available online ( _CITE_ ). Each superpixel is represented by both color and texture descriptors , based on the local RGB , hue [ 25 ] feature vectors and also the output of maximum response ( MR ) filter banks [ 22 ] ( http :// www . robots . ox . ac . uk /∼ vgg / research / texclass / filters . html ). We discretize these features using a codebook of size 64 ( other codebook sizes gave similar performance ), and then calculate the distribution [ 1 ] for each feature within each superpixel as visual words [ 3 , 6 , 10 , 11 , 20 , 23 , 24 ].__label__Method|Tool|Use
To keep the horizon low , we introduce enough wait transitions so that it takes no more than 10 transitions to wait a whole day in the busiest airport ( about 1000 flights per day ) and we set the horizon at 100 . Costs are deterministic and correspond to the time difference between the scheduled departure time of the first flight and the arrival time . We compute transition probabilities based on historical data , available from the Office of Airline Information , Bureau of Transportation Statistics , at _CITE_ Particularly , we have used on - time statistics for February 2011 . Airlines often try to conceal statistics for flights with low on - time performance by slightly changing departure times and flight numbers .__label__Material|Data|Use
And the third is the Bayesian 11 - norm sparse learning scheme that infers the optimal regularization parameters for deriving optimally sparse solutions . The results demonstrate that the proposed BSCI approach holds the potential to solve the speech dereverberation problem in real acoustic environments , which has been recognized as a very difficult problem in signal processing . The acoustic data used in this paper are available at _CITE_ Our future work includes side - by - side comparison between our BSCI approach and existing source statistics based BCI approaches . Our goal is to build a uniform framework that combines various prior knowledge about acoustic systems for best solving the speech dereverberation problem .__label__Material|Data|Use
Clustering faces . In a second experiment we applied our method to the problem of clustering face images . From the Stirling Faces database ( _CITE_ ) we selected all 68 grey - valued front views of faces and all 105 profile views . The images are rather inhomogeneous , since they show different persons with different facial expressions . Some sample images are depicted in figure 6 .__label__Material|Data|Use
However , as shown in Table 1 , our algorithm successfully picked it up 100 % of the time . The same rate of success holds even if the glass is 2 / 3 filled with water . Videos showing the robot grasping the objects , are available at _CITE_ We also applied our learning algorithm to the task of unloading items from a dishwasher . 9 Fig . 5 demonstrates that the algorithm correctly identifies the grasp on multiple objects even in the presence of clutter and occlusion . Fig .__label__Supplement|Media|Produce
The second is the multi - task RBF network , where the K tasks share the same 0 but each has its own w . In the third , we have K independent networks , each designed for a single task . We use a school data set from the Inner London Education Authority , consisting of examination records of 15362 students from 139 secondary schools . The data are available at _CITE_ This data set was originally used to study the effectiveness of schools and has recently been used to evaluate multi - task algorithms [ 2 , 3 ]. The goal is to predict the exam scores of the students based on 9 variables : year of exam ( 1985 , 1986 , or 1987 ), school code ( 1 - 139 ), FSM ( percentage of students eligible for free school meals ), VR1 band ( percentage of students in school in VR band one ), gender , VR band of student ( 3 categories ), ethnic group of student ( 11 categories ), school gender ( male , female , or mixed ), school denomination ( 3 categories ).__label__Material|Data|Use
All models are implemented in Theano [ 16 ], based on the implementation of restricted - capacity uRNNs by [ 10 ], available from https :// github . com / amarshah / complex_RNN . All code to replicate our results is available from _CITE_ All models use RMSprop [ 15 ] for optimization , except that full - capacity uRNNs optimize their recurrence matrices with a fixed learning rate using the update step ( 6 ) and optional RMSprop - style gradient normalization .__label__Method|Code|Produce
It is also worth studying how different kernel functions affect the performance of NPE . These are matters of our future inquiry . To facilitate such exploration , we make our Mturk dataset public at _CITE___label__Material|Data|Produce
The heuristic computes the error for a given validation image at level k in the pyramid as Lk ( Ik ) = min { z ;}|| Gk ( zj , u ( Ik + 1 )) − hk || 2 where { zj } is a large set of noise vectors , drawn from pnozse ( z ). In other words , the heuristic is asking , are any of the generated residual images close to the ground truth ? Torch training and evaluation code , along with model specification files can be found at _CITE_ For all models , the noise vector zk is drawn from a uniform [- 1 , 1 ] distribution .__label__Method|Code|Produce
The heuristic computes the error for a given validation image at level k in the pyramid as Lk ( Ik ) = min { z ;}|| Gk ( zj , u ( Ik + 1 )) − hk || 2 where { zj } is a large set of noise vectors , drawn from pnozse ( z ). In other words , the heuristic is asking , are any of the generated residual images close to the ground truth ? Torch training and evaluation code , along with model specification files can be found at _CITE_ For all models , the noise vector zk is drawn from a uniform [- 1 , 1 ] distribution .__label__Supplement|Document|Produce
RGS is basically an on line method which can be used in batch mode by running it in epochs on the training set . When it is run for only one epoch , T = m and the complexity is O ( m2N ). Matlab code for this algorithm ( and those that we compare with ) is available at _CITE___label__Method|Code|Compare
Fig . 4 ( top ) shows a sequence of images representing burn - in of the model as it starts from the initial condition and samples its way towards regions of high likelihood . A video demonstrating the results is available at _CITE___label__Supplement|Media|Produce
onsDimensional ityreduct ionmeth odst hatt ake lab el s orparamet ersi nto acco unth averecen tlyfo u ndaresurge nc einintere st . Our st udy wasmotiva te dby thespec ificprobl emsrela te dtoelectrophysiol og - i cald atase ts . Them ain ai mof ourmethod — demix ingparame ter dependenc ie sofhigh - dimensio nald ata sets — ma y beuse fu linot hercont ex t aswe ll . V erysimi larprobl ems ar is einf MRIda ta , for in - stan ce , andd PCAco uldprov i de ause fulalternat iv etoot herdimensional ityreduct ionmeth odss uc has C CA , P LS , orSupervi sed PCA [ 1 , 12 , 5 ]. Furthermo re , thegene ral ai mofdemix ingdependenc iesco uldlik el ybeexten de dtoot hermeth ods ( s uc hasI CA ) aswe ll . Ultimate ly , we seed PC A asap ar - ticu lard atavisualizat iontechni quet hatw illpr oveuse fu l ifademix in gofparame terdependenc iesa id sinunderstand ingda ta . The source code both for Python and Matlab can be found at _CITE_ a__label__Method|Code|Produce
In this section we present the primary experimental result of this paper , a successful application of hierarchical apprenticeship learning to the task of quadruped locomotion . Videos of the results in this section are available at _CITE___label__Supplement|Media|Produce
When the network is updated according to ( 1 ), then under certain conditions the network state becomes asymptotically independent of initial conditions . More precisely , if the network is started from two arbitrary states x ( 0 ), 31 ( 0 ) and is run with the same input sequence in both cases , the resulting state sequences x ( n ), x ( n ) converge to each other . If this condition holds , the reservoir network state will asymptotically depend only on the input history , and the network tained in a tutorial Mathematica notebook which can be fetched from _CITE_ is said to be an echo state network ( ESN ). A sufficient condition for the echo state property is contractivity of W . In practice it was found that a weaker condition suffices , namely , to ensure that the spectral radius Amax of W is less than unity .__label__Supplement|Document|Produce
For S2 , the difference between the treewidth seems negligible from the figure . This is due to the fact that the graph learned are actually sparse . Further experimental documentation is available , including how the score achieved by the algorithms evolve with time , are available from _CITE___label__Supplement|Document|Produce
The weight vector w ? in our datasets was known , so we do not evaluate EELS . This section contains a high - level description of our experimental setup , with details on our implementation , baseline algorithms , and policy classes deferred to Appendix C . Software is available at _CITE_ Data : We used two large - scale learning - to - rank datasets : MSLR [ 17 ] and all folds of the Yahoo ! Learning - to - Rank dataset [ 5 ].__label__Method|Tool|Produce
Acceleration with Structured Transforms : In Figure 3 , we analyze the speedup obtained in practice using n × n Circulant and Toeplitz - like matrices relative to a dense unstructured n × n matrix ( fully connected layer ) as a function of displacement rank and dimension n . Three scenarios are considered : inference speed per test instance , training speed as implicitly dictated by forward passes on a minibatch , and gradient computations on a minibatch . Factors such as differences in cache optimization , SIMD vectorization and multithreading between Level - 2 BLAS ( matrix - vector multiplication ), Level - 3 BLAS ( matrix - matrix multiplication ) and FFT implementations ( we use FFTW : _CITE_ ) influence the speedup observed in practice . Speedup gains start to show for dimensions as small as 512 for Circulant matrices . The gains become dramatic with acceleration of the order of 10 to 100 times for several thousand dimensions , even for higher displacement rank Toeplitz - like transforms .__label__Method|Tool|Use
We also tested the two models on 2 , 742 articles from the NIPS conference for the years 1988 – 2004 . We used the raw text versions available at _CITE_ ( 1988 – 1999 ) and http :// ai . stanford . edu /˜ gal / data . html ( 2000 – 2004 ). The first set was used as the training set and the second as the test set . The corpus was again passed through a stemmer , and stopwords and words appearing no more than 50 times were removed .__label__Material|Data|Use
This problem definition reflects the setting of the ongoing ChaLearn AutoML challenge [ 1 ]. The AutoML system we describe here won the first phase of that challenge . Here , we follow and extend the AutoML approach first introduced by AUTO - WEKA [ 2 ] ( see _CITE_ ). At its core , this approach combines a highly parametric machine learning framework F with a Bayesian optimization [ 3 ] method for instantiating F well for a given dataset . The contribution of this paper is to extend this AutoML approach in various ways that considerably improve its efficiency and robustness , based on principles that apply to a wide range of machine learning frameworks ( such as those used by the machine learning service providers mentioned above ).__label__Method|Algorithm|Extent
Proof of concept : Key Influencers in Theoretical Physics : Drawn from a KDD Cup 2003 task , this datasetis publically available at : http :// www . cs . cornell . edu / projects / kddcup / datasets . html . It consists of the latex sources of all papers in the hep - th portion of the arXiv ( _CITE_ ) In consultation with a theoretical physicist we did our analysis at a time granularity of 1 month . In total , the data spans 137 months . We created document term matrices using standard text processing techniques , over a vocabulary of 463 words chosen by running an unsupervised topic model .__label__Material|Data|Introduce
Proof of concept : Key Influencers in Theoretical Physics : Drawn from a KDD Cup 2003 task , this datasetis publically available at : http :// www . cs . cornell . edu / projects / kddcup / datasets . html . It consists of the latex sources of all papers in the hep - th portion of the arXiv ( _CITE_ ) In consultation with a theoretical physicist we did our analysis at a time granularity of 1 month . In total , the data spans 137 months . We created document term matrices using standard text processing techniques , over a vocabulary of 463 words chosen by running an unsupervised topic model .__label__Supplement|Website|Introduce
When working with fully - connected models we use stochastic GRU - style state updates rather than the stochastic residual updates in Eq . 7 . Exhaustive descriptions of the modules can be found in our code at : _CITE_ These TD modules represent each conditional p ( zi | zi − 1 , ..., z0 ) in Eq . 1 using p ( zi | hti ).__label__Method|Code|Produce
When working with fully - connected models we use stochastic GRU - style state updates rather than the stochastic residual updates in Eq . 7 . Exhaustive descriptions of the modules can be found in our code at : _CITE_ These TD modules represent each conditional p ( zi | zi − 1 , ..., z0 ) in Eq . 1 using p ( zi | hti ).__label__Supplement|Document|Produce
In section 4 . 3 , we learn human kernels to extrapolate on tasks which are difficult for Gaussian processes with standard kernels . In section 4 . 4 , we study model selection in human function learning . All human participants were recruited using Amazon ’ s mechanical turk and saw experimental materials provided at _CITE_ When we are considering stationary ground truth kernels , we use a spectral mixture for kernel learning ; otherwise , we use a non - parametric empirical estimate .__label__Method|Tool|Use
In section 4 . 3 , we learn human kernels to extrapolate on tasks which are difficult for Gaussian processes with standard kernels . In section 4 . 4 , we study model selection in human function learning . All human participants were recruited using Amazon ’ s mechanical turk and saw experimental materials provided at _CITE_ When we are considering stationary ground truth kernels , we use a spectral mixture for kernel learning ; otherwise , we use a non - parametric empirical estimate .__label__Supplement|Document|Use
where ( 2 ) approaches zero for an optimum result . Indeed , the main assumption in the original HA is that the R (`), B = 1 : S are noisy ‘ rotations ’ of a common template [ 1 , 9 ]. This paper provides a detailed description of HA methods in the supplementary materials ( _CITE_ ).__label__Supplement|Document|Produce
The advantage of our method is that we can jointly learn the optimal feature representation and the optimal domain transformation parameter , which are aware of the subsequent transductive inference procedure . Following the standard evaluation protocol in the unsupervised domain adaptation community , we evaluate our method on the digit classification task using MNIST [ 19 ] and SVHN [ 21 ] as well as the object recognition task using the Office [ 25 ] dataset , and demonstrate state of the art performance in comparison to all existing unsupervised domain adaptation methods . Learned models and the source code can be reached from the project webpage _CITE___label__Method|Code|Produce
The advantage of our method is that we can jointly learn the optimal feature representation and the optimal domain transformation parameter , which are aware of the subsequent transductive inference procedure . Following the standard evaluation protocol in the unsupervised domain adaptation community , we evaluate our method on the digit classification task using MNIST [ 19 ] and SVHN [ 21 ] as well as the object recognition task using the Office [ 25 ] dataset , and demonstrate state of the art performance in comparison to all existing unsupervised domain adaptation methods . Learned models and the source code can be reached from the project webpage _CITE___label__Supplement|Website|Produce
The advantage of our method is that we can jointly learn the optimal feature representation and the optimal domain transformation parameter , which are aware of the subsequent transductive inference procedure . Following the standard evaluation protocol in the unsupervised domain adaptation community , we evaluate our method on the digit classification task using MNIST [ 19 ] and SVHN [ 21 ] as well as the object recognition task using the Office [ 25 ] dataset , and demonstrate state of the art performance in comparison to all existing unsupervised domain adaptation methods . Learned models and the source code can be reached from the project webpage _CITE___label__Method|Algorithm|Produce
I thank Jack Crago , John Lloyd , Kirsty Aquilina , Kevin Gurney and Giovanni Pezzulo for discussions related to this research . The code used to generate the results and figures for this paper is at _CITE___label__Method|Code|Produce
The issue is particularly serious for large data , where direct second order methods cannot be used due to the computational constraints . While many approximate second - order methods are available , they rely on low - rank approximations and , as we discuss below , lead to over - regularization ( approximation bias ). In the second part of the paper we propose EigenPro iteration ( see _CITE_ for the code ), a direct and simple method to alleviate slow convergence resulting from fast eigen - decay for kernel ( and covariance ) matrices . EigenPro is a preconditioning scheme based on approximately computing a small number of top eigenvectors to modify the spectrum of these matrices . It can also be viewed as constructing a new kernel , specifically optimized for gradient descent .__label__Method|Code|Produce
Different mechanisms have been proposed , some relying on the average firing rates of the pre - and post - synaptic neurons , ( rate - based Hebbian learning ), others based on tight constraints on the time lags between pre - and post - synaptic spikes (“ Spike - Timing - Dependent - Plasticity ”). The synaptic circuits described in what follows implement a stochastic version of rate - based Hebbian learning . In the last decade , it has been realized that general constraints plausibly met by any concrete implementation of a synaptic device in a neural network , bear profound consequences on ∗ _CITE_ the capacity of the network as a memory system . Specifically , once one accepts that a synaptic element can neither have an unlimited dynamic range ( i . e . synaptic efficacy is bounded ), nor can it undergo arbitrarily small changes ( i . e .__label__Method|Algorithm|Introduce
We have found the flow - based Midflow and MQI rounding methods to be highly effective in practice on diverse classes of graphs including space - like graphs and power law graphs . Results for real - world power law graphs are shown in figure 5 . Results for a number of FE meshes can be found on the Graph Partitioning Archive website _CITE_ , which keeps track of the best nearly balanced cuts ever found for a number of classic benchmarks . Using flow - based rounding to extract cuts from spectral - type embeddings , we have found new record cuts for the majority of the largest graphs on the site , including fe body , t60k , wing , brack2 , fe tooth , fe rotor , 598a , 144 , wave , m14b , and auto . It is interesting to note that the spectral method previously did not own any of the records for these classic benchmarks , although it could have if flow - based rounding had been used instead of hyperplane rounding .__label__Supplement|Website|Produce
Three equivalence classes are shown explicitly ( highlighted ). Emergence of syntactic structures . Figure 3 shows an example of a sentence from a corpus produced by a simple artificial grammar and its ADIOS analysis ( the use of a simple grammar , constructed with Rmutt , _CITE_ , in these initial experiments allowed us to examine various properties of the model on tightly controlled data ). The abstract representation of the sample sentence in Figure 3 ( c ) looks very much like a parse tree , indicating that our method successfully identified the grammatical structure used to generate its data . To illustrate the gradual emergence of our model ’ s ability for such concise representation of syntactic structures , we show in Figure 4 , top , four trees built for the same sentence after exposing the model to progressively more data from the same corpus .__label__Method|Tool|Use
Figure 1 shows results from the simulation of the flanker task , recovering the characteristic early below - chance performance in incongruent trials . This simulation supports the assertion that our theory generalizes the flanker model of [ 5 ], though we are not sure why our scale on timesteps appears different by about 5x in spite of using what we think are equivalent parameters . A library for simulating tasks that fit in our framework and code for generating all simulation figures in this paper can be found at _CITE_ For the AX - CPT behavior , we compare qualitative patterns from our model to a heterogeneous dataset of humans performing this task ( n = 59 ) across 4 different manipulations with 200 trials per subject [ 24 ]. The manipulations were different variants of “ proactive ”- behavior inducing manipulations in the sense of [ 25 ].__label__Method|Code|Produce
1 . We show how to incorporate this information in Policy Gradient RL [ 30 ], and show that we can improve over RL that has access to the same amount of ground - truth captions . Our code and data will be released ( _CITE_ ) to facilitate more human - like training of captioning models .__label__Method|Code|Produce
1 . We show how to incorporate this information in Policy Gradient RL [ 30 ], and show that we can improve over RL that has access to the same amount of ground - truth captions . Our code and data will be released ( _CITE_ ) to facilitate more human - like training of captioning models .__label__Material|Data|Produce
We evaluated ISA on the 29 UCI datasets that Zheng and Webb used for the evaluation of LBR . On the same datasets , we also evaluated a simple Bayes classifier ( SB ) and LBR . For SB and LBR , we used the Weka implementations ( Weka v3 . 3 . 6 , _CITE_ ) with default settings [ 5 ]. We implemented the ISA algorithm as a standalone application in Java . The following settings were used for ISA : a maximum of 100 phase - 1 models , a threshold T of 0 . 001 in phase - 2 , and an upper limit of 500 models in R . For the parameter priors in Equation 10 , all αijk were set to 1 .__label__Method|Tool|Use
with probability at least 1 — S , where q = maxi I {( i , j ) E Ellis the maximum edge degree in the network , k is the number of classes in a label , and l is the number of labels . Unfortunately , we omit the proof due to lack of space . ( See a longer version of the paper at _CITE_ ) The proof uses a covering number argument analogous to previous results in SVMs [ 13 ]. However we propose a novel method for covering structured problems by constructing a cover to the loss function from a cover of the individual edge basis function differences Afx ( yi , yj ).__label__Supplement|Document|Extent
Through evolution , structure is more conserved than sequence , so that detecting even very subtle sequence similarities , or remote homology , is important for predicting function . The major methods for homology detection can be split into three basic groups : pairwise sequence comparison algorithms [ 1 , 2 ], generative models for protein families [ 3 , 4 ], and discriminative classifiers [ 5 , 6 , 7 ]. Popular sequence comparison methods such as BLAST * Supplemental information for the paper , including the data sets and Matlab source code can be found on this author ’ s web page at _CITE_ and Smith - Waterman are based on unsupervised alignment scores . Generative models such as profile hidden Markov models ( HMMs ) model positive examples of a protein family , but they can be trained iteratively using both positively labeled and unlabeled examples by pulling in close homologs and adding them to the positive set . A compromise between these methods is PSI - BLAST [ 8 ], which uses BLAST to iteratively build a probabilistic profile of a query sequence and obtain a more sensitive sequence comparison score .__label__Method|Code|Produce
Through evolution , structure is more conserved than sequence , so that detecting even very subtle sequence similarities , or remote homology , is important for predicting function . The major methods for homology detection can be split into three basic groups : pairwise sequence comparison algorithms [ 1 , 2 ], generative models for protein families [ 3 , 4 ], and discriminative classifiers [ 5 , 6 , 7 ]. Popular sequence comparison methods such as BLAST * Supplemental information for the paper , including the data sets and Matlab source code can be found on this author ’ s web page at _CITE_ and Smith - Waterman are based on unsupervised alignment scores . Generative models such as profile hidden Markov models ( HMMs ) model positive examples of a protein family , but they can be trained iteratively using both positively labeled and unlabeled examples by pulling in close homologs and adding them to the positive set . A compromise between these methods is PSI - BLAST [ 8 ], which uses BLAST to iteratively build a probabilistic profile of a query sequence and obtain a more sensitive sequence comparison score .__label__Material|Data|Produce
Through evolution , structure is more conserved than sequence , so that detecting even very subtle sequence similarities , or remote homology , is important for predicting function . The major methods for homology detection can be split into three basic groups : pairwise sequence comparison algorithms [ 1 , 2 ], generative models for protein families [ 3 , 4 ], and discriminative classifiers [ 5 , 6 , 7 ]. Popular sequence comparison methods such as BLAST * Supplemental information for the paper , including the data sets and Matlab source code can be found on this author ’ s web page at _CITE_ and Smith - Waterman are based on unsupervised alignment scores . Generative models such as profile hidden Markov models ( HMMs ) model positive examples of a protein family , but they can be trained iteratively using both positively labeled and unlabeled examples by pulling in close homologs and adding them to the positive set . A compromise between these methods is PSI - BLAST [ 8 ], which uses BLAST to iteratively build a probabilistic profile of a query sequence and obtain a more sensitive sequence comparison score .__label__Supplement|Website|Produce
Through evolution , structure is more conserved than sequence , so that detecting even very subtle sequence similarities , or remote homology , is important for predicting function . The major methods for homology detection can be split into three basic groups : pairwise sequence comparison algorithms [ 1 , 2 ], generative models for protein families [ 3 , 4 ], and discriminative classifiers [ 5 , 6 , 7 ]. Popular sequence comparison methods such as BLAST * Supplemental information for the paper , including the data sets and Matlab source code can be found on this author ’ s web page at _CITE_ and Smith - Waterman are based on unsupervised alignment scores . Generative models such as profile hidden Markov models ( HMMs ) model positive examples of a protein family , but they can be trained iteratively using both positively labeled and unlabeled examples by pulling in close homologs and adding them to the positive set . A compromise between these methods is PSI - BLAST [ 8 ], which uses BLAST to iteratively build a probabilistic profile of a query sequence and obtain a more sensitive sequence comparison score .__label__Supplement|Document|Produce
The results are presented in Figure 3 for MuJoCo and Figure 4 for Atari . Training the reward predictor offline can lead to bizarre behavior that is undesirable as measured by the true reward ( Amodei et al ., 2016 ). For instance , on Pong offline training sometimes leads our agent to avoid losing points but not to score points ; this can result in extremely long volleys ( videos at _CITE_ ). This type of behavior demonstrates that in general human feedback needs to be intertwined with RL rather than provided statically . Our main motivation for eliciting comparisons rather than absolute scores was that we found it much easier for humans to provide consistent comparisons than consistent absolute scores , especially on the continuous control tasks and on the qualitative tasks in Section 3 . 2 ; nevertheless it seems important to understand how using comparisons affects performance .__label__Supplement|Media|Produce
· j at the first hidden layer , for every document in both the training and testing sets . Feature learning for binary classification . We consider the 20 newsgroups dataset ( _CITE_ ) that consists of 18 , 774 documents from 20 different news groups , with a vocabulary of size K0 = 61 , 188 . It is partitioned into a training set of 11 , 269 documents and a testing set of 7 , 505 ones . We first consider two binary classification tasks that distinguish between the comp . sys . ibm . pc . hardware and comp . sys . mac . hardware , and between the sci . electronics and sci . med news groups .__label__Material|Data|Use
Each image is first segmented into 800 “ superpixels ”, which are local , coherent and preserve most of the structure necessary for segmentation at the scale of interest [ 19 ]. The software used for over - segmentation is discussed in [ 17 ] and is available online ( http :// www . cs . sfu . ca /∼ mori / research / superpixels /). Each superpixel is represented by both color and texture descriptors , based on the local RGB , hue [ 25 ] feature vectors and also the output of maximum response ( MR ) filter banks [ 22 ] ( _CITE_ ). We discretize these features using a codebook of size 64 ( other codebook sizes gave similar performance ), and then calculate the distribution [ 1 ] for each feature within each superpixel as visual words [ 3 , 6 , 10 , 11 , 20 , 23 , 24 ]. Since each superpixel is represented by three visual words , the mixture atoms θ ∗ j are three multinomial distributions { Mult ( Θ ∗ 1j ) (& Mult ( Θ ∗ 2j ) (& Mult ( Θ ∗ 3j )} for j / = 1 , · · , J .__label__Supplement|Document|Introduce
It has 2944 unique terms , around 179K observed words and an average of 52 unique terms per document . 3 . The NIPS data set contains the NIPS articles published between 1988 - 1999 ( _CITE_ ). It has 5005 unique terms and around 403K observed words . We randomly sample 20 % of the words for each paper and this leads to an average of 150 unique terms per document .__label__Material|Data|Introduce
4 ( d ) shows that increasing the number of trees from 1 to 5 reduces the variance and increases the accuracy , with little improvement beyond this . Here , the number of leaves per tree was kept constant at 1000 , so doubling the number of trees effectively doubles the vocabulary size . We also tested our method on the 2005 Pascal Challenge dataset , _CITE_ This contains four categories , motorbikes , bicycles , people and cars . The goal is to distinguish each category from the others .__label__Material|Data|Use
is the maximum possible pixel value and Qe is the mean - square error between the noisy and original images . We also tested the AMC - SSDA as pre - processing step in an image classification task by corrupting MNIST database of handwritten digits [ 19 ] with various types of noise and then denoising and classifying the digits with a classifier trained on the original images ( Section 4 . 2 ). Our code is available at : _CITE___label__Method|Code|Produce
Meanwhile , our results are achieved at a test - time speed of 170ms per image using ResNet - 101 , which is 2 . 5x to 20x faster than the Faster R - CNN + ResNet - 101 counterpart in [ 10 ]. These experiments demonstrate that our method manages to address the dilemma between invariance / variance on translation , and fully convolutional image - level classifiers such as ResNets can be effectively converted to fully convolutional object detectors . Code is made publicly available at : _CITE___label__Method|Code|Produce
The Adult and Web data sets have been obtained from I . Platt ' s web page http :// research . microsoft . comf jplatt / smo . htmY ; the Gauss - M data set is a two dimensional classification problem proposed in [ 3 ] to test neural networks , which comprises a gaussian random variable for each class , which highly overlap . The Banana , Diabetes and Splice data sets have been obtained from Gunnar Ratsch web page http : llsvm . first . gmd . defraetsch /. The selection of C and the RKHS has been done as indicated in [ 11 ] for Adult and Web data sets and in _CITE_ for Banana , Diabetes and Splice data sets . In Table 3 , we show the runtime complexity for each data set , where the value of q has been elected as the one that reduces the runtime complexity .__label__Material|Data|Use
We find ULURU quickly converges to the OLS estimate , although it is not able to overcome the bias induced by the corrupted datapoints despite its two - step procedure . The performance of IWS - LS relative to OLS in the airline delay problem suggests that the corrupted observation model is a more realistic modelling scenario than the standard sub - Gaussian design model for some tasks . Software is available at _CITE_ Acknowledgements . We thank David Balduzzi , Cheng Soon Ong and the anonymous reviewers for invaluable discussions , suggestions and comments .__label__Method|Tool|Produce
Experimental results are presented in Section 4 . All the technical proofs are relegated to the Appendices . Our code is available at _CITE___label__Method|Code|Produce
We trained both the EM and the margin - based formulations , using between 2 and 128 labeled points , treating all remaining points as unlabeled . We trained on 20 random splits balanced for class labels , and tested on a fixed separate set of 987 points . Results in figure 2 show that Markov random walk based algorithms have & apos ; Processed as 20news - 18827 , _CITE_ , removing rare words , duplicate documents , and performing tf - idf mapping . a clear advantage over the best SVM using only labeled data ( which had a linear kernel and = 3 ), out of linear and Gaussian kernels , different kernel widths and values of . The advantage is especially noticeable for few labeled points , but decreases thereafter .__label__Material|Data|Use
The WI observations span from January 1 , 2006 through December 31 , 2009 , and the PNW observations span from February 4 , 2006 through February 3 , 2010 . We have data from 21 and 24 sites in WI and PNW , respectively . This data is available at _CITE_ We collect wind direction and wind speed at each site , as measured at 10 meters above ground level . Since our primary motivation is wind power forecasting , we prefer wind speed measurements taken at turbine height ( approximately 50 - 100 meters above ground level ).__label__Material|Data|Use
In this case , EVD and c - EVD errors were similar , but PCP and Alt - Min - RPCA errors were less than 10 − 5 . For our second experiment , we used images of a low - rankified real video sequence as it ’ s . We chose the escalator sequence from _CITE_ since the video changes are only in the region where the escalator moves ( and hence can be modeled as being sparse ). We made it exactly low - rank by retaining its top 5 eigenvectors and projecting onto their subspace . This resulted in a data matrix L of size n × 2α with n = 20800 and α = 50 , of rank r = 5 .__label__Material|Data|Use
We first focus on the logistic regression task and compare the performance of the bound ( using the low - rank Algorithm 2 ) with first - order and second order methods such as LBFGS , conjugate gradient ( CG ) and steepest descent ( SD ). We use 4 benchmark data - sets : the SRBCT and Tumors data - sets from [ 29 ] as well as the Text and SecStr data - sets from _CITE_ For all experiments in this section , the setup is as follows . Each data - set is split into training ( 90 %) and testing ( 10 %) parts .__label__Material|Data|Use
Proof of concept : Key Influencers in Theoretical Physics : Drawn from a KDD Cup 2003 task , this datasetis publically available at : _CITE_ It consists of the latex sources of all papers in the hep - th portion of the arXiv ( http :// arxiv . org ) In consultation with a theoretical physicist we did our analysis at a time granularity of 1 month . In total , the data spans 137 months .__label__Material|Data|Use
To evaluate the proposed approach we conducted experiments on multilabel text classification data that has a natural hierarchy defined over the label set . In particular , we investigated three multilabel text classification data sets , Enron , WIPO and Reuters , obtained from _CITE_ ( see Table 1 for details ). Some preprocessing was performed on the label relations to ensure consistency with our assumptions . In particular , all implications were added to each instance to ensure consistency with the hierarchy , while mutual exclusions were defined between siblings whenever this did not create a contradiction .__label__Material|Data|Use
We first consider a data set of 300 handwritten ‘ p ’ s recorded using an INTUOS 3 WACOM digitisation tablet _CITE_ , providing trajectory data at 200Hz . The trajectory Yt we model is the normalised first differential of the data , so that the data mean was close to zero , providing the requirements for the zero state assumption in the model constraints . Three dimensional data was used , x - position , y - position , and pressure .__label__Method|Tool|Use
We first consider a data set of 300 handwritten ‘ p ’ s recorded using an INTUOS 3 WACOM digitisation tablet _CITE_ , providing trajectory data at 200Hz . The trajectory Yt we model is the normalised first differential of the data , so that the data mean was close to zero , providing the requirements for the zero state assumption in the model constraints . Three dimensional data was used , x - position , y - position , and pressure .__label__Material|Data|Use
This approach was entitled multiple kernel learning ( MKL ). Research in the subsequent years focused on speeding up the initially demanding optimization algorithms [ 22 , 26 ]— ignoring the fact that empirical evidence for the superiority of MKL over trivial baseline approaches ( not optimizing the kernel ) was missing . In 2008 , negative results concerning the accuracy of MKL in practical applications accumulated : at the NIPS 2008 MKL workshop [ 6 ] several researchers presented empirical evidence showing that traditional MKL rarely helps in practice and frequently is outperformed by a regular SVM using a uniform kernel combination , see _CITE_ Subsequent research ( e . g ., [ 10 ]) revealed further negative evidence and peaked in the provocative question “ Can learning kernels help performance ?” posed by Corinna Cortes in an invited talk at ICML 2009 [ 5 ]. Consequently , despite all the substantial progress in the field of MKL , there remained an unsatisfied need for an approach that is really useful for practical applications : a model that has a good chance of improving the accuracy ( over a plain sum kernel ).__label__Method|Algorithm|Introduce
The PROP - diff , PROP - WL and the WL kernel were each run with 10 iterations . In the CSM kernel , the clique size parameter was set to k = 5 . Our kernel implementations and datasets ( with the exception of AIRWAYS ) can be found at _CITE_ Classification experiments were made on four datasets : ENZYMES , PROTEINS , AIRWAYS and SYNTHETIC . ENZYMES and PROTEINS are sets of proteins from the BRENDA database [ 22 ] and the dataset of Dobson and Doig [ 23 ], respectively .__label__Method|Code|Produce
The PROP - diff , PROP - WL and the WL kernel were each run with 10 iterations . In the CSM kernel , the clique size parameter was set to k = 5 . Our kernel implementations and datasets ( with the exception of AIRWAYS ) can be found at _CITE_ Classification experiments were made on four datasets : ENZYMES , PROTEINS , AIRWAYS and SYNTHETIC . ENZYMES and PROTEINS are sets of proteins from the BRENDA database [ 22 ] and the dataset of Dobson and Doig [ 23 ], respectively .__label__Material|Data|Produce
Section 7 concludes the paper . Due to space limit , we omit all detailed proofs . A complete version of this work is available at _CITE___label__Supplement|Document|Produce
Experimental results and a discussion conclude the paper . Some proofs have been omitted due to space constraints . They can be found in an extended version of this paper ( available at_CITE_ ).__label__Supplement|Document|Produce
The advantage of being able to remember is much smaller here due to the speed at which the KT base model can learn , although not insignificant . It is also worth noting that a performance improvement is obtained even though each individual observation is by itself not informative ; the FMN process is exploiting the statistical similarity of the outcomes across time . ( Online Task Identification ) A video demonstrating real - time segmentation of Atari frames can be found at : _CITE_ Here we see that the ( low complexity ) FMN quickly learns 45 game specific models , and performs an excellent job of routing experience to the appropriate model . These results provide evidence that this technique can scale to long , high dimensional input sequences using state of the art density models .__label__Supplement|Media|Produce
As long as one head k imagines Q (˜ N , 2 ) & gt ; Q (˜ N , 2 ) then TD bootstrapping can propagate this signal back to s = 1 through the target network to drive deep exploration . The expected time for these estimates at n to propagate to at least one head grows gracefully in n , even for relatively small K , as our experiments show . We expand upon this intuition with a video designed to highlight how bootstrapped DQN demonstrates deep exploration _CITE_ We present further evaluation on a difficult stochastic MDP in Appendix C .__label__Supplement|Media|Produce
One would hope that extra structure should allow us to obtain more statistically efficient solutions . In this work , we focus on the case of bandable precision matrices , which capture ∗ Addison graduated from Yale in May 2017 . Up - to - date contact information may be found at _CITE___label__Supplement|Document|Produce
Specifically , the translation that required adding more details to the image was usually harder ( e . g . night to day ). Additional results are available in _CITE_ Synthetic to real . In Figure 3 , we showed several example results achieved by applying the proposed framework to translate images between the synthetic images in the SYNTHIA dataset [ 23 ] and the real images in the Cityscape dataset [ 2 ].__label__Supplement|Document|Produce
We conducted experiments with stock price data obtained from _CITE_ We downloaded data for the following stocks : MSFT , HPQ and WMT . The data consists of trades made throughout a given date in chronological order .__label__Material|Data|Use
For the eigenspace representation , each target image region is resized to 32 × 32 patch , and the number of eigenvectors used in all experiments is set to 16 though fewer eigenvectors may also work well . Implemented in MATLAB with MEX , our algorithm runs at 4 frames per second on a standard computer with 200 particles . We present some tracking results in this section and more tracking results as well as videos can be found at _CITE___label__Supplement|Document|Produce
Thus , a total of 20 , 000 AMT HITs were collected , and a total of 100 , 000 images were shown to the participants . On average , each participant took approximately one minute to finish each HIT , spending about 3 seconds per query image . The dataset is publicly available at _CITE___label__Material|Data|Use
Here we propose a diagonal + rank 1 approximation . Our diagonal + rank 1 quasi - Newton forward - backward splitting algorithm is listed in Algorithm 1 ( with details for the quasi - Newton update in Algorithm 2 , see § 4 for details ). These algorithms are listed as simply as possible to emphasize their important components ; the actual software used for numerical tests is open - source and available at _CITE___label__Method|Tool|Use
We trained the networks using an implementation based on Caffe [ 25 ]. Details on the training , the hyperparameter settings , and an analysis of the performance depending on the network architecture is provided in the supplementary material . Our code and training data are available at _CITE_ . We applied the feature representation to images of arbitrary size by convolutionally computing the responses of all the network layers except the top softmax . To each feature map , we applied the pooling method that is commonly used for the respective dataset : 1 ) 4 - quadrant max - pooling , resulting in 4 values per feature map , which is the standard procedure for STL - 10 and CIFAR - 10 [ 26 , 10 , 27 , 12 ]; 2 ) 3 - layer spatial pyramid , i . e .__label__Method|Code|Produce
We trained the networks using an implementation based on Caffe [ 25 ]. Details on the training , the hyperparameter settings , and an analysis of the performance depending on the network architecture is provided in the supplementary material . Our code and training data are available at _CITE_ . We applied the feature representation to images of arbitrary size by convolutionally computing the responses of all the network layers except the top softmax . To each feature map , we applied the pooling method that is commonly used for the respective dataset : 1 ) 4 - quadrant max - pooling , resulting in 4 values per feature map , which is the standard procedure for STL - 10 and CIFAR - 10 [ 26 , 10 , 27 , 12 ]; 2 ) 3 - layer spatial pyramid , i . e .__label__Material|Data|Produce
We tested PEA regularization in three scenarios : supervised learning on MNIST digits , semi - supervised learning on MNIST digits , and semi - supervised transfer learning on a dataset from the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models [ 13 ]. Full implementations of our methods , written with THEANO [ 3 ], and scripts / instructions for reproducing all of the results in this section are available online at : _CITE___label__Method|Code|Produce
4 . The Conf . abstracts set data contains abstracts ( including papers and posters ) from six international conferences : CIKM , ICML , KDD , NIPS , SIGIR and WWW ( _CITE_ ). It has 3733 unique terms , around 173K observed words and an average of 46 unique terms per document . The data are from 2005 - 2008 .__label__Material|Data|Introduce
Our experiments included evaluations on several classification datasets , segmentation databases and simulations . Due to space limitations , we provide a brief summary here . Our first set of experiments illustrates an application to several datasets from the UCI Machine Learning Repository : ( 1 ) Iris dataset , ( 2 ) Soybean dataset and ( 3 ) Wine dataset ; these include ground truth data , see _CITE_ . To create the ensemble , we used a set of [ 4 , 10 ] clustering schemes ( by varying the clustering criterion and / or algorithm ) from the CLUTO clustering toolkit . The multiple solutions comprised the input ensemble , our model was then used to determine a agreement maximizing solution .__label__Material|Data|Use
In this subsection , we use four benchmark datasets for the evaluation . There are one document dataset and three gene expression datasets participating in the experiment , the property of which is introduced in details as below . Reuters21578 dataset is processed and downloaded from _CITE_ It contains 8293 documents in 65 topics . Each document is depicted by its frequency on 18933 terms .__label__Material|Data|Use
Experiments are performed on two real - world data sets : subsets of Microsoft Research ( MSRC ) data ( _CITE_ ) and UIUC - Sport data from [ 15 , 16 ], the latter images originally obtained from the Flickr website and available online ( http :// vision . cs . princeton . edu / lijiali / event dataset /). For the MSRC dataset , 10 categories of images with manual annotations are selected : “ tree ”, “ building ”, “ cow ”, “ face ”, “ car ”, “ sheep ”, “ flower ”, “ sign ”, “ book ” and “ chair ”. The number of images in the “ cow ” class is 45 , and in the “ sheep ” class there are 35 ; there are 30 images in all other classes .__label__Material|Data|Use
Finally , using our method – which we dub Bayesian Optimization with Hamiltonian Monte Carlo Artificial Neural Networks ( BOHAMIANN ) – we demonstrate state - of - the - art performance for a wide range of optimization tasks . This includes multi - task BO , parallel optimization of deep residual networks , and deep reinforcement learning . An implementation of our method can be found at _CITE___label__Method|Code|Produce
Videos of these behaviors can be found at _CITE_ These behaviors were trained using feedback from the authors .__label__Supplement|Media|Produce
We also find that a low rank version is able to achieve approximately 23 × compression of the original data , with the optimal solution very close to the full rank optimum . Our method is a superior predictor to the existing regional model for visual system data , and the success of the low rank version suggests that this approach will be able to reveal whole - brain structural connectivity at unprecedented scale . All of our supplemental material and data processing and optimization code is available for download from : _CITE___label__Method|Code|Produce
We also find that a low rank version is able to achieve approximately 23 × compression of the original data , with the optimal solution very close to the full rank optimum . Our method is a superior predictor to the existing regional model for visual system data , and the success of the low rank version suggests that this approach will be able to reveal whole - brain structural connectivity at unprecedented scale . All of our supplemental material and data processing and optimization code is available for download from : _CITE___label__Material|Data|Produce
We also find that a low rank version is able to achieve approximately 23 × compression of the original data , with the optimal solution very close to the full rank optimum . Our method is a superior predictor to the existing regional model for visual system data , and the success of the low rank version suggests that this approach will be able to reveal whole - brain structural connectivity at unprecedented scale . All of our supplemental material and data processing and optimization code is available for download from : _CITE___label__Supplement|Document|Produce
This work was partially supported by NSF CAREER award IIS – 0546857 ( MACP ), NSF IIS – 0535140 and EC MCEXT – 025481 ( CS ). CMU data : _CITE_ ( created with funding from NSF EIA – 0196217 ). OSU data : http :// accad . osu . edu / research / mocap / mocap data . htm .__label__Material|Data|Use
Moreover , efficient algorithms are available to generate Pólya - gamma random variates [ 21 ]. Our Gibbs updates for the remaining parameters and latent variables ( νn , un , vn , and B ) are described in the supplementary material . A Python implementation of our inference algorithm is available at _CITE___label__Method|Code|Produce
And on a 10 %- subsample of the CIFAR - 10 dataset , we achieve a 4 . 0 accuracy point gain over a standard heuristic augmentation approach and are competitive with comparable semi - supervised approaches . Additionally , we show empirical results suggesting that the proposed approach is robust to misspecified TFs . Our hope is that the proposed method will be of practical value to practitioners and of interest to researchers , so we have open - sourced the code at _CITE___label__Method|Code|Produce
Section 4 describes our helicopter platform and our experimental results . Section 5 concludes the paper . Movies of our autonomous helicopter flights are available at the following webpage : _CITE___label__Supplement|Media|Produce
3 . The results of the two algorithms seem qualitatively similar , while Fast - VDP computed its results much faster than VDP . In a second real data experiment we clustered documents from citeseer ( _CITE_ ). The dataset has 30 , 696 documents , with a vocabulary size of 32 , 473 words .__label__Material|Data|Use
3 . The results of the two algorithms seem qualitatively similar , while Fast - VDP computed its results much faster than VDP . In a second real data experiment we clustered documents from citeseer ( _CITE_ ). The dataset has 30 , 696 documents , with a vocabulary size of 32 , 473 words .__label__Supplement|Website|Use
However , COCO - QA questions are actually easier to answer than DAQUAR from a human point of view . This encourages the model to exploit salient object relations instead of exhaustively searching all possible relations . COCO - QA dataset can be downloaded at _CITE___label__Material|Data|Produce
In home scenes we obtained a precision of 83 . 12 % and 70 . 03 % recall , and in the office scenes we obtain 87 . 92 % precision and 71 . 93 % recall . Object Detection : We finally use our algorithm on two mobile robots , mounted with Kinects , for completing the goal of finding objects such as a keyboard in cluttered office scenes . The following video shows our robot successfully finding a keyboard in an office : _CITE_ In conclusion , we have proposed and evaluated the first model and learning algorithm for scene understanding that exploits rich relational information from the full - scene 3D point cloud . We applied this technique to object labeling problem , and studied affects of various factors on a large dataset . Our robotic application shows that such inexpensive RGB - D sensors can be extremely useful for scene understanding for robots .__label__Supplement|Media|Produce
More details concerning the experimental setup can be found at http :// www1 . cs . columbia . edu / compbio / svm - pairwise . In all experiments , we use an SVM classifier with a small soft margin parameter , set as in [ 7 ] . The SVM computations are performed using the freely available Spider Matlab machine learning package available at _CITE_ More information concerning the experiments , including data and source code scripts , can be found at http :// www . kyb . tuebingen . mpg . de / bs / people / weston / semiprot . Semi - supervised setting .__label__Method|Code|Use
We fix the hyper - parameters , but resampling techniques [ 2 ] can easily be incorporated . All results are averaged over 10 sample paths . Source code can be downloaded from _CITE___label__Method|Code|Produce
This work was supported by NSF CCF - 0830780 , CCF - 0917274 , DMS0915228 , and IIS - 1117965 at UTA ; and by NSF IIS - 1117335 , NIH R01 LM011360 , UL1 RR025761 , U01 AG024904 , RC2 AG036535 , R01 AG19771 , and P30 AG10133 - 18S1 at IU . Data used in the work were obtained from the ADNI database . ADNI funding information is available at _CITE_ to apply / ADNI DSP Policy . pdf .__label__Supplement|Document|Introduce
To illustrate the general idea , a first example of GIOHMM is provided by the bidirectional IOHMMs ( Figure 1 ) introduced in [ 2 ] to process sequences and predict protein structural features , such as secondary structure . Unlike standard HMMs or IOHMMS used , for instance in speech recognition , this architecture is based on two hidden markov chains running in opposite directions to leverage the fact that biological sequences are spatial objects rather than temporal sequences . Bidirectional IOHMMs have been used to derive a suite of structural feature predictors [ 12 , 13 , 4 ] available through _CITE_ These predictors have accuracy rates in the 75 - 80 % range on a per amino acid basis .__label__Method|Algorithm|Introduce
The use of soft attention over both types of inputs made strong generalization possible . In particular , on a family of block stacking tasks , our neural network policy was able to perform well on novel block configurations which were not present in any training data . Videos of our experiments are available at _CITE___label__Supplement|Media|Produce
Soccer data source : _CITE_ We modeled player motion using ( 4 ) with F and Q derived from an NCV model [ 1 , Ch . 1 . 5 ].__label__Method|Code|Produce
All the experiments used the ADAM optimizer [ 29 ], and the detailed experimental set - up ( batch size , learning rate , etc .) can be found in the appendix . The implementation of all the experiments in Python is released at _CITE_ fS ( θ ). The second proposal also applies this subset average likelihood approxmass - covering zero - forcing__label__Method|Code|Produce
Moreover , pKBR and kRegBayes run faster than KBR . The total running times for 50 random datasets of pKBR , kRegBayes and KBR are respectively 601 . 3s , 677 . 5s and 3667 . 4s . Camera position recovery In this experiment , we build a scene containing a table and a chair , which is derived from classchair . pov ( _CITE_ ). With a fixed focal point , the position of the camera uniquely determines the view of the scene . The task of this experiment is to estimate the position of the camera given the image .__label__Supplement|Website|Extent
We have borrowed the chunking and shrinking ideas from the SVAilIght [ 6 ] for our computer program . To test these two programs several data sets have been used . The Adult and Web data sets have been obtained from I . Platt ' s web page _CITE_ ; the Gauss - M data set is a two dimensional classification problem proposed in [ 3 ] to test neural networks , which comprises a gaussian random variable for each class , which highly overlap . The Banana , Diabetes and Splice data sets have been obtained from Gunnar Ratsch web page http : llsvm . first . gmd . defraetsch /. The selection of C and the RKHS has been done as indicated in [ 11 ] for Adult and Web data sets and in http :// svm . first . gmd . der raetsch / for Banana , Diabetes and Splice data sets .__label__Material|Data|Use
We have borrowed the chunking and shrinking ideas from the SVAilIght [ 6 ] for our computer program . To test these two programs several data sets have been used . The Adult and Web data sets have been obtained from I . Platt ' s web page _CITE_ ; the Gauss - M data set is a two dimensional classification problem proposed in [ 3 ] to test neural networks , which comprises a gaussian random variable for each class , which highly overlap . The Banana , Diabetes and Splice data sets have been obtained from Gunnar Ratsch web page http : llsvm . first . gmd . defraetsch /. The selection of C and the RKHS has been done as indicated in [ 11 ] for Adult and Web data sets and in http :// svm . first . gmd . der raetsch / for Banana , Diabetes and Splice data sets .__label__Supplement|Website|Use
In our platform , no clock is provided for model evaluations thus all arithmetics need to be executed in pure combinational logic . Taking advantage of combinational logic allows all model evaluations to be 1 ) fast , the evaluation time depends entirely on the propagating and settling time of signals , which is on the order of microseconds , and 2 ) parallel , each model is evaluated on its own circuit without waiting for any other results . Our implementations of adder and multiplier are inspired by the open source project “ Free FloatingPoint Madness ”, available at _CITE_ Please contact the authors of this paper if the modified code is needed .__label__Method|Code|Extent
It is , therefore , interesting to see how the minimum degree of rate fluctuation depends on the non - Poissonian feature of spike trains . In this study , we investigate the extent to which the non - Poissonian feature of spike trains affects the encoding efficiency of rate fluctuations . In addition , we address the question of how the de ∗ _CITE_ tectability of rate fluctuations depends on the encoding efficiency . For this purpose , we introduce the Kullback - Leibler ( KL ) divergence to measure the encoding efficiency , and assume that spike sequences are generated by time - rescaled renewal processes . With the aid of analytical and numerical studies , we suggest that the lower bound of detectable rate fluctuations , below which the empirical Bayes decoder cannot detect the rate fluctuations , is uniquely determined by the KL divergence .__label__Supplement|Document|Produce
We consider first a simple classification problem where the goal is to classify whether a particular book is of liberal political inclination or not . The features of each book are given by the words in the Amazon . com front page for that particular book . The choice of books , labels , and relationships are given in the data collected by Valdis Krebs and available at _CITE_ . The data containing book features can be found at http :// www . statslab . cam . ac . uk /- silva . There are 105 books , 43 of which are labeled as liberal books .__label__Material|Data|Use
We thank the Gatsby Charitable Foundation for generous funding . We thank Ryan Adams and Iain Murray for code and comments ; and Jakob Macke and Lars Buesing for useful discussions . The grasshopper data was collected by Ariel Rokem at Andreas Herz ’ s lab and provided through the CRCNS program ( _CITE_ ).__label__Method|Tool|Use
We thank the Gatsby Charitable Foundation for generous funding . We thank Ryan Adams and Iain Murray for code and comments ; and Jakob Macke and Lars Buesing for useful discussions . The grasshopper data was collected by Ariel Rokem at Andreas Herz ’ s lab and provided through the CRCNS program ( _CITE_ ).__label__Material|Data|Use
The random working set selection from the samples not fulfilling the KKT conditions is the best option if the working is be large , because it reduces the number of chunks to be solved . This strategy benefits from the IRWLS procedure , which allows to work with large training data set . All these modifications have been concreted in the SVC " tht solving procedure , publicly available at _CITE___label__Method|Algorithm|Produce
In our final set of experiments we employed KHA to denoise a human walking motion trajectory from the CMU motion capture database ( _CITE_ ), converted to Cartesian coordinates via Neil Lawrence ’ s Matlab Motion Capture Toolbox ( http :// www . dcs . shef . ac . uk /∼ neil / mocap /). The experimental setup was similar to that of Tangkuampien and Suter [ 15 ]: Gaussian noise was added to the frames of the original motion , then KHA with 25 PCs was used to denoise them . The results are shown in Figure 4 .__label__Material|Data|Use
If required the FGβ score can be converted back to an Fβ score at the end . The second recommendation is to use Precision - Recall - Gain curves instead of PR curves , and the third to use AUPRG which is easier to calculate than AUPR due to linear interpolation , has a proper interpretation as an expected F - Gain score and allows performance assessment over a range of operating points . To assist practitioners we have made R , Matlab and Java code to calculate AUPRG and PRG curves available at _CITE_ We are also working on closer integration of AUPRG as an evaluation metric in OpenML and performance visualisation platforms such as ViperCharts [ 15 ]. As future work we mention the interpretation of AUPRG as a measure of ranking performance : we are working on an interpretation which gives non - uniform weights to the positives and as such is related to Discounted Cumulative Gain .__label__Method|Code|Produce
Figure 3 shows results for a multi - view experiment where the task is two distinguish between two different views of a car and background . 3The images were obtained from http :// www . vision . caltech . edu / html - files / archive . html and the car side images from http :// l2r . cs . uiuc . edu / cogcomp / Data / Car /. Notice , that since our algorithm does not currently allow for the recognition of multiple instances of an object we test it on a partition of the the training set in _CITE_ and not on the testing set in that site . The animals data set is a subset of Caltech ’ s 101 categories data set .__label__Material|Data|Use
We presented , in a sadly condensed fashion , three novel learning algorithms for symbol dynamics . A detailed treatment of the Efficiency Sharpening algorithm is given in [ 2 ], and a Matlab toolbox for it can be fetched from _CITE_ The numerical investigations reported here were done using this toolbox . Our numerical simulations demonstrate that there is an altogether new world of faster and often statistically more efficient algorithms for sequence modelling than Baum - Welch / SE - HMMs .__label__Method|Tool|Use
The features of each book are given by the words in the Amazon . com front page for that particular book . The choice of books , labels , and relationships are given in the data collected by Valdis Krebs and available at http :// www - personal . umich . edu / mejn / netdata . The data containing book features can be found at _CITE_ There are 105 books , 43 of which are labeled as liberal books . The relationships are pairs of books which are frequently purchased together by a same customer .__label__Material|Data|Produce
We present experimental results on four publicly available datasets : the bouncing balls [ 9 ], polyphonic music [ 10 ], motion capture [ 7 ] and state - of - the - Union [ 30 ]. To assess the performance of the TSBN model , we show sequences generated from the model , and report the average log - probability that the model assigns to a test sequence , and the average squared one - step - ahead prediction error per frame . Code is available at _CITE_ The TSBN model with W3 = 0 and W4 = 0 is denoted Hidden Markov SBN ( HMSBN ), the deep TSBN with stochastic hidden layer is denoted DTSBN - S , and the deep TSBN with deterministic hidden layer is denoted DTSBN - D . Model parameters were initialized by sampling randomly from N ( 0 , 0 . 0012I ), except for the bias parameters , that were initialized as 0 . The TSBN model is trained using a variant of RMSprop [ 6 ], with momentum of 0 . 9 , and a constant learning rate of 10 − 4 .__label__Method|Code|Produce
A lower value for y pushes up the confidence of true positive examples , allowing the model more examples to learn from , is thus a way to deal with ground - truth false negatives . Although some of the selection of these parameters is a bit ad - hoc , we assert that our results still provide a good first - pass baseline approach for this dataset . The code is available at _CITE_ During training , we input one day ’ s simulation at a time ( 8 time steps ; 16 variables ). The semisupervised experiments reconstruct all 8 time steps , predicting bounding boxes for the 4labelled timesteps , while the supervised experiments reconstruct and predict bounding boxes only for the 4 labelled timesteps . Table 3 shows Mean Average Precision ( mAP ) for each experiment .__label__Method|Code|Produce
The system is scaleable to very large problems with very large weight arrays . Current research is aimed at showing that the system is scaleable , evaluating methods for the acceleration of the pre - and post processing tasks and considering greater integration of the elements of the processor through VLSI . For more details of the AURA project and the hardware described in this paper see _CITE___label__Supplement|Document|Produce
We then apply our algorithm for computing LSA on the entire Wikipedia – a computation task hitherto not possible with state of the art algorithms . We see this work as a theoretical foundation and practical toolbox for a range of dimensionality reduction problems , and we believe that our algorithms will be used to construct many other coresets in the future . Our project codebase is open - sourced and can be found here : _CITE___label__Method|Code|Produce
This work was partially supported by NSF CAREER award IIS – 0546857 ( MACP ), NSF IIS – 0535140 and EC MCEXT – 025481 ( CS ). CMU data : http :// mocap . cs . cmu . edu ( created with funding from NSF EIA – 0196217 ). OSU data : _CITE_ data . htm .__label__Material|Data|Use
For each object in a mini - batch , we include projections from all 24 views as supervision . The models including the perspective transformer nets are implemented using Torch [ 3 ]. To download the code , please refer to the project webpage : _CITE_ Experimental Design . As mentioned in the formulation , there are several variants of the model depending on the hyper - parameters of learning objectives λpT ,, j and λ ,,,,.__label__Method|Code|Produce
For each object in a mini - batch , we include projections from all 24 views as supervision . The models including the perspective transformer nets are implemented using Torch [ 3 ]. To download the code , please refer to the project webpage : _CITE_ Experimental Design . As mentioned in the formulation , there are several variants of the model depending on the hyper - parameters of learning objectives λpT ,, j and λ ,,,,.__label__Supplement|Website|Produce
It should be clear that we have left several important questions open . Perhaps the most obvious such question concerns the use of non - information theoretic objective functions . It turns out that many of our results apply with only modest changes if the experiment is instead designed to minimize something like the Bayes mean - square error ( perhaps defined only locally if O has a nontrivial manifold structure ), for example : in this case , the results in sections 3 . 1 and 3 . 2 remain completely unchanged , while the statement of our main theorem requires only slight changes in the asymptotic variance formula ( see _CITE_ ). Thus it seems our results here can add very little to any discussion of what objective function is “ best ” in general . We briefly describe a few more open research directions below .__label__Method|Algorithm|Compare
A number of heuristics recommended in [ 33 ] that greatly improve the running time in practice have also been incorporated . Despite the changes , our implementation is theoretically correct and also outputs an upper bound on the error by giving a feasible point to the dual program . Our implementation is available at _CITE_ In the figure , we plot average running times ( with error bars denoting standard deviation ) for B2 - Isotonic Regression on DAGs , where the underlying graphs are 2 - d grid graphs and random regular graphs ( of constant degree ). The edges for 2 - d grid graphs are all oriented towards one of the corners .__label__Method|Code|Produce
In practice we initialize our VBEM algorithm with a Laplace - EM algorithm , and we initialize each E - step in VBEM with a Laplace approximation , which empirically gives substantial runtime advantages , and always produces a sensible optimum . With the above steps , we have a fully specified learning and inference algorithm , which we now use to analyze real neural data . Code can be found at _CITE___label__Method|Code|Produce
In area MT , neurons preferentially respond to the movement directions of visual inputs [ 21 ]. We analyzed the neural data recorded from area MT of a rhesus monkey when random dots were presented . These neural data are available from the Neural Signal Archive ( _CITE_ ), and detailed experimental setups are described by Britten et al . [ 22 ].__label__Material|Data|Use
As additional evaluations , we also tested our model on the SUPPORT2 and RHC datasets ( available at _CITE_ ), which record the survival time for patients hospitalized with severe illnesses . SUPPORT2 contains over 9000 patients ( 32 % censored ) while RHC contains over 5000 patients ( 35 % censored ). Table 4 ( top ) shows the MSE on survival probability prediction over the SUPPORT2 dataset and RHC dataset ( we omit classification accuracy due to lack of space ).__label__Material|Data|Use
Evaluations on four diverse tasks clearly show the model outperforms models without communication , fully - connected models , and models using discrete communication . Despite the simplicity of the broadcast channel , examination of the traffic task reveals the model to have learned a sparse communication protocol that conveys meaningful information between agents . Code for our model ( and baselines ) can be found at _CITE_ One aspect of our model that we did not fully exploit is its ability to handle heterogenous agent types and we hope to explore this in future work . Furthermore , we believe the model will scale gracefully to large numbers of agents , perhaps requiring more sophisticated connectivity structures ; we also leave this to future work .__label__Method|Code|Produce
Thus , the total database consists of 96 digit utterances . The specifics of this database are explained in ( Movellan , 1995 ). The database is available at _CITE_ Visual processing We have tried a wide variety of visual processing approaches on this database , including decomposition with local Gaussian templates ( Movellan , 1995 ), PCA - based templates ( Gray , Movellan & Sejnowski , 1997 ), and Gabor energy templates ( Movellan & Prayaga , 1996 ). To date , best performance was achieved with the local Gaussian approach .__label__Material|Data|Introduce
In the resulting data set , there are 204 patients with available genomic and clinical biomarker data . We implement kernel k - means clustering and its multiview variants in Matlab . Our implementations are publicly available at _CITE_ We solve the QP problems of the multiview variants using the Mosek optimization software ( Mosek , 2014 ). For all methods , we perform 10 replications of k - means with different initializations as the last step and use the solution with the lowest sum - of - squares cost to decide cluster memberships .__label__Method|Code|Produce
OST performs comparably or slightly inferiorly to PLCA but with an impressive gain in computational time (- 3000x speedup ). Best overall performance is obtained with OSTe + noise with an average - 10 % performance gain over PLCA and - 750x speedup . A Python implementation of OST and real - time demonstrator are available at _CITE___label__Method|Code|Produce
All the results shown in Table I are averaged over 10 random permutations of the training sequence . The columns marked 6Notice that B and C in part I do not satisfy this relationship . & apos ; Available on Y . LeCun & apos ; s home page : _CITE_ & quot ; Corn & apos ; s & quot ; give the total number of corrections made in the training phase for the 10 labels . The first three rows of Table 1 are taken from [ 4 , 12 , 13 ].__label__Supplement|Website|Produce
In future , it would be interesting to see how the proposed approach scales with more complex environments , diverse object collections , different manipulation skills and to other non - manipulation based tasks , such as navigation . Other directions for future investigation include the use of forward model for planning and developing better strategies for data collection than random interaction . Supplementary Materials : and videos can be found at _CITE_ Acknowledgement : We thank Alyosha Efros for inspiration and fruitful discussions throughout this work . The title of this paper is partly influenced by the term “ pokebot & quot ; that Alyosha has been using for several years .__label__Supplement|Document|Produce
In future , it would be interesting to see how the proposed approach scales with more complex environments , diverse object collections , different manipulation skills and to other non - manipulation based tasks , such as navigation . Other directions for future investigation include the use of forward model for planning and developing better strategies for data collection than random interaction . Supplementary Materials : and videos can be found at _CITE_ Acknowledgement : We thank Alyosha Efros for inspiration and fruitful discussions throughout this work . The title of this paper is partly influenced by the term “ pokebot & quot ; that Alyosha has been using for several years .__label__Supplement|Media|Produce
Yet it is easy to see how our algorithm could be extended to other , much better performing models . For instance , models in which multiple sources are modeled jointly by a multivariate GSM , or bilinear models with two sets of latent variables . Code for training and evaluating overcomplete linear models is available at _CITE___label__Method|Code|Use
CDM was learnt for a Japanese OCR environment . Specifically , there were 3018 functions f in the environment F , each one a classifier for a different Kanji character . A database containing 90 , 918 segmented , machine - printed Kanji characters scanned from various sources was purchased from the CEDAR group at the State University of New York , Buffalo The quality of the images ranged from clean to very degraded ( see _CITE_ The main reason for choosing Japanese OCR rather than English OCR as a test - bed was the large number of distinct characters in Japanese . Recall from Theorem 2 that to get good generalisation from a learnt CDM , sufficiently many functions must be sampled from the environment . If the environment just consisted of English characters then it is likely that & quot ; sufficiently many & quot ; characters would mean all characters , and so it would be impossible to test the learnt CDM on novel characters not seen in training .__label__Material|Data|Introduce
We studied the performance of the nonequilibrium marginal likelihood estimators on various challenging probabilistic models including Markov random fields and Gaussian mixture models . A python package implementing the work simulations and evidence estimators can be downloaded from _CITE___label__Method|Code|Use
4 on a multiclass categorization task and compared them to previously studied algorithms for multiclass categorization . We compared our algorithms to the single - prototype and multiprototype Max - Update algorithms from [ 9 ] and to the Mira algorithm [ 2 ]. The experiments were performed on the task of email classification using the Enron email dataset ( Available at _CITE_ ). The learning goal was to correctly classify email messages into user defined folders . Thus , the instances in this dataset are email messages , while the set of classes are the user defined folders denoted by { 1 , ... , k }.__label__Material|Data|Use
We provide code for tensor biclustering methods in the following link : _CITE___label__Method|Code|Produce
We created 4 , 000 identical concepts ( four for each leaf node ) using the protocol above , and recruited participants online through Amazon Mechanical Turk ( AMT , _CITE_ ) to obtain the human ground truth data . For each concept , an AMT HIT ( a single task presented to the human participants ) is formed with five example images and twenty query images , and the participants were asked whether each query belongs to the concept represented by the examples . Each HIT was completed by five unique participants , with a compensation of $ 0 . 05 USD per HIT .__label__Method|Tool|Use
This output distribution can be seen as a saliency map from the point of view of the person inside the picture . To train and evaluate our model , we also introduce GazeFollow , a large - scale benchmark dataset for gaze - following . Our model , code and dataset are available for download at _CITE_ Related Work ( Saliency ): Although strongly related , there are a number of important distinctions between gaze - following [ 3 ] and saliency models of attention [ 8 ]. In traditional models of visual attention , the goal is to predict the eye fixations of an observer looking at a picture , while in gazefollowing the goal is to estimate what is being looked at by a person inside a picture .__label__Method|Code|Produce
This output distribution can be seen as a saliency map from the point of view of the person inside the picture . To train and evaluate our model , we also introduce GazeFollow , a large - scale benchmark dataset for gaze - following . Our model , code and dataset are available for download at _CITE_ Related Work ( Saliency ): Although strongly related , there are a number of important distinctions between gaze - following [ 3 ] and saliency models of attention [ 8 ]. In traditional models of visual attention , the goal is to predict the eye fixations of an observer looking at a picture , while in gazefollowing the goal is to estimate what is being looked at by a person inside a picture .__label__Material|Data|Produce
This output distribution can be seen as a saliency map from the point of view of the person inside the picture . To train and evaluate our model , we also introduce GazeFollow , a large - scale benchmark dataset for gaze - following . Our model , code and dataset are available for download at _CITE_ Related Work ( Saliency ): Although strongly related , there are a number of important distinctions between gaze - following [ 3 ] and saliency models of attention [ 8 ]. In traditional models of visual attention , the goal is to predict the eye fixations of an observer looking at a picture , while in gazefollowing the goal is to estimate what is being looked at by a person inside a picture .__label__Method|Algorithm|Produce
We used Tulipsl ( Movellan , 1995 ), a database consisting of 96 movies of 9 male and 3 female undergraduate students from the Cognitive Science Department at the University of California , San Diego . For each student two sample utterances were taken for each of the digits & quot ; one & quot ; through & quot ; four & quot ;. The database is available at _CITE_ We compared the performance of diffusion networks and HMMs using two different image processing techniques ( contours and contours plus intensity ) in combination with 2 different recognition engines ( HMMs and diffusion networks ). The image processing was performed by Luettin and colleagues ( Luettin , 1997 ).__label__Material|Data|Use
This maximizes the free - energy bound for samples drawn from our model , but without changing We measured quantitative performance of MatNets on three datasets : Omniglot [ 13 ], and CIFAR 10 [ 12 ]. We used the 28x28 version of Omniglot described in [ 2 ], which can be found at : All quantitative experiments measured performance in MNIST , _CITE___label__Method|Tool|Use
Qualitative Evaluation : Prediction video . The prediction videos of our models and baselines are available in the supplementary material and at the following website : _CITE_ Asseeninthe videos , the proposed models make qualitatively reasonable predictions over 30 – 500 steps depending on the game . In all games , the MLP baseline quickly diverges , and the naFf baseline fails to predict the controlled object .__label__Supplement|Website|Produce
Qualitative Evaluation : Prediction video . The prediction videos of our models and baselines are available in the supplementary material and at the following website : _CITE_ Asseeninthe videos , the proposed models make qualitatively reasonable predictions over 30 – 500 steps depending on the game . In all games , the MLP baseline quickly diverges , and the naFf baseline fails to predict the controlled object .__label__Supplement|Document|Produce
Qualitative Evaluation : Prediction video . The prediction videos of our models and baselines are available in the supplementary material and at the following website : _CITE_ Asseeninthe videos , the proposed models make qualitatively reasonable predictions over 30 – 500 steps depending on the game . In all games , the MLP baseline quickly diverges , and the naFf baseline fails to predict the controlled object .__label__Supplement|Media|Produce
Most significantly , we compare clarans with methods uni , bf , km ++ and afk - mc2 for K - means initialization , and show that it provides significant reductions in initialization and final MSEs in § 5 . We thus provide a conceptually simple initialization scheme which is demonstrably better than km ++, which has been the de facto initialization method for one decade now . Our source code at _CITE_ is available under an open source license . It consists of a C ++ library with Python interface , with several examples for diverse data types ( sequence data , sparse and dense vectors ), metrics ( Levenshtein , l1 , etc .) and potentials ( quadratic as in K - means , logarithmic , etc .__label__Method|Code|Produce
We used a soft - max operation with an increasing temperature parameter to model the non - differentiable color channel selection at each point , which allowed us to train the pattern effectively . Finally , we demonstrated that our learned pattern enabled better reconstructions than past designs . An implementation of our method , along with trained models , data , and results , is available at our project page at _CITE_ Our results suggest that learning measurement strategies jointly with computational inference is both useful and possible . In particular , our approach can be used directly to learn other forms of optimized multiplexing patterns — e . g ., spatio - temporal multiplexing for video , viewpoint multiplexing in lightfield cameras , etc .__label__Material|Data|Produce
We used a soft - max operation with an increasing temperature parameter to model the non - differentiable color channel selection at each point , which allowed us to train the pattern effectively . Finally , we demonstrated that our learned pattern enabled better reconstructions than past designs . An implementation of our method , along with trained models , data , and results , is available at our project page at _CITE_ Our results suggest that learning measurement strategies jointly with computational inference is both useful and possible . In particular , our approach can be used directly to learn other forms of optimized multiplexing patterns — e . g ., spatio - temporal multiplexing for video , viewpoint multiplexing in lightfield cameras , etc .__label__Supplement|Website|Produce
We used a soft - max operation with an increasing temperature parameter to model the non - differentiable color channel selection at each point , which allowed us to train the pattern effectively . Finally , we demonstrated that our learned pattern enabled better reconstructions than past designs . An implementation of our method , along with trained models , data , and results , is available at our project page at _CITE_ Our results suggest that learning measurement strategies jointly with computational inference is both useful and possible . In particular , our approach can be used directly to learn other forms of optimized multiplexing patterns — e . g ., spatio - temporal multiplexing for video , viewpoint multiplexing in lightfield cameras , etc .__label__Method|Algorithm|Produce
We used a soft - max operation with an increasing temperature parameter to model the non - differentiable color channel selection at each point , which allowed us to train the pattern effectively . Finally , we demonstrated that our learned pattern enabled better reconstructions than past designs . An implementation of our method , along with trained models , data , and results , is available at our project page at _CITE_ Our results suggest that learning measurement strategies jointly with computational inference is both useful and possible . In particular , our approach can be used directly to learn other forms of optimized multiplexing patterns — e . g ., spatio - temporal multiplexing for video , viewpoint multiplexing in lightfield cameras , etc .__label__Supplement|Document|Produce
In this section we conduct experiments in different settings to validate the robustness of our spectral regularizers . We compare our approach against two baselines : Lasso and greedy FR . We use two different datasets for the experiments , the mnist data ( _CITE_ ) and a simulation data ( for which , results are presented in the supplementary material ). The way we synthesize a regression problem out of the mnist dataset is as follows . Each image is regarded as a feature vector ( of size 784 ) consisting of the pixel intensities .__label__Material|Data|Use
Perplexities for holdout words . In addition to examining the performance of the PGBN for unsupervised feature learning , we also consider a more direct approach that we randomly choose 30 % of the word tokens in each document as training , and use the remaining ones to calculate per - heldoutword perplexity . We consider the NIPS12 ( _CITE_ ) corpus , limiting the vocabulary to the 2000 most frequent terms . We set η ( t ) = 0 . 05 and Ct = 500 for all t , set B1 = 1000 and Bt = 500 for t ≥ 2 , and consider five random trials . Among the Bt + Ct Gibbs sampling iterations used to train layer t , we collect one sample per five iterations during the last 500 iterations , for each of which we draw the topics { 0 ( 1 ) k } k and topics weights B ( 1 ) j , to compute the per - heldout - word perplexity using Equation ( 34 ) of [ 13 ].__label__Material|Data|Use
We now present some more insight to how bootstrapped DQN drives deep exploration in Atari . In each game , although each head Q & apos ;,.., Qio learns a high scoring policy , the policies they find are quite distinct . In the video _CITE_ we show the evolution of these policies simultaneously for several games . Although each head performs well , they each follow a unique policy . By contrast , ‘- greedy strategies are almost indistinguishable for small values of ‘ and totally ineffectual for larger values .__label__Supplement|Media|Produce
Our generations compare favorably with leading approaches , despite being a simple model , e . g . lacking the GAN losses or probabilistic formulations of other video generation approaches . Source code is available at _CITE___label__Method|Code|Produce
We show that a network with Leaky ReLU [ 17 ] and Batch Normalization [ 11 ] coupled with long - horizon training and progressive curriculum beats the rule - based built - in AI more than 70 % of the time in full - game Mini - RTS . We also show stronger performance in others games . ELF and its RL platform , is open - sourced at _CITE___label__Method|Code|Use
We perform a systematic evaluation of these variants by using humans to judge photorealism and a perceptual distance metric [ 52 ] to assess output diversity . Code and data are available at _CITE___label__Method|Code|Produce
We perform a systematic evaluation of these variants by using humans to judge photorealism and a perceptual distance metric [ 52 ] to assess output diversity . Code and data are available at _CITE___label__Material|Data|Produce
We release the complete details of the models at _CITE_ imageqa - public .__label__Supplement|Document|Produce
Infinite Impulse Response ( IIR ) filters have a significant advantage over Finite Impulse Response ( FIR ) filters in signal processing : the length of the impulse response is uncoupled from the number of filter parameters . The length of the impulse response is related to the memory depth of a system , and hence IIR filters allow a greater memory depth than FIR filters of the same order . However , IIR filters are * _CITE___label__Method|Tool|Introduce
The above joint analysis of text and votes was restricted to 1989 - 2008 , since the documents ( legislation ) were only available for those years . However , the dataset contains votes on all legislation from 1789 to the present , and we now analyze the vote data from 1789 - 1988 . Figure 5 shows snapshots in time of the latent space for voters and legislation , for the House of Representatives ( similar results have been computed for the Senate , and are omitted for brevity ; as supplemental material , at _CITE_ we present movies of how legislation and congressman evolve across all times , for both the House and Senate ). Five features were inferred , with the two highest - variance features chosen for the axes . The blue symbols denote Democratic legislators , or legislation sponsored by a Democrat , and the red points correspond to Republicans .__label__Supplement|Document|Produce
The L2 regularization constant for each problem was set by hand to ensure f was not in the big data regime n > L / µ ; as noted above , all the methods perform essentially the same when n > L / µ . The constant used is noted beneath each plot . Open source code to exactly replicate the experimental results is available at _CITE_ Algorithm scaling with respect to n The key property that distinguishes accelerated FIG methods from their non - accelerated counterparts is their performance scaling with respect to the dataset size . For large datasets on well - conditioned problems we expect from the theory to see little difference between the methods .__label__Method|Code|Produce
a function of training steps . ( b ) The optimal resource appropriation policy for a single agent on this map . At convergence , the agent we study nearly learns this policy : _CITE_ ri : S x A1 x · · · x AN -+ R for player i . Finally , let us write Oi = { oi | s E S , oi = O ( s , i )} be the observation space of player i .__label__Supplement|Document|Use
The reason is the synthetic functions are nonstationary but SE - MPGP uses a stationary SE kernel . Hence we perform 5 - SENN - MPGP with a nonstationary kernel to show that our MPGP is competitive with SSGP , and much better with shorter running time than SPGP . Global Surface Temperature Dataset : We present here a preliminary analysis of the Global Surface Temperature Dataset in January 2011 ( _CITE_ ). We first gather the training data with 100 collections . For each collection , we randomly select 90 data points where the input vector is the longitude and latitude location , the output is the temperature ( oC ).__label__Material|Data|Use
We compare HMP to SIFT based single layer sparse coding because of its success in both computer vision and machine learning communities [ 24 , 23 , 5 , 6 ]. We extract SIFT with 16x16 image patches over dense regular grids with spacing of 8 pixels . We use the publicly available dense SIFT code at _CITE_ [ 14 ]. We perform sparse coding feature extraction using 1 , 000 visual words learned from 1 , 000 , 000 SIFT features , and compute image - level features by running spatial pyramid max pooling on 1 x 1 , 2 x 2 and 4 x 4 sub - regions [ 24 ].__label__Method|Code|Use
This is the main tool of experimental nonlinear dynamics [ 8 ]; but the assumption of determinism is crucial and false , for almost any interesting neural system . While classical state - space reconstruction won ’ t work on stochastic processes , such processes do have state - space representations [ 11 ], and , in the special case of discretevalued , discrete - time series , there are ways to reconstruct the state space . Here we use the CSSR algorithm , introduced in [ 12 ] ( code available at _CITE_ ). This produces causal state models , which are stochastic automata capable of statistically - optimal nonlinear prediction ; the state of the machine is a minimal sufficient statistic for the future of the observable process [ 13 ]. 1 The basic idea is to form a set of states which should be ( 1 ) Markovian , ( 2 ) sufficient statistics for the next observable , and ( 3 ) have deterministic transitions ( in the automata - theory sense ). The algorithm begins with a minimal , one - state , IID model , and checks whether these properties hold , by means of hypothesis tests .__label__Method|Code|Use
Figure 1 ( c ) plots the convergence of objective value against the runtime when n = 4000 . BADMM converges faster than ADMM even when the initial point is further from the optimum . BADMM vs Gurobi : Gurobi ( _CITE_ ) is a highly optimized commercial software where linear programming solvers have been efficiently implemented . We run Gurobi on two settings : a Mac laptop with 8G memory and a server with 86G memory , respectively . For comparison , BADMM is run in parallel on a Tesla M2070 GPU with 5G memory and 448 cores1 .__label__Method|Tool|Compare
Outputs of the program , taking the form of posterior samples , are indicated by the return values . There is a finite set of sample and observe statements in a program source code , but the number of times each statement is called can vary between executions . We refer the reader to _CITE_ for more details .__label__Supplement|Document|Produce
( iv ) Furthermore , with Precision - Recall - Gain curves it is possible to calibrate a model for Fp in the sense that the predicted score for any instance determines the value of p for which the instance is on the Fp decision boundary . ( v ) We give experimental evidence in Section 4 that this matters by demonstrating that the area under traditional Precision - Recall curves can easily favour models with lower expected F1 score than others . Proofs of the formal results are found in the Supplementary Material ; see also _CITE___label__Supplement|Document|Produce
Figure 5 shows the velocities and positions of the helicopter under our learned policy and under the human pilot ’ s control . As we see , our controller was able to keep the helicopter flying more stably than was a human pilot . Videos of the helicopter flying are available at _CITE___label__Supplement|Media|Produce
We empirically analyzed the accuracy of the algorithms proposed here against each other and against the available implementations of TWILP ( _CITE_ ) and K & P ( http :// www . cs . helsinki . fi / u / jazkorho / aistats - 2013 /) on a collection of data sets from the UCI repository . The S + K & P and S2 algorithms were implemented ( purely ) in Matlab . The data sets were selected so as to span a wide range of dimensionality , and were preprocessed to have variables discretized over the median value when needed .__label__Method|Code|Compare
30000 samples of natural scenes of 12 × 12 pixels were given as the observed signals X . That is , N and M were 144 and 30000 . Original natural scenes were downloaded at_CITE_ Thenumberof layers L was set 720 , where one layer means one pair of the mapping and the local - ICA phases . For comparison , the experiments without the mapping phase were carried out , where the mapping Y was randomly generated .__label__Material|Data|Use
In this paper , we did not evaluate the use of AUTO - SKLEARN for interactive machine learning with an expert in the loop and weeks of CPU power , but we note that that mode has also led to a third place in the human track of the same challenge . As such , we believe that AUTO - SKLEARN is a promising system for use by both machine learning novices and experts . The source code of AUTO - SKLEARN is available under an open source license at _CITE_ Our system also has some shortcomings , which we would like to remove in future work . As one example , we have not yet tackled regression or semi - supervised problems .__label__Method|Code|Produce
The optimization procedure and the kernels κj are identical to the ones used for processing the SVHN dataset in the classification task . The pipeline also includes a pre - processing step , where we remove from input images a local mean component obtained by convolving the images with a 5 x 5 averaging box filter ; the mean component is added back after up - scaling . For the evaluation , we consider three datasets : Set5 and Set14 are standard for super - resolution ; Kodim is the Kodak Image database , available at _CITE_ , which contains high - quality images with no compression or demoisaicing artefacts . The evaluation procedure follows [ 7 , 8 , 26 , 27 ] by using the code from the author ’ s web page . We present quantitative results in Table 2 .__label__Material|Data|Use
Here we demonstrate the method on the auto - mpg problem from the UCI dataset . This provides a natural scenario for demonstrating covariate shift . The auto - mpg data can be found at _CITE_ and involves the problem of predicting the city cycle fuel consumption of cars . One of the attributes is a class label dictating the origin of a particular car . To demonstrate covariate shift we can consider the prediction task trained on cars from one place of origin and tested on cars from another place of origin .__label__Material|Data|Use
Higly textured objects on the ground do not detract the system from the correct response ( center - right ). One scenario where the vehicle occasionally made wrong decisions is when the sun is in the field of view : the system seems to systematically drive towards the sun , whenever the sun is low on the horizon ( right ). Videos of these sequences are available at _CITE___label__Supplement|Media|Produce
10 - 8 and Ek We run the code provided in _CITE_ and use default parameter settings to report the VB results . We also implemented the Rec - MCEM approach but only observed shrinkage of edges , not nodes .__label__Method|Code|Use
We tested the VHEM algorithm on hierarchical motion clustering , where each of the input HMMs to be clustered is estimated on a sequence of motion capture data from the Motion Capture dataset ( _CITE_ ). In particular , we start from K1 = 56 motion examples from 8 different classes (“ jump ”, “ run ”, ‘ jog ‘”, “ walk 1 ” and “ walk 2 ” which are from two different subjects , “ basket ”, “ soccer ”, “ sit ”), and learn a HMM for each of them , forming the first level of the hierarchy . A tree - structure is formed by successively clustering HMMs with the VHEM algorithm , and using the learned cluster centers as the representative HMMs at the new level .__label__Material|Data|Use
We also tried learning regularizations for the Bethe Hessian , and found it succeeds in repairing Bethe Hessian when Bethe Hessian has localization problem . These indicate that our scheme of regularization - learning is a general spectral approach for hard inference problems . A ( Matlab ) demo of our method can be found at _CITE___label__Method|Code|Produce
An implementation of semi - CRFs is available at _CITE_ , and a NER package using this package is available on http :// minorthird . sourceforge . net .__label__Method|Code|Use
This is the same rate as for a wellspecified case where W  2 ([ 0 , 1 ] d ) was assumed for the construction of weighted points . Namely , we have shown that these methods are adaptive to the unknown smoothness of the integrand . For the algorithm by Bach [ 2 ], we conducted simulation experiments to support this observation , by using code available from _CITE_ The setting is what we have described with d = 1 , and weights are obtained without regularization as in [ 2 ]. The result is shown in Figure 1 , where r (= a ) denotes the assumed smoothness , and s (= a0 ) is the ( unknown ) smoothness of an integrand .__label__Method|Code|Use
Kernel ridge regression was used to estimate the values of a *, k *, l * as to minimize Equation 12 for different user - specified LMA - Effort vectors ¯ e . The recovered parameter values were used to synthesize animations with the specified desired styles . Videos of these automatically generated motions as well as additional results can be viewed at _CITE_ . In order to test the generalization ability of our system , the target styles in this experiment were chosen to be considerably different from those in the training set . All of the synthesized sequences were visually inspected by LMA experts and , for the great majority , they were found to be consistent with the style target labels .__label__Supplement|Document|Produce
Kernel ridge regression was used to estimate the values of a *, k *, l * as to minimize Equation 12 for different user - specified LMA - Effort vectors ¯ e . The recovered parameter values were used to synthesize animations with the specified desired styles . Videos of these automatically generated motions as well as additional results can be viewed at _CITE_ . In order to test the generalization ability of our system , the target styles in this experiment were chosen to be considerably different from those in the training set . All of the synthesized sequences were visually inspected by LMA experts and , for the great majority , they were found to be consistent with the style target labels .__label__Supplement|Media|Produce
Figure 5 shows our predicted distribution is very close to the ground - truth distribution . It also shows that a variational autoencoder helps to capture the true distribution of future frames . ‡ Our project page : _CITE___label__Supplement|Website|Produce
The state vector sj for each agent is thus a concatenation of all these vectors , having dimension 32 × | n |× | l |× | r |. In Table 2 ( left ), we show the probability of failure of a variety of different model Φ and module f pairs . Compared to the baseline models , CommNet significantly reduces the failure rate for all module types , achieving the best performance with LSTM module ( a video showing this model before and after training can be found at _CITE_ ). We also explored how partial visibility within the environment effects the advantage given by communication . As the vision range of each agent decreases , the advantage of communication increases as shown in Fig .__label__Supplement|Media|Produce
We showed experiments for several challenging tasks on real datasets up to 0 . 6PB size with hundreds billions samples and features to demonstrate its efficiency . We believe that this third - generation parameter server is an important and useful building block for scalable machine learning . Finally , the source codes are available at _CITE___label__Method|Code|Produce
Stimuli that produce bistability are characterized by having several distinct interpretations that are in some sense equally plausible . Given the successes of Bayesian inference as a model of perception ∗ _CITE_ ( for instance [?, ?, ? ]), these observations suggest that bistability is intimately connected with making perceptual decisions in the presence of a multi - modal posterior distribution , as previously noted by several authors [?, ?]. However , typical Bayesian models of perceptual inference have no dynamics , and probabilistic inference per se provides no reason for spontaneous switching , raising the possibility that switching stems from idiosyncracies in the brain ’ s implementation of probabilistic inference , rather than from general principles .__label__Method|Algorithm|Extent
After each node in the trellis we apply batch normalization [ 9 ], and regularize the model with weight decay of 10 − 4 , but did not apply dropout [ 30 ]. We use the validation set to determine the optimal number of training epochs , and then train a final model from the train and validation data and report performance on the test set . We release our Caffe - based implementation at _CITE___label__Method|Code|Produce
We also use a larger minibatch size ( 64 ) which we found to be more efficient on our hardware ( Amazon Elastic Compute Cloud g2 . 2xlarge GPU instance ). Apart from these changes we follow [ 25 ] as closely as possible in terms of parameter settings and evaluation methods . However , we use a Python / Theano / Lasagne reimplementation of their work , adapted from the implementation available at _CITE_ , so there may be small additional differences in implementation . Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders . Using weight normalization the algorithm progresses more quickly and reaches a better final result .__label__Method|Code|Use
Second , we add non - probabilistic features for information that is not easily modeled generatively , such as alignments of expressed sequence tags ( ESTs ). We developed Conrad , a gene predictor and highly optimized CRF engine . Conrad is freely available with an open source license at _CITE_ We applied Conrad to predict genes in the fungal human pathogen Cryptococcus neoformans . Our baseline comparative model is as accurate as Twinscan [ 9 , 10 ], the most accurate gene predictor trained for C . neoformans .__label__Supplement|License|Other
We implement the credit assignment compiler in Vowpal - Wabbit ( _CITE_ ), a fast online learning library , and show that the credit assignment compiler achieves outstanding empirical performance both in accuracy and in speed for several application tasks . This provides strong simple baselines for future research and demonstrates the compiler approach to solving complex prediction problems may be of broad interest . Details experimental settings are in appendices .__label__Method|Tool|Use
Implementation . We implemented LDAMP and LDIT , using the DnCNN denoiser [ 39 ], in both TensorFlow and MatConvnet [ 47 ], which is a toolbox for Matlab . Public implementations of both versions of the algorithm are available at _CITE___label__Method|Code|Produce
As a dissimilarity measure , we use the L2 - distance estimated by LSDD and the Kullback - Leibler ( KL ) divergence estimated by the KL importance estimation procedure ( KLIEP ) [ 2 , 3 ]. We set k = 10 and r = 50 . First , we use the IPSJ SIG - SLP Corpora and Environments for Noisy Speech Recognition ( CENSREC ) dataset ( _CITE_ ). This dataset is provided by the National Institute of Informatics , Japan that records human voice in a noisy environment such as a restaurant . The top graphs in Figure 7 ( a ) display the original timeseries ( true change points were manually annotated ) and change scores obtained by KLIEP and LSDD .__label__Material|Data|Use
In our experiments we used seven different real data sets . They are all taken from UCI Machine Learning Repository at _CITE_ . For a description of the data sets see [ 6 ). For the Iris , Sonar , Liver and Vote data we perform leave - one - out cross - validation to measure performance , since the number of available data is limited for these data sets .__label__Material|Data|Use
There exists a very large literature on neural circuits for translation - invariant pattern recognition see http :// www . cnl . salk . edurwiskott / Bibliographiesfinvariances . htrnl . Unfortunately there exists substantial disagreement regarding the interpretation of existing approaches see _CITE_ Virtually all positive results are based on computer simulations of small circuits , or on learning algorithms for concrete neural networks with a fixed input size n on the order of 20 or 30 , without an analysis how the required number of gates and the area or volume occupied by wires scale up with the input size . The computational performance of these networks is often reported in an anecdotical manner .__label__Supplement|Document|Introduce
a squashing function ( exponential of the Digamma function ) before being normalized . We refer the reader to the original paper for more detail ( see also _CITE_ Publications . htm for a bug fix in the Digamma function implementation ).__label__Supplement|Document|Produce
Sound sources used were : broadband white noise ; recordings of instruments and voices from the RWC Music Database ( http :// staff . aist . go . jp / m . goto / RWC - MDB /); and recordings of vowel - consonant - vowel sounds ( Lorenzi et al ., 1999 ). All sounds were of 1 second duration and were presented at 80 dB SPL . Sounds were filtered by head - related impulse responses ( HRIRs ) from the IRCAM LISTEN HRTF Database ( _CITE_ ). This database includes 187 approximately evenly spaced locations at all azimuths in 15 degree increments ( except for high elevations ) and elevations from - 45 to 90 degrees in 15 degree increments . HRIRs from this and other databases do not provide sufficiently accurate timing information at frequencies below around 150Hz , and so subsequent cochlear filtering was restricted to frequencies above this point .__label__Material|Data|Use
Our research for an optimal set of RNN architectures and hyperparameters for pre - miRNA identification involved an exploration of the design space spanned by the components of our methodology . The result of this research is a technique with demonstrable advantages over other state - of - the - art alternatives in terms of both cross - validation results but also the generalization ability ( i . e ., performance on test data ). The source code for the proposed method is available at _CITE___label__Method|Code|Produce
In this section , we empirically compare LLCA with the spectral clustering approach of [ 10 ] as well as with k - means clustering . For the last discretization step of LLCA ( cf . section 3 . 6 ), we use the same code contained in the implementation of the spectral clustering algorithm , available at _CITE___label__Method|Code|Use
SSOM , however , performs only slightly worse ( approx . 1 dB ) than STVQ . Considering the fact that SSOM is computationally much less demanding than STVQ & apos ; The Lenna Story can be found at _CITE_ An Annealed Self - Organizing Map for Source Channel Coding 435 ( 0 ( N ) for encoding ) - due to the omission of the convolution with hrs in Eq . ( 4 ) - the result demonstrates the efficiency of SSOM for source channel coding . Figure 4 also shows the generalization behavior of a SSOM codebook optimized for a BER of 0 . 05 ( rectangles ).__label__Method|Tool|Introduce
where ∇ wL is the gradient with respect to the weights w as used normally . Backpropagation using weight normalization thus only requires a minor modification to the usual backpropagation equations , and is easily implemented using standard neural network software , either by directly specifying the network in terms of the v , g parameters and relying on auto - differentiation , or by applying ( 3 ) in a post - processing step . We provide reference implementations using both approaches for Theano , Tensorflow and Keras at _CITE_ Unlike with batch normalization , the expressions above are independent of the minibatch size and thus cause only minimal computational overhead . An alternative way to write the gradient is__label__Method|Code|Produce
The image size is 512 x 512 . We subsample the images down to a size of 100 x 100 = 10000 . ORL ( available at _CITE_ ), contains 400 face images of 40 persons . The image size is 92 x 112 . PIE is a subset of the CMU – PIE face image dataset ( available at http :// www . ri . cmu . edu / projects / project 418 . html ).__label__Material|Data|Use
The bound estimated by GP is used to reduce the number of candidates . Since the bound estimated by GP is known , we can ignore the candidates of the bounds that are looser than the bound estimated by GP . The source code of the proposed algorithm is publicly available at _CITE___label__Method|Code|Produce
The estimated density functions are depicted in figure 1 , where grey - values are proportional to on a grid . From the images one can see , that the QDE better captures the global structure of the distribution while the KDE is more sensitive to local variations in the data . In a second experiment we trained PCA - QDEs with - dimensional real - world data ( images ) which had been derived from the MNIST database of handwritten digits ( _CITE_ ). For each digit class a - point training set and a - point test set were used to compare the PCA - QDE with__label__Material|Data|Extent
There are 22 , 894 atom symmetry classes , which when paired with reaction condition yields 29 , 104 ( a , c ) tuples . Of these 29 , 104 ( a , c ) tuples , 1 , 262 have label srcreact = 1 , and 1 , 786 have label sinkreact = 1 . Atom and MO interaction data is available at our chemoinformatics portal ( _CITE_ ) under Supplements .__label__Material|Data|Produce
There are 22 , 894 atom symmetry classes , which when paired with reaction condition yields 29 , 104 ( a , c ) tuples . Of these 29 , 104 ( a , c ) tuples , 1 , 262 have label srcreact = 1 , and 1 , 786 have label sinkreact = 1 . Atom and MO interaction data is available at our chemoinformatics portal ( _CITE_ ) under Supplements .__label__Supplement|Website|Produce
lem arising from the use of a hyper - Laplacian prior , by using a splitting approach that allows the non - convexity to become separable over pixels . Using a LUT to solve this sub - problem allows for orders of magnitude speedup in the solution over existing methods . Our Matlab implementation is available online at _CITE_ A potential drawback to our method , common to the TV and Bl approaches of [ 22 ], is its use of frequency domain operations which assume circular boundary conditions , something not present in real images . These give rise to boundary artifacts which can be overcome to some extend with edge tapering operations .__label__Method|Code|Produce
Graph - based methods are amongst the most popular and aim to construct a graph connecting similar observations ; label information propagates through the graph from labelled to unlabelled nodes by finding the minimum energy ( MAP ) configuration ( Blum et al ., 2004 ; Zhu et al ., 2003 ). Graph - based approaches are sensitive to the graph structure and require eigen - analysis of the graph Laplacian , which limits the scale to which these methods can be applied – though efficient spectral methods are now available ( Fergus et al ., 2009 ). Neural network - based approaches combine unsupervised and supervised learning For an updated version of this paper , please see _CITE_ by training feed - forward classifiers with an additional penalty from an auto - encoder or other unsupervised embedding of the data ( Ranzato and Szummer , 2008 ; Weston et al ., 2012 ). The Manifold Tangent Classifier ( MTC ) ( Rifai et al ., 2011 ) trains contrastive auto - encoders ( CAEs ) to learn the manifold on which the data lies , followed by an instance of TangentProp to train a classifier that is approximately invariant to local perturbations along the manifold . The idea of manifold learning using graph - based methods has most recently been combined with kernel ( SVM ) methods in the Atlas RBF model ( Pitelis et al ., 2014 ) and provides amongst most competitive performance currently available .__label__Supplement|Document|Produce
Despite its limited size , through user study we show our algorithm is realizable in practice on high DoF manipulators . We hope this motivates researchers to build robotic systems capable of learning from non - expert users . For more details and video , please visit : _CITE___label__Supplement|Document|Produce
Despite its limited size , through user study we show our algorithm is realizable in practice on high DoF manipulators . We hope this motivates researchers to build robotic systems capable of learning from non - expert users . For more details and video , please visit : _CITE___label__Supplement|Media|Produce
We systematically evaluate three types of density estimator ( MADE , Real NVP and MAF ) in terms of density estimation performance on a variety of datasets . Code for reproducing our experiments ( which uses Theano [ 29 ]) can be found at _CITE_ MADE . We consider two versions : ( a ) a MADE with Gaussian conditionals , denoted simply by MADE , and ( b ) a MADE whose conditionals are each parameterized as a mixture of C Gaussians , denoted by MADE MoG .__label__Method|Code|Produce
This means that we have a trade - off of fast computation per iteration and slow convergence for SGD versus slow computation per iteration and fast convergence for gradient descent . Although the fast computation means it can reach an approximate solution relatively quickly , and thus has been proposed by various researchers for large scale problems Zhang [ 2004 ], Shalev - Shwartz et al . [ 2007 ] ( also see Leon Bottou ’ s Webpage _CITE_ ), the convergence slows down when we need a more accurate solution . In order to improve SGD , one has to design methods that can reduce the variance , which allows us to use a larger learning rate ηt . Two recent papers Le Roux et al .__label__Supplement|Website|Introduce
The strong convexity constant µ is the regularization parameter . For all methods , we consider step - sizes supported by the theory as well as larger step - sizes that may work better in practice . Our C ++/ Cython implementation of all methods considered in this section is available at _CITE_ Choices of step - sizes . For both S - MISO and SGD , we use the step - size strategy mentioned in Section 3 and advised by [ 5 ], which we have found to be most effective among many heuristics__label__Method|Code|Produce
Our model provides a novel compromise between point estimators given by the faithfulness assumptions and bounds based on instrumental variables . We believe such an approach should become a standard item in the toolbox of anyone who needs to perform an observational study . R code is available at _CITE_ Unlike risky Bayesian approaches that put priors directly on the parameters of the unidentifiable latent variable model P ( Y , X , W , U | Z ), the constrained Dirichlet prior does not suffer from massive sensitivity to the choice of hyperparameters , as discussed at length by [ 18 ] and the Supplementary Material . By focusing on bounds , WPP keeps inference more honest , providing a compromise between a method purely based on faithfulness and purely theory - driven analyses that overlook competing models suggested by independence constraints .__label__Method|Code|Produce
Efficient on - line variants can be implemented by using standard update formulas for matrix inversion by partitioning . For an implementation of the VEGAS algorithm see [ 9 ]. The R package “ car ” provides a comfortable implementation of quantile - quantile plots and robust line fitting ( see also _CITE_ ).__label__Method|Code|Use
What do Jesus and Darwin have in common ? Other than being associated with two different views on the origin of man , they also have colleges at Cambridge University named after them . If these two names are entered as a query into GoogleTM Sets ( _CITE_ ) it returns a list of other colleges at Cambridge . GoogleTM Sets is a remarkably useful tool which encapsulates a very practical and interesting problem in machine learning and information retrieval . 1 Consider a universe of items D . Depending on the application , the set D may consist of web pages , movies , people , words , proteins , images , or any other object we may wish to form queries on . The user provides a query in the form of a very small subset of items Dc C D . The assumption is that the elements in Dc are examples of some concept / class / cluster in the data .__label__Method|Tool|Introduce
We thank Jakob Macke , Pierre Garrigues , and Greg Stephens for helpful comments and stimulating discussions , as well as Alexander Ecker and Andreas Hoenselaar for last minute advice . An implementation of the DG model in Matlab and R will be avaible at our website _CITE___label__Method|Code|Produce
We thank Jakob Macke , Pierre Garrigues , and Greg Stephens for helpful comments and stimulating discussions , as well as Alexander Ecker and Andreas Hoenselaar for last minute advice . An implementation of the DG model in Matlab and R will be avaible at our website _CITE___label__Supplement|Website|Produce
It was not robust to small errors in the box position estimation because it was trained on perfect state coming from the simulation . After retraining the policy with gaussian noise ( std = 1cm ) added to observations10 the success rate increased to 5 / 5 . The video showing some of the trials is available at _CITE___label__Supplement|Media|Produce
Also , we are analyzing the rate , as a function of sample size , at which estimates of the lower probability bound Q converge to the true value . Finally , the proposed minimax probability machine regression framework is a new formulation of the regression problem , and therefore its properties can only be fully understood through extensive experimentation . We are currently applying MPMR to a wide variety of regression problems and have made Matlab / C source code available ( _CITE_ ) for others to do the same .__label__Method|Code|Produce
The blobby support surface ( left figure ) was computed from 1000 randomly chosen sample points with the slab SVM . In the middle we show a color coding of the residual values of all sample points ( cf . _CITE_ for color images ). In the right figure we show the surface that we get after applying support vector regression using the residual values .__label__Supplement|Media|Produce
Therefore , computing each li essentially only requires the addition of N numbers from a look - up table . We need to do this for all M = | M | illuminants , where summations for different illuminants can be carried out in parallel . Our implementation takes roughly 0 . 3 seconds for a 9 mega - pixel image , on a modern Intel 3 . 3GHz CPU with 6 cores , and is available at _CITE_ This empirical version of our approach bears some similarity to the Bayesian method of [ 14 ] that is based on priors for illuminants , and for the likelihood of different true reflectance values being present in a scene . However , the key difference is our modeling of true chromaticity conditioned on luminance that explicitly makes estimation agnostic to the absolute scale of intensity values .__label__Method|Code|Produce
We consider two different types of high dimensional time series , a human motion capture data set consisting of different walks and high resolution video sequences . The experiments are intended to explore the various properties of the model and to evaluate its performance in different tasks ( prediction , reconstruction , generation of data ). Matlab source code for repeating the following experiments and links to the video files are available on - line from _CITE___label__Method|Code|Produce
We consider two different types of high dimensional time series , a human motion capture data set consisting of different walks and high resolution video sequences . The experiments are intended to explore the various properties of the model and to evaluate its performance in different tasks ( prediction , reconstruction , generation of data ). Matlab source code for repeating the following experiments and links to the video files are available on - line from _CITE___label__Supplement|Media|Produce
For improving single - MLP performance , one might employ two layers of hidden nodes ( rather than one large hidden layer ; see the letter problem below ), which increases nB while reducing nA , rendering Algorithm 2 less efficient ( i . e ., slower ). Alternatively , one might introduce direct connections between the input and terminal output layers , which increases CA , the column size of Ak , retaining nice parameter separability . Yet another approach ( if applicable ) is to use a “ comple ∗ The floating - point operation counts were measured by using PAPI ( Performance Application Programming Interface ); see _CITE_ mentary mixtures of Z MLP - experts ” model ( or a neuro - fuzzy modular network ) that combines Z smaller - size MLPs complementarily ; the associated residual vector to be minimized becomes : r ( 6 ) = y ( 6 ) − t = [ EZi = 1 wioi ] − t , where scalar wi , the ith output of the integrating unit , is the ith ( normalized ) mixing proportion assigned to the outputs ( F - vector oi ) of expert - MLP i . Note that each expert learns “ residuals ” rather than “ desired outputs ” ( unlike in the committee method below ) in the sense that only the final combined outputs y must come close to the desired ones t . That is , there are strong coupling effects ( see page 80 in [ 5 ]) among all experts ; hence , it is crucial to consider the global Hessian across all experts to optimize them simultaneously [ 7 ].__label__Method|Algorithm|Use
We imposed the graph based prior mentioned in Section 2 . 2 . To build our similarity graph we used the English - French and English - Portuguese dictionaries from _CITE_ , augmented with translations from Google Translate for the most frequent words in our dataset . As described earlier , each word corresponds to a vertex , with an edge7 whenever two words match in the dictionary . In our model 0 = exp ( ykv + yv ), so we want to keep both ykv and yv reasonably low to avoid numerical problems , as a large value of either would lead to overflows .__label__Material|Data|Use
To test our methods on a set of correlated variables , we selected 56 genes associated with the oxidative phosphorlylation pathway in the KEGG database [ 1 ]. We discretized the expression measurements of each gene into three levels ( down , same , up ) as in [ 15 ]. We obtained the 1990 US census data set from the UC Irvine data repository ( _CITE_ ). The data set includes 68 discretized attributes such as age , income , occupation , work status , etc . We randomly selected 5k entries from the 2 . 5M available entries in the entire data set .__label__Material|Data|Use
These criteria are given by the penalty term , which consists of the number of parameters and samples6 , and the maximum log - likelihood . Because the maximum log - likelihood is equivalent to the residual error , our method can approximate these criteria . Python code of our algorithm is available at : _CITE___label__Method|Code|Produce
This dataset is important to us , because we are interested in conducting neuroscience research concerning this brain region . Even to those with no interest in piriform cortex , the dataset could be useful for research on image segmentation algorithms . Therefore we make the annotated dataset publicly available ( _CITE_ ).__label__Material|Data|Produce
In this situation , we can benefit from the available training data to learn a specialized AL strategy for an application . In most of the experiments , we use Random Forest ( RF ) classifiers for f and a RF regressor for g . The state of the learning process � t at time t consists of the following features : a ) predicted probability p ( y = 0 | Lt , x ); b ) proportion of class 0 in Lt ; c ) out - of - bag cross - validated accuracy of ft ; d ) variance of feature importances of ft ; e ) forest variance computed as variance of trees ’ predictions on Ut ; f ) average tree depth of the forest ; g ) size of Lt . For additional implementational details , including examples of the synthetic datasets , parameters of the data generation algorithm and features in the case of GP classification , we refer the reader to the supplementary material . The code is made available at _CITE_ Baselines and protocol We consider the three versions of our approach : a ) LAL - independent - 2D , LALINDEPENDENT strategy trained on a synthetic dataset of cold start ; b ) LAL - iterative - 2D , LALITERATIVE strategy trained on a synthetic dataset of cold start ; c ) LAL - independent - WS , LALINDEPENDENT strategy trained on warm start representative data . We compare them against the following 4 baselines : a ) Rs , random sampling ; b ) Us , uncertainty sampling ; c ) Kapoor [ 16 ], an algorithm that balances exploration and exploitation by incorporating mean and variance estimation of the GP classifier ; d ) ALBE [ 11 ], a recent example of meta - AL that adaptively uses a combination of strategies , including Us , Rs and that of Huang et al .__label__Method|Code|Produce
Together with the peak conductance values , the midpoint voltages VI and slopes s of these Boltzmann functions adapt to the statistics of stimuli . For simplicity , all time constants for the dendritic conductances are set to a constant 5 msec . For additional details and parameter values , see _CITE_ Hodgkin - Huxley models can exhibit complex behaviors on several timescales , such as firing patterns consisting of & quot ; bursts & quot ;— sequences of multiple spikes interspersed with periods of silence . We will , however , focus on models of regularly spiking cells that adapt to a sustained stimulus by spiking periodically .__label__Supplement|Document|Produce
Unlike vanilla DQN , bootstrapped DQN can know what it doesn ’ t know . In an application where executing a poorly - understood action is dangerous this could be crucial . In the video _CITE_ we visualize this ensemble policy across several games . We find that the uncertainty in this policy is surprisingly interpretable : all heads agree at clearly crucial decision points , but remain diverse at other less important steps .__label__Supplement|Media|Produce
Due to space limitations , we omit proofs from this version of the paper . Complete proofs may be found in the extended version , which is available on the author ’ s Web page ( _CITE_ ). To prove ( a ), we show that when the search is at a node v whose least common ancestor with the target has height h , there is a high probability that v has a link into the sub - tree of height h − 1 containing the target . In this way , the search reaches the target in logarithmically many steps .__label__Supplement|Website|Produce
Due to space limitations , we omit proofs from this version of the paper . Complete proofs may be found in the extended version , which is available on the author ’ s Web page ( _CITE_ ). To prove ( a ), we show that when the search is at a node v whose least common ancestor with the target has height h , there is a high probability that v has a link into the sub - tree of height h − 1 containing the target . In this way , the search reaches the target in logarithmically many steps .__label__Supplement|Document|Produce
This material includes the treatment of two additional problems : the NP - complete Maximum Cut Problem [ 11 ] and an NP - complete problem known as the multiprocessor document allocation problem ( MDAP ). Also in the full version of this paper is a substantially more thorough exposition of the material presented here . The reader is encouraged to refer to [ 10 ], available on the World Wide Web at _CITE_ , juelsi .__label__Supplement|Document|Produce
Co - authored papers gave fractional counts evenly to all authors . All words occurring in six or more documents were included , except for stopwords giving a vocabulary size of 13649 . The NIPS text data is available at _CITE_ . in terms of , the probability that version of picks versionof :__label__Material|Data|Produce
At the same time , it is also usually able to correctly estimate the depth of large and texture - less planar regions ( but , see column 6 for an example failure case ). Our overall inference method ( network predictions and globalization ) takes 24 seconds per - image when using an NVIDIA Titan X GPU . The source code for implementation , along with a pre - trained network model , are available at _CITE___label__Method|Code|Produce
At the same time , it is also usually able to correctly estimate the depth of large and texture - less planar regions ( but , see column 6 for an example failure case ). Our overall inference method ( network predictions and globalization ) takes 24 seconds per - image when using an NVIDIA Titan X GPU . The source code for implementation , along with a pre - trained network model , are available at _CITE___label__Method|Algorithm|Produce
Additionally , convergence of CONCORD - ISTA and CONCORD - FISTA will be illustrated . Section 4 . 2 has timing results from analyzing a real breast cancer dataset with outliers . Comparisons are made to the coordinate - wise CONCORD implementation in gconcord package for R available at _CITE_ For implementing the proposed algorithms , we can take advantage of existing linear algebra libraries . Most of the numerical computations in Algorithms 1 and 2 are linear algebra operations , and , unlike the sequential coordinate - wise CONCORD algorithm , CONCORD - ISTA and CONCORD - FISTA implementations can solve increasingly larger problems as more and more scalable and efficient linear algebra libraries are made available .__label__Method|Code|Compare
In each experiment an image was artificially contaminated with independent Gaussian pixel noise of some predetermined variance and denoised using 20 iterations of the proposed algorithm . To reduce artifacts at the boundaries we used “ reflective boundary extensions ”. The images were obtained from _CITE_ to ensure comparison on the same set of images . In table 1 we compare performance between the PoEdges and GSM based denoising algorithms on six test images and ten different noise levels . In figure 3 we compare results on__label__Supplement|Media|Compare
After training a 256 - component speech model on clean speech , we used the adaptive version of ALGONQUIN to denoise noisy test utterances on two tasks : the publically available Aurora limited vocabulary speech recognition task ( _CITE_ ); the Wall Street Journal ( WSJ ) large vocabulary speech recognition task , with Microsoft & apos ; s Whisper speech recognition system . We obtained results on all 48 test sets from partitions A and 13 of the Aurora database . Each set contains 24 , 000 sentences that have been corrupted from one of 4 different noise types and one of 6 different signal to noise ratios .__label__Supplement|Website|Extent
components ) 100 000 points in dimension 5 in well under a second on a standard desktop computer . Our Matlab implementation of GI - ICA is available for download at _CITE_ Finally , we observe that our method is partially compatible with the robust cumulants introduced in [ 20 ]. We briefly discuss how GI - ICA can be extended using these noise - robust techniques for ICA to reduce the impact of sparse noise .__label__Method|Code|Produce
In order to better evaluate the empirical performance , four data sets are used in our study including the Wine Quality , CASP , Year Prediction datasets ( _CITE_ ) and the census - house dataset ( http :// www . cs . toronto . edu / delve / data / census - house / desc . html ). The detailed information about the data sets are showed in Table 2 . Firstly , each data set is standardized by subtracting its mean and dividing its standard deviation .__label__Material|Data|Use
5 and 6 and we provide results on smaller corrupted datasets ( to show the performance of IWS - LS ) as well as non - corrupted data simulated according to [ 13 ] in § SI . 7 . Airline delay dataset The dataset consists of details of all commercial flights in the USA over 20 years . Dataset along with visualisations available from _CITE_ Selecting the first ntrain = 13 , 000 US Airways flights from January 2000 ( corresponding to approximately 1 . 5 weeks ) our goal is to predict the delay time of the next ntest = 5 , 000 US Airways flights . The features in this dataset consist of a binary vector representing origin - destination pairs and a real value representing distance ( p = 170 ).__label__Material|Data|Produce
5 and 6 and we provide results on smaller corrupted datasets ( to show the performance of IWS - LS ) as well as non - corrupted data simulated according to [ 13 ] in § SI . 7 . Airline delay dataset The dataset consists of details of all commercial flights in the USA over 20 years . Dataset along with visualisations available from _CITE_ Selecting the first ntrain = 13 , 000 US Airways flights from January 2000 ( corresponding to approximately 1 . 5 weeks ) our goal is to predict the delay time of the next ntest = 5 , 000 US Airways flights . The features in this dataset consist of a binary vector representing origin - destination pairs and a real value representing distance ( p = 170 ).__label__Supplement|Media|Produce
A significant advantage of our design is that long range interaction can be easily coupled . As 3D data is becoming more accessible , we believe that our method will stimulate more work on feature learning from 3D data . We open - source our code at _CITE_ for encouraging future developments .__label__Method|Code|Produce
We now present our analysis on the Futures price returns . This dataset was downloaded from _CITE_ We focus on the Top - 26 most liquid instruments being traded at the Chicago Mercantile Exchange ( CME ). The instruments span different sectors like Energy , Agriculture , Currencies , Equity Indices , Metals and Interest Rates .__label__Material|Data|Use
In this case , convergence with probability one ( w . p . 1 ) to a local optimum can be established for arbitrary differentiable policy classes under mild assumptions [ 22 , 13 , 19 ]. On the other hand , while the greedy value function approach is often considered to possess practical advantages in terms of convergence speed and representational flexibility , its behavior in the proximity of an optimum is currently not well understood . It is well known that interactively operated approximate hard - greedy An extended version of this paper with full proofs and additional background material is available at _CITE_ and http :// users . ics . aalto . fi / pwagner /. value function methods can fail to converge to any single policy and instead become trapped in sustained policy oscillation or policy chattering , which is currently a poorly understood phenomenon [ 6 , 7 ]. This applies to both non - optimistic and optimistic policy iteration ( value iteration being a special case of the latter ).__label__Supplement|Document|Produce
More importantly , well - clustered datasets are abundant in big - data scenarios . For instance , although there are more than 1 billion of users on Facebook , the intrinsic “ feature vectors ” of these users can be naturally categorized by the users ’ occupations , nationalities , etc . As another example , although there ∗ The full version of this paper can be found on _CITE_ † These two authors equally contribute to this paper . 30th Conference on Neural Information Processing Systems ( NIPS 2016 ), Barcelona , Spain .__label__Supplement|Document|Produce
The experimental task was online handwriting recognition , using the IAM - OnDB handwriting database [ 12 ], which is available for public download from _CITE_ fki / iamondb / For CTC , we record both the character error rate , and the word error rate using Algorithm 1 with a language model and a dictionary . For the HMM system , the word error rate is quoted from the literature [ 13 ]. Both the character and word error rate are defined as the total number of insertions , deletions and substitutions in the algorithm ’ s transcription of test set , divided by the combined length of the target transcriptions in the test set .__label__Material|Data|Use
It is fairly straightforward to write down observation functions for a given model . The moment polynomials can then be derived by computing expectations under the model – this step can be compared to deriving gradients for EM . We implemented Polymom for several mixture models in Python ( code : _CITE_ ). We used CVXOPT to handle the SDP and the random projections algorithm from to extract solutions . In Table 3 , we show the relative error maxk ||✓ k - ✓⇤ k || 2 /||✓⇤ k || 2 averaged over 10 random models of each class .__label__Method|Code|Produce
We present experimental results for the proposed SPLD on two tasks : event detection and action recognition . We demonstrate that our approach significantly outperforms SPL on three real - world challenging datasets . The code is at ( _CITE_ ).__label__Method|Code|Produce
In this section we report on matrix completion on real data sets . We observe a percentage of the ( user , rating ) entries of a matrix and the task is to predict the unobserved ratings , with the assumption that the true matrix has low rank . The datasets we considered were MovieLens 100k and MovieLens 1M ( _CITE_ ), which consist of user ratings of movies , and Jester 1 and Jester 3 ( http :// goldberg . berkeley . edu / jesterdata /), which consist of users and ratings of jokes ( Jester 2 showed essentially identical performance to Jester 1 ). Following [ 4 ], for MovieLens we uniformly sampled p = 50 % of the available entries for each user for training , and for Jester 1 and Jester 3 we sampled 20 , respectively 8 , ratings per user , and we used 10 % for validation . The error was measured as normalized mean absolute error , Iltrue - s /( r_ - ' where rmin and rmax are lower and upper bounds for the ratings [ 4 ].__label__Material|Data|Use
The methods follow , e . g ., example 3 . 2 . 12 of [ 5 ] — basically , a generalization of the classical theorem on the asymptotic distribution of the maximum likelihood estimator in regular parametric families . Again , see the longer draft at _CITE_ , liam for the precise definition of the approximation error and the full expression for a ( k0 ). We have developed an algorithm for the computation of argmaxvMN ( V ), and numerical results show that 1 : 4 can be competitive with spike - triggered average or covariance techniques even in cases in which \- kcoRR ) are zero . We present a brief application of Ko in section 4 .__label__Supplement|Document|Introduce
This work is supported by the NSF ( CRCNS 26 - 1004 - 04xx ), an HFSP award to IRF ( 26 - 6302 - 87 ), and the Simons Foundation through the Simons Collaboration on the Global Brain . The authors acknowledge the Texas Advanced Computing Center ( TACC ) at The University of Texas at Austin ( URL : _CITE_ ) for providing HPC resources that have contributed to the research results reported within this paper .__label__Supplement|Website|Use
For all methods , the parameters C and p for Indefinite SVM are tuned by cross - validation and we terminate the algorithm if the relative change of the objective value is less than 10 − 6 . In Table 3 , we report the average test set accuracy (%) and CPU time ( seconds ) across different algorithms : smooth optimization method ( SMM ), simplified projected gradient method ( SPGM ), analytic center cutting plane method ( ACCPM ), and semi - infinite quadratically constrained linear programming ( SIQCLP ). For the QCLP sub - problem in the SIQCLP method , we use Mosek software package ( _CITE_ ). We can see that test accuracies are statistically the same across different algorithms , which validates our analysis on the objective function . In particular , we observe that SMM is consistently more efficient than other methods , especially for a large number of training samples .__label__Method|Tool|Use
For the univariate densities , we use a standard Gaussian kernel density estimator ( see , for example , [ Bowman and Azzalini , 1997 ]). Using an identical procedure , we learn a linear Gaussian BN baseline where Xi ∼ N ( apai , Qi ) so that each variable Xi is normally distributed around a linear combination of its parents Pai ( see [ Koller and Friedman , 2009 ] for details on this standard approach to structure learning ). For the GCBN model , we also compare to Nonparametric BP ( NBP ) [ Sudderth et al ., 2010a ] using D . Bickson ’ s code [ Bickson , 2008 ] and A . Ihlers KDE Matlab package ( _CITE_ ihler / code / kde . html ), which relies on a mixture of Gaussians for message representation . In this case , since our univariate densities are constructed using Gaussian kernels , there is no approximation in the NBP representation and all approximations are due to message computations . To carry out message products , we tried all 7 sampling - based methods available in the KDE package .__label__Method|Code|Use
Also , the number of words in a document D will usually be much smaller than the vocabulary size V . The final model , which we refer to as Document NADE ( DocNADE ), is illustrated in Figure 1 . A pseudocode for computing p ( v ) and the parameter learning gradients for a given document is provided in the supplementary material and our code is available here : _CITE___label__Method|Code|Produce
Recognizing that popular performance metrics for evaluating generative models all subject to issues [ 7 ], we adopted a pair image generation performance metric for comparison . Many details including the network architectures and additional experiment results are given in the supplementary materials . An implementation of CoGAN is available in _CITE_ Digits : We used the MNIST training set to train CoGANs for the following two tasks . Task A is about learning a joint distribution of a digit and its edge image .__label__Method|Code|Produce
Of particular scienti £ c interest to astronomers is the identi £ cation and cataloging of sky objects with a “ bent - double ” morphology , indicating clusters of galaxies ([ 8 ], see Figure 1 ). Due to the very large number of observed deep - sky radio sources , ( on the order of 106 so far ) it is infeasible for the astronomers to label all of them manually . The data from the FIRST Survey ( _CITE_ ) is available in both raw image format and in the form of a catalog of features that have been automatically derived from the raw images by an image analysis program [ 8 ]. Each entry corresponds to a single detectable “ blob ” of bright intensity relative to the sky background : these entries are called Figure 1 : 4 examples of radio - source galaxy images . The two on the left are labelled as “ bent - doubles ” and the two on the right are not .__label__Material|Data|Use
We used 141 modules given by Lee et al . [ 3 ] as groups of gene traits , and extracted unique 1 , 260 SNPs from 2 , 956 SNPs for our analysis . For prior biological knowledge on SNPs used for adaptive multi - task Lasso , we downloaded 12 features from Saccharomyces Genome Database ( _CITE_ ) including 11 discrete and 1 continuous feature ( conservation score ). For a discrete feature , we set its value as ft = s ( 2 ) if the feature is found on the j - th SNP , ft = s ( 1 ) otherwise . For conservation score , we set ft = s ( score ).__label__Material|Data|Use
Sound sources used were : broadband white noise ; recordings of instruments and voices from the RWC Music Database ( _CITE_ ); and recordings of vowel - consonant - vowel sounds ( Lorenzi et al ., 1999 ). All sounds were of 1 second duration and were presented at 80 dB SPL . Sounds were filtered by head - related impulse responses ( HRIRs ) from the IRCAM LISTEN HRTF Database ( http :// recherche . ircam . fr / equipes / salles / listen / index . html ).__label__Material|Data|Use
We demonstrate LTP by applying it to learn a linear mapping from image features to poses while LTP could be used to learn more sophisticated models . We will show that the algorithms learned by LTP are more generalizable both across subjects and over time on the same subject respectively . In this experiment , we use six walking sequences from CMU MoCap database ( _CITE_ ). The data are from 3 subjects , with sequences 1 & 2 from the first subject , sequences 3 & 4 from the second subject and sequences 5 & 6 from the third subject . Each sequence consists of about 70 frames .__label__Material|Data|Use
Operators : We implemented 1D calibrators and multilinear interpolation over a lattice as new C ++ operators in TensorFlow [ 15 ] and express each layer as a computational graph node using these new and existing TensorFlow operators . Our implementation is open sourced and can be found in _CITE_ We use the Adam optimizer [ 16 ] and batched stochastic gradients to update model parameters . After each batched gradient update , we project parameters to satisfy their monotonicity constraints .__label__Method|Code|Produce
For our algorithms , we swept values of a in { 2 − 1 , 2 − 2 , ..., 2 − 10 } from largest to smallest and chose the solution that produced the least number of stars . Furthermore , we warm - started the symmetric algorithm with the star - pattern solution of the asymmetric algorithm to make it converge more quickly . We first explored six standard data - sets from UCI _CITE_ in the uniform anonymity setting . Figure 3 ( a ) summarizes the results where utility is plotted against 6 . Fewer stars imply greater utility and larger 6 implies higher anonymity .__label__Material|Data|Use
mating this lifetime , we hope it might be possible to form a link between the hidden states and the underlying physical process that governs the dynamics of switching . Despite the apparent limitation of Poisson statistics , it is a simple matter to generalize our model to hidden state distributions with long tails ( e . g ., power - law lifetime distributions ): By cascading many hidden states into a chain ( with fixed CIFs ), a power - law distribution can be approximated by the combination of multiple exponentials with different lifetimes . Our code is available at _CITE___label__Method|Code|Produce
lenge [ 10 ]. For each drug , we had 14 features that describes their chemical and physical properties such as molecular weight , XLogP3 and hydrogen bond donor count , and were downloaded from National Center for Biotechnology Information ( _CITE_ ). For the cell line features , we ran principle component analysis ( PCA ) and used the top 45 principal components that accounted for more than 99 . 99 % of the total data variance . We compared the four different methods with four different q values : 20 - 50 %.__label__Supplement|Website|Use
In this section , we present experimental results to compare CLIME - ADMM with existing algorithms and show its scalability . In all experiments , we use the low rank property of the sample covariance matrix and do not assume any other special structures . Our algorithm is implemented in a shared - memory architecture using OpenMP ( _CITE_ ) and a distributed - memory architecture using OpenMPI ( http :// www . open - mpi . org ) and ScaLAPACK [ 15 ] ( http :// www . netlib . org / scalapack /).__label__Method|Tool|Use
We used three public benchmark datasets [ 4 ] named human , cross - species , and new . The positive pre - miRNA sequences in all three datasets were obtained from miRBase [ 25 ] ( release 18 ). For the negative training sets , we obtained noncoding RNAs other than pre - miRNAs and exonic regions of protein - coding genes from NCBI ( _CITE_ ), fRNAdb [ 23 ], NONCODE [ 24 ], and__label__Material|Data|Use
Meanwhile , our method waives nearly all computational burdens of SS at test - time — the effective running time for proposals is just 10 milliseconds . Using the expensive very deep models of [ 19 ], our detection method still has a frame rate of 5fps ( including all steps ) on a GPU , and thus is a practical object detection system in terms of both speed and accuracy ( 73 . 2 % mAP on PASCAL VOC 2007 and 70 . 4 % mAP on 2012 ). Code is available at _CITE___label__Method|Code|Produce
These systems include previously studied gravitational and billiards systems [ 3 , 1 ] with the added challenge of natural image backgrounds . For example videos of these systems , see the Supplementary Material or visit ( _CITE_ ). One limitation of the above systems is that the positions , masses , and radii of all objects are either visually observable in every frame or global constants . Furthermore , while occlusion is allowed , the objects have the same radius so total occlusion never occurs .__label__Supplement|Document|Produce
These systems include previously studied gravitational and billiards systems [ 3 , 1 ] with the added challenge of natural image backgrounds . For example videos of these systems , see the Supplementary Material or visit ( _CITE_ ). One limitation of the above systems is that the positions , masses , and radii of all objects are either visually observable in every frame or global constants . Furthermore , while occlusion is allowed , the objects have the same radius so total occlusion never occurs .__label__Supplement|Media|Produce
2 ), and the LS ( Eq . 3 ) approaches . In the simulation , the source sequence ( s ) was a sentence of speech ( approximately 1 . 5 seconds ), and the filters ( h1 and h2 ) were two measured RIRs from York MARDY database ( _CITE_ sap / mardy . htm ) but down - sampled to 16 kHz ( from originally 48 kHz ). The original filters in the database were not sparse , but they had many tiny coefficients which were in the range of measurement uncertainty . To make the simulated filters sparse , we simply zeroed out those coefficients whose amplitudes were less than 2 % of the maximum .__label__Material|Data|Use
All the code used for performing the experiments is available from _CITE_ 6By one - shot we mean that , given the algorithm above , each experiment was only run once with one setting of the random seed and the values of and given . If we were producing a visualisation for only one dataset this would leave us open to the criticism that our one - shot result was ‘ lucky ’.__label__Method|Code|Use
Thanks to Helmut Schwegler and Robert P . O & apos ; Shea for interesting discussions . Image data courtesy of G . Medoni , UCS Institute for Robotics & Intelligent Systems , B . Bolles , AIC , SRI International , and G . Sommer , Kiel Cognitive Systems Group , Christian - Albrechts - Universitat Kiel . An internetbased implementation of the algorithm presented in this paper is available at _CITE___label__Method|Code|Produce
I am grateful to Rodney Douglas and Kevan Martin for their support , and to Shih - Chii Liu and Stefano Fusi for constructive comments on the manuscript . Some of the ideas that led to the design and implementation of the circuits presented were inspired by the Telluride Workshop on Neuromorphic Engineering ( _CITE_ ).__label__Supplement|Website|Extent
code positions , spread across 70 files and 650 procedures . We randomly selected 100 of the 1500 valid traces for each revision as training data . Both data sets are available at _CITE_ Evaluation criterion . Following the evaluation in [ 1 ], we evaluate how well the models are able to guide the user into the vicinity of a defective code position .__label__Material|Data|Use
G - means and X - means overfit the non - Gaussian datasets , while PG - means and BKM both perform excellently in the number of clusters learned and in learning the true labels according to the VI metric . We tested all of these algorithms on the U . S . Postal Service handwritten digits dataset ( both the train and test portions , obtained from _CITE_ ). Each example is a grayscale image of a handwritten digit . There are 9298 examples in the dataset , and each example has 256 pixels ( 16 pixels on a side ).__label__Material|Data|Use
G - means and X - means overfit the non - Gaussian datasets , while PG - means and BKM both perform excellently in the number of clusters learned and in learning the true labels according to the VI metric . We tested all of these algorithms on the U . S . Postal Service handwritten digits dataset ( both the train and test portions , obtained from _CITE_ ). Each example is a grayscale image of a handwritten digit . There are 9298 examples in the dataset , and each example has 256 pixels ( 16 pixels on a side ).__label__Supplement|Website|Use
It is important to note that the choice of r in the description of Algorithm 1 is a sufficient - not necessary - condition to prove our theoretical bounds . Indeed , a much smaller choice of r , for example r = 10k , is often sufficient for good empirical results . We first experimented with a NIPS documents dataset ( see _CITE_ and [ 10 ]). The data consist of a 184 x 6314 document - term matrix A , with Aij denoting the number of occurrences of the j - th term in the i - th document . Each document is a paper that appeared in the proceedings of NIPS 2001 , 2002 , or 2003 , and belongs to one of the following three topic categories : ( i ) Neuroscience , ( ii ) Learning Theory , and ( iii ) Control and Reinforcement Learning .__label__Material|Data|Use
We test our algorithm on both toy and real world examples , on which we find our method tends to outperform a variety of baseline methods . Our code is available at _CITE_ Stein - Variational - Gradient - Descent . For all our experiments , we use RBF kernel k ( x , x & apos ;) = exp (− 1h || x − x & apos ;|| 22 ), and take the bandwidth to be h = med2 / log n , where med is the median of the pairwise distance between the current points { xi } n i = 1 ; this is based on the intuition that we would have Ej k ( xi , xj ) ≈ n exp (− 1 hmed2 ) = 1 , so that for each xi the contribution from its own gradient and the influence from the other points balance with each other . Note that in this way , the bandwidth h actually changes adaptively across the iterations .__label__Method|Code|Produce
In the experiments , we used the optimizer LOQO ( _CITE_ ). This has the serendipitous advantage that the primal variables b and E can be recovered as the dual variables of the Wolfe dual ( 9 ) ( i . e . the double dual variables ) fed into the optimizer .__label__Method|Tool|Use
We know of no significant disadvantage in using the GTM algorithm in place of the SOM . While we believe the SOM procedure is superseded by the GTM algorithm , is should be noted that the SOM has provided much of the inspiration for developing GTM . A web site for GTM is provided at : _CITE_ which includes postscript files of relevant papers , software implementations in Matlab and C , and example data sets used in the development of the GTM algorithm .__label__Supplement|Website|Produce
marked with ( pos ) or ( neg ) were in the learning set for this focal image . Full rankings for all experimental runs can be browsed at _CITE_ nips2006 . we are not fully exploiting that in these experiments . Instead of using the full pairwise combination of all in - and out - of - class images , we select triplets using elementary feature distances .__label__Supplement|Document|Produce
More experiments and results on real data sets can be found on our web - page _CITE___label__Material|Data|Produce
More experiments and results on real data sets can be found on our web - page _CITE___label__Supplement|Website|Produce
More experiments and results on real data sets can be found on our web - page _CITE___label__Supplement|Document|Produce
1 Task 10 . This is thus related to the supervision in Task 5 except the learner must request help . In our experiments we constructed the ten supervision tasks for the two datasets which are all available for download at _CITE_ They were built in the following way : for each task we consider a fixed policy1 for performing actions ( answering questions ) which gets questions correct with probability era ,, ( i . e . the chance of getting the red text correct in Figs .__label__Material|Data|Use
, u , r Let , r =  T , rgu , r . The following theorem establishes that the difference between the average cost ,˜ r associated with an optimal solution (˜ r , ˜ s1 , ˜ s2 ) to the LP and the optimal average cost  is proportional to the minimal error that can be attained given the choice of basis functions . A proof of this theorem is provided in the appendix of a version of this paper available at _CITE_ .__label__Supplement|Document|Produce
Additional numerical tests evaluating performance of the proposed scheme relative to TWF / WF are presented in this section . For fairness , all pertinent algorithmic parameters involved in each scheme are set to their default values . The Matlab implementations of TGGF are available at _CITE_ The initial estimate was found based on 50 power iterations , and was subsequently refined by T = 103 gradient - like iterations in each scheme . Left panel in Fig .__label__Method|Code|Produce
The problem of inference , recovering the topic distributions from such a collection of documents , is provably NP - hard . Existing literature pursues techniques such as variational methods [ 2 ] or MCMC procedures [ 3 ] for approximating the maximum likelihood estimates . ∗ _CITE_ Given the intractability of the problem one needs further assumptions on topics to derive polynomial time algorithms which can provably recover topics . A possible ( strong ) assumption is that each document has only one topic but the collection can have many topics . A document with only one topic is sometimes referred as a pure topic document .__label__Method|Algorithm|Introduce
Of course , our model is a general statistical model and given an expressive feature map , it can learn any ground truth . In this view the experiments suggest the relative performance of the compared models . The dataset , and an implementation of our model , is available at _CITE_ Table 1 shows some statistics of the dataset . We use two kinds of features , byte - level and instruction - level features .__label__Method|Code|Produce
Of course , our model is a general statistical model and given an expressive feature map , it can learn any ground truth . In this view the experiments suggest the relative performance of the compared models . The dataset , and an implementation of our model , is available at _CITE_ Table 1 shows some statistics of the dataset . We use two kinds of features , byte - level and instruction - level features .__label__Material|Data|Produce
Then , all moved to watch the table tennis game ( 710th frame : one gaze concurrence ). Our method correctly evaluates the gaze concurrences at the location where people look . All results are best seen in the videos from the following project website ( _CITE_ ).__label__Supplement|Website|Produce
Then , all moved to watch the table tennis game ( 710th frame : one gaze concurrence ). Our method correctly evaluates the gaze concurrences at the location where people look . All results are best seen in the videos from the following project website ( _CITE_ ).__label__Supplement|Media|Produce
We show the best performing NHERD variant (‘ project ’ and ‘ exact ’) in each case . [ Crammer and Lee , 2010 ]) ( project and exact variants3 ), and perceptron algorithm with margin ( PAM ) which was shown to be robust to label noise by Khardon and Wachman [ 2007 ]. We use the standard UCI classification datasets , preprocessed and made available by Gunnar R ¨ atsch ( _CITE_ ). For kernelized algorithms , we use Gaussian kernel with width set to the best width obtained by tuning it for a traditional SVM on the noise - free data . For ˜ Blog , we use p + 1 and p − 1 that give the best accuracy in cross - validation .__label__Material|Data|Use
[ 14 , 15 ]. We ' As compiled by David Lewis from the AT & T Research Lab in 1987 . The data can be found at _CITE_ have used the “ ModApte ” split , leading to 9603 training and 3299 test documents . After preprocessing , our training set contained 5561 distinct terms . Performance Evaluation .__label__Material|Data|Use
Active Sampling : One advantage of probabilistic models is that they lend themselves to implementing active methods ( e . g ., Infomax [ 4 ]) for selecting which images should be re - labeled next . We are currently pursuing the development of control policies for optimally choosing whether to obtain more labels for a particular item – so that the inferred Z label for that item becomes more certain – versus obtaining more labels from a particular labeler – so that his / her accuracy a may be better estimated , and all the images that he / she labeled can have their posterior probability estimates of Z improved . A software implementation of GLAD is available at _CITE___label__Method|Code|Produce
Code is available at : _CITE___label__Method|Code|Produce
The multiclass generalization of l1 - MKL proposed in [ 33 ] ( MCMKL ) is state of the art methodology in predicting protein subcellular localization , an important cell biology problem that concerns the estimation of where a protein resides in a cell so that , for example , the identification of drug targets can be aided . We use three multiclass datasets : PSORT +, PSORT - and PLANT provided by the authors of [ 33 ] at _CITE_ together with a dictionary of 69 kernels derived with biological insight : 2 kernels on phylogenetic trees , 3 kernels based on similarity to known proteins ( BLAST E - values ), and 64 kernels based on amino - acid sequence patterns . The statistics of the three datasets are as follows : PSORT + has 541 proteins labeled with 4 location classes , PSORT - has 1444 proteins in 5 classes and PLANT is__label__Material|Data|Use
All source code is made available under a public license . It consists of generic C ++ code which can be extended to various data types and metrics , compiling to a shared library with extensions in Cython for a Python interface . It can currently be found in the git repository _CITE___label__Method|Code|Produce
In this section we present experimental results on several multi - class problems : segment , satimage , and letter from the Statlog collection [ 9 ], USPS [ 4 ], and MNIST [ 7 ]. All data sets are available at _CITE_ t . Their numbers of classes are 7 , 6 , 26 , 10 , and 10 , respectively . From thousands of instances in each data , we select 300 and 500 as our training and testing sets . We consider support vector machines ( SVM ) with RBF kernel e − xi − xj  2 as the binary classifier .__label__Material|Data|Use
∗ Data used in preparation of this article were obtained from the Alzheimer ’ s Disease Neuroimaging Initiative ( ADNI ) database ( adni . loni . ucla . edu ). As such , the investigators within the ADNI contributed to the design and implementation of ADNI and / or provided data but did not participate in analysis or writing of this report . A complete listing of ADNI investigators can be found at : _CITE_ to apply / ADNI Acknowledgement List . pdf .__label__Supplement|Document|Introduce
The learning rate was set so that initially the change in the filters was approximately . In figure ( 2a ) we show a small subset of the inverse - filters given by the pseudo - inverse of , where is the matrix used for sphering the data . & apos ; Collected from _CITE___label__Material|Data|Use
SSGP is similar to the recent models of Le et al . [ 8 ] and Rahimi and Recht [ 9 ], except it learns the locations of the point masses through marginal likelihood optimization . We use the SSGP implementation provided by the authors at _CITE_ To further test the importance of the fast inference ( section 2 . 1 ) used in GPatt , we compare to a GP which uses the SMP kernel of section 2 but with the popular fast FITC [ 10 , 24 ] inference , which uses inducing inputs , and is implemented in GPML ( http :// www . gaussianprocess . org / gpml ). We also compare to GPs with the popular squared exponential ( SE ), rational quadratic ( RQ ) and Mat ´ ern ( MA ) ( with 3 degrees of freedom ) kernels , catalogued in Rasmussen and Williams [ 1 ], respectively for smooth , multi - scale , and finitely differentiable functions .__label__Method|Code|Use
Our algorithms could be useful in various applications . For example , improving the spatial accuracy of weakly supervised learners [ 7 , 11 ] by using thousands of windows per image , using more complex kernels and detecting more classes in kernel SVM object detectors [ 13 , 24 ], and enabling image retrieval systems to search at the window level with any descriptor , rather than returning entire images or be constrained to bag - of - words descriptors [ 20 ]. To encourage these applications , we release our source code at _CITE___label__Method|Code|Produce
CIFAR 10 The result is shown in Figure 1 ( 5 ). We compare our algorithm to a neural net with two convolution layers ( after contrast normalization and max - pooling layers ) and two local layers achieving 11 % test error . The specification is at _CITE_ Our method achieves comparable performance but much faster . ImageNet The result is shown in Figure 1 ( 6 ).__label__Supplement|Document|Produce
In Section 4 . 1 , we will describe the process to collect the data , and the method to monitor the quality of annotations . Some statistics and examples of the dataset will be given in Section 4 . 2 . The latest dataset is available on the project page : _CITE___label__Material|Data|Produce
In Section 4 . 1 , we will describe the process to collect the data , and the method to monitor the quality of annotations . Some statistics and examples of the dataset will be given in Section 4 . 2 . The latest dataset is available on the project page : _CITE___label__Supplement|Website|Produce
are used with and without 3 leading supervised metric learning algorithms — resulting in an overall total of 26 competitive baselines . Our code is implemented in Matlab and is freely available at _CITE_ Datasets and Baselines . We evaluate all approaches on 8 document datasets in the settings of news categorization , sentiment analysis , and product identification , among others .__label__Method|Code|Produce
A later motion - capture session was used to create a 3D morphable model of her face , consisting of a set of 5 morph bases ( k = 5 ). Twenty experts were initialized randomly near the correct pose on frame 1 of the video and propagated using G - flow inference ( assuming an uninformative background ). See _CITE_ for video . Figure 2 shows the distribution of experts for three frames . In each frame , every expert has a hypothesis about the pose ( translation , rotation , scale , and morph coefficients ).__label__Supplement|Media|Produce
We evaluate the residual transfer network against state of the art transfer learning and deep learning methods . Codes and datasets will be available at _CITE___label__Method|Code|Produce
We evaluate the residual transfer network against state of the art transfer learning and deep learning methods . Codes and datasets will be available at _CITE___label__Material|Data|Produce
Each of these images has been subdivided into sixteen non - overlapping images , obtaining a data set with 640 images . Sixteen Gabor filters were used to characterise these images , so that each image is represented by a 16 - dimensional feature vector [ 14 ]. The database extracted from the UCI repository ( _CITE_ ) consists of 2 , 310 outdoor images . The images are subdivided into seven data classes ( brickface , sky , foliage , cement , window , path , and grass ). Nineteen colour and spatial features characterise each image .__label__Material|Data|Use
[ 8 ] and Rahimi and Recht [ 9 ], except it learns the locations of the point masses through marginal likelihood optimization . We use the SSGP implementation provided by the authors at http :// www . tsc . uc3m . es /˜ miguel / downloads . php . To further test the importance of the fast inference ( section 2 . 1 ) used in GPatt , we compare to a GP which uses the SMP kernel of section 2 but with the popular fast FITC [ 10 , 24 ] inference , which uses inducing inputs , and is implemented in GPML ( _CITE_ gpml ). We also compare to GPs with the popular squared exponential ( SE ), rational quadratic ( RQ ) and Mat ´ ern ( MA ) ( with 3 degrees of freedom ) kernels , catalogued in Rasmussen and Williams [ 1 ], respectively for smooth , multi - scale , and finitely differentiable functions . Since GPs with these kernels cannot scale to the large datasets we consider , we combine these kernels with the same fast inference techniques that we use with GPatt , to enable a comparison . 3 Moreover , we stress test each of these methods in terms of speed and accuracy , as a function of available data and extrapolation range , and number of components .__label__Method|Code|Use
We show all the test examples on the first fold of cross validation where k - NN makes an error in estimating the correct digit whilst KDE does not ( 73 mistakes ) and vice - versa ( 23 mistakes ). We chose them by viewing the complete results by eye ( and are thus somewhat subjective ). The complete results can be found at _CITE_ same as the competing algorithms ). We found a and - y for KDE and k for k - NN by another level of 5 - fold cross validation .__label__Supplement|Document|Produce
Videos of these reconstructions have been included in the supplementary material . Best viewed in color . Videos of these samples can be seen at _CITE___label__Supplement|Media|Produce
This shows again that our model can learn discriminative part distributions for each class . Figure 3 shows results for a multi - view experiment where the task is two distinguish between two different views of a car and background . 3The images were obtained from _CITE_ and the car side images from http :// l2r . cs . uiuc . edu / cogcomp / Data / Car /. Notice , that since our algorithm does not currently allow for the recognition of multiple instances of an object we test it on a partition of the the training set in http :// l2r . cs . uiuc . edu / cogcomp / Data / Car / and not on the testing set in that site . The animals data set is a subset of Caltech ’ s 101 categories data set .__label__Supplement|Media|Use
In this paper we exploit this novel connection to show an interesting application of the SVM setup for identfying large dense subgraphs . More specifically we make the following contributions . ∗ Relevant code and datasets can be found on _CITE___label__Method|Code|Produce
In this paper we exploit this novel connection to show an interesting application of the SVM setup for identfying large dense subgraphs . More specifically we make the following contributions . ∗ Relevant code and datasets can be found on _CITE___label__Material|Data|Produce
Future work includes establishing theoretical consistency and uniform convergence rates for the empirical estimators , for example via using recent analysis of random Fourier Features with tight bounds [ 21 ], and a thorough experimental study in the ABC - MCMC context where we see a lot of potential for KMC . It might also be possible to use KMC as a precomputing strategy to speed up classical HMC as in [ 27 ]. For code , see _CITE___label__Method|Code|Produce
Note that the set of active nodes in the diffusion process is a random variable , and the expectation of its size is monotone and submodular [ 16 ]. We use two real - world data sets : ego - Facebook and Weibo . ego - Facebook is downloaded from _CITE_ , and Weibo is crawled from a Chinese microblogging__label__Material|Data|Use
Two problem sizes , 300 / 500 and 800 / 1 , 000 for training / testing , are used . 20 training / testing splits are generated and the testing error rates are averaged . All data used are available at _CITE_ We use the same four ways in Section 6 . 1 to generate Ii . All of them have | I1 |,: ,: | Im |.__label__Material|Data|Use
Psychologists have designed various instruments intended to measure whether individuals exhibit these traits . We consider a survey in which subjects rate fifty statements , such as , “ I am the life of the party ”, on a five point scale : ( 1 ) disagree , ( 2 ) slightly disagree , ( 3 ) neutral , ( 4 ) slightly agree , and ( 5 ) agree . _CITE_ The data consist of answers to these questions from about ten thousand test - takers . The test was designed with the intention that each question should belong to a cluster according to which personality trait the question gauges . Is it true that there are five factors that strongly predict the answers to these questions ?__label__Material|Data|Use
Table 1 also reports the average computing time of SAGE and SAM - LRB . We can see that , by avoiding the log - sum - exp calculation , SAM - LRB ( fixed ) performs more than 7 times faster than SAGE , while SAM - LRB ( optimized ) pays for updating the variational variables . method SAGE SAM - LRB ( fixed ) SAM - LRB ( optimized ) time cost ( minutes ) 3 . 8 0 . 6 3 . 3 We now apply our unsupervised SAM - LRB model to the benchmark NIPS data _CITE_ . Following the same preprocessing and evaluation as in [ 10 , 26 ], we have a training set of 1986 documents with 237 , 691 terms , and a testing set of 498 documents with 57 , 427 terms . For consistency , SAM - LRB is still compared with Dirichlet - Multinomial model ( variational LDA model with symmetric Dirichlet prior ) and SAGE .__label__Material|Data|Use
We have also performed experiments with different values of a and 6 . Despite the fact that larger a and 6 increase label independence on the graph structure and undermine the effectiveness of both V / E - optimality heuristics , we have seen that whenever the V - optimality establishes a superiority over random selections , E - optimality yields better performance . The active learning heuristics to be compared are : _CITE_ number of papers bearing both scholars ’ names . The largest connected component has 1711 nodes and 2898 edges . The node labels were hand assigned in Ji & Han ( 2012 ) to one of the four expertise areas of the scholars : machine learning , data mining , information retrieval , and databases .__label__Method|Algorithm|Compare
Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques , we recommend it as a new standard for topic modeling . Topic models such as latent Dirichlet allocation ( LDA ) [ 3 ] have been recognized as useful tools for analyzing large , unstructured collections of documents . There is a significant body of work applying LDA to an wide variety of tasks including analysis of news articles [ 14 ], study of the history of scientific ideas [ 2 , 9 ], topic - based search interfaces _CITE_ and navigation tools for digital libraries [ 12 ]. In practice , users of topic models are typically faced with two immediate problems : First , extremely common words tend to dominate all topics . Second , there is relatively little guidance available on how to set T , the number of topics , or studies regarding the effects of using a suboptimal setting for T . Standard practice is to remove “ stop words ” before modeling using a manually constructed , corpus - specific stop word list and to optimize T by either analyzing probabilities of held - out documents or resorting to a more complicated nonparametric model .__label__Supplement|Website|Introduce
We evaluate our approach , Multivariate Prediction Games ( MPG ), on the three performance measures of interest in this work : precision at k , F - score , and DCG . Our primary point of comparison is with structured support vector machines ( SSVM )[ 27 ] to better understand the trade - offs between convexly approximating the loss function with the hinge loss versus adversarially approximating the training data using our approach . We employ an optical recognition of handwritten digits ( OPTDIGITS ) dataset [ 17 ] ( 10 classes , 64 features , 3 , 823 training examples , 1 , 797 test examples ), an income prediction dataset (‘ a4a ’ ADULT _CITE_ [ 17 ] ( two classes , 123 features , 3 , 185 training examples , 29 , 376 test examples ), and query - document pairs from the million query TREC 2007 ( MQ2007 ) dataset of LETOR4 . 0 [ 23 ] ( 1700 queries , 41 . 15 documents on average per query , 46 features per document ). Following the same evaluation method used in [ 27 ] for OPTDIGITS , the multi - class dataset is converted into multiple binary datasets and we report the macro - average of the performance of all classes on test data . For OPTDIGITS / ADULT , we use a random 31 of the training data as a holdout validation data to select the L2 regularization parameter trade - off C E 12 − 6 , 2 − 5 ,..., 2 }.__label__Material|Data|Use
The tree optimizer with Direct Search and grid search as subordinate optimizers resulted in error rates of 31 . 72 % and 33 . 33 %. In this application , the proposed method enjoys only modest gains of — 2 % because the variables are tightly coupled , as indicated by the denseness of the graph and the thickness of the junction tree . The third application was to tune the hyperparameters of a multi - class SVM classifier on the RCV1v2 text categorization dataset _CITE_ . This dataset consists of a training set of 23 , 149 documents and a test set of 781 , 265 documents each labeled with one of 101 topics Lewis et al . ( 2004 ).__label__Material|Data|Use
The models are chosen for comparability , since they all embed nodes into a Euclidean latent space . Experiments for all three models were performed using reference implementations by the respective authors . _CITE_ The network exhibits stochastic equivalence ( visible as block structure in the matrix ) and homophily ( concentration of points around the diagonal ). Right : Maximum a posteriori estimate of the function Θ , corresponding to the function in Fig . 1 ( middle ).__label__Method|Code|Use
Finally , our third claim is that data programming is an intuitive and productive framework for domain - expert users , and we report on our initial user studies . Relation Mention Extraction Tasks In the relation mention extraction task , our objects are relation mention candidates x = ( e1 , e2 ), which are pairs of entity mentions e1 , e2 in unstructured text , and our goal is to learn a model that classifies each candidate as either a true textual assertion of the relation R ( e1 , e2 ) or not . We examine a news application from the 2014 TAC - KBP Slot Filling challenge _CITE_ , where we extract relations between real - world entities from articles [ 2 ]; a clinical genomics application , where we extract causal relations between genetic mutations and phenotypes from the scientific literature ; and a pharmacogenomics application where we extract interactions between genes , also from the scientific literature [ 21 ]; further details are included in the Appendix . For each application , we or our collaborators originally built a system where a training set was programmatically generated by ordering the labeling functions as a sequence of if - then - return statements , and for each candidate , taking the first label emitted by this script as the training label . We refer to this as the if - then - return ( ITR ) approach , and note that it often required significant domain expert development time to tune ( weeks or more ).__label__Method|Algorithm|Extent
We also see that GFM has a huge advantage over MEUF regarding the time complexity . All the computations are performed on a typical desktop machine . The results on four commonly used benchmark datasets _CITE_ with known training and test sets are presented in Table 1 , which also includes some basic statistics of these datasets . We additionally present results of the binary relevance ( BR ) approach which trains an independent classifier for each label ( we used the same base learner as in PCC ). We also apply the MEUF method on marginals delivered by BR .__label__Material|Data|Use
In each of the experiments , we used the training regime suggested by the original work , together with a momentum SGD optimizer . We use a batch of 4096 samples as & quot ; large batch & quot ; ( LB ) and a small batch ( SB ) of either 128 ( F1 , C1 , VGG , Resnet44 , C3 , Alexnet ) or 256 ( WResnet ). We compare the original training baseline for small and large batch , as well as the following methods _CITE_ : iterations for each batch size used - effectively multiplying the number of epochs by the relative size of the large batch . Results . Following our experiments , we can establish an empirical basis to our claims .__label__Method|Algorithm|Compare
The conflictual points are peripheral in the projection onto the negative eigenspace centered around the bulk of points whose dissimilarities are essentially Euclidean . Note that because of the huge weights , these effects are largely exaggerated in comparison to real world judgments . Twenty gray - scale 256 x 256 - pixel images of faces were generated from the MPI - face database _CITE_ . All faces were normalized to have the same mean and standard deviation of pixel intensities , the same area , and were aligned such that the cross - correlation of each face to a mean face of the database was maximal . Faces were presented at an angle of 15 degrees and were illuminated primarily with ambient light together with an additional but weak point source at 65 degrees azimuth and 25 degree eccentricity .__label__Material|Data|Use
We empirically evaluate IAF by applying the idea to improve variational autoencoders . Please see appendix C for details on the architectures of the generative model and inference models . Code for reproducing key empirical results is available online _CITE_ . In this expermiment we follow a similar implementation of the convolutional VAE as in ( Salimans et al ., 2014 ) with ResNet ( He et al ., 2015 ) blocks . A single layer of Gaussian stochastic units of dimension 32 is used .__label__Method|Code|Produce
While the latter is a strongly performing method , it also suffers from scalability problems . Learning a model of term correlations over a large vocabulary is a considerable challenge that requires a large amount of training data . Standard retrieval datasets like TREC _CITE_ or LETOR [ 22 ] contain only a few hundred training queries , and are hence too small for that purpose . Moreover , some datasets only provide pre - processed features like tf , idf or BM25 , and not the actual words . Click - through from web search engines could provide valuable supervision .__label__Material|Data|Introduce
As a result , we can approximate any sufficiently smooth bounded function of u arbitrarily well , given sufficiently many training classification problems . To validate our method , we evaluated its ability to learn parameter functions on a variety of email and webpage classification tasks in which the number of classes , K , was large ( K = 10 ), and the number of number of training examples per class , m / K , was small ( m / K = 2 ). We used the dmoz Open Directory Project hierarchy , the 20 Newsgroups dataset , the Reuters - 21578 dataset , and the Industry Sector dataset _CITE_ . classifications using non - discriminative methods : the learned g , Naive Bayes , and TFIDF , respectively . Columns 5 - 7 give the corresponding values for the discriminative methods : softmax regression , 1 - vs - all SVMs , and multiclass SVMs .__label__Material|Data|Use
We analyze the link between “ Vote ” and “ PChange ”. Though the marginal correlation between them ( without X ) is only 0 . 0389 , which is the second lowest absolute pairwise correlation , the link is firstly recovered by SLasso . It has been suggested that there is indeed a connection _CITE_ . This shows that after taking features into account , the dependence structure of response variables may change and hidden relations could be discovered . The main factors in this case are “ percentage of housing unit change ” ( X1 ) and “ population percentage of people over 65 ” ( X2 ).__label__Supplement|Document|Introduce
Moreover , according to the theory , the experiments suggest that the number of performed epochs increases if the number of available training points increases . Real data . We tested the kernelized version of our algorithm ( see Remark 1 and Appendix A ) on the cpuSmall _CITE_ , Adult and Breast Cancer Wisconsin ( Diagnostic ) real - world datasets . We considered a subset of Adult , with n = 1600 . The results are shown in Figure 2 .__label__Material|Data|Use
In Tab . 2 we report the mean angle the object would need to be rotated ( on a fixed 3D axis ) to move from the predicted to the ground truth pose [ 12 ]. All the models were implemented using TensorFlow _CITE_ [ 1 ] and were trained with Stochastic Gradient Descent plus momentum [ 27 ]. Our initial learning rate was multiplied by 0 . 9 every 20 , 000 steps ( mini - batches ). We used batches of 32 samples from each domain for a total of 64 and the input images were mean - centered and rescaled to [− 1 , 1 ].__label__Method|Code|Use
We now expect 1000 modes in this data set , corresponding to the number of possible triples of digits . Again , to focus the evaluation on the difference in the learning algorithms , we use the same generator architecture for all methods . In particular , the generator architecture is an off - the - shelf standard implementation _CITE_ of DCGAN [ 17 ]. measure ( IvOM ), and sample quality ( as measured by KL ) on Stacked - MNIST and CIFAR . VEEGAN captures the most modes and also achieves the highest quality .__label__Method|Code|Use
All kernels are normalized to have a unit length in the feature space . Moreover , we use 10 - fold cross validation with a binary C - Support Vector Machine ( SVM ) where the C value for each fold is independently tuned using training data from that fold . In order to exclude random effects of the fold assignments , this experiment is repeated 10 times and average prediction accuracy of 10 experiments with their standard deviations are reported _CITE_ . In our first experiment , we compare the base kernels with their smoothed variants . As can be seen from Table 1 , smoothing improves the classification accuracy of every base kernel on every dataset with majority of the improvements being statistically significant with p & lt ; 0 . 05 .__label__Supplement|Document|Produce
The number of hidden neurons in the MLP network was 40 . The learning algorithm was run for 2000 iterations . _CITE_ ferent approximations plotted against reference values evaluated by sampling . The left subfigure shows the values from experiments using the proposed approximation and the right subfigure from experiments using the Taylor approximation . Fig .__label__Method|Algorithm|Use
We assume that we have already completed 50 iterations of an optimization of the same model on the related USPS digits task . The USPS data is only 1 / 6 the size of MNIST and each image contains 16 x 16 pixels , so it is considerably cheaper to evaluate . Convolutional neural networks on pixels We applied convolutional neural networks _CITE_ ( CNNs ) to the Street View House Numbers ( SVHN ) [ 21 ] dataset and bootstrapped from a previous run of Bayesian optimization using the same model trained on CIFAR - 10 [ 22 , 6 ]. At the time , this model represented the state - of - the - art . The SVHN dataset has the same input dimension as CIFAR - 10 , but is 10 times larger .__label__Method|Algorithm|Use
For prediction in the autonomous setup , A � is used to propagate the state wT forward to make predictions with no feedback , and in the observer setup , a Kalman filter ( Algorithm 4 in the supplementary ) with N determined using Proposition 2 , and locations picked randomly , is used to propagate wT forward to make predictions . We also compare with a baseline GP ( denoted by ‘ original GP ’), which is the sparse GP model trained using all of the available data . Our first dataset , the Intel Berkeley research lab temperature data , consists of 50 wireless temperature sensors in indoor laboratory region spanning 40 . 5 meters in length and 31 meters in width _CITE_ . Training data consists of temperature data on March 6th 2004 at intervals of 20 minutes ( beginning 00 : 20 hrs ) which totals to 72 timesteps . Testing is performed over another 72 timesteps beginning 12 : 20 hrs of the same day .__label__Material|Data|Use
We use four problem libraries suggested in [ 12 ]. MIK [ 13 ] is a set of MILP problems with knapsack constraints . Regions and Hybrid are sets of problems of determining the winner of a combinatorial auction , generated from different distributions by the Combinatorial Auction Test Suite ( CATS ) _CITE_ [ 14 ]. CORLAT [ 15 ] is a real dataset used for the construction of a wildlife corridor for grizzly bears in the Northern Rockies region . The number of variables ranges from 300 to over 1000 ; the number of constraints ranges from 100 to 500 .__label__Method|Tool|Use
The datasets used in this work are two collections of Wikipedia articles : one is in English and Japanese , the other is in English , Japanese , and Spanish , and articles in each collection are connected across languages via inter - language links , as of November 2 , 2009 . We extracted text content from the original Wikipedia articles , removing link information and revision history information . We used WP2TXT _CITE_ for this purpose . For English articles , we removed 418 types of standard stop words [ 12 ]. For Spanish articles , we removed 351 types of standard stop words [ 13 ].__label__Method|Tool|Use
In our implementation of the BCD method , we alternate between computing the proximity operator with respect to the rows and to the columns . Since the BCD method allows us to compute a duality gap when solving the proximal problem , we can run the method until the duality gap is below a given error threshold εk to find an xk + 1 satisfying ( 4 ). In our experiments , we used the four data sets examined by [ 33 ] _CITE_ and we choose λrow = . 01 and λcol = . 01 , which yielded approximately 25 – 40 % non - zero entries in X ( depending on the data set ). Rather than assuming we are given the Lipschitz constant L , on the first iteration we set L to 1 and following [ 2 ] we double our estimate anytime g ( xk ) & gt ; g ( yk − 1 ) + ( g & apos ;( yk − 1 ), xk − yk − 1 ) + ( L / 2 )|| xk − yk − 1 || . We tested three different ways to terminate the approximate proximal problem , each parameterized by a parameter α : Note that all three strategies lead to global convergence in the case of the basic proximal - gradient method , the first two give a convergence rate up to some fixed optimality tolerance , and in this paper we have shown that the first one ( for large enough α ) yields a convergence rate for an arbitrary optimality tolerance .__label__Material|Data|Use
We then run MAP inference on the factor graph using the LP formulation in [ 9 ] and compare the quality of the solutions obtained by Thetis with a Gibbs sampling - based approach [ 26 ]. We follow the LP - rounding algorithm in [ 16 ] to solve the MAP estimation problem . For entity linking , we use the TAC - KBP 2010 benchmark _CITE_ . The input graphical model has 12K boolean random variables and 17K factors . For text chunking , we use the CoNLL 2000 shared task .__label__Material|Data|Use
Data . We obtained microfinance data of Bolivia from several sources , such as ASOFIN , the apex body of MFIs in Bolivia , and the Central Bank of Bolivia . _CITE_ We were only able to collect somewhat coarse , region - level data ( June 2011 ). It consists of eight MFIs operating in 10 regions . Computational Results .__label__Material|Data|Use
We take meta search as the target application , and use the LETOR [ 13 ] benchmark datasets in the experiments . LETOR is a public collection created for ranking research . _CITE_ There are two meta search datasets in LETOR , MQ2007 - agg and MQ2008 - agg . In addition to using them , we also composed a smaller dataset from MQ2008 - agg , referred to as MQ2008 - small , by selecting queries with no more than 8 documents from the MQ2008 - agg dataset . This small dataset is used to perform detailed investigations on the CPS model and other baseline models .__label__Material|Data|Use
For sequence labeling we experiment on POS and ChineseOCR . The POS dataset is a subset of Penn treebank that contains 3 , 808 sentences , 196 , 223 words , and 45 POS labels . The HIT - MW _CITE_ ChineseOCR dataset is a hand - written Chinese character dataset from [ 17 ]. The dataset has 12 , 064 hand - written sentences , and a total of 174 , 074 characters . The vocabulary ( label ) size is 3 , 039 .__label__Material|Data|Use
To lower down the risk of overfitting , we allow the weight sharing of the word embedding layer between the LSTMs in the first and third components . We also adopt the transposed weight sharing scheme as proposed in [ 25 ], which allows the weight sharing between word embedding layer and the fully connected Softmax layer . To train our method , we construct a large - scale Freestyle Multilingual Image Question Answering dataset _CITE_ ( FM - IQA , see details in Section 4 ) based on the MS COCO dataset [ 21 ]. The current version of the dataset contains 158 , 392 images with 316 , 193 Chinese question - answer pairs and their corresponding English translations . To diversify the annotations , the annotators are allowed to raise any question related to the content of the image .__label__Material|Data|Use
We split our dataset into 5 , 000 discharge summaries for training and 1 , 000 for testing . The text of the discharge summaries was tokenized with NLTK . _CITE_ A fixed vocabulary was formed by taking the top 10 , 000 tokens with highest document frequency ( exclusive of names , places and other identifying numbers ). The study was approved by the Institutional Review Board and follows HIPAA ( Health Insurance Portability and Accountability Act ) privacy guidelines . Amazon . com , an online retail store , organizes its catalog of products in a mulitply - rooted hierarchy and provides textual product descriptions for most products .__label__Method|Tool|Use
The Office part consists of 4 , 652 images in 31 categories collected from three distinct domains ( tasks ): Amazon ( A ), which contains images downloaded from amazon . com , Webcam ( W ) and DSLR ( D ), which are images taken by Web camera and digital SLR camera under different environmental variations . This dataset is organized by selecting the 10 common categories shared by the Office dataset and the Caltech - 256 ( C ) dataset [ 12 ], hence it yields four multi - class learning tasks . Office - Home _CITE_ [ 26 ] This dataset is to evaluate transfer learning algorithms using deep learning . It consists of images from 4 different domains : Artistic images ( A ), Clip Art ( C ), Product images ( P ) and Real - World images ( R ). For each domain , the dataset contains images of 65 object categories collected in office and home settings .__label__Material|Data|Introduce
Datasets . Our tasks are based on two families of graphs . The first family of instances ( frb59 - 26 - 1 to frb59 - 26 - 5 ) was obtained from Bhoslib _CITE_ ( Benchmark with Hidden Optimum Solutions ); they are considered difficult problems [ 25 ]. The instances in this family are similar ; the first is reported in the figures of this section , while the remainder appear in Appendix E . The second family of instances are social networking graphs obtained from the Stanford Network Analysis Platform ( SNAP ) . System Setup .__label__Material|Data|Use
We compiled Markov networks into three types of arithmetic circuits . The first compilation ( AC1 ) is to decomposable and deterministic ACs using ace [ Chavira and Darwiche , 2008 ]. _CITE_ The second compilation ( AC2 ) is also to decomposable and deterministic ACs , but using the approach proposed in Choi et al . [ 2013 ]. The third compilation is to PSDDs as discussed above .__label__Method|Tool|Use
by adding instance noise [ 24 ] or by using the Wasserstein - divergence [ 4 , 11 ]: while these strategies try to ensure the existence of Nash - equilibria , our paper deals with their computation and the numerical difficulties that can arise in practice . In summary , our contributions are as follows : The proofs for the theorems in this paper can be found the supplementary material . _CITE_ In this section we first revisit the concept of Generative Adversarial Networks ( GANs ) from a divergence minimization point of view . We then introduce the concept of a smooth ( non - convex ) two - player game and define the terminology used in the rest of the paper . Finally , we describe simultaneous gradient ascent , the de - facto standard algorithm for finding Nash - equilibria of such games , and derive some of its properties .__label__Supplement|Document|Produce
But an understanding of how these methods relate and when one method is preferable to another is still lacking . Here , we present a novel unified approach to interpreting model predictions . _CITE_ Our approach leads to three potentially surprising results that bring clarity to the growing space of methods : The best explanation of a simple model is the model itself ; it perfectly represents itself and is easy to understand . For complex models , such as ensemble methods or deep networks , we cannot use the original model as its own best explanation because it is not easy to understand . Instead , we must use a simpler explanation model , which we define as any interpretable approximation of the original model .__label__Method|Algorithm|Produce
Models . We compare the base exponential family embeddings ( EFE ) model ( Rudolph et al ., 2016 ) with our context selection procedure . We implement the amortized inference network described in Section 2 . 3 _CITE_ , for different values of the prior hyperparameter a ( Eq . 5 ) ( see below ). For the movie data , in which the ratings range from 0 to 3 , we use a binomial conditional distribution ( Eq .__label__Method|Code|Produce
After data cleaning , the dataset is with T = 122 , V = 16 . International Disaster ( ID ) : The International Disaster dataset contains essential core data on the occurrence and effects of over 22 , 000 mass disasters in the world from 1900 to the present day . A count matrix with T = 115 and V = 12 is built from the events of disasters occurred in Europe from the year 1902 to 2016 , classified according to their disaster types . Annual Sheep Population ( ASP ) _CITE_ : The Annual Sheep Population contains the sheep population in England & Wales from the year 1867 to 1939 yearly . The data matrix is with T = 73 , V = 1 . We set K = 3 for ID and ASP datasets , while set K = 10 for the others .__label__Material|Data|Use
We focussed on the prototype “ house - price - 16L ”, composed of 16 variables . We derived this prototype by including all the other variables related to these 16 variables . The final dataset is then composed of 37 variables , split up into 10 groups _CITE_ . We randomly selected 8000 observations for training and left the 14732 for testing . We divided the training observations into 10 distinct datasets .__label__Material|Data|Use
Another important aspect of multi - class settings is that the use of more classes which is discriminated by the BCI device only at lower accuracy is likely to confuse the user . Current BCI research strives for enhanced information transfer rates . Several options are available : ( 1 ) training of the BCI users , which can be somewhat tedious if up to 300 hours of training would be necessary , ( 2 ) invasive BCI techniques , which we consider not applicable for healthy human test subjects , ( 3 ) improved machine learning and signal processing methods where , e . g ., new filtering , feature extraction and sophisticated classifiers are constantly tuned and improved _CITE_ , ( 4 ) faster trial speeds and finally ( 5 ) more classes among which the BCI user is choosing . This work analysed the theoretical and practical implications of using more than two classes , and also psychological issues were shortly discussed . In essence we found that higher a ITR is achieved with three classes , however , it seems unlikely that it can be increased by moving above four classes .__label__Method|Algorithm|Introduce
More specifically , we attempted to construct the subset of features G ⊂ X that minimizes the normalized mean squared regression error ( NMSE ) of a Gaussian process regressor . We do so by selecting the feature x ( i ) maximizing dependence between the feature set Gi = { Gi − 1 , x ( i )} and the target variable y at each iteration i ∈ { 1 ,... 10 }, such that G0 = {∅} and x ( i ) ∈/ Gi − 1 . We considered 12 heterogeneous datasets , obtained from the UCI dataset repository , the Gaussian process web site Data and the Machine Learning data set repository _CITE_ . Random training / test partitions are computed to be disjoint and equal sized . Since G can be multi - dimensional , we compare RDC to the non - linear methods dCor , HSIC and CHSIC .__label__Material|Data|Use
Here , we show that the Z statistic , Z = Ei ( m2Xi − 3 � 2i ) 2 −( m22Xi + m21Yi ), which is the core of our testing algorithm , can accurately dism1 m2 ( Xi + Yi ) tinguish whether two words are very similar based on surprisingly small samples of the contexts in which they occur . Specifically , for each pair of words , a , b that we consider , we select m1 random occurrences of a and m2 random occurrences of word b from the Google books corpus , using the Google Books Ngram Dataset . _CITE_ We then compare the sample of words that follow a with the sample of words that follow b . Henceforth , we refer to these as samples of the set of bi - grams involving each word . Figure 1 ( a ) illustrates the Z statistic for various pairs of words that range from rather similar words like “ smart ” and “ intelligent ”, to essentially identical word pairs such as “ grey ” and “ gray ” ( whose usage differs mainly as a result of historical variation in the preference for one spelling over the other ); the sample size of bi - grams containing the first word is fixed at m1 = 1 , 000 , and the sample size corresponding to the second word varies from m2 = 50 through m2 = 1 , 000 .__label__Material|Data|Use
Our question becomes whether it is possible to use SALSTMs for tasks that have a substantial number of words , such as web articles or emails and where the content consists of many different topics . For that purpose , we carry out the next experiments on the 20 newsgroups dataset [ 17 ]. _CITE_ There are 11 , 293 documents in the training set and 7 , 528 in the test set . We use 15 % of the training documents as a validation set . Each document is an email with an average length of 267 words and a maximum length of 11 , 925 words .__label__Material|Data|Use
Finally , while we discuss the main application of our results in the context of advertising exchanges , our model and results apply to the broad space of platforms that serve as intermediaries between buyers and sellers , and help run many repeated auctions over time . The issue of dynamic revenue sharing also arises when Amazon or eBay act as a platform and splits revenues from a sale with the sellers , or when ride - sharing services such as Uber or Lyft split the fare paid by the passenger between the driver and the platform . Uber for example mentions in their website _CITE_ that : “ Drivers using the partner app are charged an Uber Fee as a percentage of each trip fare . The Uber Fee varies by city and vehicle type and helps Uber cover costs such as technology , marketing and development of new features within the app .” We propose different designs of auctions and revenue sharing policies in exchanges and analyze them both theoretically and empirically on data from a major ad exchange . We compare against the naïve policy described above .__label__Supplement|Website|Introduce
The dropout is added to some fully connected layers or locally connected layers . The rectified linear activation function is used for all neurons . All the experiments are conducted using the cuda - convnet library _CITE_ . The training procedure is similar to [ 9 ] using mini - batch SGD with momentum ( 0 . 9 ). The size of mini - batch is fixed to 128 .__label__Method|Code|Use
10 for the MAP estimation is − 1 2D wTw . Our dataset contains covtype ( D = 581 , 012 , p = 54 ), obtained from the LIBSVM data website . _CITE_ We separate 5K examples as the test set . We test two types of learning rates , constant and decayed . For constant rates , we explore ρt ∈ { 0 . 01 , 0 . 05 , 0 . 1 , 0 . 2 , 0 . 5 , 1 }.__label__Material|Data|Use
10 for the MAP estimation is − 1 2D wTw . Our dataset contains covtype ( D = 581 , 012 , p = 54 ), obtained from the LIBSVM data website . _CITE_ We separate 5K examples as the test set . We test two types of learning rates , constant and decayed . For constant rates , we explore ρt ∈ { 0 . 01 , 0 . 05 , 0 . 1 , 0 . 2 , 0 . 5 , 1 }.__label__Supplement|Website|Use
In this section , we demonstrate the behaviors of MATk learning coupled with different individual losses for binary classification and regression on synthetic and real datasets , with minimizing the average loss and the maximum loss treated as special cases for k = n and k = 1 , respectively . For simplicity , in all experiments , we use homogenized linear prediction functions f ( x ) = wT x with parameters w and the Tikhonov regularizer Q ( w ) = 1 2C || w || , and optimize the MATk learning objective with the stochastic gradient descent method given in ( 4 ). Binary Classification : We conduct experiments on binary classification using eight benchmark datasets from the UCI _CITE_ and KEEL data repositories to illustrate the potential effects of using ATk loss in practical learning to adapt to different underlying data distributions . A detailed description of the datasets is given in supplementary materials . The standard individual logistic loss and hinge loss are combined with different aggregate losses .__label__Material|Data|Use
Moreover , we show that analysis of the marginal distribution provides an indicator for the confidence in those predictions . Finally , we investigate the convergence rate and runtime performance of the algorithm in detail . For our evaluation dataset , we collected all Wikipedia articles that appeared in the featured list _CITE_ for a two week period in Oct . 2009 , thus obtaining 2460 documents . Of these , we considered a subset of 1717 documents assigned to the 7 most popular categories . After stemming and stop - word removal , we represented the text of each document as a tf / idf - weighted word vector .__label__Supplement|Document|Use
GANs train two deep networks in concert : a generator network that maps random noise , usually drawn from a multi - variate Gaussian , to data items ; and a discriminator network that estimates the likelihood ratio of the generator network to the data distribution , and is trained using an adversarial principle . Despite an enormous amount of recent work , GANs are notoriously fickle to train , and it has been observed [ 1 , 19 ] that they often suffer from mode collapse , in which the generator network learns how to generate samples from a few modes of the data distribution but misses many other modes , even though samples from the missing modes occur throughout the training data . To address this problem , we introduce VEEGAN , _CITE_ a variational principle for estimating implicit probability distributions that avoids mode collapse . While the generator network maps Gaussian random noise to data items , VEEGAN introduces an additional reconstructor network that maps the true data distribution to Gaussian random noise . We train the generator and reconstructor networks jointly by introducing an implicit variational principle , which encourages the reconstructor network not only to map the data distribution to a Gaussian , but also to approximately reverse the action of the generator .__label__Method|Algorithm|Produce
This dataset comes from a biological domain and involves the problem of Yeast gene functional classification . We use the data studied by Elisseeff and Weston [ 5 ], which contains n = 2417 examples ( Yeast genes ) with d = 103 input features ( results from microarray experiments ). _CITE_ We follow the approach of [ 5 ] and predict each gene ’ s membership in t = 14 functional classes . For this larger dataset , we omitted the computationally expensive EM4 + SVM methods , and tuned only µ for matrix completion while fixing A = 1 . Table 5 reveals that MC - b leads to statistically significantly lower transductive label error for this biological dataset .__label__Material|Data|Use
As outlined in Section 1 , for p = 3 and small k we can take sign considerations into account : Theorem 9 ( Low rank ). There is a constant a & gt ; 0 and a sufficiently small constant y & gt ; 0 such that for any symmetric tensor T = T * + E E Rn with E satisfying ( 2 ), rank ( T *) & lt ; 2 , and Ak & gt ; _ 1 / nγ , then Algorithm 3 runs in O ( n − α ) time . Our implementation shares the same code base _CITE_ as the sketching - based robust tensor power method proposed in [ 23 ]. We ran our experiments on an i7 - 5820k CPU with 64 GB of memory in singlethreaded mode . We ran two versions of our algorithm : the version with pre - scanning scans the full tensor to accurately measure per - slice Frobenius norms and make samples for each slice in proportion to its Frobenius norm in APPROXTIVW ; the version without pre - scanning assumes that the Frobenius norm of each slice is bounded by nα �� T �� F , a E ( 0 , 1 ] and uses b / n samples per slice , where b is the total number of samples our algorithm makes , analogous to the sketch length b in [ 23 ].__label__Method|Code|Produce
Even as the tensor fluctuates in magnitude by more than a factor of two , the maximum absolute value of the mantissa P is safely prevented from overflowing . The cost of this approach is that in the last example P reaches 3 bits below the cutoff , leaving the top bits zero and using only 13 of the 16 bits for representing data . The experiments described below were performed on Nvidia GPUs using the neon deep learning framework _CITE_ . In order to simulate the flex16 + 5 data format we stored tensors using an int16 type . Computations such as convolution and matrix multiplication were performed with a set of GPU kernels which convert the underlying int16 data format to float32 by multiplying with κ , perform operations in floating point , and convert back to int16 before returning the result as well as P . The kernels also have the ability to compute only P without writing any outputs , to prevent writing invalid data during exponent initialization .__label__Method|Code|Use
Ultimately the justification for an algorithm is practical applicability . We demonstrate this based on three datasets : embedding of digits of the USPS database , the Newsgroups 20 dataset containing Usenet articles in text form , and a collection of NIPS papers from 1987 to 1999 . _CITE_ We compare “ colored ” MVU ( also called MUHSIC , maximum unfolding via HSIC ) to MVU [ 1 ] and PCA , highlighting places where MUHSIC produces more meaningful results by incorporating side information . Further details , such as effects of the adjacency matrices and a comparison to Neighborhood Component Analysis [ 6 ] are relegated to the appendix due to limitations of space . For images we use the Euclidean distance between pixel values as the base metric .__label__Material|Data|Compare
Taking the first filter , we iteratively add filters one by one to the end of the filter set , picking the filter that minimizes the group sparsity penalty . We learn three different convolutional DBN models to use as the feature representation for deep congealing . First , we learn a one - layer CRBM from the Kyoto images , _CITE_ a standard natural image data set , to evaluate the performance of congealing with self - taught CRBM features . Next , we learn a one - layer CRBM from LFW face images , to compare performance when learning the features directly on images of the object class to be aligned . Finally , we learn a two - layer CDBN from LFW face images , to evaluate performance using higher - order features .__label__Material|Data|Use
With smaller amounts , however , this isn ' t always possible , and when it is possible , it can produce worse actual running times than a disk - based approach . As our goal is to judge streaming algorithms under low memory conditions , we used the first approach , which is more fitting to such a constraint . Each algorithm _CITE_ was programmed in C / C ++, compiled with g ++, and run under Ubuntu Linux ( 10 . 04 LTS ) on HP Pavilion p6520f Desktop PC , with an AMD Athlon II X4 635 Processor running at 2 . 9 GhZ and with 6 GB main memory ( although nowhere near the entirety of this was used by any algorithm ). For StreamKM ++, the authors ' implementation [ 2 ], also in C , was used instead . With all algorithms , the reported cost is determined by taking the resulting k facilities and computing the k - means cost across the entire dataset .__label__Method|Algorithm|Produce
Speakers may start speaking or become silent at any time . Similarly to [ 23 ], we collect data from several speakers from the PASCAL ‘ CHiME ’ Speech Separation and Recognition Challenge website . _CITE_ The voice signal for each speaker consists of 4 sentences , which we append with random pauses in between each sentence . We artificially mix the data 10 times ( corresponding to 10 microphones ) with mixing weights sampled from Uniform ( 0 , 1 ), such that each microphone receives a linear combination of all the considered signals , corrupted by Gaussian noise with standard deviation 0 . 3 . We consider two scenarios , with 5 and 15 speakers , and subsample the data so that we learn from T = 1 , 354 and T = 1 , 087 datapoints , respectively .__label__Material|Data|Use
Speakers may start speaking or become silent at any time . Similarly to [ 23 ], we collect data from several speakers from the PASCAL ‘ CHiME ’ Speech Separation and Recognition Challenge website . _CITE_ The voice signal for each speaker consists of 4 sentences , which we append with random pauses in between each sentence . We artificially mix the data 10 times ( corresponding to 10 microphones ) with mixing weights sampled from Uniform ( 0 , 1 ), such that each microphone receives a linear combination of all the considered signals , corrupted by Gaussian noise with standard deviation 0 . 3 . We consider two scenarios , with 5 and 15 speakers , and subsample the data so that we learn from T = 1 , 354 and T = 1 , 087 datapoints , respectively .__label__Supplement|Website|Use
For comparison , consider the results of [ 26 ] obtained for the compression of the fully - connected layers of the Krizhevsky - type network [ 13 ] with the Fastfood method . The model achieves compression factors of 2 - 3 without decreasing the network error . In all experiments we use our MATLAB extension of the MatConvNet framework _CITE_ [ 24 ]. For the operations related to the TT - format we use the TT - Toolbox implemented in MATLAB as well . The experiments were performed on a computer with a quad - core Intel Core i5 - 4460 CPU , 16 GB RAM and a single NVidia Geforce GTX 980 GPU .__label__Method|Code|Extent
All experiments were conducted on a Linux machine with AMD Opteron 2GHz CPU and 2GB RAM . First , we evaluated the feature - sign search algorithm for learning coefficients with the L1 sparsity function . We compared the running time and accuracy to previous state - of - the - art algorithms : a generic QP solver , a modified version of LARS [ 12 ] with early stopping , grafting [ 13 ], and Chen et al .’ s interior point method [ 11 ]; _CITE_ all the algorithms were implemented in MATLAB . For each dataset , we used a test set of 100 input vectors and measured the running time10 and the objective function at convergence . Table 1 shows both the running time and accuracy ( measured by the relative error in the final objective value ) of different coefficient learning algorithms .__label__Method|Algorithm|Compare
We kept 1 , 000 , 000 such relationships to build a training set , 50 , 000 for a validation set and 250 , 000 for test . All triplets are unique and we made sure that all words appearing in the validation or test sets were occurring in the training set . _CITE_ Practical training setup . During the training phase , we optimized over the validation set various parameters , namely , the size p E 125 , 50 , 100 } of the representations , the dimension d E { 50 , 100 , 200 } of the latent decompositions ( 2 ), the value of the regularization parameter A as a fraction 11 , 0 . 5 , 0 . 1 , 0 . 05 , 0 . 011 of nT x d , the stepsize in 10 . 1 , 0 . 05 , 0 . 011 and the weighting of the negative triplets . Moreover , to speed up the training , we gradually increased the number of sampled negative verbs ( cf .__label__Material|Data|Use
To test whether this is indeed the case on real data sets , we performed experiments on a set of freely available benchmark binary classification data sets . The protein ( n = 145751 , p = 74 ) data set was obtained from the KDD Cup 2004 website , while the rcv1 ( n = 20242 , p = 47236 ) and covertype ( n = 581012 , p = 54 ) data sets were obtained from the LIBSVM data website . _CITE_ Although our method can be applied to any differentiable function , on these data sets we focus on an ` 2 - regularized logistic regression problem , with A = 1 / n . We split each dataset in two , training on one half and testing on the other half . We added a ( regularized ) bias term to all data sets , and for dense features we standardized so that they would have a mean of zero and a variance of one .__label__Material|Data|Use
To test whether this is indeed the case on real data sets , we performed experiments on a set of freely available benchmark binary classification data sets . The protein ( n = 145751 , p = 74 ) data set was obtained from the KDD Cup 2004 website , while the rcv1 ( n = 20242 , p = 47236 ) and covertype ( n = 581012 , p = 54 ) data sets were obtained from the LIBSVM data website . _CITE_ Although our method can be applied to any differentiable function , on these data sets we focus on an ` 2 - regularized logistic regression problem , with A = 1 / n . We split each dataset in two , training on one half and testing on the other half . We added a ( regularized ) bias term to all data sets , and for dense features we standardized so that they would have a mean of zero and a variance of one .__label__Supplement|Website|Use
If not specified otherwise , xeext = argmaxx a ( x ) is computed using gradientbased search with multiple restarts ( see supplementary material for details ). The code used is made publicly available at http :// www . cs . toronto . edu /˜ jasper / software . html . We first compare to standard approaches and the recent Tree Parzen Algorithm _CITE_ ( TPA ) of Bergstra et al . [ 5 ] on two standard problems . The Branin - Hoo function is a common benchmark for Bayesian optimization techniques [ 2 ] that is defined over x E R where 0 & lt ; x1 & lt ; 15 and − 5 & lt ; x2 & lt ; 15 .__label__Method|Algorithm|Compare
and trained on millions of features based on these words . The authors of [ 15 ] used a model similar to the naive full rank model ( 1 ), but for the task of image retrieval , and [ 13 ] also used a related ( regression - based ) method for advert placement . These techniques are implemented in related software to these two publications , PAMIR _CITE_ and Vowpal Wabbit . When the memory usage is too large , the latter bins the features randomly into a reduced space ( hence with random collisions ), a technique called Hash Kernels [ 25 ]. In all cases , the task of document retrieval , and the use of low - rank approximation or polynomial features is not studied .__label__Method|Tool|Use
Figure 2 summarizes ROC curves of all methods averaged over 200 simulations . We see that EPIC also outperforms the competing estimators throughout all settings . To illustrate the effectiveness of the proposed EPIC method , we adopt the breast cancer data _CITE_ , which is analyzed in Hess et al . ( 2006 ). The data set contains 133 subjects with 22 , 283 gene expression levels .__label__Material|Data|Use
Today , DNNs are almost exclusively trained on one or many very fast and power - hungry Graphic Processing Units ( GPUs ) ( Coates et al ., 2013 ). As a result , it is often a challenge to run DNNs on target low - power devices , and substantial research efforts are invested in speeding up DNNs at run - time on both general - purpose ( Gong et al ., 2014 ; Han et al ., 2015b ) and specialized computer hardware ( Chen et al ., 2014 ; Esser et al ., 2015 ). This paper makes the following contributions : The code for training and running our BNNs is available on - line ( both Theano _CITE_ and Torch framework ). In this section , we detail our binarization function , show how we use it to compute the parameter gradients , and how we backpropagate through it . Deterministic vs Stochastic Binarization When training a BNN , we constrain both the weights and the activations to either + 1 or − 1 .__label__Method|Code|Produce
3 . CiteSeer citation network . _CITE_ This is another citation graph of 3312 publications , each of which is classified into one of six classes : agents , artificial intelligence , databases , information retrieval , machine learning , human computer interaction . The network has 4732 links . We took its largest connected component , with 2109 nodes and 3665 undirected and unweighted edges .__label__Supplement|Website|Introduce
We collect the adjusted close prices of these selected indices from May 2nd , 2006 to April 12th , 2012 , and use linear interpolation to estimate the prices on those dates when the data are not available . We apply our proposed model with SVAR ( 1 ) to model the spatiotemporal variance dependencies of the data . For the contemporaneous causal structure discovery , we use LiNGAM - GC - UK , C - M , LiNGAM and Direct - LiNGAM _CITE_ to estimate the causal ordering . The discovered causal orderings of different algorithms are shown in Table 2 . From Table 2 , we see that in the causal ordering discovered by LiNGAM - GC - UK and LiNGAM , the stock indices in US , i . e ., DJI and NDX are contemporaneously affected by the indices in Europe .__label__Method|Tool|Use
We can safely bundle such exclusive features . To this end , we design an efficient algorithm by reducing the optimal bundling problem to a graph coloring problem ( by taking features as vertices and adding edges for every two features if they are not mutually exclusive ), and solving it by a greedy algorithm with a constant approximation ratio . We call the new GBDT algorithm with GOSS and EFB LightGBM _CITE_ . Our experiments on multiple public datasets show that LightGBM can accelerate the training process by up to over 20 times while achieving almost the same accuracy . The remaining of this paper is organized as follows .__label__Method|Tool|Use
All data instances in the dataset are 20 frames long ( 10 frames for the input and 10 frames for the prediction ) and contain two handwritten digits bouncing inside a 64 × 64 patch . The moving digits are chosen randomly from a subset of 500 digits in the MNIST dataset . _CITE_ The starting position and velocity direction are chosen uniformly at random and the velocity amplitude is chosen randomly in [ 3 , 5 ). This generation process is repeated 15000 times , resulting in a dataset with 10000 training sequences , 2000 validation sequences , and 3000 testing sequences . We train all the LSTM models by minimizing the cross - entropy loss using back - propagation through time ( BPTT ) [ 2 ] and RMSProp [ 24 ] with a learning rate of 10 − 3 and a decay rate of 0 . 9 .__label__Material|Data|Use
We call this Ladder network wrapped in the TAG framework Tagger . This is illustrated in Figure 3 and the corresponding pseudocode can be found in Algorithm 1 . We explore the properties and evaluate the performance of Tagger both in fully unsupervised settings and in semi - supervised tasks in two datasets _CITE_ . Although both datasets consist of images and grouping is intuitively similar to image segmentation , there is no prior in the Tagger model for images : our results ( unlike the ConvNet baseline ) generalize even if we permute all the pixels . Shapes .__label__Material|Data|Use
Clearly rescaling by the actual norms help reduce the estimation uncertainty . This phenomenon is more prominent when the true dot products are close to ± 1 , which makes sense because cos ✓ has a small slope when cos ✓ approaches ± 1 , and hence the uncertainty from angles may produce smaller distortion compared to that from norms . In the extreme case when cos ✓ = ± 1 , rescaled JL embedding can perfectly recover the true dot product _CITE_ . In the lower part of Figure 2 ( b ) we illustrate how to construct unit - norm vectors from a cone with angle ✓. Given a fixed unit - norm vector x , and a random Gaussian vector t with expected norm tan (✓/ 2 ), we construct new vector y by randomly picking one from the two possible choices x + t and —( x + t ), and then renormalize it .__label__Method|Algorithm|Extent
It has two classes and three domains , as shown in Figure 1 . The two source domains D1 and D2 were created to have both conditional and marginal probability differences with the target domain data so as to provide an ideal testbed for the proposed domain adaptation methodology . The three real - world datasets used are 20 Newsgroups _CITE_ , Sentiment Analysis and another dataset of multi - dimensional feature vectors extracted from SEMG ( Surface electromyogram ) signals . The 20 Newsgroups dataset is a collection of approximately 20 , 000 newsgroup documents , partitioned ( nearly ) evenly across 20 different categories . We represented each document as a binary vector of the 100 most discriminating words determined by Weka ’ s info - gain filter [ 22 ].__label__Material|Data|Use
Hamming loss ), but not for non - decomposable F1 or Jaccard metrics . We refer to this as Binary Relevance ( BR ) [ 15 ]. We use four benchmark multilabel datasets _CITE_ in our experiments : ( i ) SCENE , an image dataset consisting of 6 labels , with 1211 training and 1196 test instances , ( ii ) BIRDS , an audio dataset consisting of 19 labels , with 323 training and 322 test instances , ( iii ) EMOTIONS , a music dataset consisting of 6 labels , with 393 training and 202 test instances , and ( iv ) CAL500 , a music dataset consisting of 174 labels , with 400 training and 100 test instances . We perform logistic regression ( with L2 regularization ) on a separate subsample to obtain estimates of ˆ �..( x ) of P ( Y .. = 11x ), for each label m ( as described in Section 4 ). All the methods we evaluate rely on obtaining a good estimator for the conditional probability .__label__Material|Data|Use
We denote our proposed sparse embedded k - means clustering algorithm as SE for short . This section evaluates the performance of the proposed method on four real - world data sets : COIL20 , SECTOR , RCV1 and ILSVRC2012 . The COIL20 [ 20 ] and ILSVRC2012 [ 21 ] data sets are collected from website34 , and other data sets are collected from the LIBSVM website _CITE_ . The statistics of these data sets are presented in the Supplementary Materials . We compare SE with several other dimensionality reduction techniques : After dimensionality reduction , we run all methods on a standard k - means clustering package , which is from website with default parameters .__label__Material|Data|Use
We denote our proposed sparse embedded k - means clustering algorithm as SE for short . This section evaluates the performance of the proposed method on four real - world data sets : COIL20 , SECTOR , RCV1 and ILSVRC2012 . The COIL20 [ 20 ] and ILSVRC2012 [ 21 ] data sets are collected from website34 , and other data sets are collected from the LIBSVM website _CITE_ . The statistics of these data sets are presented in the Supplementary Materials . We compare SE with several other dimensionality reduction techniques : After dimensionality reduction , we run all methods on a standard k - means clustering package , which is from website with default parameters .__label__Supplement|Website|Use
We parametrize the set A as the set of matrices obtained as an outer product of vectors from A1 = { z E Rk : zi > 0 d i } and A2 = { z E Rd : zi > 0 d i }. The LMO is approximated using a truncated power method [ 55 ], and we perform atom correction with greedy coordinate descent see , e . g ., [ 29 , 18 ], to obtain a better objective value while maintaining the same ( small ) number of atoms . We consider three different datasets : The Reuters Corpus _CITE_ , the CBCL face dataset and the KNIX dataset . The subsample of the Reuters corpus we used is a term frequency matrix of 7 , 769 documents and 26 , 001 words . The CBCL face dataset is composed of 2 , 492 images of 361 pixels each , arranged into a matrix .__label__Material|Data|Use
We implemented PSA by replacing the L - BFGS optimizer in CRF ++ [ 11 ]. For SMD , we used the implementation available in the public domain . Our SGD implementation for CRF is from Bottou _CITE_ . All the above implementations are revisions of CRF ++. Finally , we ran the original CRF ++ with default settings to obtain the performance results of LBFGS .__label__Method|Code|Produce
Figure 3 summarizes the performances of the detectors for a set of objects on isolated patches ( not whole images ) taken from the test set . The results vary in quality since some objects are harder to recognize than others , and because some objects have less training data . When we trained and tested our detector on the training / testing sets of side - views of cars from UIUC _CITE_ , we outperformed the detector of [ 1 ] at every point on the precision - recall curve ( results not shown ), suggesting that our base - line detectors can match state - of - the - art detectors when given enough training data . One way to improve the speed and accuracy of a detector is to reduce the search space , by only running the detector in locations / scales that we expect to find the object . The expected location / scale can be computed on a per image basis using the gist , as we explain below .__label__Material|Data|Use
The red and blue curves intersect at nearly the same positions in ( a )( b ) and in ( c )( d ), even though the risk minimizers in the experiments were locally optimal and regularized , making our estimation error bounds inexact . Benchmark data Table 2 summarizes the specification of benchmarks , which were downloaded from many sources including the IDA benchmark repository [ 29 ], the UCI machine learning repository , the semi - supervised learning book [ 30 ], and the European ESPRIT 5516 project . _CITE_ In Table 2 , three rows describe the number of features , the number of data , and the ratio of P data according to the true class labels . Given a random sampling of X +, X − and Xu , the test set has all the remaining data if they are less than 10 , or else drawn uniformly from the remaining data of size 10 . For benchmark data , the linear model for the artificial data is not enough , and its kernel version is employed .__label__Material|Data|Use
More details are found in Algorithm 1 , and Appendix E . 4 gives the gradient formulas . In this section , we use two text games from [ 11 ] to evaluate our proposed model and demonstrate the idea of interpreting the decision making processes : ( i ) “ Saving John ” and ( ii ) “ Machine of Death ” ( see Appendix C for a brief introduction of the two games ). _CITE_ The action spaces of both games are defined by natural languages and the feasible actions change over time , which is a setting that Q - LDA is designed for . We choose to use the same experiment setup as [ 11 ] in order to have a fair comparison with their results . For example , at each m - th experience - replay learning ( see Algorithm 1 ), we use the softmax action selection rule [ 21 , pp . 30 – 31 ] as the exploration policy to collect data ( see Appendix E . 3 for more details ).__label__Supplement|Media|Use
Our model takes around 5 days to train on a Nvidia Tesla K40m . To binarize predicted masks we simply threshold the continuous output ( using a threshold of . 1 for PASCAL and . 2 for COCO ). All the experiments were conducted using Torch7 _CITE_ . In this section , we evaluate the performance of our approach on the PASCAL VOC 2007 test set [ 7 ] and on the first 5000 images of the MS COCO 2014 validation set [ 21 ]. Our model is trained on the COCO training set which contains about 80 , 000 images and a total of nearly 500 , 000 segmented objects .__label__Method|Code|Use
Based on this experiment , we expect not only that the search space of the loss function between modern neural networks can be easily nearly convex [ 23 ], but also that regularizers , such as dropout , make the search space smooth and the point in the search space have a good accuracy in continual learning . We evaluate our approach on four experiments , whose settings are intensively used in the previous works [ 4 , 8 , 7 , 12 ]. For more details and experimental results , see Appendix D . The source code for the experiments is available in Github repository _CITE_ . Disjoint MNIST Experiment . The first experiment is the disjoint MNIST experiment [ 4 ].__label__Method|Code|Produce
We tested on widely - used graph classification benchmarks from different domains [ cf . 4 , 23 , 19 , 24 ]: MUTAG , PTC - MR , NCI1 and NCI109 are graphs derived from small molecules , PROTEINS , D & D and ENZYMES represent macromolecules , and COLLAB and REDDIT are derived from social networks . _CITE_ All data sets have two class labels except ENZYMES and COLLAB , which are divided into six and three classes , respectively . The social network graphs are unlabelled and we considered all vertices uniformly labelled . All other graph data sets come with vertex labels .__label__Material|Data|Use
The amount of � 1 regularization ( λ2 ) is selected to give an approximate / 10 nonzero coefficients . Implementation details are available in Appendix E . We chose the 3 datasets described in Table 1 Results . We compare three parallel asynchronous methods on the aforementioned datasets : PROXASAGA ( this work ), _CITE_ ASYSPCD , the asynchronous proximal coordinate descent method of Liu & Wright ( 2015 ) and the ( synchronous ) FISTA algorithm ( Beck & Teboulle , 2009 ), in which the gradient computation is parallelized by splitting the dataset into equal batches . We aim to benchmark these methods in the most realistic scenario possible ; to this end we use the following step size : 1 / 2L for PROXASAGA , 1 / L , for ASYSPCD , where L , is the coordinate - wise Lipschitz constant of the gradient , while FISTA uses backtracking line - search . The results can be seen in Figure 1 ( top ) with both one ( thus sequential ) and ten processors .__label__Material|Data|Use
Note that the same feature can be used for different events . Feature Description Position difference in position , distance to border , overlap with border ; Intensity difference in intensity histogram / sum / mean / deviation , intensity of father cell ; Shape difference in shape , difference in size , shape compactness , shape evenness ; Others division angle pattern , mass evenness , eccentricity of father cell . We evaluated the proposed method on two publicly available image sequences provided in conjunction with the DCellIQ project [ 16 ] and the Mitocheck project _CITE_ [ 12 ]. The two datasets show a certain degree of variations such as illumination , cell density and image compression artifacts ( Fig . 2 ).__label__Method|Algorithm|Extent
We observed in unreported experiments that there is little of statistical significance lost , in terms of predictive performance , when assuming a factorable form for each ˆpn . LP proceeds by propagating the approximate moments such that The required derivatives follow straightforwardly and details are included in the accompanying material . The approximate predictive distribution for a new data point x * requires a Monte Carlo estimate employing samples drawn from a K - dimensional multivariate Gaussian for which details are given in the supplementary material _CITE_ . By assuming a factorable approximate posterior , as in the variational approximation [ 3 ], a distinct simplification of the problem setting follows , where now we assume that gn ( Fn ,·) = Hk NFn , k ( µn , k , An , k ) i . e . is a factorable distribution .__label__Supplement|Document|Produce
( 6 ) does include a sum over distinct pairs of units , as well as a sum over coupled triplets of units , such triplets are excluded by the bipartite structure of the RBM . However , coupled quadruplets do contribute to the fourth - order term and therefore fourth - and higher - order approximations require much more expensive computations [ 21 ], though it is possible to utilize adaptive procedures [ 19 ]. To evaluate the performance of the proposed deterministic EMF RBM training algorithm _CITE_ , we perform a number of numerical experiments over two separate datasets and compare these results with both CD - 1 and PCD . We first use the MNIST dataset of labeled handwritten digit images [ 25 ]. The dataset is split between 60 000 training images and 10 000 test images .__label__Method|Algorithm|Use
The first application was a face detector . The program has five parameters : the size , in pixels , of the smallest face to consider , the minimum distance , in pixels , between detected faces ; a floating point subsampling rate for building a multiresolution pyramid of the input image ; a boolean flag that determines whether to apply non - maximal suppression ; and the choice of one of four wavelets to use . Our goal was to minimize the detection error rate of this program on the GENKI - SZSL dataset of 3 , 500 faces _CITE_ . Depending on the parameter settings , evaluating the accuracy of the program on this dataset takes between 2 seconds and 2 minutes . Algorithm 1 was run with a grid search as a subordinate optimizer with three discrete values along the continuous dimensions .__label__Material|Data|Use
Indeed , in a case where the optimization has to be stopped before the end of the budget is reached , one would wish to have collected part of the reward . We now evaluate the performance on test functions . _CITE_ We consider four rollout configurations R - 4 - 9 ( h = 4 , y = 0 . 9 ), R - 4 - 10 ( h = 4 , y = 1 . 0 ), R - 5 - 9 ( h = 5 , y = 0 . 9 ) and R - 5 - 10 ( h = 5 , y = 1 . 0 ) and two additional BO algorithms : PES [ 7 ] and the non - greedy GLASSES [ 5 ]. We use a squareexponential kernel for each algorithm ( hyper - parameters : maximum variance u = 4 , noise variance A = 10 − , length scale L set to 10 % of the design space length scale ). We generate 40 designs from a uniform distribution on X , and use them as 40 different initial guesses for optimization .__label__Material|Data|Use
We will compare the new approach with the AFHMM + PR [ 26 ] using the set of statistics τ described in Section 5 . 2 . The key difference between our method AFHMM + LBM and AFHMM + PR is that AFHMM + LBM models the statistics τ conditional on the number of cycles ξ . We apply AFHMM , AFHMM + PR and AFHMM + LBM to the Household Electricity Survey ( HES ) data _CITE_ . This data set was gathered in a recent study commissioned by the UK Department of Food and Rural Affairs . The study monitored 251 households , selected to be representative of the population , across England from May 2010 to July 2011 [ 27 ].__label__Material|Data|Use
We compare the performance of LIBSVM with CrossTraining using LIBSVM with s = 5 , averaging over 10 splits . The results given in figure 3 show a reduction in SVs and computation time using Cross - Training , with no loss in accuracy . Our second experiment involves the discrimination of digits 3 and 8 in the MNIST _CITE_ database . Artificial noise was introduced by swapping the labels of 0 %, 5 %, 10 % and 15 % of the examples . There are 11982 training examples and 1984 testing examples .__label__Material|Data|Use
Input : · Update u to obtain ujr , r = 1 , · · · | S | using ( 9 ); Output : The final p . s . d . matrix X ∈ RDXD , X = PJj = 1 wjZj . used USPS and MNIST handwritten digits , ORL face recognition datasets , Columbia University Image Library ( COIL20 ) _CITE_ , and UCI machine learning datasets ( datasets 7 - 13 ), Twin Peaks and Helix . The last two are artificial datasets . Experimental results are obtained by averaging over 10 runs ( except USPS - 1 ).__label__Material|Data|Use
With , the M - step is similar to the regression case , with playing the role of observed data . In all the experiments we use kernel classifiers , with Gaussian kernels , i . e ., where is a parameter that controls the kernel width . Our first experiment is mainly illustrative and uses Ripley ’ s synthetic data _CITE_ ; the optimal error rate for this problems is [ 3 ]. Table 3 shows the average test set error ( on 1000 test samples ) and the final number of kernels , for 20 classifiers learned from 20 random subsets of size 100 from the original 250 training samples . For comparison , we also include results ( from [ 20 ]) for RVM , variational RVM ( VRVM ), and SVM classifiers .__label__Material|Data|Use
The vocabulary size is 13 , 649 , and there are 2 . 3 million tokens in total . We randomly divide the corpus into a 1 , 392 - document training set and a 348 - document test set . New York Times The New York Times Annotated Corpus _CITE_ consists of over 1 . 8 million articles appearing in the New York Times between 1987 and 2007 . The vocabulary is pruned to 8 , 000 words . We hold out a randomly selected subset of 5 , 000 test documents , and use the remainder for training .__label__Material|Data|Introduce
These differences yield an important consequence : VDMGP can infer automatically the latent dimensionality K of data , but SPGP - DR is unable to , since increasing K is never going to decrease its likelihood . Thus , VDMGP follows Occam ’ s razor on the number of latent dimensions K . We will assess VDMGP on real - world datasets . For this purpose we will use the two data sets from the WCCI - 2006 Predictive Uncertainty in Environmental Modeling Competition run by Gavin Cawley _CITE_ , called Temp and SO2 . In dataset Temp , maximum daily temperature measurements have to be predicted from 106 input variables representing large - scale circulation information . For the SO2 dataset , the task is to predict the concentration of SO2 in an urban environment twenty - four hours in advance , using information on current SO2 levels and meteorological conditions .__label__Material|Data|Use
Its prediction result and time spent are then reported by taking the average together with the standard deviation over all runs . For comparison , we employ 11 state - of - the - art online kernel learning methods : perceptron [ 5 ], online gradient descent ( OGD ) [ 6 ], randomized budget perceptron ( RBP ) [ 9 ], forgetron [ 8 ] projectron , projectron ++ [ 20 ], budgeted passive - aggressive simple ( BPAS ) [ 17 ], budgeted SGD using merging strategy ( BSGD - M ) [ 7 ], bounded OGD ( BOGD ) [ 21 ], Fourier OGD ( FOGD ) and Nystrom OGD ( NOGD ) [ 16 ]. Their implementations are published as a part of LIBSVM , BudgetedSVM _CITE_ and LSOKL toolboxes . We use a Windows machine with 3 . 46GHz Xeon processor and 96GB RAM to conduct our experiments . In the first experiment , we investigate the effect of hyperparameters , i . e ., budget size B , merging size k and random feature dimension D ( cf .__label__Method|Tool|Use
This is equivalent to using the L1 ; 1 norm in our framework which , as discussed above , cannot enforce group sparsity among different features over all tasks . In this section , we study our methods empirically on two cancer classification applications using microarray gene expression data . We compare our methods with three related methods : multi - task feature learning ( MTFL ) [ 1 ] _CITE_ , multi - task feature selection using l1 ; 2 regularization [ 16 ] , and multitask feature selection using l1 ;,, regularization [ 20 ] . We first conduct empirical study on a breast cancer classification application . This application consists of three learning tasks with data collected under different platforms [ 21 ].__label__Method|Algorithm|Compare
In this experiment , an interactive machine translation scenario is simulated where a given machine translation system is adapted to user style and domain based on feedback to predicted translations . Domain adaptation from Europarl to NewsCommentary domains using the data provided at the WMT 2007 shared task is performed for French - to - English translation . _CITE_ The MT experiments are based on the synchronous context - free grammar decoder cdec [ 5 ]. The models use a standard set of dense and lexicalized sparse features , including an out - of and an in under F1 - score , and for machine translation under BLEU . Higher is better for both scores .__label__Material|Data|Use
‘ Books ’ concerns data collected from the Book - Crossing community about users providing ratings on books where we extracted the bipartite network from the ratings . ‘ Citations ’ is the co - authorship network based on preprints posted to Condensed Matter section of ArXiv between 1995 and 1999 [ 15 ]. ‘ Movielens100k ’ contains information about users rating particular movies _CITE_ from which we extracted the bipartite network . Finally , ‘ IMDB ’ contains information about actors co - starring a movie . The sizes of the different networks are given in We evaluate the fit of four different models on these datasets .__label__Material|Data|Introduce
Our goal is not to reproduce the success of deep learning in face verification [ 7 , 14 ], but to stress the importance of robust training and to compare the proposed Euc - DRT objective with popular alternatives . Note also that it is difficult to compare with deep learning methods when training sets are proprietary [ 12 – 14 ]. We adopt the experimental framework used in [ 2 ], and train a deep network on the WDRef dataset , where each face is described using a high dimensional LBP feature [ 3 ] ( available at _CITE_ ) that is reduced to a 5000 - dimensional feature using PCA . The WDRef dataset is significantly smaller than the proprietary datasets typical of deep learning , such as the 4 . 4 million labeled faces from 4030 individuals in [ 14 ], or the 202 , 599 labeled faces from 10 , 177 individuals in [ 12 ]. It contains 2 , 995 subjects with about 20 samples per subject .__label__Method|Algorithm|Use
Moreover , we define J ( π ) = 1211VF ( π ) 112 2 and assume that in the above neighborhood , F is sufficiently smooth so that there is a constant L > 0 such that 11VJ ( π & apos ;)− VJ ( π ) 112 & lt ; L11π & apos ;− π112 for any π , π & apos ; in the neighborhood of π *. Then using the step - size η = δ / L in Algorithm 1 , we have That is , the squared norm of the gradient VF ( π ) decreases geometrically . Here we discuss principled extensions of the heuristic proposed in [ 10 ] and real / fake statistics discussed by Larsen and Sønderby _CITE_ . Furthermore we discuss practical advice that slightly deviate from the principled viewpoint . Goodfellow et al .__label__Method|Algorithm|Extent
Moreover , we employed the Weisfeiler - Lehman subtree kernel [ 13 ], denoted as KWL , as the state - of - the - art graph kernel , which has a parameter h of the number of iterations . We first compared the geometric random walk kernel KGR to other kernels in graph classification . The classification accuracy of each graph kernel was examined by 10 - fold cross validation with multiclass C - support vector classification ( libsvm _CITE_ was used ), in which the parameter C for CSVC and a parameter ( if one exists ) of each kernel were chosen by internal 10 - fold cross validation ( CV ) on only the training dataset . We repeated the whole experiment 10 times and reported average Second , the two random walk kernels KGR and Kk × show greater accuracy than naive linear kernels on edge and vertex histograms , which indicates that halting is not occurring in these datasets . It is also noteworthy that employing a Gaussian RBF kernel on vertex - edge histograms leads to a clear improvement over linear kernels on all three datasets .__label__Method|Code|Use
Each 6 - core processor is linked to a 32 GB memory bank with independent memory controllers leading to a total system memory of 256 GB ( 32 x 8 ) that can be globally addressed from each core . The four sockets are interconnected using HyperTransport - 3 technology . Datasets A variety of datasets were chosen _CITE_ for experimentation ; these are summarized in Table 1 . We consider four datasets : ( i ) NEWS20 contains about 20 , 000 UseNet postings from 20 newsgroups . The data was gathered by Ken Lang at Carnegie Mellon University circa 1995 .__label__Material|Data|Use
The red and blue curves intersect at nearly the same positions in ( a )( b ) and in ( c )( d ), even though the risk minimizers in the experiments were locally optimal and regularized , making our estimation error bounds inexact . Benchmark data Table 2 summarizes the specification of benchmarks , which were downloaded from many sources including the IDA benchmark repository [ 29 ], the UCI machine learning repository , the semi - supervised learning book [ 30 ], and the European ESPRIT 5516 project . _CITE_ In Table 2 , three rows describe the number of features , the number of data , and the ratio of P data according to the true class labels . Given a random sampling of X +, X − and Xu , the test set has all the remaining data if they are less than 10 , or else drawn uniformly from the remaining data of size 10 . For benchmark data , the linear model for the artificial data is not enough , and its kernel version is employed .__label__Material|Data|Use
The second , Cluster - Based LML ( CBLML ), is also a variant of PLML without weight learning . Here we learn one local metric for each cluster and we assign a weight of one for a basis metric Mbi if the corresponding cluster of Mbi contains the instance , and zero otherwise . Finally , we also compare against four state of the art metric learning methods LMNN [ 15 ], BoostMetric [ 13 ] , GLML [ 11 ] and LMNN - MM [ 15 ] _CITE_ . The former two learn a single global metric and the latter two a number of local metrics . In addition to the different metric learning methods , we also compare PLML against multi - class SVMs in which we use the one - against - all strategy to determine the class label for multi - class problems and select the best kernel with inner cross validation .__label__Method|Algorithm|Compare
The red and blue curves intersect at nearly the same positions in ( a )( b ) and in ( c )( d ), even though the risk minimizers in the experiments were locally optimal and regularized , making our estimation error bounds inexact . Benchmark data Table 2 summarizes the specification of benchmarks , which were downloaded from many sources including the IDA benchmark repository [ 29 ], the UCI machine learning repository , the semi - supervised learning book [ 30 ], and the European ESPRIT 5516 project . _CITE_ In Table 2 , three rows describe the number of features , the number of data , and the ratio of P data according to the true class labels . Given a random sampling of X +, X − and Xu , the test set has all the remaining data if they are less than 10 , or else drawn uniformly from the remaining data of size 10 . For benchmark data , the linear model for the artificial data is not enough , and its kernel version is employed .__label__Material|Data|Use
The vector u obtained in this way is the low dimensional representation of the transaction with starter files in S . To determine whether file fi is relevant , we compute the probability p ( xi = 1 | u , V ) = σ ( u · vi ) and recommend the file if this probability exceeds some threshold . ( We tune the threshold on held - out transactions from the development history ). We evaluated our models on three datasets _CITE_ constructed from check - in records of Mozilla Firefox , Eclipse Subversive , and Gimp . These open - source projects use software configuration management ( SCM ) tools which provide logs that allow us to extract binary vectors indicating which files were changed during a transaction . Our experimental setup and results are described below .__label__Material|Data|Use
[ Zero + SVM ] Impute missing features by filling in zeros . After imputation , an SVM is trained using the available ( noisy ) labels in QY for that task , and predictions are made for the rest of the labels . All SVMs are linear , trained using SVMlin _CITE_ , and the regularization parameter is tuned using 5 - fold cross validation separately for each task . The range of parameter values considered was { 10 − , 10 − ,..., 10 , 10 }. Evaluation Method : To evaluate performance , we consider two measures : transductive label error , i . e ., the percentage of unobserved labels predicted incorrectly ; and relative feature imputation error ( Ki /∈ nX ( xij − ˆxij ) ) / Ki for each parameter setting , we report the mean performance ( and standard deviation in parenthesis ) of different algorithms over 10 random trials .__label__Method|Tool|Use
For the Conceptnet and the Movielens datasets , we used only two train / test splits and at most 30 clusters , which made our experiments faster . We report test root mean squared error ( RMSE ) and the area under the precision recall curve ( AUC ) [ 9 ]. For the IRM _CITE_ we make predictions as follows . The IRM partitions the data into blocks ; we compute the smoothed mean of the observed entries of each block and use it to predict the test entries in the same block . We first applied BCTF to the Animals , Kinship , and the UML datasets using 20 and 40 - dimensional vectors .__label__Method|Code|Use
An efficient C - code implementation for Matlab of the proposed table completion tool is also released on the authors website . In recent years , probabilistic modeling has become an attractive option for building database management systems since it allows estimating missing values , detecting errors , visualizing the data , and providing probabilistic answers to queries [ 19 ]. BayesDB , _CITE_ for instance , is a database management system that resorts to Crosscat [ 18 ], which originally appeared as a Bayesian approach to model human categorization of objects . BayesDB provides missing data estimates and probabilistic answer to queries , but it only considers Gaussian and multinomial likelihood functions . In the literature , probabilistic low - rank matrix factorization approaches have been broadly applied to table completion ( see , e . g ., [ 14 , 15 , 21 ])__label__Material|Data|Introduce
In this section , we empirically compare our algorithms with the existing algorithms in terms of clustering performance and computational time ( on a single desktop ). For NSN , we used the fast implementation described in Section A . 1 . The compared algorithms are K - means , K - flats , SSC , LRR , SCC , TSC _CITE_ , and SSC - OMP . The numbers of replicates in K - means , K - flats , and the K means used in the spectral clustering are all fixed to 10 . The algorithms are compared in terms of Clustering error ( CE ) and Neighborhood selection error ( NSE ), defined as where ⇧ L is the permutation space of [ L ].__label__Method|Algorithm|Compare
The parameters of this layers are { q ( n ) The gradient of layer output w . r . t . input is computed as  x ( n ) . We train and test ADMM - Net on brain and chest MR images _CITE_ . For each dataset , we randomly take 100 images for training and 50 images for testing . ADMM - Net is separately learned for each sampling ratio .__label__Material|Data|Use
The success on the IMDB dataset convinces us to test our methods on another sentiment analysis task to see if similar gains can be obtained . The benchmark of focus in this experiment is the Rotten Tomatoes dataset [ 26 ]. _CITE_ The dataset has 10 , 662 documents , which are randomly split into 80 % for training , 10 % for validation and 10 % for test . The average length of each document is 22 words and the maximum length is 52 words . Thus compared to IMDB , this dataset is smaller both in terms of the number of documents and the number of words per document .__label__Material|Data|Use
For all data sets , training time was limited to at most 1 , 000 epochs over the training set . The best models were selected by early stopping using the mean predicted ranks on the validation sets ( raw setting ). An open - source implementation of TransE is available from the project webpage _CITE_ . Overall results Tables 3 displays the results on all data sets for all compared methods . As expected , the filtered setting provides lower mean ranks and higher hits @ 10 , which we believe are a clearer evaluation of the performance of the methods in link prediction .__label__Method|Code|Produce
For all data sets , training time was limited to at most 1 , 000 epochs over the training set . The best models were selected by early stopping using the mean predicted ranks on the validation sets ( raw setting ). An open - source implementation of TransE is available from the project webpage _CITE_ . Overall results Tables 3 displays the results on all data sets for all compared methods . As expected , the filtered setting provides lower mean ranks and higher hits @ 10 , which we believe are a clearer evaluation of the performance of the methods in link prediction .__label__Supplement|Website|Produce
We created a single count matrix with one column per year . The dataset is downloaded from Gal ’ s page , with T = 17 , V = 14036 , with 3280697 events for the matrix . Ebola corpus ( EBOLA ) _CITE_ : EBOLA corpus contains the data for the 2014 Ebola outbreak in West Africa every day from Mar 22th , 2014 to Jan 5th 2015 , each column represents the cases or deaths in a West Africa country . After data cleaning , the dataset is with T = 122 , V = 16 . International Disaster ( ID ) : The International Disaster dataset contains essential core data on the occurrence and effects of over 22 , 000 mass disasters in the world from 1900 to the present day . A count matrix with T = 115 and V = 12 is built from the events of disasters occurred in Europe from the year 1902 to 2016 , classified according to their disaster types .__label__Material|Data|Use
All the model parameters have a physical interpretation and thus expert knowledge was used to set priors which produce realistic wind fields . We will also use ( 10 ) to help set ( hyper ) priors using real data in Z °. MCMC using the Metropolis algorithm ( Neal , 1993 ) is used to sample from ( 10 ) using the NETLAB _CITE_ library . Convergence of the Markov chain is currently assessed using visual inspection of the univariate sample paths since the generating parameters are known , although other diagnostics could be used ( Cowles and Carlin , 1996 ). We find that the procedure is insensitive to the initial value of the GP parameters , but that the parameters describing the location of the front ( Of , cif ) need to be initialised & apos ; close & apos ; to the correct values if the chain is to converge on a reasonable time - scale .__label__Method|Code|Use
A highly relevant application is matrix completion - given a matrix with missing values we would like to be able to complete it by utilizing similarities between rows and columns . This problem is at the core of collaborative filtering applications , but may also appear in other fields . We test our algorithm on the publicly available MovieLens 100K dataset _CITE_ . The dataset consists of 100 , 000 ratings on a five - star scale for 1 , 682 movies by 943 users . We take the five non - overlapping splits of the dataset into 80 % train on 20 % test size provided at the MovieLens web site .__label__Material|Data|Use
The regularization parameter A for AR - DESPOT was selected offline by running the algorithm with a training set distinct from the online test set . The discount factor is y = 0 . 95 . For POMCP , we used the implementation from the original authors _CITE_ , but modified it in order to support very large number of observations and strictly follow the 1 - second time limit for online planning . We evaluated the algorithms on four domains , including a very large one with about 1056 states ( Table 1 ). In summary , compared with AEMS2 , AR - DESPOT is competitive on smaller POMDPs , but scales up much better on large POMDPs .__label__Method|Code|Use
We implement an importance sampling approach by repeatedly drawing r ∼ o for some density o ; we find that o = kPLY k2 + N ( 0 , Q ) works well in practice . Given samples r1 , ... , rB ∼ o we then estimate In this section we present results from experiments on simulated and real data , performed in R [ 11 ]. _CITE_ We fix sample size n = 500 and G = 50 groups each of size 10 . For each trial , we generate a design matrix X with i . i . d . N ( 0 , 1 / n ) entries , set O with its first 50 entries ( corresponding to first s = 5 groups ) equal to τ and all other entries equal to 0 , and set Y = XO + N ( 0 , In ).__label__Supplement|Document|Produce
Our algorithms are shown in practice to have accuracy comparable to a Gibbs sampler in terms of topic estimation , which requires the number of topics be given . Moreover , they are one of the fastest among several state of the art parametric techniques . _CITE_ Statistical consistency of our estimator is established under some conditions . A well - known challenge associated with topic modeling inference can be succinctly summed up by the statement that sampling based approaches may be accurate but computationally very slow , e . g ., Pritchard et al . ( 2000 ); Griffiths & Steyvers ( 2004 ), while the variational inference approaches are faster but their estimates may be inaccurate , e . g ., Blei et al .__label__Method|Algorithm|Introduce
However , we see that the proposed local metric highly outperforms the discriminative nearest neighbor performance in a high dimensional space appropriately . We note that this example is ideal for GLML , and it shows much improvement compared to the other methods . The other experiments consist of the following benchmark datasets : UCI machine learning repository _CITE_ datasets ( Ionosphere , Wine ), and the IDA benchmark repository ( German , Image , Waveform , Twonorm ). We also used the USPS handwritten digits and the TI46 speech dataset . For the USPS data , we resized the images to 8 × 8 pixels and trained on the 64 - dimensional pixel vector data .__label__Material|Data|Use
matrix X ∈ RDXD , X = PJj = 1 wjZj . used USPS and MNIST handwritten digits , ORL face recognition datasets , Columbia University Image Library ( COIL20 ) , and UCI machine learning datasets ( datasets 7 - 13 ), Twin Peaks and Helix . The last two are artificial datasets _CITE_ . Experimental results are obtained by averaging over 10 runs ( except USPS - 1 ). We randomly split the datasets for each run .__label__Material|Data|Use
For MaR and Random , the number of iterations is larger , but still comparable to the sparsity level of k = 20 . We now present results on two gene expression cancer datasets . The first dataset _CITE_ contains expression values from patients with two different types cancers related to leukemia . The second dataset contains expression levels from patients with and without prostate cancer . The matrix X contains the gene expression values and the vector y is an indictor of the type of cancer a patient has .__label__Material|Data|Use
Each bill is an example and the variables are the votes of the 453 congressmen , which can be yes , no , or present . The movie review data contains 1000 positive and 1000 negative movie reviews . We first applied the Porter stemmer and then used the Scikit Learn CountVectorizer , _CITE_ which counts all 1 - and 2 - grams , while omitting the standard Scikit Learn stop words . We selected the 1000 most frequent n - grams in the training data to serve as the features . For all data sets , we divided the data into a single train , tune , and test partition .__label__Method|Tool|Use
Finally , note that the SOFT ( HARD ) penalty algorithm tends to perform better when adversaries are biased towards low ( high ) degrees , and this insight can be used to aid the choice of the reputation algorithm to be employed in different scenarios . Real Datasets . Next , we evaluated our algorithm on some standard datasets : ( a ) TREC _CITE_ : a collection of topic - document pairs labeled as relevant or non - relevant by workers on AMT . We consider two versions : stage2 and task2 . ( b ) NLP [ 17 ]: consists of annotations by AMT workers for different NLP tasks ( 1 ) rte - which provides binary judgments for textual entailment , i . e .__label__Material|Data|Use
( 1993 ) ( a deterministic space carving strategy ), FIPS Mendes et al . ( 2004 ) ( a biologically inspired randomized algorithm ), and MEGA Hazen & Gupta ( 2009 ) ( a multiresolution search strategy with numerically computed gradients ). We use a publicly available implementation of Direct Search _CITE_ , and an implementation of FIPS and MEGA available from the authors of MEGA . We set the number of particles for FIPS and MEGA to the square of the dimension of the problem plus one , following the recommendation of their authors . As the subordinate optimizer for Algorithm 1 , we use a simple grid search for all our experiments .__label__Method|Code|Use
Also , in case of Caltech - 5 and Oxford flowers datasets , the accuracies reported are the testset accuracies averaged over 10 such randomly sampled training and test datasets . Since the Caltech - 101 dataset has large number of classes and the experiments are computationally intensive ( 100 choose 2 classifiers need to be built in each case ), the results are averaged over 3 sets of training and test datasets only . In case of the Caltech datasets , five feature descriptors _CITE_ were employed : SIFT , OpponentSIFT , rgSIFT , C - SIFT , Transformed Color SIFT . Whereas in case of Oxford flowers dataset , following strategy of [ 11 , 10 ], seven feature descriptors were employed . Using each feature descriptor , nine kernels were generated by varying the width - parameter of the Gaussian kernel .__label__Method|Tool|Use
This suggests that the VP - tree could be applicable to a wide class of non - metric spaces . We run experiments on a Linux server equipped with Intel Core i7 2600 ( 3 . 40 GHz , 8192 KB of L3 CPU cache ) and 16 GB of DDR3 RAM ( transfer rate is 20GB / sec ). The software ( including scripts that can be used to reproduce our results ) is available online , as a part of the Non - Metric Space Library _CITE_ [ 5 ]. The code was written in C ++, compiled using GNU C ++ 4 . 7 ( optimization flag - Ofast ), and executed in a single thread . SIMD instructions were enabled using the flags - msse2 - msse4 . 1 - mssse3 .__label__Method|Code|Produce
For APG , we implement two variants , where APG - D refers to the variant with the dual averaging style of update on one sequence of points ( i . e ., Algorithm 1 ) and APG - F refers to the variant of the FISTA style [ 2 ]. Similarly , we also implement the two variants for HOPS . We conduct experiments for solving three problems : ( 1 ) an ` 1 - norm regularized hinge loss for linear classification on the w1a dataset _CITE_ ; ( 2 ) a total variation based ROF model for image denoising on the Cameraman picture ; ( 3 ) a nuclear norm regularized absolute error minimization for low - rank and sparse matrix decomposition on a synthetic data . More details about the formulations and experimental setup can be found in the supplement . To make fair comparison , we stop each algorithm when the optimality gap is less than a given a and count the number of iterations and the running time that each algorithm requires .__label__Material|Data|Use
This is equivalent to using the L1 ; 1 norm in our framework which , as discussed above , cannot enforce group sparsity among different features over all tasks . In this section , we study our methods empirically on two cancer classification applications using microarray gene expression data . We compare our methods with three related methods : multi - task feature learning ( MTFL ) [ 1 ] , multi - task feature selection using l1 ; 2 regularization [ 16 ] , and multitask feature selection using l1 ;,, regularization [ 20 ] _CITE_ . We first conduct empirical study on a breast cancer classification application . This application consists of three learning tasks with data collected under different platforms [ 21 ].__label__Method|Algorithm|Compare
In this paper , we have preferred to embed these ideas into another online algorithm and start with a higher baseline performance . We have chosen to use the Margin Infused Relaxed Algorithm ( MIRA ) as our baseline algorithm since it has exhibited good generalization performance in previous experiments [ 1 ] and has the additional advantage that it is designed to solve multiclass classification problem directly without any recourse to performing reductions . The algorithms were evaluated on three natural datasets : mnist _CITE_ , usps and letter . The characteristics of these datasets has been summarized in Table 1 . A comprehensive overview of the performance of various algorithms on these datasets can be found in a recent paper [ 2 ].__label__Material|Data|Use
After every Nµ iterations , we set µ := max { µ · gµ , ¯ µ }; i . e ., we simply reduce µ by a constant factor gµ every Nµ iterations until a desired lower bound on µ is achieved . We compare ALM ( i . e ., Algorithm 3 with the above stopping criteria and µ updates ), with the projected subgradient method ( PSM ) proposed by Duchi et al . in [ 13 ] and implemented by Mark Schmidt and the smoothing method ( VSM ) _CITE_ proposed by Lu in [ 17 ], which are considered to be the state - of - the - art algorithms for solving SICS problems . The per - iteration complexity of all three algorithms is roughly the same ; hence a comparison of the number of iterations is meaningful . The parameters used in PSM and VSM are set at their default values .__label__Method|Algorithm|Compare
We use two text datasets : NIPS full papers and New York Times news articles . We eliminate a minimal list of 347 English stop words and prune rare words based on tf - idf scores and remove documents with fewer than five tokens after vocabulary curation . We also prepare two non - textual item - selection datasets : users ’ movie reviews from the Movielens 10M Dataset , _CITE_ and music playlists from the complete Yes . com dataset . 10 We perform similar vocabulary curation and document tailoring , with the exception of frequent stop - object elimination . Playlists often contain the same songs multiple times , but users are unlikely to review the same movies more than once , so we augment the movie dataset so that each review contains 2 x ( stars ) number of movies based on the half - scaled rating information that varies from 0 . 5 stars to 5 stars . Statistics of our datasets are shown in Table 2 .__label__Material|Data|Use
The first type of baseline we considered is based on machine translation . We used a machine translation tool on the Japanese query , and then applied TFIDF or LSI . We considered three methods of machine translation : Google ’ s API _CITE_ or Fujitsu ’ s ATLAS was used to translate each query document , or we translated each word in the Japanese dictionary using ATLAS and then applied this word - based translation to a query . We also compared to CL - LSI [ 9 ] trained on all 90 , 000 Jap - Eng pairs from the training set . For PSI , we considered two cases : ( i ) apply the ATLAS machine translation tool first , and then use PSI trained on the task in Section 4 . 1 , e . g .__label__Method|Tool|Use
Needless to say that all the sets Ns , ... , N5 have zero intersection with each other . We repeated each experiment three times , where in every time we used a different Ni as the test set , for i E { 1 , 2 , 3 }. Suppose Ns is the test set , then for the training set we used two configurations : We use a network architecture suggested by [ 24 ], using an available tensorflow implementation _CITE_ . It should be noted that we did not change any parameters of the network architecture or the optimization process , and use the default parameters in the implementation . Since the amount of male and female subjects in the dataset is not balanced , we use an objective of maximizing the balanced accuracy [ 9 ] - the average accuracy obtained on either class .__label__Method|Code|Use
Deep congealing gives a significant improvement over SIFT congealing . Using a CDBN representation learned with a group sparsity penalty , leading to learned filters with topographic organization , consistently gives a higher accuracy of one to two percentage points . We compare with two supervised alignment systems , the fiducial points based system of [ 3 ], _CITE_ and LFW - a . Note that LFW - a was produced by a commercial alignment system , in the spirit of [ 3 ], but with important differences that have not been published [ 2 ]. Congealing with a one - layer CDBN trained on faces , with topology , gives verification accuracy significantly higher than using images produced by [ 3 ], and comparable to the accuracy using LFW - a images .__label__Method|Tool|Compare
Because there is natural variance in skew , rotation , style , etc in hand written digits , the trained CapsNet is moderately robust to small affine transformations of the training data . To test the robustness of CapsNet to affine transformations , we trained a CapsNet and a traditional convolutional network ( with MaxPooling and DropOut ) on a padded and translated MNIST training set , in which each example is an MNIST digit placed randomly on a black background of 40 x 40 pixels . We then tested this network on the affNIST _CITE_ data set , in which each example is an MNIST digit with a random small affine transformation . Our models were never trained with affine transformations other than translation and any natural transformation seen in the standard MNIST . An under - trained CapsNet with early stopping which achieved 99 . 23 % accuracy on the expanded MNIST test set achieved 79 % accuracy on the affnist test set .__label__Material|Data|Use
In this experiment , we attempted to recover 2D geographical relationships between weather stations from recorded monthly precipitation patterns . Data were obtained by averaging month - by - month annual precipitation records from 2005 – 2014 at 400 weather stations scattered across the US ( see Fig . 6 ) _CITE_ . Thus , the data set comprised 400 12 - dimensional vectors . The goal of the experiment is to recover the two - dimensional topology of the weather stations ( as given by their latitude and longi a weather station is a 12 - dimensional vector of monthly precipitation measurements .__label__Material|Data|Use
The red and blue curves intersect at nearly the same positions in ( a )( b ) and in ( c )( d ), even though the risk minimizers in the experiments were locally optimal and regularized , making our estimation error bounds inexact . Benchmark data Table 2 summarizes the specification of benchmarks , which were downloaded from many sources including the IDA benchmark repository [ 29 ], the UCI machine learning repository , the semi - supervised learning book [ 30 ], and the European ESPRIT 5516 project . _CITE_ In Table 2 , three rows describe the number of features , the number of data , and the ratio of P data according to the true class labels . Given a random sampling of X +, X − and Xu , the test set has all the remaining data if they are less than 10 , or else drawn uniformly from the remaining data of size 10 . For benchmark data , the linear model for the artificial data is not enough , and its kernel version is employed .__label__Material|Data|Use
We compare our approach , Prox - QN , with four other methods , Proximal Gradient ( Prox - GD ), OWLQN [ 23 ], SGD [ 21 ] and BCD [ 16 ]. For OWL - QN , we directly use the OWL - QN optimizer developed by Andrew et al . _CITE_ , where we set the memory size as m = 10 , which is the same as that in Prox - QN . For SGD , we implement the algorithm proposed by Tsuruoka et al . [ 21 ], and use cumulative B1 penalty with learning rate 77k = 770 /( 1 + k / N ), where k is the SGD iteration and N is the number of samples .__label__Method|Tool|Use
Besides , unless specified otherwise , we fix the training set size to 2 , 000 and the code length K to 24 . The Wiki data set , generated from Wikipedia featured articles , consists of 2 , 866 image - text pairs . _CITE_ In each pair , the text is an article describing some events or people and the image is closely related to the content of the article . The images are represented by 128 - dimensional SIFT [ 28 ] feature vectors , while the text articles are represented by the probability distributions over 10 topics learned by a latent Dirichlet allocation ( LDA ) model [ 29 ]. Each pair is labeled with one of 10 semantic classes .__label__Material|Data|Introduce
Results with more components showed identical trends ( see supplementary material ). Experiment 3 : Terrain data ( two dimensional input space , exponentiated quadratic kernel ) In the final experiment we tested the tree based appoximation using a spatial dataset in which terrain altitude was measured as a function of geographical position . _CITE_ We considered a 20km by 30km region ( 400 × 600 datapoints ) and tested prediction on 80 randomly positioned missing blocks of size 1km by 1km ( 20x20 datapoints ). In total , this translates into about 200k / 40k training / test points . We used an exponentiated quadratic kernel with different length - scales in the two input dimensions , comparing a tree - based approximation , which was constructed as described in section 2 . 1 , to the pseudo - point approximation methods considered in the first experiment .__label__Material|Data|Use
In particular , the evolutional dropout achieves relative improvements over 10 % on the testing performance and over 50 % on the convergence speed compared to the standard dropout . Finally , we make a comparison between the evolutional dropout and the batch normalization . For batch normalization , we use the implementation in Caffe _CITE_ . We compare the evolutional dropout with the batch normalization on CIFAR - 10 data set . The network structure is from the Caffe package and can be found in the supplement , which is different from the one used in the previous experiment .__label__Method|Code|Use
Figure 3 shows the squared Frobenius error 11 P �- P * 11 F + 11S �-� S *⌦ 11 F ( see Theorem 3 for details ) across a range of probabilities p . We see that the squared error scales approximately linearly with 1 / p , as predicted by our theory . To illustrate the application of our method to a specific application , we consider chlorine concentration data from a network of sensors . _CITE_ The data contains a realistic simulation of chlorine concentration measurements from n = 166 sensors in a hydraulic system over d = 4310 time points . We assume D is well approximated with a low - rank + sparse decomposition . We then compress the data using the orthogonal model ( 4 ) and study the performance of our estimators ( 2 ) for varying m . In order to evaluate performance , we use 80 % of the entries to fit the model , 10 % as a validation set for selecting tuning parameters , and the final 10 % as a test set .__label__Material|Data|Use
For a fair comparison , we use the same parameter ranges and fully connected layers for our network ( c . f . the supplementary material for more details ), and adopt results of GAN and mode regularized GAN ( Reg - GAN ) from [ 5 ]. For evaluation , we first train a simple , yet effective 3 - layer convolutional nets _CITE_ that can obtain 0 . 65 % error on MNIST testing set , and then employ it to predict the label probabilities and compute MODE scores for generated samples . Fig . 3 ( left ) shows the distributions of MODE scores obtained by three models .__label__Method|Algorithm|Use
Simulation . To evaluate the lower bound in Theorem 3 . 1 and its approximation for B = 1 , we simulate the first - timestamp estimator on regular trees . _CITE_ Figure 2 illustrates the simulation results for B = 1 compared to the approximation above . Each data point is averaged over 5 , 000 trials . Empirically , the lower bound appears to be tight , especially as d grows .__label__Method|Code|Produce
A single wrong correspondence between two consecutive frames may reduce the electrode ’ s score dramatically , while being unnoticed by the single frame score . In most cases the algorithm gives reasonably evolving clustering , even when it disagrees with the manual solution . Examples can be seen at the authors ’ web site _CITE_ . Low matching scores between the manual and the automatic clustering may result from inherent ambiguity in the data . As a preliminary assessment of this hypothesis we obtained a second , independent , manual clustering for the data set for which we got the lowest match scores .__label__Supplement|Website|Introduce
A single wrong correspondence between two consecutive frames may reduce the electrode ’ s score dramatically , while being unnoticed by the single frame score . In most cases the algorithm gives reasonably evolving clustering , even when it disagrees with the manual solution . Examples can be seen at the authors ’ web site _CITE_ . Low matching scores between the manual and the automatic clustering may result from inherent ambiguity in the data . As a preliminary assessment of this hypothesis we obtained a second , independent , manual clustering for the data set for which we got the lowest match scores .__label__Supplement|Document|Introduce
MMAP estimation is difficult as it corresponds to the optimization of an intractable integral , such that the optimization target is expensive to evaluate and gives noisy results . Current PPS inference engines are typically unsuited to such settings . We therefore introduce BOPP _CITE_ ( Bayesian optimization for probabilistic programs ) which couples existing inference algorithms from PPS , like Anglican [ 29 ], with a new Gaussian process ( GP ) [ 22 ] based Bayesian optimization ( BO ) [ 11 , 15 , 20 , 23 ] package . To demonstrate the functionality provided by BOPP , we consider an example application of engineering design . Engineering design relies extensively on simulations which typically have two things in common : the desire of the user to find a single best design and an uncertainty in the environment in which the designed component will live .__label__Method|Tool|Produce
It consists of images from 4 different domains : Artistic images ( A ), Clip Art ( C ), Product images ( P ) and Real - World images ( R ). For each domain , the dataset contains images of 65 object categories collected in office and home settings . Real World Product Clipart Art ImageCLEF - DA _CITE_ This dataset is the benchmark for ImageCLEF domain adaptation challenge , organized by selecting the 12 common categories shared by the following four public datasets ( tasks ): Caltech - 256 ( C ), ImageNet ILSVRC 2012 ( I ), Pascal VOC 2012 ( P ), and Bing ( B ). All three datasets are evaluated using DeCAF7 [ 9 ] features for shallow methods and original images for deep methods . We compare MRN with standard and state - of - the - art methods : Single - Task Learning ( STL ), MultiTask Feature Learning ( MTFL ) [ 2 ], Multi - Task Relationship Learning ( MTRL ) [ 31 ], Robust MultiTask Learning ( RMTL ) [ 5 ], and Deep Multi - Task Learning with Tensor Factorization ( DMTL - TF ) [ 27 ].__label__Material|Data|Introduce
For comparison , we performed experiments with the method of Levin et al . [ 8 ] as one of the state - of - the - art methods . _CITE_ Figure 4 shows an example and Table 2 summarizes the results . The Hessian regularizer clearly outperformed the KRR and the Laplacianbased regression and produced slightly better results than those of Levin et al . [ 8 ].__label__Method|Algorithm|Compare
For each hospitalization , trained medical coders review the information in the discharge summary and assign a series of diagnoses codes . Coding follows the ICD - 9 - CM controlled terminology , an international diagnostic classification for epidemiological , health management , and clinical purposes . _CITE_ The ICD - 9 codes are organized in a rooted - tree structure , with each edge representing an is - a relationship between parent and child , such that the parent diagnosis subsumes the child diagnosis . For example , the code for “ Pneumonia due to adenovirus ” is a child of the code for “ Viral pneumonia ,” where the former is a type of the latter . It is worth noting that the coding can be noisy .__label__Method|Code|Introduce
The known structures are only used for performance measurement and not during learning . Embedding algorithms may be used to study the structure of document databases . Here we used the NIPS 0 - 12 database supplied by Roweis _CITE_ , and augmented it with data from NIPS volumes 13 - 17 . The last three volumes also contain an indicator of the document ’ s topic ( AA for algorithms and architecture , LT for learning theory , NS for neuroscience etc .). We first used CODE to embed documents and words into R .__label__Material|Data|Use
Tissues known to secrete endocrine hormones ( dashed borders ) cluster together . Following Heller et al . [ 10 ], we also compare the three models on 20 - newsgroups , _CITE_ a multilevel hierarchy first dividing into general areas ( rec , space , and religion ) before specializing into areas such as baseball or hockey . This true hierarchy is inset in the bottom right of Figure 4 , and we assume each edge has the same length . We apply latent Dirichlet allocation [ 31 ] with 50 topics to this corpus , and use the topic distribution for each document as the document feature .__label__Material|Data|Use
We also compare our method against Kanatani and Matsunaga ’ s [ 8 ] algorithm ( henceforth , the “ KM ” method ) in estimating the number of independent motions in the video sequences . Note that KM per se does not perform motion segmentation . For the sake of objective comparisons we use only implementations available publicly _CITE_ . Following [ 18 ], motion segmentation performance is evaluated in terms of the labelling error of the point trajectories , where each point in a sequence has a ground truth label , i . e . classification error = number of mislabeled points total number of points Unlike [ 18 ], we also emphasize on the ability of the methods in recovering the number of motions .__label__Method|Code|Use
The set of possible hyperparameter values was determined on early validation tests using subsets of the citation and Reddit data that we then discarded from our analyses . The appendix contains further implementation details . _CITE_ on the full test set ( 79 , 534 nodes ). B : Model performance with respect to the size of the sampled neighborhood , where the “ neighborhood sample size ” refers to the number of neighbors sampled at each depth for K = 2 with S1 = S2 ( on the citation data using GraphSAGE - mean ). Our first two experiments are on classifying nodes in evolving information graphs , a task that is especially relevant to high - throughput production systems , which constantly encounter unseen data .__label__Supplement|Document|Produce
The Hessian of a Gaussian distribution is then given by the expression : This expression is then used to learn the optimal local metric . We compare the performance of our method ( GLML — Generative Local Metric Learning ) with recent metric learning discriminative methods which report state - of - the - art performance on a number of datasets . These include Information - Theoretic Metric Learning ( ITML ) [ 3 ], Boost Metric ( BM ) [ 21 ], and Largest Margin Nearest Neighbor ( LMNN ) _CITE_ [ 26 ]. We used the implementations downloaded from the corresponding authors ’ websites . We also compare with a local metric given by the Fisher kernel [ 12 ] assuming a single Gaussian for the generative model and using the location parameter to derive the Fisher information matrix .__label__Method|Algorithm|Use
We call this STD2 . In all experiments we run the Gibbs sampler for a total of 3000 iterations , with the number of topics fixed to 20 , and keep the last sample . After a burn - in of 500 iterations , the optimisation over the word smoothing coefficients is done every 100 iterations , using an off - the - shelf L - BFGS [ 12 ] optimizer . _CITE_ . We repeat every experiment 5 times with different randomisations . Evaluation of topic models is an open problem – recent work [ 7 ] suggests that popular measures based on held - out likelihood , such as perplexity , do not capture whether topics are coherent or not .__label__Method|Tool|Use
: y in RBF kernel ), and the bound parameter A in the projection constraint . In the experiment , C , y , and A are selected from { 0 . 01 , 0 . 1 , 1 , 10 , 50 , 100 }, { 0 . 2 , 0 . 4 , 0 . 6 , 0 . 8 , 1 . 0 } and { 0 . 01 , 0 . 1 , 1 , 10 , 100 } respectively . We employ the MOSEK toolbox _CITE_ to solve the resulted QCQP problem ( 19 ). The other experiment uses the same parameter setting . The ten - times 10 - fold cross validation results ( except Diverse Density ) are shown in Table 1 .__label__Method|Tool|Use
Second , by observing the ‘ weights ’ of the convex combination we can distinguish the strong from the weak candidate kernels . We proceed by discussing the details of the experimental design interleaved with our results . We used the USPS dataset _CITE_ of 16x16 images of handwritten digits with pixel values ranging between - 1 and 1 . We present the results for 5 pairwise classification tasks of varying difficulty and for odd vs . even digit classification . For pairwise classification , the training set consisted of the first 200 images for each digit in the USPS training set and the number of labeled points was chosen to be 4 , 8 or 12 ( with equal numbers for each digit ).__label__Material|Data|Use
The architecture uses rectified linear units ( ReLU ), and gated recurrent units ( GRU ) [ 20 ], which have similar performance to long short - term memory [ 21 ] ( LSTM ) [ 22 ]. Unless stated otherwise , we set the standard deviation of noise added to the channel to σ = 2 , which was found to be essential for good performance . _CITE_ RIAL and DIAL share the same individual model architecture . For brevity , we describe only the DIAL model here . As illustrated in Figure 2 , each agent consists of a recurrent neural network ( RNN ), unrolled for T time - steps , that maintains an internal state h , an input network for producing a task embedding z , and an output network for the Q - values and the messages m . The input for agent a is defined as a tuple of ( oat , mat − 1 , uat − 1 , a ).__label__Method|Algorithm|Introduce
The datasets where purposely are selected with various sizes in order to clearly expose the differences among scalable capabilities of the models . Three of which are large - scale datasets with hundreds of thousands and millions of data points ( year : 515 , 345 ; poker : 1 , 025 , 010 ; and airlines : 5 , 929 , 413 ), whilst the rest are medium size databases ( ijcnn1 : 141 , 691 and cod - rna : 331 , 152 ). These datasets can be downloaded from LIBSVM _CITE_ and UCI websites , except the airlines which was obtained from American Statistical Association ( ASA ). For the airlines dataset , our aim is to predict whether a flight will be delayed or not under binary classification setting , and how long ( in minutes ) the flight will be delayed in terms of departure time under regression setting . A flight is considered delayed if its delay time is above 15 minutes , and non - delayed otherwise .__label__Material|Data|Use
We collect the adjusted close prices of these selected indices from May 2nd , 2006 to April 12th , 2012 , and use linear interpolation to estimate the prices on those dates when the data are not available . We apply our proposed model with SVAR ( 1 ) to model the spatiotemporal variance dependencies of the data . For the contemporaneous causal structure discovery , we use LiNGAM - GC - UK , C - M , LiNGAM _CITE_ and Direct - LiNGAM to estimate the causal ordering . The discovered causal orderings of different algorithms are shown in Table 2 . From Table 2 , we see that in the causal ordering discovered by LiNGAM - GC - UK and LiNGAM , the stock indices in US , i . e ., DJI and NDX are contemporaneously affected by the indices in Europe .__label__Method|Tool|Use
All other aspects of GP inference remain the same . All of the experiments in this paper were performed using the standard GPML toolbox ; code to perform all experiments is available at the author ’ s website . _CITE_ Plate [ 6 ] constructs a form of additive GP , but using only the first - order and Dth order terms . This model is motivated by the desire to trade off the interpretability of first - order models , with the flexibility of full - order models . Our experiments show that often , the intermediate degrees of interaction contribute most of the variance .__label__Method|Code|Produce
All other aspects of GP inference remain the same . All of the experiments in this paper were performed using the standard GPML toolbox ; code to perform all experiments is available at the author ’ s website . _CITE_ Plate [ 6 ] constructs a form of additive GP , but using only the first - order and Dth order terms . This model is motivated by the desire to trade off the interpretability of first - order models , with the flexibility of full - order models . Our experiments show that often , the intermediate degrees of interaction contribute most of the variance .__label__Supplement|Website|Produce
In some settings , the λ that yields the best generalization error scales like O ( 1 / Vn ), hence p = O ( Tr ( K )/ Vn ) is sufficient . On the other hand , if the columns are sampled uniformly , one would get p = O ( dmof ) = O ( n maxi li ( λ )). We test our results based on several datasets : one synthetic regression problem from [ 3 ] to illustrate the importance of the λ - ridge leverage scores , the Pumadyn family consisting of three datasets pumadyn - 32fm , pumadyn - 32fh and pumadyn - 32nh _CITE_ and the Gas Sensor Array Drift Dataset from the UCI database . The synthetic case consists of a regression problem on the interval X = [ 0 , 1 ] where , given a sequence ( xi ) 1 ≤ i ≤ n and a sequence of noise ( Ei ) 1 ≤ i ≤ n , we observe the sequence The function f belongs to the RKHS F generated by the kernel k ( x , y ) = 1 ( 2β )! B2β ( x − y − Lx − yI ) where B2β is the 2β - th Bernoulli polynomial [ 3 ]. One important feature of this regression problem is the distribution of the points ( xi ) 1 ≤ i ≤ n on the interval X : if they are spread uniformly over the interval , the λ - ridge leverage scores ( li ( λ )) 1 ≤ i ≤ n are uniform for every λ & gt ; 0 , and uniform column sampling is optimal in this case .__label__Material|Data|Use
This results in 3 , 505 , 366 games between 206 , 059 unique players . Note that a large proportion of games was collected between 1987 and 2006 ( see Figure 2 ( left )). Our implementation of the TrueSkill through Time algorithms was done in F # _CITE_ and builds a factor graph with approximately 11 , 700 , 000 variables and 15 , 200 , 000 factors ( TTT ) or 18 , 500 , 000 variables and 27 , 600 , 000 factors ( TTT - D ). The whole schedule allocates no more than 6 GB ( TTT ) or 11 GB ( TTT - D ) and converges in less than 10 minutes ( TTT )/ 20 minutes ( TTT - D ) of CPU time on a standard Pentium 4 machine . The code for this analysis will be made publicly available .__label__Method|Code|Use
Utilizing our general algorithm for a hard version of the multinomial HDP is straightforward . In order to apply the hard hierarchical algorithm to topic modeling , we simply utilize the discrete KL - divergence in the hard exponential family HDP , since topic modeling for text uses a multinomial distribution for the data likelihood . To test topic modeling using our asymptotic approach , we performed analyses using the NIPS 1 - 12 _CITE_ and the NYTimes [ 15 ] datasets . For the NIPS dataset , we use the whole dataset , which contains 1740 total documents , 13649 words in the vocabulary , and 2 , 301 , 375 total words . For the NYTimes NIPS NYTimes 1 neurons , memory , patterns , activity , re - team , game , season , play , games , point , sponse , neuron , stimulus , firing , cortex , re - player , coach , win , won , guy , played , playcurrent , pattern , spike , stimuli , delay , re - ing , record , final sponses 2 neural , networks , state , weight , states , re - percent , campaign , money , fund , quarter , sults , synaptic , threshold , large , time , sys - federal , public , pay , cost , according , intems , activation , small , work , weights come , half , term , program , increase 3 training , hidden , recognition , layer , per - president , power , government , country , formance , probability , parameter , error , peace , trial , public , reform , patriot , ecospeech , class , weights , trained , algorithm , nomic , past , clear , interview , religious , approach , order early 4 cells , visual , cell , orientation , cortical , con - family , father , room , line , shares , recount , nection , receptive , field , center , tuning , told , mother , friend , speech , expression , low , ocular , present , dominance , fields won , offer , card , real 5 energy , solution , methods , function , solu - company , companies , stock , market , busitions , local , equations , minimum , hopfield , ness , billion , firm , computer , analyst , intemperature , adaptation , term , optimiza - dustry , internet , chief , technology , custion , computational , procedure tomer , number 6 noise , classifier , classifiers , note , margin , right , human , decision , need , leadership , noisy , regularization , generalization , hy - foundation , number , question , country , pothesis , multiclasses , prior , cases , boost - strike , set , called , support , law , train ing , fig , pattern dataset , we randomly sampled 2971 documents with 10171 vocabulary words , and 853 , 451 words in total ; we also eliminated low - frequency words ( those with less than ten occurrences ).__label__Material|Data|Use
To close this section , let us stress that other approaches ( cf ., for instance , [ 9 , 7 , 1 ]) recently introduced in sparse learning and estimation may potentially be useful for the problem of robust estimation . We implemented the algorithm in MatLab , using the SeDuMi package for solving LPs [ 28 ]. We applied our algorithm of robust estimation to the well - known dinosaur sequence _CITE_ . which consists procedure ( b ) and by our procedure . ( d ) Boxplots of the errors when estimating the camera centers by our procedure ( left ) and by the KK - procedure .__label__Material|Data|Use
The model achieves compression factors of 2 - 3 without decreasing the network error . In all experiments we use our MATLAB extension of the MatConvNet framework [ 24 ]. For the operations related to the TT - format we use the TT - Toolbox _CITE_ implemented in MATLAB as well . The experiments were performed on a computer with a quad - core Intel Core i5 - 4460 CPU , 16 GB RAM and a single NVidia Geforce GTX 980 GPU . We report the running times and the memory usage at the forward pass of the TT - layer and the baseline fully - connected layer in Table 3 .__label__Method|Code|Use
Additionally , in initial user studies we observed that data programming may be an easier way for non - experts to create machine learning models when training data is limited or unavailable . Many of the major machine learning breakthroughs of the last decade have been catalyzed by the release of a new labeled training dataset . _CITE_ Supervised learning approaches that use such datasets have increasingly become key building blocks of applications throughout science and industry . This trend has also been fueled by the recent empirical success of automated feature generation approaches , notably deep learning methods such as long short term memory ( LSTM ) networks [ 14 ], which ameliorate the burden of feature engineering given large enough labeled training sets . For many real - world applications , however , large hand - labeled training sets do not exist , and are prohibitively expensive to create due to requirements that labelers be experts in the application domain .__label__Material|Data|Introduce
and trained on millions of features based on these words . The authors of [ 15 ] used a model similar to the naive full rank model ( 1 ), but for the task of image retrieval , and [ 13 ] also used a related ( regression - based ) method for advert placement . These techniques are implemented in related software to these two publications , PAMIR and Vowpal Wabbit _CITE_ . When the memory usage is too large , the latter bins the features randomly into a reduced space ( hence with random collisions ), a technique called Hash Kernels [ 25 ]. In all cases , the task of document retrieval , and the use of low - rank approximation or polynomial features is not studied .__label__Method|Tool|Use
Note that the sequences used for training were only 20 frames long . The model ’ s predictions look qualitatively better than most published generated sequences . _CITE_ Further results and data can be found on the project website at http :// www . ccc . cs . uni - frankfurt . de / people / vincent - michalski / grammar - cells A major long - standing problem in sequence modeling is dealing with long - range correlations . It has been proposed that deep learning may help address this problem by finding representations that capture better the abstract , semantic content of the inputs [ 22 ].__label__Method|Algorithm|Compare
This part tests the above three algorithms with synthetic data . To make a fair comparison , some implementation details are clarified as follows : ( 1 ) Since domain transformations are not considered in Li ’ s work , we assume the synthetic data are well aligned . ( 2 ) To eliminate the influence of different optimization methods , RASL is implemented with the following four optimization methods : APG ( Accelerated Proximal Gradient ), APGP ( Accelerated Proximal Gradient with partial SVDs ), ALM ( Augmented Lagrange Multiplier ) and IALM ( Inexact Augmented Lagrange Multiplier ) _CITE_ . Moreover , since RASL is applied to one mode of the tensor , to make it more competitive , we apply RASL to each mode of the tensor and take the mode that has the minimal reconstruction error . For synthetic data , we first randomly generate two data tensors : ( 1 ) a pure low - rank tensor Lo ∈ R50 × 50 × 50 whose rank is ( 10 , 10 , 10 ); ( 2 ) an error tensor E ∈ R50 × 50 × 50 in which only a fraction c of entries are non - zero ( To ensure the error to be sparse , the maximal value of c is set to 40 %).__label__Method|Algorithm|Use
Our algorithm differs in the use of kd - trees and in the way we handle truncation : we only assume that the variational distributions are fixed at their priors after a certain level . Experiments show that speedups relative to the standard variational algorithm can be significant . Evidenced by three recent workshops _CITE_ , nonparametric Bayesian methods are gaining popularity in the machine learning community . In each of these workshops computational efficiency was mentioned as an important direction for future research . In this paper we propose computational speedups for Dirichlet Process ( DP ) mixture models [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], with the purpose of improving their applicability in modern day data - mining problems where millions of data - cases are no exception .__label__Supplement|Paper|Extent
Combining this constraint with the usual 1 - norm SVM constraints and allowing different regularisation constants gives the following optimisation : Let ˆwA , ˆwB , ˆbA , ˆbB be the solution to this optimisation problem . The final SVM - 2K decision function is then h ( x ) = sign ( f ( x )), where � � f ( x ) = 0 . 5 h ˆwA , φA ( x ) i + ˆbA + h ˆwB , φB ( x ) i + ˆbB = 0 . 5 ( fA ( x ) + fB ( x )) . Applying the usual Lagrange multiplier techniques we arrive at the following dual problem : The performance of the algorithms developed in this paper we evaluated on PASCAL Visual Object Classes ( VOC ) challenge dataset test1 _CITE_ . This is a new dataset consisting of four object classes in realistic scenes . The object classes are , motorbikes ( M ), bicycles ( B ), people ( P ) and cars ( C ) with the dataset containing 684 training set images consisting of ( 214 , 114 , 84 , 272 ) images in each class and 689 test set images with ( 216 , 114 , 84 , 275 ) for each class .__label__Material|Data|Use
Note that since ui depends only on the rank order of | si |, the results would be the same if the signs are discarded by taking s i . To test the behavior of our model , we applied it to small patches taken from digitized natural images . The image dataset is available on the World Wide Web from Bruno Olshausen _CITE_ . It contains ten 512x512 pre - whitened images . We took 151 , 290 evenly distributed 20x20 image patches .__label__Material|Data|Use
Note that since ui depends only on the rank order of | si |, the results would be the same if the signs are discarded by taking s i . To test the behavior of our model , we applied it to small patches taken from digitized natural images . The image dataset is available on the World Wide Web from Bruno Olshausen _CITE_ . It contains ten 512x512 pre - whitened images . We took 151 , 290 evenly distributed 20x20 image patches .__label__Supplement|Website|Use
More specifically , we record multiple peo318 { xtm , stm } M ‡ m = 1 . by the vector xi set of micophones Given the rcorded sinal the t the state of the i - th particle at time t . We also ait ∈{ 1 , ... , P } in order to denote the particle that precedes the gnals Speakers may stat speakng or become sil t corresponds to the index of the ancestor particle of xi i - th particle at time t . That is , ai collect data from several speakers from the ASCAL t . Let alsoxi1 : t xi the particle trajctory that s recursively defined as hallenge website . _CITE_ The voice signal for each speak example to clarify the notation . and the emission In Step 1 , we follow the slice sampling scheme for inference in BNP models based on the Indian bu et process ( IBP ) [ 19 , 23 ], which ectively transforms the model into a finite factorial model with = M + + Mnew parallel chains . Step 2 consists in sampling the elements of the matrices S and X given the current value of the global variables .__label__Material|Data|Produce
More specifically , we record multiple peo318 { xtm , stm } M ‡ m = 1 . by the vector xi set of micophones Given the rcorded sinal the t the state of the i - th particle at time t . We also ait ∈{ 1 , ... , P } in order to denote the particle that precedes the gnals Speakers may stat speakng or become sil t corresponds to the index of the ancestor particle of xi i - th particle at time t . That is , ai collect data from several speakers from the ASCAL t . Let alsoxi1 : t xi the particle trajctory that s recursively defined as hallenge website . _CITE_ The voice signal for each speak example to clarify the notation . and the emission In Step 1 , we follow the slice sampling scheme for inference in BNP models based on the Indian bu et process ( IBP ) [ 19 , 23 ], which ectively transforms the model into a finite factorial model with = M + + Mnew parallel chains . Step 2 consists in sampling the elements of the matrices S and X given the current value of the global variables .__label__Supplement|Website|Produce
Once the corpus C has been split into two subsets C1 and C2 , each of these subsets may Algorithm 2 Splitting a corpus into two parts still contain the full set of topics but the topic distribution will differ in the two : topics similar to the first pseudo - center will be predominant in the first subset , the others in the second . By recursively iterating this process , we obtain a binary tree where topic distributions in the nodes with higher depth are expected to be more concentrated on fewer topics . In the next sections , we assess the validity of this approach on both synthetic and real - world data _CITE_ . In order to test the ability of SIDIWO to recover latent structures in data , we generate a dataset distributed as a single topic model ( with a vocabulary of 100 words ) whose 8 topics have an intrinsic hierarchical structure depicted in Figure 1a . In this figure , topics are on the x - axis , words on the y - axis , and green ( resp .__label__Material|Data|Use
In our case , the level - set projection is a convex quadratic problem with ` p - norm constraints and can again be approximated by successive second - order Taylor expansions . In this section we study non - sparse MKL in terms of efficiency and accuracy . _CITE_ We apply the method of [ 21 ] for ` 1 - norm results as it is contained as a special case of our cutting plane strategy . We write B ,,,- norm MKL for a regular SVM with the unweighted - sum kernel K = Em Km . We demonstrate the efficiency of our implementations of non - sparse MKL .__label__Method|Algorithm|Use
Note that the same feature can be used for different events . Feature Description Position difference in position , distance to border , overlap with border ; Intensity difference in intensity histogram / sum / mean / deviation , intensity of father cell ; Shape difference in shape , difference in size , shape compactness , shape evenness ; Others division angle pattern , mass evenness , eccentricity of father cell . We evaluated the proposed method on two publicly available image sequences provided in conjunction with the DCellIQ project _CITE_ [ 16 ] and the Mitocheck project [ 12 ]. The two datasets show a certain degree of variations such as illumination , cell density and image compression artifacts ( Fig . 2 ).__label__Method|Algorithm|Extent
It has two classes and three domains , as shown in Figure 1 . The two source domains D1 and D2 were created to have both conditional and marginal probability differences with the target domain data so as to provide an ideal testbed for the proposed domain adaptation methodology . The three real - world datasets used are 20 Newsgroups , Sentiment Analysis _CITE_ and another dataset of multi - dimensional feature vectors extracted from SEMG ( Surface electromyogram ) signals . The 20 Newsgroups dataset is a collection of approximately 20 , 000 newsgroup documents , partitioned ( nearly ) evenly across 20 different categories . We represented each document as a binary vector of the 100 most discriminating words determined by Weka ’ s info - gain filter [ 22 ].__label__Material|Data|Use
This section investigates behavior of Algorithm 2 on the basis of the simulation model used in [ 9 ], and we compare the proposed method with state - of - the - art methods : the SDP relaxation method [ 9 ] and the QPBO and QBPOI methods [ 11 ]. We use SDPA 7 . 3 . 8 to solve SDP problems and use the implementation of QPBO and QPBOI written by Kolmogolov . _CITE_ QPBO methods computes partial labeling , i . e ., there might remain unlabeled variables , and we set unlabeled variables to 0 in our experiments . For computing a minimum s - t cut , we use Dinic ’ s algorithm [ 6 ]. All experiments were conducted in a machine equipped with Intel ( R ) Xeon ( R ) CPU E5 - 2699 v3 @ 2 . 30GHz , 768GB RAM .__label__Method|Code|Use
Algorithm 1 Blocked Gibbs sampler for GP - modulated renewal process on the interval [ 0 , T ] Input : Set of event times G , set of thinned times ˜ Gprev and l instantiated at G U ˜ Gprev . The gamma prior on λ ∗ is conjugate to the Poisson , resulting in a gamma posterior . We resampled the GP hyperparameters using slice sampling [ 23 ] _CITE_ , while parameters of the hazard function were updated using Metropolis - Hastings moves along with equation ( 8 ). The inferential bottleneck in our model is the Gaussian process : sampling a GP on a set of points is , in the worst case , cubic in the size of that set . In our model , each iteration sees on average | G |+ 2 | E | values of the GP , where | G | is the number of observations and | E | is the average number of points sampled from the subordinating Poisson process .__label__Method|Algorithm|Use
The proof of the lemma can be found in the supplementary material . One can take note that the problem ( P2 ) boils down to computing ( w , Q ) as a solution to � ✓ nII ( In − Π )(✓ nω − Y ) II ,, & lt ; Au , minimize IIωII1 subject to and then setting � θ = ( ATA )− AT ( Y − ✓ n w ). For the empirical evaluation we use a synthetic dataset with randomly drawn Gaussian design matrix X and the real - world dataset fountain - P11 _CITE_ , on which we apply our methodology for computing the fundamental matrices between consecutive images . We randomly generated a n x p matrix X with independent entries distributed according to the standard normal distribution . Then we chose a vector β * E Rp that has exactly s nonzero elements all equal to one .__label__Material|Data|Use
The number of samples needed depends on the number of the fragments . In practice we find that n samples are sufficient for n fragments . Figure 6 shows the boundary extraction , grouping , and motion estimation results of our system for both real and synthetic examples _CITE_ . All the results are generated using the same parameter settings . The algorithm is implemented in MATLAB , and the running time varies from ten seconds to a few minutes , depending on the number of the boundary fragments found in the image .__label__Material|Data|Use
Fig . 2 visualizes the structure learning process . _CITE_ This example is similar to that above but includes some uncorrelated random variables to show how they are treated by CorEx . We set b = 5 clusters of variables but we used m = 10 hidden variables . At each iteration , t , we show which hidden variables , Yj , are connected to input variables , Xi , through the connectivity matrix , a ( shown on top ).__label__Method|Algorithm|Introduce
If updated instead , SAM - LRB ( optimized ) performs promisingly the best . We then proceed to test the multifaceted modeling by SAM - LRB . Same as [ 10 ], we choose a publicly - available dataset of political blogs describing the 2008 U . S . presidential election _CITE_ [ 11 ]. Out of the total 6 political blogs , three are from the right and three are from left . There are 20 , 827 documents and a vocabulary size of 8284 .__label__Material|Data|Use
We define a concept graph as a rooted , directed graph where the nodes represent thematic units ( called concepts ) and the edges represent relationships between concepts . Concept graphs are useful for summarizing document collections and providing a visualization of the thematic content and structure of large document sets - a task that is difficult to accomplish using only keyword search . An example of a concept graph is Wikipedia ’ s category graph _CITE_ . Figure 1 shows a small portion of the Wikipedia category graph rooted at the category MACHINE LEARNING . From the graph we can quickly infer that the collection of machine learning articles in Wikipedia focuses primarily on evolutionary algorithms and Markov models with less emphasis on other aspects of machine learning such as Bayesian networks and kernel methods .__label__Supplement|Document|Introduce
The datasets where purposely are selected with various sizes in order to clearly expose the differences among scalable capabilities of the models . Three of which are large - scale datasets with hundreds of thousands and millions of data points ( year : 515 , 345 ; poker : 1 , 025 , 010 ; and airlines : 5 , 929 , 413 ), whilst the rest are medium size databases ( ijcnn1 : 141 , 691 and cod - rna : 331 , 152 ). These datasets can be downloaded from LIBSVM and UCI websites , except the airlines which was obtained from American Statistical Association ( ASA _CITE_ ). For the airlines dataset , our aim is to predict whether a flight will be delayed or not under binary classification setting , and how long ( in minutes ) the flight will be delayed in terms of departure time under regression setting . A flight is considered delayed if its delay time is above 15 minutes , and non - delayed otherwise .__label__Material|Data|Use
Today , DNNs are almost exclusively trained on one or many very fast and power - hungry Graphic Processing Units ( GPUs ) ( Coates et al ., 2013 ). As a result , it is often a challenge to run DNNs on target low - power devices , and substantial research efforts are invested in speeding up DNNs at run - time on both general - purpose ( Gong et al ., 2014 ; Han et al ., 2015b ) and specialized computer hardware ( Chen et al ., 2014 ; Esser et al ., 2015 ). This paper makes the following contributions : The code for training and running our BNNs is available on - line ( both Theano and Torch framework _CITE_ ). In this section , we detail our binarization function , show how we use it to compute the parameter gradients , and how we backpropagate through it . Deterministic vs Stochastic Binarization When training a BNN , we constrain both the weights and the activations to either + 1 or − 1 .__label__Method|Code|Produce
We demonstrate competitive results with state - of - the - art methods for both of them . Our image - based rendering engine requires two different training sources : 1 ) a 2D source of images with 2D pose annotations and 2 ) a MoCap 3D source . We consider two different datasets for each : for 3D poses we use the CMU Motion Capture Dataset _CITE_ and the Human3 . 6M 3D poses [ 13 ], and for 2D pose annotations the MPII - LSP - extended dataset [ 24 ] and the Human3 . 6M 2D poses and images . MoCap 3D source . The CMU Motion Capture dataset consists of 2500 sequences and a total of 140 , 000 3D poses .__label__Material|Data|Use
Our experiments are conducted on 4 public datasets : POS , ChineseOCR , RCV1 - regions , and EURLex ( directory codes ). For sequence labeling we experiment on POS and ChineseOCR . The POS dataset is a subset of Penn treebank _CITE_ that contains 3 , 808 sentences , 196 , 223 words , and 45 POS labels . The HIT - MW ChineseOCR dataset is a hand - written Chinese character dataset from [ 17 ]. The dataset has 12 , 064 hand - written sentences , and a total of 174 , 074 characters .__label__Material|Data|Use
We further pre - computed the scores of candidate parent sets , which were fed as input into each system evaluated . Finally , we used the EVASOLVER partial MaxSAT solver , for inferring ordering constraints . In our first set of experiments , we compared our approach with the ILP - based system of GOBNILP , _CITE_ where we encoded ancestral constraints using linear constraints , based on [ Cussens , 2008 ]; note again that both are exact approaches for structure learning . In Table 1 , we supplied both systems with decomposable constraints inferred via projection ( which empowers the oracle for searching the EC tree , and provides redundant constraints for the ILP ). In Table 2 , we withheld the projected constraints .__label__Method|Tool|Compare
Similarly with the group normal - Jeffreys prior , we will replace the distribution over W at each layer with the masked variational posterior mean during test time : where m is a binary mask determined according to the aforementioned threshold , MW are the means of q (˜ W ) and µz , σ z are the means and variances of the local log - normals over zi . Furthermore , similarly to the group normal - Jeffreys approach , we will use the variational posterior marginal variances : to compute the final bit precision for the entire weight matrix ˆW . We validated the compression and speed - up capabilities of our models on the well - known architectures of LeNet - 300 - 100 [ 39 ], LeNet - 5 - Caffe _CITE_ on MNIST [ 40 ] and , similarly with [ 49 ], VGG [ 61 ] 10 on CIFAR 10 [ 36 ]. The groups of parameters were constructed by coupling the scale variables for each filter for the convolutional layers and for each input neuron for the fully connected layers . We provide the algorithms that describe the forward pass using local reparametrizations for fully connected and convolutional layers with each of the employed approximate posteriors at appendix F . For the horseshoe prior we set the scale τ0 of the global half - Cauchy prior to a reasonably small value , e . g .__label__Method|Tool|Use
The SDPP samples , however , are more diverse , tending to cover more of the space while still respecting the quality scores — they are still smooth , and still tend to start near the middle position . To demonstrate that SDPPs effectively model characteristics of real - world data , we apply them to a multiple - person pose estimation task . Our dataset consists of 73 still frames taken from various TV shows , each approximately 720 by 540 pixels in size _CITE_ . As much as possible , the selected frames contain three or more people at similar scale , all facing the camera and without serious occlusions . Sample images from the dataset are shown in Figure 4 .__label__Material|Data|Introduce
To be specific , the denoising MSE using electroGP is only 1 . 8 ˆ 10 - , comparing to 63 . 37 using NLM and 61 . 79 using IsD . The MSE of reconstructing the entirely missing frame 53 using electroGP is 2 ˆ 10 - compared to 13 using LI . An online video of the super - resolution result using electroGP can be found in this link _CITE_ . The frame per second ( fps ) of the generated video under electroGP was tripled compared to the original one . Though over two thirds of the frames are pure generations from electroGP , this new video flows quite smoothly .__label__Supplement|Media|Produce
We compute the mean squared error between the true angle and the approximated angles from DLSH and DSBLSH respectively . Note that after computing the Hamming distance , we divide the result by C = K / 7r and get the approximated angle . We conduct the experiment on the following datasets : 1 ) Photo Tourism patch dataset [ 26 ], Notre Dame , which contains 104 , 106 patches , each of which is represented by a 128D SIFT descriptor ( Photo Tourism SIFT ); and 2 ) MIR - Flickr _CITE_ , which contains 25 , 000 images , each of which is represented by a 3125D bag - of - SIFT - feature histogram ; For each dataset , we further conduct a simple preprocessing step as in [ 12 ], i . e . mean - centering each data sample , so as to obtain additional mean - centered versions of the above datasets , Photo Tourism SIFT ( mean ), and MIR - Flickr ( mean ). The experiment on these mean - centered datasets will test the performance of SBLSH when the angles of data pairs are not constrained in ( 0 , 7r / 2 ].__label__Material|Data|Use
To find a “ good ” bi - clustering of the ratings matrix , minimization of Tmdl defined in ( 2 ) is done by scanning cluster cardinalities | C | and | D | and optimizing 6min as defined in ( 3 ) for each fixed pair of | C |, | D |. The minimum of Tmdl is obtained at | C |,: 13 and | D |,: 6 with beyond 1 % sensitivity to small changes in | C | and in | D | both in Tmdl values and in prediction accuracy . See supplementary material _CITE_ for visualization of the solution at | C |= 4 and | D |= 3 . To measure the accuracy of our algorithm we use mean absolute error ( MAE ) metrics , which is commonly used for evaluation on this dataset [ 11 ]. The mean absolute error is defined as : MAE = N EN 1 | zi − ri |, where zi - s are the predicted and ri - s are the actual ratings .__label__Supplement|Document|Produce
To demonstrate the effectiveness of our split - merge moves , we compare three algorithms : batch variational inference ( bHDP ), online variational inference without split - merge ( oHDP ), and online variational inference with split - merge ( oHDP - SM ). On the NIPS corpus we also compare these three methods to collapsed Gibbs sampling ( CGS ) and the CRF - style oHDP model ( oHDP - CRF ) proposed by [ 4 ]. _CITE_ We test the models on one synthetic and two real datasets : Bars A 20 - topic bars dataset of the type introduced in [ 18 ], where topics can be viewed as bars on a 10 x 10 grid . The vocabulary size is 100 , with a training set of 2000 documents and a test set of 200 documents , 250 words per document . NIPS 1 , 740 documents from the Neural Information Processing Systems conference proceedings , 1988 - 2000 .__label__Method|Algorithm|Compare
We used node skip parameter AV ( v ) = 0 . 5 for all nodes v in each comparison . We used SVM [ 11 ] as a kernel - based machine learning algorithm . We evaluated the performance of the comparison methods with question type TIME TOP , ORGANIZATION , LOCATION , and NUMEX , which are defined in the CRL QA - data _CITE_ . Table 2 shows the average F - measure as evaluated by 5 - fold cross validation . n in Table 2 indicates the threshold of an attribute ’ s number , that is , we evaluated only those HiASs that contain less than n - attributes for each kernel calculation .__label__Material|Data|Extent
In protein fold identification [ 15 ], the low conservation of primary sequence in protein superfamilies such as Thioredoxin - fold ( Trx - fold ) makes conventional modeling methods , such as Hidden Markov Models difficult to use . MIL can be used to identify new Trx - fold proteins naturally , in which each protein sequence is considered as a bag , and some of its subsequences are considered as instances . Here , we use a benchmark protein dataset _CITE_ . In each protein ’ s primary sequence , first of all , the primary sequence motif ( typically CxxC ) are found . Then , a window of size 214 around it are extracted and aligned .__label__Material|Data|Use
Our implementation is in C ++, using the Bundle Methods for Risk Minimization ( BMRM ) of [ 18 ] as a base . Source code is available under the Mozilla Public License . _CITE_ In our first set of experiments we compared our model to published results on the Macro - F1 score . We strived to make our comparison as broad as possible , but we limited ourselves to methods with published results on public datasets , where the experimental setting was described in enough detail to allow us to make a fair comparison . We therefore compared our model to Canonical Correlation Analysis [ 3 ] ( CCA ), Binary Method [ 9 ] ( BM ), Classifier Chains [ 4 ] ( CC ), Subset Mapping [ 19 ] ( SM ), Meta Stacking [ 12 ] ( MS ), Ensembles of Binary Method [ 4 ] ( EBM ) , Ensembles of Classifier Chains [ 4 ] ( ECC ), Ensembles of Pruned Sets [ 11 ] ( EPS ) and Random K Label Subsets [ 10 ] ( RAKEL ).__label__Method|Code|Produce
The true rating matrix is then R * = UVT . We generate the observed rating matrix R from R * by adding Gaussian noise N ( 0 , Q ) to the true ratings . We use five real world datasets as follows : Movielens 100k , Movielens 1M , Yahoo Music , Book crossing _CITE_ and EachMovie as shown in Table 1 . There are no current approaches available that simultaneously learn both the user and item factors by sampling from the posterior in a bandit setting . From the currently available algorithms , we choose two kinds of baseline methods - one that sequentially updates the the posterior of the user features only while fixing the item features to a point estimate ( ICF ) and another that updates the MAP estimates of user and item features via stochastic gradient descent ( SGD - Eps ).__label__Material|Data|Use
The Hessian of a Gaussian distribution is then given by the expression : This expression is then used to learn the optimal local metric . We compare the performance of our method ( GLML — Generative Local Metric Learning ) with recent metric learning discriminative methods which report state - of - the - art performance on a number of datasets . These include Information - Theoretic Metric Learning ( ITML ) _CITE_ [ 3 ], Boost Metric ( BM ) [ 21 ], and Largest Margin Nearest Neighbor ( LMNN ) [ 26 ]. We used the implementations downloaded from the corresponding authors ’ websites . We also compare with a local metric given by the Fisher kernel [ 12 ] assuming a single Gaussian for the generative model and using the location parameter to derive the Fisher information matrix .__label__Method|Algorithm|Use
Any additional parameters of pe ( e ) such as scale and mean may be obtained in a maximum likelihood ( ML ) sense by maximizing L . Note that the nonlinear optimization is not subject to constraints ; the constraints apply only in the quadratic optimization . In our first experiment we use mineral data from the United . States Geological Survey ( USGS ) _CITE_ to build artificial mixtures for evaluating our unsupervised unmixing framework . Three target endmembers where chosen ( Almandine WS479 , Montmorillonite + Illi CM42 and Dickite NMNH106242 ). A spectral scene of 100 samples was constructed by creating a random mixture of the three minerals .__label__Supplement|Website|Introduce
Due to conjugacy , the posterior distribution over β is also normal , and the gradients of the log - likelihood and the log - prior are given by Vβ log ( P ( yi | xi , β )) = −( yi − βT xi ) xi and Vβ log ( P ( β )) = − Aβ . We ran experiments on 11 standard UCI regression datasets , summarized in Table 1 . _CITE_ In each case , we set the prior precision A = 1 , and we partitioned our dataset into training ( 70 %), validation ( 10 %), and test ( 20 %) sets . The validation set is used to select the step size parameters , and we report the mean square error ( MSE ) evaluated on the test set , using 5 - fold cross - validation . The average test MSE on a subset of datasets is reported in Figure 1 .__label__Material|Data|Use
In contrast to this method , our approach does not require to specify a prior for W , leads to simple updates for W that are directly comparable to IS - NMF and experiments will reveal that our approach embeds model order selection as well , by automatically pruning unnecessary columns of W , without resorting to the nonparametric framework . In this section , we study the performances of MJLE and MMLE methods on both synthetical and real - world datasets . _CITE_ The prior hyperparameters are fixed to αk = 1 , γk = 0 ( exponential distribution ) and Ok = 1 , i . e ., hk ,,, — exp (— hk ,,,). We used 5000 algorithm iterations and nonnegative random initializations in all cases . In order to minimize the odds of getting stuck in local optima , we adapted the deterministic annealing method proposed in [ 15 ] for MMLE .__label__Material|Data|Use
All of the above approaches were evaluated on two standard and challenging datasets : Oxford Flower - 102 and Caltech - UCSD Bird200 . The train - test splits are fixed for both datasets and are provided on their respective websites . _CITE_ The Oxford Flower - 102 dataset contains 8189 images of 102 different flower species . It is a challenging dataset due to significant scale and illumination changes ( see figure 4 ). The results are presented in table 1 ( a ).__label__Material|Data|Use
Once the corpus C has been split into two subsets C1 and C2 , each of these subsets may Algorithm 2 Splitting a corpus into two parts still contain the full set of topics but the topic distribution will differ in the two : topics similar to the first pseudo - center will be predominant in the first subset , the others in the second . By recursively iterating this process , we obtain a binary tree where topic distributions in the nodes with higher depth are expected to be more concentrated on fewer topics . In the next sections , we assess the validity of this approach on both synthetic and real - world data _CITE_ . In order to test the ability of SIDIWO to recover latent structures in data , we generate a dataset distributed as a single topic model ( with a vocabulary of 100 words ) whose 8 topics have an intrinsic hierarchical structure depicted in Figure 1a . In this figure , topics are on the x - axis , words on the y - axis , and green ( resp .__label__Material|Data|Use
In addition , our model reasons about the question ( and consequently the image via the co - attention mechanism ) in a hierarchical fashion via a novel 1 - dimensional convolution neural networks ( CNN ). Our model improves the state - of - the - art on the VQA dataset from 60 . 3 % to 60 . 5 %, and from 61 . 6 % to 63 . 3 % on the COCO - QA dataset . By using ResNet , the performance is further improved to 62 . 1 % for VQA and 65 . 4 % for COCO - QA . _CITE_ . Visual Question Answering ( VQA ) [ 2 , 6 , 14 , 15 , 27 ] has emerged as a prominent multi - discipline research problem in both academia and industry . To correctly answer visual questions about an image , the machine needs to understand both the image and question .__label__Material|Data|Use
Each row corresponds to a motion sequence to which we would expect the second layer features to be roughly invariant . From this visualization , non - trivial invariances are observed such as non - linear warping , rotation , local non - affine changes and large scale translations . A video animation of this visualization is also available online _CITE_ . without ( top ) and with ( bottom ) temporal slowness . ( Right ) visualization of second layer features ( patch size 32x32 ), with each row corresponding to one pooling unit .__label__Supplement|Media|Produce
We also give an ex - post privacy analysis of the classical AboveThreshold privacy tool , modifying it to allow for queries chosen depending on the database . Finally , we apply our approach to two common objective functions , regularized linear and logistic regression , and empirically compare our noise reduction methods to ( i ) inverting the theoretical utility guarantees of standard private ERM algorithms and ( ii ) a stronger , empirical baseline based on binary search . _CITE_ Differential Privacy [ 7 , 8 ] enjoys over a decade of study as a theoretical construct , and a much more recent set of large - scale practical deployments , including by Google [ 10 ] and Apple [ 11 ]. As the large theoretical literature is put into practice , we start to see disconnects between assumptions implicit in the theory and the practical necessities of applications . In this paper we focus our attention on one such assumption in the domain of private empirical risk minimization ( ERM ): that the data analyst first chooses a privacy requirement , and then attempts to obtain the best accuracy guarantee ( or empirical performance ) that she can , given the chosen privacy constraint .__label__Supplement|Paper|Compare
For each experiment , we refer to the supplementary material for model architectures and additional results . Common points are : i ) discriminators ’ outputs with softplus activations : f ( x ) = ln ( 1 + ex ), i . e ., positive version of ReLU ; ( ii ) Adam optimizer [ 16 ] with learning rate 0 . 0002 and the first - order momentum 0 . 5 ; ( iii ) minibatch size of 64 samples for training both generator and discriminators ; ( iv ) Leaky ReLU with the slope of 0 . 2 ; and ( v ) weights initialized from an isotropic Gaussian : N ( 0 , 0 . 01 ) and zero biases . Our implementation is in TensorFlow [ 1 ] and we have published a version for reference _CITE_ . We now present our experiments on synthetic data followed by those on large - scale real - world datasets . In the first experiment , we reuse the experimental design proposed in [ 20 ] to investigate how well our D2GAN can deal with multiple modes in the data .__label__Method|Code|Produce
Note that the sequences used for training were only 20 frames long . The model ’ s predictions look qualitatively better than most published generated sequences . _CITE_ Further results and data can be found on the project website at http :// www . ccc . cs . uni - frankfurt . de / people / vincent - michalski / grammar - cells A major long - standing problem in sequence modeling is dealing with long - range correlations . It has been proposed that deep learning may help address this problem by finding representations that capture better the abstract , semantic content of the inputs [ 22 ].__label__Method|Algorithm|Compare
We show that Foveated Feature Congestion ( FFC ) clutter scores ( r ( 44 ) = − 0 . 82 f 0 . 04 , p < 0 . 0001 ) correlate better with target detection ( hit rate ) than regular Feature Congestion ( r ( 44 ) = − 0 . 19 f 0 . 13 , p = 0 . 0774 ) in forced fixation search ; and we extend foveation to other clutter models showing stronger correlations in all cases . Thus , our model allows us to enrich clutter perception research by computing fixation specific clutter maps . Code for building peripheral representations is available _CITE_ . What is clutter ? While it seems easy to make sense of a cluttered desk vs an uncluttered desk at a glance , it is hard to quantify clutter with a number .__label__Method|Code|Produce
1 ( d ) does not fully match the true covariance , it clearly captures the nonstationary effects . As a testbed for the proposed methods , we consider an information filtering task . The goal is to predict individual users ’ preferences for a large collection of art images _CITE_ , where each user rated a random subset out of a total of 642 paintings , with ratings “ like ” (+ 1 ), “ dislike ”(− 1 ), or “ not sure ” ( 0 ). In total , ratings from M = 190 users were collected , where each user had rated 89 paintings on average . Each image is also described by a 275dimensional feature vector ( containing correlogram , color moments , and wavelet texture ).__label__Material|Data|Use
Algorithm 1 The CCCP algorithm for solving MMS In order to evaluate the proposed algorithm , we first perform experiments on several artificial datasets created from standard machine learning databases . Finally , we test our algorithm on one of the examples motivating our study — learning a face recognition system from news images weakly annotated by their associated captions . We benchmark MMS against the following baselines : We implemented our MMS algorithm in MATLAB _CITE_ , and used a value of the 1 / N for the regularization parameter A in all our experiments . In ( 1 ) we used A ( zm , ym ) = 1 ( zm # ym ). For a fair comparison , we used linear kernel for all the methods .__label__Method|Tool|Use
We will show in the next section that the CR1 rules fail to lift the theory in Figure 4a , which is in 2 - WFOMC . Note that there are also useful theories that are not in 2 - WFOMC , such as those containing the transitive relation friends ( X , Y ) ∧ friends ( Y , Z ) ⇒ friends ( X , Z ). To complement the theoretical results of the previous section , we extended the WFOMC implementation _CITE_ with the domain recursion rule . We performed experiments with the theory in Figure 4a , which is a version of the friends and smokers model [ 11 ] extended with the symmetric relation of Equation 3 . We evaluate the performance querying P ( smokes ( bob )) with increasing domain size , comparing our approach to the existing WFOMC implementation and its propositional counterpart , which first grounds the theory and then compiles it with the c2d compiler [ 13 ] to a propositional d - DNNF circuit .__label__Method|Code|Extent
This confidently beats the “ magic barrier ” of 0 . 73 reported in the collaborative filtering literature [ 11 ]. The root mean squared error ( RMSE ) measured for the same clustering with a mean of z values within each section c , d taken for prediction yields 0 . 96 ( with a deviation below 0 . 01 ). This is much better than 1 . 165 RMSE reported for a dataset 20 times larger [ 20 ] and quite close to 0 . 9525 RMSE reported by Netflix for a dataset 1000 times larger of a similar nature _CITE_ . A new model independent approach to the analysis of data given in the form of samples of a function Z ( X , Y ) rather than samples of co - occurrence statistics of X and Y is introduced . From a theoretical viewpoint the approach is a much required extension of the Information Bottleneck method that allows for its application to entirely new domains .__label__Material|Data|Introduce
For the nulls , if the set of selected groups contains the support of the true model , which is nearly always true for higher signal levels τ , then the two are equivalent ( as kPLµk2 = hdirL ( Y ), µi = 0 ), and coverage is at the target level . At low signal levels τ , however , a true group is occasionally missed , in which case kPLµk2 & gt ; hdirL ( Y ), µi strictly . We examine the 2015 California county health data _CITE_ which was also studied by Loftus and Taylor [ 9 ]. We fit a linear model where the response is the log - years of potential life lost and the covariates are the 34 predictors in this data set . We first let each predictor be its own group ( i . e ., group size 1 ) and run the three algorithms considered in Section 3 .__label__Material|Data|Use
However , we see that the proposed local metric highly outperforms the discriminative nearest neighbor performance in a high dimensional space appropriately . We note that this example is ideal for GLML , and it shows much improvement compared to the other methods . The other experiments consist of the following benchmark datasets : UCI machine learning repository datasets ( Ionosphere , Wine ), and the IDA benchmark repository _CITE_ ( German , Image , Waveform , Twonorm ). We also used the USPS handwritten digits and the TI46 speech dataset . For the USPS data , we resized the images to 8 × 8 pixels and trained on the 64 - dimensional pixel vector data .__label__Material|Data|Use
To highlight the model ’ s ability to resolve visual references , we first perform experiment with a synthetic dataset that is explicitly designed to contain ambiguous expressions and strong inter - dependency among questions in the visual dialog . We then show that the model also works well in the real VisDial [ 1 ] benchmark . Experimental Setting We create a synthetic dataset , called MNIST Dialog _CITE_ , which is designed for the analysis of models in the task of visual reference resolution with ambiguous expressions . Each image in MNIST Dialog contains a 4 x 4 grid of MNIST digits and each MNIST digit in the grid has four randomly sampled attributes , i . e ., color = { red , blue , green , purple , brown }, bgcolor = { cyan , yellow , white , silver , salmon }, number = { x | 0 & lt ; x & lt ; 9 } and style = { flat , stroke }, as illustrated in Figure 1 . Given the generated image from MNIST Dialog , we automatically generate questions and answers about a subset of the digits in the grid that focus on visual reference resolution .__label__Material|Data|Use
SVD + SMM is a two - step procedure : 1 ) extracting low - dimensional representations of words by using a singular value decomposition ( SVD ), and 2 ) learning a support measure machine using the distribution of extracted representations of words appearing in each document with the same kernels as the latent SMM . word2vec + SMM employs the representations of words learnt by word2vec [ 7 ] and uses them for the SMM as in SVD + SMM . Here we use pre - trained 300 dimensional word representation vectors from the Google News corpus , which can be downloaded from the author ’ s website _CITE_ . Note that word2vec + SMM utilizes an additional resource to represent the latent vectors for words unlike the latent SMM , and the learning of word2vec requires n - gram information about documents , which is lost in the BoW representation . With SVMs , we use a Gaussian RBF kernel with parameter y and a quadratic polynomial kernel , and the features are represented as BoW .__label__Material|Data|Use
SVD + SMM is a two - step procedure : 1 ) extracting low - dimensional representations of words by using a singular value decomposition ( SVD ), and 2 ) learning a support measure machine using the distribution of extracted representations of words appearing in each document with the same kernels as the latent SMM . word2vec + SMM employs the representations of words learnt by word2vec [ 7 ] and uses them for the SMM as in SVD + SMM . Here we use pre - trained 300 dimensional word representation vectors from the Google News corpus , which can be downloaded from the author ’ s website _CITE_ . Note that word2vec + SMM utilizes an additional resource to represent the latent vectors for words unlike the latent SMM , and the learning of word2vec requires n - gram information about documents , which is lost in the BoW representation . With SVMs , we use a Gaussian RBF kernel with parameter y and a quadratic polynomial kernel , and the features are represented as BoW .__label__Supplement|Website|Use
‘ Citations ’ is the co - authorship network based on preprints posted to Condensed Matter section of ArXiv between 1995 and 1999 [ 15 ]. ‘ Movielens100k ’ contains information about users rating particular movies from which we extracted the bipartite network . Finally , ‘ IMDB ’ contains information about actors co - starring a movie _CITE_ . The sizes of the different networks are given in We evaluate the fit of four different models on these datasets . First , the stable IBP [ 18 ] with parameters ( αIBP , τIBP , σIBP ) ( S - IBP ).__label__Material|Data|Introduce
Here Nv , J is the number of times v occurs in the observation wJ . Computing the expectation over this Dirichlet posterior gives us the following Bayesian estimate for PV : To provide a meaningful comparison with previously - reported results , we use , without any modification , the dataset provided by Duygulu et al . [ 4 ] _CITE_ . This allows us to compare the age annotation . Our model ( CRM ) substantially outperforms all other models .__label__Material|Data|Use
Some statistics and examples of the dataset will be given in Section 4 . 2 . The latest dataset is available on the project page : http :// idl . baidu . com / FM - IQA . html We start with the 158 , 392 images from the newly released MS COCO [ 21 ] training , validation and testing set as the initial image set . The annotations are collected using Baidu ’ s online crowdsourcing server _CITE_ . To make the labeled question - answer pairs diversified , the annotators are free to give any type of questions , as long as these questions are related to the content of the image . The question should be answered by the visual content and commonsense ( e . g ., we are not expecting to get questions such as “ What is the name of the person in the image ?”).__label__Supplement|Website|Use
We chose the learning rate to be γt = ( 1 + Vt )− , for stochastic mirror ascent to update the posterior approximation ; the learning rate for the stochastic gradient ascent to update the hyperparameters is set to 10 − γt . We evaluate the models in terms of the normalized mean squared error ( nMSE ) on a held - out test set after 500 iterations . We performed experiments on three real - world robotic datasets datasets , kin40k , SARCOS _CITE_ , KUKA , and three variations of iVSGPR : iVSGPR5 , iVSGPR10 , and iVSGPRada . For the kin40k and SARCOS datasets , we also implemented VSGPR ∗ -, which uses stochastic variational inference to update m ˜ svi and S ˜ but fixes hyperparameters and inducing points as the solution to the batch variational sparse GPR [ 26 ] with all of the training data . Because VSGPR ∗ svi reflects the perfect scenario of performing stochastic approximation under the selected learning rate , we consider it as the optimal goal we want to approach .__label__Material|Data|Use
The dataset is downloaded from Gal ’ s page , with T = 17 , V = 14036 , with 3280697 events for the matrix . Ebola corpus ( EBOLA ) : EBOLA corpus contains the data for the 2014 Ebola outbreak in West Africa every day from Mar 22th , 2014 to Jan 5th 2015 , each column represents the cases or deaths in a West Africa country . After data cleaning , the dataset is with T = 122 , V = 16 . International Disaster ( ID ) _CITE_ : The International Disaster dataset contains essential core data on the occurrence and effects of over 22 , 000 mass disasters in the world from 1900 to the present day . A count matrix with T = 115 and V = 12 is built from the events of disasters occurred in Europe from the year 1902 to 2016 , classified according to their disaster types . Annual Sheep Population ( ASP ) : The Annual Sheep Population contains the sheep population in England & Wales from the year 1867 to 1939 yearly .__label__Material|Data|Use
The second , Cluster - Based LML ( CBLML ), is also a variant of PLML without weight learning . Here we learn one local metric for each cluster and we assign a weight of one for a basis metric Mbi if the corresponding cluster of Mbi contains the instance , and zero otherwise . Finally , we also compare against four state of the art metric learning methods LMNN [ 15 ], BoostMetric [ 13 ] _CITE_ , GLML [ 11 ] and LMNN - MM [ 15 ] . The former two learn a single global metric and the latter two a number of local metrics . In addition to the different metric learning methods , we also compare PLML against multi - class SVMs in which we use the one - against - all strategy to determine the class label for multi - class problems and select the best kernel with inner cross validation .__label__Method|Algorithm|Compare
We also test our methods on Named Entity Recognition ( NER ) in CoNLL 2003 [ 46 ] and OntoNote 5 . 0 [ 20 ] datasets using the CNN from Strubell et al . [ 45 ]. _CITE_ Similar to Question Type , the model is too complex for our approaches . So we ( i ) only use 3 layers instead of 4 layers , ( ii ) reduce the number of filters from 300 to 100 , ( iii ) add 0 . 001 L2 regularization , ( iv ) make the 50 dimension word embedding from Collobert et al . [ 9 ] non - trainable .__label__Method|Algorithm|Use
In Tab . 1 , we compare our method to conventional compressive sensing MRI methods on brain data . These methods include Zero - filling [ 22 ], TV [ 2 ], RecPF [ 4 ], SIDWT _CITE_ , and also the state - of - the - art methods such as PBDW [ 6 ], PANO [ 10 ], FDLCP [ 8 ] and BM3D - MRI [ 11 ]. For ADMM - Net , we initialize the filters in each stage to be eight 3 × 3 DCT basis ( the average DCT basis is discarded ). Compared with the baseline methods such as Zero - filling , TV , RecPF and SIDWT , our proposed method produces the best quality with comparable reconstruction speed .__label__Method|Algorithm|Compare
The left column of Fig . 2 presents a screen from the annotation process , while the right column shows how we inform annotators about their mistakes . As an alternative to human annotators , we propose an automatic method to evaluate samples , which we find to correlate well with human evaluation : We apply the Inception model _CITE_ [ 20 ] to every generated image to get the conditional label distribution p ( y | x ). Images that contain meaningful objects should have a conditional label distribution p ( y | x ) with low entropy . Moreover , we expect the model to generate varied images , so the marginal f p ( y | x = G ( z )) dz should have high entropy .__label__Method|Algorithm|Use
More specifically , we attempted to construct the subset of features G ⊂ X that minimizes the normalized mean squared regression error ( NMSE ) of a Gaussian process regressor . We do so by selecting the feature x ( i ) maximizing dependence between the feature set Gi = { Gi − 1 , x ( i )} and the target variable y at each iteration i ∈ { 1 ,... 10 }, such that G0 = {∅} and x ( i ) ∈/ Gi − 1 . We considered 12 heterogeneous datasets , obtained from the UCI dataset repository , the Gaussian process web site Data _CITE_ and the Machine Learning data set repository . Random training / test partitions are computed to be disjoint and equal sized . Since G can be multi - dimensional , we compare RDC to the non - linear methods dCor , HSIC and CHSIC .__label__Material|Data|Use
The class of the remaining images of the test fold was then predicted to be the one with highest SVM score among the the 10 previously trained binary SVMs . Splitting our data into test and training sets was led through a 3 - fold cross validation ( roughly 332 training images and 168 for testing ), averaging the test error on 5 random fold splits of the original data . Those results were obtained using the spider toolbox _CITE_ and graphically displayed in figure ( 2 ). Note that the best testing errors were reached using a σ value of 0 . 12 with an η parameter within 0 . 008 and 0 . 02 , this error being roughly 19 . 5 % with a standard deviation inferior to 1 % in all the region corresponding to an error lower than 22 %. To illustrate the sensibility of our method to the number of sampled points in τ we show in the same figure the decrease of this error when the number of sampled points ranges from 10 to 30 with independently chosen random points for each computation .__label__Method|Tool|Use
Interestingly , the performance was also robust with respect to these choices . Experiment 4 : Scene / object recognition . Our final set of experiments used the data from the Graz dataset _CITE_ , as well as the dataset proposed in [ 21 ]. In both tests , we used Latent Dirichlet allocation ( LDA ) [ 4 ] as the generative model . The free energy for LDA is derived in [ 4 ].__label__Material|Data|Use
In this section , we demonstrate the behaviors of MATk learning coupled with different individual losses for binary classification and regression on synthetic and real datasets , with minimizing the average loss and the maximum loss treated as special cases for k = n and k = 1 , respectively . For simplicity , in all experiments , we use homogenized linear prediction functions f ( x ) = wT x with parameters w and the Tikhonov regularizer Q ( w ) = 1 2C || w || , and optimize the MATk learning objective with the stochastic gradient descent method given in ( 4 ). Binary Classification : We conduct experiments on binary classification using eight benchmark datasets from the UCI and KEEL _CITE_ data repositories to illustrate the potential effects of using ATk loss in practical learning to adapt to different underlying data distributions . A detailed description of the datasets is given in supplementary materials . The standard individual logistic loss and hinge loss are combined with different aggregate losses .__label__Material|Data|Use
We set each training sequence to have the length of 50 . Quality of fit is evaluated by the bits - per - character ( BPC ) metric , which is loge of perplexity . text8 dataset : Another dataset used for character level language modelling is the text8 dataset _CITE_ , which contains 100M characters from Wikipedia with an alphabet size of 27 . We follow the setting from [ 23 ] and each training sequence has length of 180 . adding problem : The adding problem ( and the following copying memory problem ) was introduced in [ 10 ]. For the adding problem , each input has two sequences with length of T where the first sequence are numbers sampled from uniform [ 0 , 1 ] and the second sequence are all zeros except two elements which indicates the position of the two elements in the first sequence that should be summed together .__label__Material|Data|Introduce
Each 6 - core processor is equipped with a three - level memory hierarchy as follows : ( i ) 64 KB of L1 cache for data and 512 KB of L2 cache that are private to each core , and ( ii ) 12 MB of L3 cache that is shared among the 6 cores . Each 6 - core processor is linked to a 32 GB memory bank with independent memory controllers leading to a total system memory of 256 GB ( 32 x 8 ) that can be globally addressed from each core . The four sockets are interconnected using HyperTransport - 3 technology _CITE_ . Datasets A variety of datasets were chosen for experimentation ; these are summarized in Table 1 . We consider four datasets : ( i ) NEWS20 contains about 20 , 000 UseNet postings from 20 newsgroups .__label__Method|Algorithm|Use
The matrix O is initialized as follows : ∀ i = 1 .. ml and ∀ Q in {− 1 , 1 }, Oσi = 1 if Q = yi , 0 otherwise , ∀ i = ml + 1 .. mu and ∀ Q in {− 1 , 1 }, Oσi = 0 . 5 and we learn an optimal separator : Here c1 and c2 are balance constants between the labeled and unlabeled set : when the number of unlabeled instances become greater than the number of labeled instances , we need to reduce the importance of the unlabeled set in the learning procedure because there exists the risk that the labeled set will be ignored . We consider the provided labels to be correct , so we keep the corresponding lO fixed during the iterations of the algorithm and estimate uO by optimizing P2 ( Xu , ht + ). The iterative algorithm with O - SVM is implemented in Python using Cvxopt ( for optimizing O - SVM ) and Cvxpy _CITE_ with its Ecos solver [ 9 ]. For each dataset , we show in Figure 1 the accuracy of the two methods with an increasing proportion of labeled data . The different approaches are compared on the same kernel , either the linear or the gaussian , the one that gives higher overall accuracy .__label__Method|Tool|Use
All kernels were implemented in C ++ with Eigen library and compiled with gcc 4 . 8 . 2 . Datasets . We collected five real - world graph classification benchmark datasets : _CITE_ ENZYMES , NCI1 , NCI109 , MUTAG , and D & D , which are popular in the graph - classification literature [ 13 , 14 ]. ENZYMES and D & D are proteins , and NCI1 , NCI109 , and MUTAG are chemical compounds . Statistics of these datasets are summarized in Table 1 , in which we also show the maximum of maximum degrees of product graphs maxG , G ′ G ∆× for each dataset G . We consistently used Amax = ( maxG , G ′ G ∆×)− as the upper bound of A in geometric random walk kernels , in which the gap was less than one order as the lower bound of A .__label__Material|Data|Use
For targets that are near the swimmer , the behaviour must also include various turns and jerks , quite different from steady - state swimming , which maneuver the nose into contact with the target . Our experience during interaction with the controller , as detailed below , leads us to believe that the behavioral variety that would be exhibited by a hypothetical exact optimal controller for this system to be extremely large . In order to asses the controllers we constructed a real - time interaction package _CITE_ . By dragging the target with a cursor , a user can interact with controlled swimmers of 3 to 10 links with a state dimension varying from 10 to 24 , respectively . Even with controllers composed of a single trajectory , the swimmers perform quite well , turning , tracking and braking on approach to the target .__label__Method|Code|Produce
Therefore , the relaxation of discrete codes to continuous codes offers a partial remedy by assigning different importance weight to each binary classifier while taking into account the statistical correlations between the binary classifiers . In this section we describe experiments we performed comparing discrete and continuous output codes . We selected eight multiclass datasets , seven from the UCI repository ) and the mnist dataset available from AT & T _CITE_ . When a test set was provided we used the original split into training and test sets , otherwise we used 5 - fold cross validation for evaluating the test error . Since we ran multiple experiments with 3 different codes , 7 kernels , and two base - learners , we used a subset of the training set for mn 1st , letter , and shuttle .__label__Material|Data|Use
We experimentally find that using a smaller learning rate ( e . g ., 1e − 4 ) for the weights in the output layer is crucial to obtain good performance . To verify the effectiveness , we apply the proposed model to the task of video SR , and present both quantitative and qualitative results as follows . We use 25 YUV format video sequences _CITE_ as our training set , which have been widely used in many video SR methods [ 13 , 16 , 21 ]. To enlarge the training set , model training is performed in a volumebased way , i . e ., cropping multiple overlapped volumes from training videos and then regarding each volume as a training sample . During cropping , each volume has a spatial size of 32x32 and a temporal step of 10 .__label__Material|Data|Use
BADMM vs Gurobi : Gurobi ( http :// www . gurobi . com /) is a highly optimized commercial software where linear programming solvers have been efficiently implemented . We run Gurobi on two settings : a Mac laptop with 8G memory and a server with 86G memory , respectively . For comparison , BADMM is run in parallel on a Tesla M2070 GPU with 5G memory and 448 cores _CITE_ . We experiment with large scale problems and use m = n = 11 , 5 , 10 , 151 x 210 . Table 1 shows the runtime and the objective values of BADMM and Gurobi , where a ‘-’ indicates the algorithm did not terminate .__label__Method|Code|Compare
The matrix O is initialized as follows : ∀ i = 1 .. ml and ∀ Q in {− 1 , 1 }, Oσi = 1 if Q = yi , 0 otherwise , ∀ i = ml + 1 .. mu and ∀ Q in {− 1 , 1 }, Oσi = 0 . 5 and we learn an optimal separator : Here c1 and c2 are balance constants between the labeled and unlabeled set : when the number of unlabeled instances become greater than the number of labeled instances , we need to reduce the importance of the unlabeled set in the learning procedure because there exists the risk that the labeled set will be ignored . We consider the provided labels to be correct , so we keep the corresponding lO fixed during the iterations of the algorithm and estimate uO by optimizing P2 ( Xu , ht + ). The iterative algorithm with O - SVM is implemented in Python using Cvxopt ( for optimizing O - SVM ) and Cvxpy _CITE_ with its Ecos solver [ 9 ]. For each dataset , we show in Figure 1 the accuracy of the two methods with an increasing proportion of labeled data . The different approaches are compared on the same kernel , either the linear or the gaussian , the one that gives higher overall accuracy .__label__Method|Tool|Use
1 ). 2 ) NCM : Nearest mean classifier from [ 11 ], which uses the class mean as category embeddings ( uc = xµc ). We use the code provided by the authors _CITE_ . 3 ) LME : A base large - margin embedding ( Eq . 3 ) solved using alternating optimization .__label__Method|Code|Use
For unsupervised cases , we perform not only localization but also classification according to object types . The PASCAL 06 dataset is so challenging to use that only very rare previous work has used it for unsupervised localization . For comparison , we ran publicly available code of one of the state - of - the - art techniques proposed by Russell et al _CITE_ [ 18 ] in the identical setting . The PASCAL dataset consists of { train + val + test }. However , our approach requires only images as an input , and thus all of the { train + val + test } images are used without discrimination between them .__label__Method|Code|Compare
O which is split into two terms based on dependence on each parameter : ( 1 ) expected log - likelihood for updating V by arg maxV Eq ( X ) q ( C )[ log p ( y | C , x , V , G )]; and ( 2 ) negative KL divergence between the prior and the posterior on x for updating α by arg maxα Eq ( X ) q ( C )[ log p ( x | G , α ) − log q ( x )]. The update rules for each hyperparameter are given in the Appendix . The full EM algorithm _CITE_ starts with an initial value of O . In the E - step , given q ( C ), compute q ( x ) as in Eq . ( 9 ).__label__Method|Algorithm|Introduce
For comparison , consider the results of [ 26 ] obtained for the compression of the fully - connected layers of the Krizhevsky - type network [ 13 ] with the Fastfood method . The model achieves compression factors of 2 - 3 without decreasing the network error . In all experiments we use our MATLAB extension _CITE_ of the MatConvNet framework [ 24 ]. For the operations related to the TT - format we use the TT - Toolbox implemented in MATLAB as well . The experiments were performed on a computer with a quad - core Intel Core i5 - 4460 CPU , 16 GB RAM and a single NVidia Geforce GTX 980 GPU .__label__Method|Code|Use
Figure 1 shows the “ Amari Index ” [ 1 ] of estimated W by three methods , at several factors a and sample sizes , with ten runs for every condition . In each run , the true mixing matrix was given by inverting W randomly generated from standard Gaussian and then row - normalized to have unit norms . The three methods were : 1 ) FastICA _CITE_ with the tanh nonlinearity , 2 ) Our method ( symmetric model ) without energy - dependence ( NoDep ) initialized by FastICA , and 3 ) Our full method ( symmetric model ) initialized by NoDep . NoDep was the same as the full method except that the off - diagonal elements of H was kept zero . Note that our two algorithms used exactly the same criterion for termination of algorithm , while FastICA used a different one .__label__Method|Algorithm|Use
2 and 3 on four data sets . In the ABALONE data set [ 1 ] with 4177 examples , the goal is to predict the age of Abalones based on 8 inputs . The KIN8NM data set _CITE_ represents the forward dynamics of an 8 link all - revolute robot arm , based on 8192 examples . The goal is to predict the distance of the end - effector from a target , given the twist angles of the 8 links as features . KIN40K represents the same task , yet has a lower noise level than KIN8NM and contains 40000 examples .__label__Material|Data|Introduce
Our variance measurement experiments in Table 1 includes a comparison to the estimator featured in [ 19 ], which we found to be much higher variance than the baseline RGE . In this section we empirically examine the variance properties of RV - RGEs and stochastic optimization for two real - data examples — a hierarchical Poisson GLM and a Bayesian neural network . _CITE_ surements were taken for λ values at three points during the optimization algorithm ( early , mid , late ). The parenthetical rows labeled “ MC abs ” denote the absolute value of the standard Monte Carlo reparameterization gradient estimator . The other rows compare estimators relative to the pure MC RGE variance — a value of 100 indicates equal variation L = 10 samples , a value of 1 indicates a 100 - fold decrease in variance ( lower is better ).__label__Method|Algorithm|Use
The first column contains query words , on the right are the 5 words with smallest Euclidean distance in the embedded space . We can see that they are quite relevant . Cross Language Retrieval [ 16 ] is the task of retrieving documents in a target language E given a query in a different source language F . For example , Google provides such a service _CITE_ . This is an interesting case for word - based learning to rank models which can naturally deal with this task without the need for machine translation as they directly learn the correspondence between the two languages from bi - lingual labeled data in the form of tuples R . The use of a non - symmetric lowrank model like ( 3 ) also naturally suits this task ( however in this case adding the identity does not make sense ). We therefore also provide a case study in this setting .__label__Supplement|Website|Introduce
Since the Caltech - 101 dataset has large number of classes and the experiments are computationally intensive ( 100 choose 2 classifiers need to be built in each case ), the results are averaged over 3 sets of training and test datasets only . In case of the Caltech datasets , five feature descriptors were employed : SIFT , OpponentSIFT , rgSIFT , C - SIFT , Transformed Color SIFT . Whereas in case of Oxford flowers dataset , following strategy of [ 11 , 10 ], seven feature descriptors _CITE_ were employed . Using each feature descriptor , nine kernels were generated by varying the width - parameter of the Gaussian kernel . The kernels can be grouped based on the feature descriptor they were generated from and the proposed formulation can be employed to construct classifiers well - suited for object categorization .__label__Method|Tool|Use
Input : · Update u to obtain ujr , r = 1 , · · · | S | using ( 9 ); Output : The final p . s . d . matrix X ∈ RDXD , X = PJj = 1 wjZj . used USPS and MNIST handwritten digits , ORL face recognition datasets , Columbia University Image Library ( COIL20 ) , and UCI machine learning datasets _CITE_ ( datasets 7 - 13 ), Twin Peaks and Helix . The last two are artificial datasets . Experimental results are obtained by averaging over 10 runs ( except USPS - 1 ).__label__Material|Data|Use
On this data set , our method performs competitively with RVM and VRVM and much better than SVM ( specially in terms of sparseness ). To allow the comparisons , we chose , as in [ 20 ]. Table 3 also reports the numbers of errors achieved by the proposed method and by several state - of - the - art techniques on three well - known benchmark problems : the Pima Indians diabetes _CITE_ , the Leptograpsus crabs _CITE_ , and the Wisconsin breast cancer ( WBC ). For the WBC , we report average results over 30 random partitions ( 300 / 269 training / testing , as in [ 26 ]). All the inputs are normalized to zero mean and unit variance , and the kernel width was set to , for the Pima and crabs problems , and to for the WBC .__label__Method|Algorithm|Use
In the initial version of these data sets , each variable had four values , which were binarized using a 1 - of - n encoding . Complex queries To evaluate complex queries , we used voting data from GovTrac . us and Pang and Lee ’ s Movie Review data set . _CITE_ The voting data contains all 1764 votes in the House of Representatives from the 110th Congress . Each bill is an example and the variables are the votes of the 453 congressmen , which can be yes , no , or present . The movie review data contains 1000 positive and 1000 negative movie reviews .__label__Material|Data|Use
Since our label noise is artificial , we can also measure the misclassification rate on the unmodified testing set ( right figure ). This measurement shows a slight loss of accuracy without statistical significance . Finally the cross - training algorithm was applied to real data sets from both the ANU repository _CITE_ and from the the UCI repository . Experimental results were quite disappointing until we realized that the discarding steps tends to produce training sets with very different numbers of examples for each class . To alleviate this problem , after training each SVM , we choose the value of the threshold b * in ( 2 ) which achieves the best validation performance .__label__Material|Data|Use
We implement this by starting from a fully pruned network and greedily adding the splits that most decrease KL divergence . After every 10 splits , we check the number of edges by compiling the candidate network to an AC using the C2D compiler . _CITE_ We stop when the number of edges exceeds our prespecified bound . The second approach we tried is learning a circuit from a set of generated samples . The samples themselves are generated using forward sampling , in which each variable in the BN is sampled in topological order according to its conditional distribution given its parents .__label__Method|Tool|Use
Its prediction result and time spent are then reported by taking the average together with the standard deviation over all runs . For comparison , we employ 11 state - of - the - art online kernel learning methods : perceptron [ 5 ], online gradient descent ( OGD ) [ 6 ], randomized budget perceptron ( RBP ) [ 9 ], forgetron [ 8 ] projectron , projectron ++ [ 20 ], budgeted passive - aggressive simple ( BPAS ) [ 17 ], budgeted SGD using merging strategy ( BSGD - M ) [ 7 ], bounded OGD ( BOGD ) [ 21 ], Fourier OGD ( FOGD ) and Nystrom OGD ( NOGD ) [ 16 ]. Their implementations are published as a part of LIBSVM , BudgetedSVM and LSOKL _CITE_ toolboxes . We use a Windows machine with 3 . 46GHz Xeon processor and 96GB RAM to conduct our experiments . In the first experiment , we investigate the effect of hyperparameters , i . e ., budget size B , merging size k and random feature dimension D ( cf .__label__Method|Tool|Use
Noun - phrase Chunking . The experimental setting for chunking is the same as in [ 19 ]. Following [ 16 ], conditional random fields ( CRF ) are applied to the noun phrase chunking task on the CoNLL2000 dataset _CITE_ . The implemented set of feature templates is a simplified version of [ 16 ] and leads to around 2M active features . Training under full information with a log - likelihood objective yields 0 . 935 F1 .__label__Material|Data|Use
We first constructed artificial data , by generating two classes from two Gaussian clouds in 10 dimensions with means ( 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ) and (− 1 , − 1 , − 1 , − 1 , − 1 , 0 , 0 , 0 , 0 , 0 ) and standard deviation 4 . We trained a linear SVM for differing amounts of training points , selecting C via cross validation . We compare the performance of LIBSVM _CITE_ with CrossTraining using LIBSVM with s = 5 , averaging over 10 splits . The results given in figure 3 show a reduction in SVs and computation time using Cross - Training , with no loss in accuracy . Our second experiment involves the discrimination of digits 3 and 8 in the MNIST database .__label__Method|Tool|Compare
We now present results on two gene expression cancer datasets . The first dataset contains expression values from patients with two different types cancers related to leukemia . The second dataset _CITE_ contains expression levels from patients with and without prostate cancer . The matrix X contains the gene expression values and the vector y is an indictor of the type of cancer a patient has . Although this is a classification problem , we treat it as a recovery problem .__label__Material|Data|Use
Since the entire correlation graph is too large , we build a 3 - layer hierarchy by clustering the learned topics , with their learned correlation strength as the similarity measure . Fig . 4 shows a part of the hierarchy _CITE_ , where the subgraph A represents the top layer with 10 clusters . The subgraphs B and C are two second layer clusters ; and D and E are two correlation subgraphs consisting of leaf nodes ( i . e ., learned topics ). To represent their semantic meanings , we present 4 most frequent words for each topic ; and for each topic cluster , we also show most frequent words by building a hyper - topic that aggregates all the included topics .__label__Method|Algorithm|Introduce
1 ( d )), as well as the coverage of countries / movies ( Fig . 1 ( a - b )). This data set is from ACL Anthology Network _CITE_ . It consists of a paper citation network and a researcher citation network . Here , the nodes are papers or researchers ; and the edges indicate the citation relationship .__label__Material|Data|Use
We compared PSA with plain SGD and SMD [ 1 ] to evaluate PSA ’ s performance for training conditional random fields ( CRF ). We implemented PSA by replacing the L - BFGS optimizer in CRF ++ [ 11 ]. For SMD , we used the implementation available in the public domain _CITE_ . Our SGD implementation for CRF is from Bottou . All the above implementations are revisions of CRF ++.__label__Method|Code|Use
After every Nµ iterations , we set µ := max { µ · gµ , ¯ µ }; i . e ., we simply reduce µ by a constant factor gµ every Nµ iterations until a desired lower bound on µ is achieved . We compare ALM ( i . e ., Algorithm 3 with the above stopping criteria and µ updates ), with the projected subgradient method ( PSM ) proposed by Duchi et al . in [ 13 ] and implemented by Mark Schmidt _CITE_ and the smoothing method ( VSM ) proposed by Lu in [ 17 ], which are considered to be the state - of - the - art algorithms for solving SICS problems . The per - iteration complexity of all three algorithms is roughly the same ; hence a comparison of the number of iterations is meaningful . The parameters used in PSM and VSM are set at their default values .__label__Method|Code|Compare
We ran 1000 simulations , where each simulation is a random split of the dataset . We employed the MILP formulation , and used Gurobi software [ 4 ] in order to find a response , where the running time of the solver was limited to one minute . _CITE_ Our findings are reported in Table 1 . Notice that against both opponent strategies , and even in case where the opponent had seen the test set , the agent still gets more than 50 % of the points . In both scenarios , LAE guarantees the opponent more than LSE .__label__Method|Tool|Use
This dataset has 1 , 899 users and 20 , 296 directed contact edges between users , with timestamps for each node arrival and edge creation event . This longitudinal network spans April to October of 2004 . The METAFILTER data set is from a community weblog where users can share links and discuss Web content _CITE_ . This dataset has 51 , 362 users and 76 , 791 directed contact edges between users . The continuous - time observation spans 8 / 31 / 2007 to 2 / 5 / 2011 .__label__Material|Data|Introduce
See a few examples of images above . We used half of the examples as a training set and the other half as a test set . We calculated HoG features ([ 11 ]) from the images _CITE_ . We then trained , using GECO , a depth - 2 polynomial network on the resulting features . We used 40 neurons in the hidden layer .__label__Material|Data|Use
Other more advanced optimization methods , such as L - BFGS and Newton ’ s method are also applicable . In this section , we demonstrate our algorithm on a set of real - world graphical models from recent UAI inference challenges , including two diagnostic Bayesian networks with 203 and 359 variables and max domain sizes 7 and 6 , respectively , and several MRFs for pedigree analysis with up to 1289 variables , max domain size of 7 and clique size 5 . _CITE_ We construct marginal MAP problems on these models by randomly selecting half of the variables to be max nodes , and the rest as sum nodes . We implement several algorithms that optimize the same primal marginal MAP bound , including our GDD ( Algorithm 1 ), the WMB algorithm in [ 16 ] with ibound = 1 , which uses the same cliques and a fixed point heuristic for optimization , and an off - the - shelf L - BFGS implementation that directly optimizes our decomposed bound . For comparison , we also computed several related primal bounds , including standard mini - bucket [ 2 ] and elimination reordering [ 27 , 38 ], limited to the same computational limits ( ibound = 1 ).__label__Method|Algorithm|Produce
We choose the ‘ best ’ results for GSADMM ( p = 1 ) and RBSUMM ( p = 1 , α = p √ 1 10 ) and sADMM ( p = 1 ). PDMMs perform better than RBSUMM and sADMM . Note the public available code of sADMM _CITE_ does not have dual update , i . e ., Ti = 0 . sADMM should be the same as PDMM3 if Ti = _CITE_3 . Since Ti = 0 , sADMM is the slowest algorithm . Without tuning the parameters of PDMM , GSADMM converges faster than PDMM .__label__Method|Code|Use
SimpleSearch scores each Wikipedia article by the TF - IDF weighted sum of words that co - occur in the articles and a query and returns top - K articles . Second , we use Lucene , a popular open source information retrieval library , in its default configuration on the whole Wikipedia dump . Lastly , we use Google Search API _CITE_ , while restricting the domain to wikipedia . org . Each system is evaluated by document recall at K ( Recall @ K ). We vary K to be 1 , 4 or 40 .__label__Supplement|Website|Use
Here , the cyclic index of A � was determined to be 2 , so N was set to 2 for the kernel observer with feedback . Note that here , even the autonomous kernel observer outperforms PCLSK and LEIS overall , and the kernel observer with feedback N = 2 does so significantly , which is why we did not include results with N > 2 . The second dataset is the Irish wind dataset , consisting of daily average wind speed data collected from year 1961 to 1978 at 12 meteorological stations in the Republic of Ireland _CITE_ . The prediction error is in box - plot form in Figure 5a and as a time - series in Figure 5b . Again , the cyclic index of A � was determined to be 2 .__label__Material|Data|Introduce
We have tested the performance of the proposed method on mixtures of different voice and music signals . The sample rate of the mixtures is 22 . 05kHz . Audio files for all the experiments are accessible at the website _CITE_ . Figure 2 shows experimental results . In experiments 1 and 2 , the mixed signals consist of one voice signal and one music signal .__label__Supplement|Website|Produce
We have tested the performance of the proposed method on mixtures of different voice and music signals . The sample rate of the mixtures is 22 . 05kHz . Audio files for all the experiments are accessible at the website _CITE_ . Figure 2 shows experimental results . In experiments 1 and 2 , the mixed signals consist of one voice signal and one music signal .__label__Supplement|Media|Produce
In addition to the base kernels , we also compare our smoothed kernels with the random walk kernel [ 7 ], the Ramon - G ¨ artner subtree [ 18 ], and p - step random walk kernel [ 24 ]. The Random Walk , p - step Random Walk and Ramon - G ¨ artner are written in Matlab and obtained from [ 22 ]. All other kernels were coded in Python except Pitman - Yor smoothing which is coded in C ++ _CITE_ . We used a parallel implementation for smoothing the counts of Weisfeiler - Lehman kernel for efficiency . All kernels are normalized to have a unit length in the feature space .__label__Method|Code|Produce
Namely , we will show that our methods have benefits over the existing parametric and nonparametric HMM algorithms in terms of speed and accuracy . Synthetic Data . First we compare our nonparametric algorithm with the Beam Sampler for the iHMM _CITE_ . A sequence of length 3000 was generated over a varying number of hidden states with the all - zeros transition matrix except that Ti , i + 1 = 0 . 8 and Ti , i + 2 = 0 . 2 ( when i + 1 & gt ; K , the total number of states , we choose j = i + 1 mod K and let Ti , j = 0 . 8 , and similarly for i + 2 ). Observations were sampled from symmetric Gaussian distributions with means of { 3 , 6 , ... , 3K } and a variance of 0 . 9 .__label__Method|Algorithm|Compare
Baselines In addition to several state - of - the - art published results and ablated variants of our method , we also compare to two baselines : ( 1 ) SIGNATURE RF : random forests trained on classattribute signatures as described in Sec . 3 . 2 . 1 , without an attribute uncertainty model , and ( 2 ) DAP : Direct Attribute Prediction [ 8 , 9 ], which is a leading attribute - based zero - shot object recognition method widely used in the literature [ 8 , 3 , 18 , 30 , 8 , 23 , 19 , 29 ]. _CITE_ Controlled noise experiments Our approach is designed to overcome the unreliability of attribute classifiers . To glean insight into how it works , we first test it with controlled noise in the test images ’ attribute predictions . We start with hypothetical perfect attribute classifier scores ˆam ( x ) = Ak ( m ) for x in class k , then progressively add noise to represent increasing errors in the predictions .__label__Method|Algorithm|Compare
1 ( d ) indicates , a similar pattern emerges . News Recommendation For this experiment , we use the Yahoo ! Webscope dataset R6A _CITE_ . The dataset provides a list of records , each containing a time stamp , a user ID , a news article ID and a Boolean value that indicates whether the user clicked on the news article that was presented to her . The feature vectors of the users and the articles are also provided .__label__Material|Data|Use
For English articles , we removed 418 types of standard stop words [ 12 ]. For Spanish articles , we removed 351 types of standard stop words [ 13 ]. As for Japanese articles , we removed function words , such as symbols , conjunctions and particles , using part - of - speech tags annotated by MeCab _CITE_ . The statistics of the datasets after preprocessing are shown in Tables 1 and 2 . We assumed each set of Wikipedia articles connected via inter - language links between two ( or translation for each Japanese word follows in parentheses , except for Japanese proper nouns .__label__Method|Tool|Use
Note that the recovery error ranges from 0 to 1 . Real data . We use the last . fm 1K dataset _CITE_ , which contains the list of songs listened by heavy users of Last . Fm . We use the top 25 artist genres as the states of the Markov chain . We consider the ten heaviest users in the data set , and for each user , consider the first 3001 state transitions that change their state .__label__Material|Data|Use
We randomly select 100 images from each category . This image collection has been previously used to analyze an image segmentation method based on spatially dependent Pitman - Yor ( PY ) processes [ 10 ], and we compare both methods using an identical feature set . Each image is first divided into approximately 1000 superpixels [ 15 , 20 ] _CITE_ using the normalized cut algorithm [ 9 ]. We describe the texture of each superpixel via a local texton histogram [ 21 ], using band - pass filter responses quantized to 128 bins . A 120 - bin HSV color histogram is also computed .__label__Material|Data|Use
[ 5 ], which assigns the regression weights w with a Gaussian prior p0 ( w | α ) = Af ( w , α − ) and p0 ( α ) = Gamma ( α , 1 , 0 . 01 ). The inference is applied on posterior p ( x | D ) with x = [ w , log α ]. We compared our algorithm with the no - U - turn sampler ( NUTS ) _CITE_ [ 29 ] and non - parametric variational inference ( NPV ) [ 5 ] on the 8 datasets ( N > 500 ) used in Gershman et al . [ 5 ], and find they tend to give very similar results on these ( relatively simple ) datasets ; see Appendix for more details . We further test the binary Covertype dataset with 581 , 012 data points and 54 features .__label__Method|Algorithm|Compare
The dataset ‘ Boards ’ contains information about members of the boards of Norwegian companies sitting at the same board in August 2011 . ‘ Forum ’ is a forum network about web users contributing to the same forums . ‘ Books ’ concerns data collected from the Book - Crossing community about users providing ratings on books _CITE_ where we extracted the bipartite network from the ratings . ‘ Citations ’ is the co - authorship network based on preprints posted to Condensed Matter section of ArXiv between 1995 and 1999 [ 15 ]. ‘ Movielens100k ’ contains information about users rating particular movies from which we extracted the bipartite network .__label__Material|Data|Introduce
Finally , as an ideal yardstick , we also implement a full online SVM algorithm (“ Online - SVM ”) ( Shalev - Shwartz & Singer , 2006 ), which updates all the support vectors in each trial , and is thus computationally extremely intensive as will be revealed in our study . To extensively examine the performance , we test all the algorithms on a number of benchmark datasets from web machine learning repositories . All of the datasets can be downloaded from LIBSVM website _CITE_ , UCI machine learning repository 2 and MIT CBCL face datasets 3 . Due to space limitation , we randomly choose six of them in our discussions , including “ german ”, “ splice ”, “ spambase ”, “ MITFace ”, “ a7a ”, and “ w7a ”. To make a fair comparison , all algorithms adopt the same experimental setup .__label__Material|Data|Use
Finally , as an ideal yardstick , we also implement a full online SVM algorithm (“ Online - SVM ”) ( Shalev - Shwartz & Singer , 2006 ), which updates all the support vectors in each trial , and is thus computationally extremely intensive as will be revealed in our study . To extensively examine the performance , we test all the algorithms on a number of benchmark datasets from web machine learning repositories . All of the datasets can be downloaded from LIBSVM website _CITE_ , UCI machine learning repository 2 and MIT CBCL face datasets 3 . Due to space limitation , we randomly choose six of them in our discussions , including “ german ”, “ splice ”, “ spambase ”, “ MITFace ”, “ a7a ”, and “ w7a ”. To make a fair comparison , all algorithms adopt the same experimental setup .__label__Supplement|Website|Use
We present a posterior inference algorithm based on Gibbs sampling , and establish posterior consistency of our regression model . Our method is evaluated with extensive experiments on simulated data and demonstrated to be able to identify meaningful interactions in applications in genetics and retail demand forecasting . _CITE_ A fundamental challenge in supervised learning , particularly in regression , is the need for learning functions which produce accurate prediction of the response , while retaining the explanatory power for the role of the predictor variables in the model . The standard linear regression method is favored for the latter requirement , but it fails the former when there are complex interactions among the predictor variables in determining the response . The challenge becomes even more pronounced in a high - dimensional setting – there are exponentially many potential interactions among the predictors , for which it is simply not computationally feasible to resort to standard variable selection techniques ( cf .__label__Method|Algorithm|Produce
In this prior - free framework , we obtain competitive shape and illumination estimation results under a variety of models and lighting conditions , requiring fewer assumptions than competing methods . The generic viewpoint assumption ( GVA ) [ 5 , 9 , 21 , 22 ] postulates that what we see in the world is not seen from a special viewpoint , or lighting condition . Figure 1 demonstrates this idea with the famous Necker cube example _CITE_ . A three dimensional cube may be observed with two vertices or edges perfectly aligned , giving rise to a two dimensional interpretation . Another possibility is a view that exposes only one of the faces of the cube , giving rise to a square .__label__Method|Algorithm|Use
We refer to ” Ncuts ” as the original normalization D − / KD − / , by ” RE ” to the iterative application of the original normalization ( which is proven to converge to a doubly stochastic matrix [ 11 ]), by ” L1 ” to the L1 doubly - stochastic normalization ( which we have shown is equivalent to Ratio - cuts ) and by ” Frobenius ” to the iterative Frobenius scheme based on Von - Neumann ’ s lemma described in Section 4 . We also included a ” None ” field which corresponds to no normalization being applied . We begin with evaluating the clustering quality obtained under the different normalization methods taken over a number of well studied datasets from the UCI repository _CITE_ . The data - sets are listed in Table 1 together with some of their characteristics . The best performance ( lowest error rate ) IIxi − xj 11 is presented in Boldface .__label__Material|Data|Use
Algorithm 1 Incremental LGR We evaluate our LGR on inverse dynamics learning tasks , using data from two robotic platforms : a SARCOS anthropomorphic arm and a KUKA lightweight arm . For both robots , learning the inverse dynamics involves learning a map from the joint positions q ( rad ), velocities q ˙ ( rad / 3 ) and accelerations q ¨ ( rad / 3 ), to torques T ( Nm ) for each of 7 joints ( degrees of freedom ). We compare to two methods previously used for inverse dynamics learning : LWPR – an extension of LWR for high dimensional spaces [ 31 ] – and I - SSGPR _CITE_ [ 13 ] – an incremental version of Sparse Spectrum GPR . I - SSGPR differs from LGR and LWPR in that it is a global method and does not learn the distance metric online . Instead , I - SSGPR needs offline training of hyperparameters before it can be used online .__label__Method|Algorithm|Compare
text categorization [ 26 ]. In this subsection , we foll ow Gong et al . [ 19 ] to consider Sparse LR with a We compare monotone APG ( mAPG ) and nonmonotone APG ( nmAPG ) with monotone GIST _CITE_ ( mGIST ), nonmonotone GIST ( nmGIST ) [ 19 ] and IFB [ 22 ]. We test the performance on the real - sim data set , which contains 72309 samples of 20958 dimensions . We follow [ 19 ] to set λ = 0 . 0001 , θ = 0 . 1λ and the starting point as zero vectors .__label__Method|Algorithm|Compare
The running time of ADMM increase rapidly with the data size while the greedy algorithm stays steady , which confirms the speedup advantage of the greedy algorithm . We conduct cokriging and forecasting experiments on four real - world datasets : USHCN The U . S . Historical Climatology Network Monthly ( USHCN ) _CITE_ dataset consists of monthly climatological data of 108 stations spanning from year 1915 to 2000 . It has three climate variables : ( 1 ) daily maximum , ( 2 ) minimum temperature averaged over month , and ( 3 ) total monthly precipitation . CCDS The Comprehensive Climate Dataset ( CCDS ) is a collection of climate records of North America from [ 18 ].__label__Material|Data|Use
See Appendix C . 5 for a discussion on the orders . Algorithm 1 Joint diagonalization ( JD ) algorithm for GP / DICA cumulants ( or LDA moments ) In this section , ( a ) we compare experimentally the GP / DICA cumulants with the LDA moments and ( b ) the spectral algorithm [ 3 ], the tensor power method [ 4 ] ( TPM ), the joint diagonalization ( JD ) algorithm from Algorithm 1 , and variational inference for LDA [ 1 ]. Real data : the associated press ( AP ) dataset , from D . Blei ’ s web page , with N = 2 , 243 documents and M = 10 , 473 vocabulary words and the average document length Lb = 194 ; the NIPS papers dataset _CITE_ [ 28 ] of 2 , 483 NIPS papers and 14 , 036 words , and Lb = 1 , 321 ; the KOS dataset , from the UCI Repository , with 3 , 430 documents and 6 , 906 words , and Lb = 136 . Semi - synthetic data are constructed by analogy with [ 29 ]: ( 1 ) the LDA parameters D and c are learned from the real datasets with variational inference and ( 2 ) toy data are sampled from a model of interest with the given parameters D and c . This provides the ground truth parameters D and c . For each setting , data are sampled 5 times and the results are averaged . We plot error bars that are the minimum and maximum values .__label__Material|Data|Use
Note that while the BNBP sampler in ( 12 ) is fully collapsed , the direct assignment sampler of the HDP - LDA in ( 14 ) is only partially collapsed as neither the globally shared Dirichlet process Ge nor the concentration parameter α are marginalized out . To derive a collapsed sampler for the HDP - LDA that marginalizes out Ge ( but still not α ), one has to use the Chinese restaurant franchise [ 6 ], which has cumbersome book - keeping as each word is indirectly linked to its topic via a latent table index . We consider the JACM , PsyReview _CITE_ , and NIPS12 corpora , restricting the vocabulary to terms that occur in five or more documents . The JACM corpus includes 536 documents , with V = 1539 unique terms and 68 , 055 total word counts . The PsyReview corpus includes 1281 documents , with V = 2566 and 71 , 279 total word counts .__label__Material|Data|Use
Therefore , we had only one parameter , binwidth size , chosen from the set { 0 . 001 , 0 . 0001 , 0 . 00001 }. This results in F feature vector dimension in range 100 − 1000 , 000 with feature matrix sparsity & gt ; 90 % in all cases . Our FGSD code is available at github _CITE_ . Datasets : We employed wide variety of datasets considered as benchmark [ 1 , 34 , 21 , 26 ] in graph classification task to evaluate the quality of produce FGSD graph features . We adopted 7 bioinformatics datasets : Mutag , PTC , Proteins , NCI1 , NCI109 , D & D , MAO and 5 social network datasets : Collab , REDDIT - Binary , REDDIT - Multi - 5K , IMDB - Binary , IMDB - Multi .__label__Method|Code|Produce
The algorithms under comparison are : ( i ) CCIT - Algorithm 3 in our paper where we use XGBoost [ 6 ] as the classifier . In our experiments , for each data - set we boot - strap the samples and run our algorithm B times . The results are averaged over B bootstrap runs _CITE_ . ( ii ) KCIT - Kernel CI test from [ 32 ]. We use the Matlab code available online .__label__Method|Algorithm|Produce
Section 5 presents experiments on KTH and KITTI datasets with comparison to related attention - based trackers . Section 6 discusses the results and intriguing properties of our framework and Section 7 concludes the work . Code and results are available online _CITE_ . A number of recent studies have demonstrated that visual content can be captured through a sequence of spatial glimpses or foveation [ 22 , 12 ]. Such a paradigm has the intriguing property that the computational complexity is proportional to the number of steps as opposed to the image size .__label__Method|Code|Produce
Section 5 presents experiments on KTH and KITTI datasets with comparison to related attention - based trackers . Section 6 discusses the results and intriguing properties of our framework and Section 7 concludes the work . Code and results are available online _CITE_ . A number of recent studies have demonstrated that visual content can be captured through a sequence of spatial glimpses or foveation [ 22 , 12 ]. Such a paradigm has the intriguing property that the computational complexity is proportional to the number of steps as opposed to the image size .__label__Supplement|Document|Produce
Unless otherwise specified , all model parameters are chosen via 5 - fold cross validation . We use three datasets for our experiments . Details are given below : Landmine Detection _CITE_ consists of 19 tasks collected from different landmine fields . Each task is a binary classification problem : landmines (+) or clutter (−) and each example consists of 9 features extracted from radar images with four moment - based features , three correlation - based features , one energy ratio feature and a spatial variance feature . Landmine data is collected from two different terrains : tasks 1 - 10 are from highly foliated regions and tasks 11 - 19 are from desert regions , therefore tasks naturally form two clusters .__label__Material|Data|Use
Let { wk } be the iterates generated by Algorithm 1 , with αk = α ∈ ( 0 , µ1 This result bounds the average norm of the gradient of F after the first L − 1 iterations , and shows that the iterates spend increasingly more time in regions where the objective function has a small gradient . In this Section , we present numerical results that evaluate the proposed robust multi - batch L - BFGS scheme ( Algorithm 1 ) on logistic regression problems . Figure 2 shows the performance on the webspam dataset _CITE_ , where we compare it against three methods : ( i ) multi - batch L - BFGS without enforcing sample consistency ( L - BFGS ), where gradient differences are computed using different samples , i . e ., yk = gSk + 1 k ; ( ii ) multi - batch gradient descent ( Gradient Descent ), which is obtained by setting Hk = I in Algorithm 1 ; and , ( iii ) serial SGD , where at every iteration one sample is used to compute the gradient . We run each method with 10 different random seeds , and , where applicable , report results for different batch ( r ) and overlap ( o ) sizes . The proposed method is more stable than the standard L - BFGS method ; this is especially noticeable when r is small .__label__Material|Data|Use
In this section , we evaluate how well neurally - guided procedural models capture image - based constraints . We implemented our prototype system in the WebPPL probabilistic programming language [ 4 ] using the adnn neural network library . _CITE_ All timing data was collected on an Intel Core i7 - 3840QM machine with 16GB RAM running OSX 10 . 10 . 5 . In experiments which require target images , we use the following image collections : We augment the dataset with a horizontally - mirrored copy of each image , and we annotate each image with a starting point and direction from which to initialize the execution of a procedural model . Figure 3 shows some representative images from each collection .__label__Method|Code|Use
We used the standard SURF features [ 21 ] with 2000 visual words as the original domain and the recently proposed DeCAF features [ 25 ] extracted from the activation of a deep convolutional network trained in a fully supervised fashion as the privileged domain . The DeCAF features have 4096 dimensions . All features are provided with the AwA dataset _CITE_ . We again performed PCA for dimensionality reduction in the original and privileged domains and only kept the top 50 principal components , as well as standardised the data . Attributes as privileged information : Following the experimental setting of [ 6 ], we also used images as the original domain and attributes as the privileged domain .__label__Material|Data|Produce
Despite the preprocessing , ASSUMPTION - FREE K - MC clearly outperforms K - MC on the data sets with large values for a ( X ) ( CSN , KDD and SONG ). The additional effort of computing the nonuniform proposal is compensated by a substantially lower expected quantization error for a given chain size . For the other data sets , ASSUMPTION - FREE K - MC _CITE_ is initially disadvantaged by the cost of computing the proposal distribution . However , as m increases and more time is spent computing the Markov chains , it either outperforms K - MC ( RNA and SUSY ) or matches its performance ( WEB ). Table 3 details the practical significance of the proposed algorithm .__label__Material|Data|Introduce
We randomly generated inputs as Aij ∼ U [− 1 , 1 ], di ∼ U [ 0 , 1 ], and bi ∼ U [− 1 , 1 ] for i , j ∈ [ n ], where U [ a , b ] denotes the uniform distribution with the support [ a , b ]. After that , we solved ( 1 ) by using Algorithm 1 and compared it with the exact solution obtained by QP . _CITE_ The result ( Figure 1 ) show the approximation errors were evenly controlled regardless of n , which meets the error analysis ( Theorem 4 . 2 ). Application to kernel methods Next , we considered the kernel approximation of the Pearson divergence [ 21 ]. The problem is defined as follows .__label__Method|Algorithm|Compare
It is well established that many web - related objects such as web directories and RSS directories admit a ( hierarchical ) categorization , and web directories aim to do this in a semi - automated fashion . For instance , it is desirable , when building a categorizer for the Yahoo ! directory _CITE_ , to take into account other web directories such as DMOZ . Although the tasks are clearly related , their label sets are not identical . For instance , some section heading and sub - headings may be named differently in the two directories .__label__Material|Data|Extent
We investigate the performance of various inference algorithms on five real - world datasets . The NASCAR [ 18 ] and sushi [ 24 ] datasets contain multiway partial rankings . The YouTube , GIFGIF and chess datasets _CITE_ contain pairwise comparisons . Among those , the chess dataset is particular in that it features 45 % of ties ; in this case we use the extension of the Bradley – Terry model proposed by Rao and Kupper [ 23 ]. We preprocess each dataset by discarding items that are not part of the largest strongly connected component in the comparison graph .__label__Material|Data|Use
The estimator is also non - parametric ˜ MNIST digits . We trained a DAE on the binarized MNIST data ( thresholding at 0 . 5 ). A Theano _CITE_ ( Bergstra et al ., 2010 ) implementation is available . The 784 - 2000 - 784 auto - encoder is trained for 200 epochs with the 50000 training examples and salt - and - pepper noise ( probability 0 . 5 of corrupting each bit , setting it to 1 or 0 with probability 0 . 5 ). It has 2000 tanh hidden units and is trained by minimizing cross - entropy loss , i . e ., maximum likelihood on a factorized Bernoulli reconstruction distribution .__label__Method|Code|Produce
( 6 ). The models were implemented using the Theano [ 20 ], Lasagne [ 5 ] and Parmesan frameworks . The source code is available at github _CITE_ For MNIST , we used a sigmoid output layer to predict the mean of a Bernoulli observation model and leaky rectifiers ( max ( x , 0 . 1x )) as nonlinearities in the MLP ’ s . The models were trained for 2000 epochs with a learning rate of 0 . 001 on the complete training set . Models using warm - up used Nt = 200 .__label__Method|Code|Produce
The first type of baseline we considered is based on machine translation . We used a machine translation tool on the Japanese query , and then applied TFIDF or LSI . We considered three methods of machine translation : Google ’ s API or Fujitsu ’ s ATLAS _CITE_ was used to translate each query document , or we translated each word in the Japanese dictionary using ATLAS and then applied this word - based translation to a query . We also compared to CL - LSI [ 9 ] trained on all 90 , 000 Jap - Eng pairs from the training set . For PSI , we considered two cases : ( i ) apply the ATLAS machine translation tool first , and then use PSI trained on the task in Section 4 . 1 , e . g .__label__Method|Tool|Use
Specifically , we have re - labeled subsets of the Caltech Motorcycle and Faces database , to obtain the subordinates of sport and cross motorcycles , and male and female faces . For these data sets we have increased the weight of the location model , as mentioned in section 2 . 2 . We took the subordinate classes of grand piano and electric guitar from the Caltech 101 dataset _CITE_ and supplemented them with classes of upright piano and classical guitar collected using google images . Finally , we used subsets of the chairs and furniture background used in [ 2 ] to define classes of dining and living room chairs , dining and coffee tables . Example images from the data sets can be seen in Fig .__label__Material|Data|Use
On the balance of bias and variance , MLE is clearly the best choice . Finally , we compare the estimators on three popular manifold datasets ( Table 1 �: the Swiss roll , and two image datasets shown on Fig . 3 : the Isomap face database , and the hand rotation sequence _CITE_ used in [ 14 ]. For the Swiss roll , the MLE again provides the best combination of bias and variance . The face database consists of images of an artificial face under three changing conditions : illumination , and vertical and horizontal orientation .__label__Material|Data|Use
We have generated the indefinite similarity matrices as prescribed in [ 16 ] for each of the following data sets : Sonar , Liver disorder , Ionosphere , Diabetes and Heart . We have used the same similarity matrices as in [ 6 ]: for the data sets Amazon , AuralSonar , Yeast - SW - 5 - 7 and Yeast - SW - 5 - 12 . To test the proposed multiple similarity based formulations we experimented on a subset of the SCOP database [ 9 ] taken from Protein Classification Benchmark Collection _CITE_ . Considering proteins having & lt ; 40 % sequence identity , we randomly select 8 super - families which have at least 45 proteins . We compute 3 different pairwise similarity measure for proteins : Psi - BLAST [ 1 ], Smith - Waterman [ 14 ] and Needleman - Wunsch [ 11 ].__label__Material|Data|Use
For all our experiments we use a GPU - cluster interconnected with InfiniBand . Each node has 4 Titan GPU processors where each local worker corresponds to one GPU processor . The center variable of the master is stored and updated on the centralized parameter server [ 2 ] _CITE_ . To describe the architecture of the convolutional neural network , we will first introduce a notation . Let ( c , y ) denotes the size of the input image to each layer , where c is the number of color channels and y is both the horizontal and the vertical dimension of the input .__label__Supplement|Website|Use
We acquired public map data from http :// www . openstreetmap . org /, i . e ., the undirected graph representing the streets ( edges ) and intersections ( nodes ) of San Francisco . We projected the GPS data onto the San Francisco graph using the map - matching API of the graphhopper package . _CITE_ For more on map - matching , see , e . g ., [ Froehlich and Krumm , 2008 ]. To partition the graph of San Francisco into regions , we obtained a publicly available dataset of traffic analysis zones , produced by the California Metropolitan Transportation Commission . 10 These zones correspond to small area neighborhoods and communities of the San Francisco Bay Area . To facilitate the compilation of regions into SDDs , we further split these zones in half until each region was compilable ( horizontally if the region was taller than it was wide , or vertically otherwise ).__label__Method|Code|Use
In case of MixNorm - MKL , the MD based algorithm ( section 3 ) was used to solve the formulation . The SVM problem arising at each step of mirror - descent is solved using the libsvm software . L1 - MKL is solved using simpleMKL _CITE_ . L2 - MKL is solved using libsvm and serves as a baseline for comparison . In all cases , the hyper - parameters of the various formulations were tuned using suitable cross - validation procedures and the accuracies reported denote testset accuracies achieved by the respective classifiers using the tuned set of hyper - parameters .__label__Method|Tool|Use
A collaborative filtering dataset can be interpreted as the incomplete observation of a ratings matrix with columns corresponding to users and rows corresponding to items . The goal is to infer the unobserved entries of this ratings matrix . We evaluate DFC on two of the largest publicly available collaborative filtering datasets : MovieLens 10M ( m = 4K , n = 6K , s > 10M ) and the Netflix Prize dataset _CITE_ ( m = 18K , n = 480K , s > 100M ). To generate test sets drawn from the training distribution , for each dataset , we aggregated all available rating data into a single training set and withheld test entries uniformly at random , while ensuring that at least one training observation remained in each row and column . The algorithms were then run on the remaining training portions and evaluated on the test portions of each split .__label__Material|Data|Use
Specifically , we tested two settings in our experiments : For each of the settings we trained our dual - NMT algorithm for one week . We set the beam search size to be 2 in the middle translation process . All the hyperparameters in the experiments were set by cross validation . We used the BLEU score [ 8 ] as the evaluation metric , which are computed by the multi - bleu . perl script _CITE_ . Following the common practice , during testing we used beam search [ 12 ] with beam size of 12 for all the algorithms as in many previous works . We report the experimental results in this section .__label__Method|Algorithm|Use
The second dataset is Boston Housing with n = 506 , d = 10 and p = 1 . We add 10 irrelevant variables randomly drawn from Uniform ( 0 , 1 ) to evaluate the variable selection performance . The third one , Space ga _CITE_ , is an election data with spatial coordinates on 3107 US counties . Our task is to predict the x , y coordinates of each county given 5 variables regarding voting information . For Space ga , we normalize the responses to [ 0 , 1 ].__label__Material|Data|Use
Higher is better ; again we see that the learned models are quite accurate , and generalize well . Second set . We also tested on a two more complicated POMDPs called Cheesemaze and Maze 4x3 _CITE_ . For both problems , exact inference is intractable , and so we used approximate inference . We experimented with loopy belief propagation ( LBP ) [ 12 ], naive mean field ( or variational mean field , VMF ), and log - determinant relaxations ( LDR ) [ 10 ].__label__Material|Data|Use
However , when outliers start to appear , the result of least squares is significantly skewed , while the results of classic robust statistics methods , Huber , L1 and LTS , indeed turn out to be more robust than the least squares , but nevertheless are still affected significantly . Both implementations of the new method performs comparably to the the non - convex Geman - McClure loss while substantially improving the alternating strategy under the L1 loss . Note that the latter improvement clearly demonstrates that Next , we conducted an experiment on four real datasets taken from the StatLib repository _CITE_ and DELVE . 10 For each data set , we randomly selected 108 points as the training set , and another random 1000 points as the test set . Here the regularization constant is tuned by 10 - fold cross validation . To seed outliers , 5 % of the training set are randomly chosen and their X and y values are multiplied by 100 and 10000 , respectively .__label__Material|Data|Use
In particular , the methods we consider are BUCB [ 9 ], B - EST , UCB - PE / UCB - DPP - MAX [ 8 ], EST - PE / EST - DPP - MAX , UCB - DPP - SAMPLE , EST - DPP - SAMPLE and UCB with local penalization ( LP - UCB ) [ 11 ]. We used the publicly available code for BUCB and PE . The code was modified to include the code for the EST counterparts using code for EST _CITE_ . For LP - UCB , we use the publicly available GPyOpt codebase 3 and implemented the MCMC algorithm by [ 1 ] for k - DPP sampling with e = 0 . 01 as the variation distance error . We were unable to compare against PPES as the code was not publicly available .__label__Method|Code|Use
Each categorical feature is converted to as many binary features as its cardinality . The dataset contains 32 , 561 training and 16 , 281 test instances each with 123 features . _CITE_ In Figure 2 , we compare the test error of perturbed aggregate classifiers trained over data from five parties for different values of c . We consider three situations : all parties with equal datasets containing 6512 instances ( even split , n ( 1 ) = 20 % of n ), parties with datasets containing 4884 , 6512 , 6512 , 6512 , 8141 instances ( n ( 1 ) = 15 % of n ), and parties with datasets containing 3256 , 6512 , 6512 , 6512 , 9769 instances ( n ( 1 ) = 10 % of n ). We also compare with the error of the classifier trained using combined training data and its perturbed version satisfying differential privacy . We chose the value of the regularization parameter A = 1 and the results displayed are averaged over 200 executions .__label__Material|Data|Introduce
These two steps are repeated until L ( θ , ϕ ) converges . For the Multimodal RTM learning , we just additionally calculate the gradients of θmodl for each modality l in the M - step while the updating rules are not changed . After the learning is completed , according to Equation 6 the prediction for new videos can be easily obtained : We test our models on the Unstructured Social Activity Attribute ( USAA ) dataset _CITE_ for social group activity recognition . Firstly , we present quantitative evaluations of RTM in the case of different modalities and comparisons with other supervised topic models ( namely MedLDA and gClassRBM ). Secondly , we compare Multimodal RTM with some baselines in the case of plentiful and sparse training data respectively .__label__Material|Data|Use
For CDNA and STP , we used 10 transformers . While we show stills from the predicted videos in the figures , the qualitative results are easiest to compare when the predicted videos can be viewed side - by - side . For this reason , we encourage the reader to examine the video results on the supplemental website _CITE_ . Code for training the model is also available on the website . Training details : We trained all models using the TensorFlow library [ 1 ], optimizing to convergence using ADAM [ 13 ] with the suggested hyperparameters .__label__Supplement|Website|Produce
For CDNA and STP , we used 10 transformers . While we show stills from the predicted videos in the figures , the qualitative results are easiest to compare when the predicted videos can be viewed side - by - side . For this reason , we encourage the reader to examine the video results on the supplemental website _CITE_ . Code for training the model is also available on the website . Training details : We trained all models using the TensorFlow library [ 1 ], optimizing to convergence using ADAM [ 13 ] with the suggested hyperparameters .__label__Supplement|Media|Produce
We then built a histogram for each image , counting the number of patches that fall in each cell . Text data . _CITE_ The second data set was collected by Tsoumakas et al . [ 11 ] from del . icio . us , a social bookmarking service in which users assign descriptive textual tags to web pages . The set contains about 16000 labeled web page and 983 unique labels .__label__Material|Data|Use
and DMOZ web directory . We detail those experiments in turn in the following sections . Datasets MNIST data set _CITE_ consists of 28 × 28 - size images of hand - written digits from 0 through 9 . We use a small sample of the available training set to simulate the situation when we only have limited number of labeled examples and test the performance on the entire available test set . In this experiment , we look at a binary n - task ( n ∈ { 3 , 5 , 7 , 10 }) problem .__label__Material|Data|Use
However , the specific functional form in ( 14 ) allows O ( 1 ) time per function evaluation as fi ( x ; w ) = b ( xi ) IIk ( exp ( wk j # i Sk ( xi , xj ))), where the inner term E j # i Sk ( xi , xj ) in the RHS does not depend on w and can be precomputed . Thus after the precomputation step , each function evaluation is as efficient as that for a pointwise ranking function . As the base pointwise rankers b , we use those provided by RankLib _CITE_ : MART , RankNet , RankBoost , AdaRank , Coordinate Ascent ( CA ), LambdaMART , ListNet , Random Forests , Linear regression . We refer the reader to the RankLib website for details on these . Results We use the LETOR 3 . 0 collection [ 23 ], which contains the OHSUMED dataset and the Gov collection : HP2003 / 04 , TD2003 / 04 , NP2003 / 04 , which respectively correspond to the listwise Homepage Finding , Topic Distillation and Named Page Finding tasks .__label__Method|Code|Use
On MNIST , we used a non - homogeneous polynomial kernel of degree four , which gave us our best results . ( See also [ 9 ].) Small data sets with few classes The wine , iris , and balance data sets are small data sets , with less than 500 training examples and just three classes , taken from the UCI Machine Learning Repository _CITE_ . On data sets of this size , a distance metric can be learned in a matter of seconds . The results in Fig .__label__Material|Data|Use
From the validation curves in Figure 1 ( b ), we see that both MI - RNN , simple and MI - RNN - general yield much better performance compared to vanilla - RNN , and MI - RNN - general has a faster convergence speed compared to MI - RNN - simple . We also compared our results to the previously published models in Table 1 , bottom left panel , where MI - RNN - general achieves a test BPC of 1 . 39 , which is to our knowledge the best result for RNNs on this task without complex gating / cell mechanisms . In addition to the Penn - Treebank dataset , we also perform character level language modeling on two larger datasets : text8 _CITE_ and Hutter Challenge Wikipedia . Both of them contain 100M characters from Wikipedia while text8 has an alphabet size of 27 and Hutter Challenge Wikipedia has an alphabet size of 205 . For both datasets , we follow the training protocols in [ 12 ] and [ 1 ] respectively .__label__Material|Data|Use
is diagonal , the true likelihood of y given g factorizes over each datapoint : P ( y | g ) = Hi = 1 P ( yi | gi ), and standard EP algorithms for Gaussian process classification can be used [ 8 ] ( with the variance given by EE . instead of EE , and kernel matrix R instead of K ). The final algorithm defines a whole new class of relational models , depends on a single hyperparameter p which can be optimized by grid search in [ 0 , 1 ], and requires virtually no modification of code written for EP - based Gaussian process classifiers _CITE_ . We now compare three different methods in relational classification tasks . We will compare a standard Gaussian process classifier ( GPC ), the relational Gaussian process ( RGP ) of [ 2 ] and our method , the mixed graph Gaussian process ( XGP ).__label__Method|Algorithm|Introduce
Neither of worked well , the former being to be too sensitive to the value of e which is in agreement with the observations made by [ 11 ] and the latter constraining the model by using a single a and b . We do not discuss this any further due to lack of space . Throughout our experiements , we used 4 popular benchmark datasets ( Table 1 ) with the recommended train - test splits - CLEF [ 8 ], NEWS20 , LSHTC -{ small , large } _CITE_ , IPC . First , to evaluate the speed advantage of the variational inference , we compare the full variational { M1 , M2 , M3 }- var and partial MAP { M1 , M2 , M3 - map } inference 5 for the three variants of HBLR to the MCMC sampling based inference of CorrMNL [ 18 ]. For CorrMNL , we used the implementation as provided by the authors .__label__Material|Data|Use
The Amazon dataset [ 16 ] originally consisted of 595 products with daily snapshots of writing / voting trajectories from Oct 2012 to Mar 2013 . After eliminating duplicate products and products with fewer than five reviews or fragmented trajectories , 363 products are left . For the StackExchange dataset _CITE_ , we filter out questions from each community with fewer than five answers besides the answer chosen by the question owner . We drop communities with fewer than 100 questions after pre - processing . Many of these are “ Meta ” forums where users discuss policies and logistics for their original forums .__label__Material|Data|Use
This is followed by a search in [ 0 , tr ], by interval nesting or by interpolation of the collected function and gradient values , e . g . with cubic splines . _CITE_ As the line search is only an auxiliary step within a larger iteration , it need not find an exact root of f ; it suffices to find a point ‘ sufficiently ’ close to a minimum . The Wolfe [ 21 ] conditions are a widely accepted formalization of this notion ; they consider t acceptable if it fulfills and interpolation ( ➄ , ➏ ), but receives unreliable , noisy function and gradient values . These are used to construct a GP posterior ( top .__label__Method|Algorithm|Use
If T is weighted , the max model for T n CSBP ( Z ) is a max model for T , and it has the same weight in both theories . We pitted our term and term equivalent SBP formulations against other inference systems to demonstrate their efficacy . For each domain we tested on , we compared the following algorithms : ( 1 ) Vanilla : running an exact MaxSAT solver on the grounded instance ( 2 ) Shatter : running an exact MaxSAT solver on the grounded instance with SBPs added by the Shatter tool [ 1 ], ( 3 ) Term : running an exact MaxSAT solver on the grounded instance with SBPs added by our term symmetry detection algorithm , ( 4 ) Tequiv : running an exact MaxSAT solver on the grounded instance with SBPs added by our term equivalent symmetry detection algorithm ( 5 ) RockIt : running RockIt [ 26 ] on the MLN , ( 6 ) PTP : running the PTP algorithm [ 14 ] for marginal inference 1 on the MLN and ( 7 ) MWS : running the approximate MaxWalkSAT algorithm used by Alchemy - 2 _CITE_ on the grounded instance . These algorithms were chosen so that our methods would be compared against a variety of other techniques : ground techniques including regular MaxSAT ( Vanilla ), propositional symmetrybreaking ( Shatter ), local search ( MWS ), and lifted techniques including cutting planes ( RockIt ) and lifted rules ( PTP ). It should be noted that all the algorithms except MWS and RockIt are exact .__label__Method|Tool|Use
Social media and social networking sites are increasingly used by people to express their opinions , give their “ hot takes ”, on the latest breaking news , political issues , sports events , and new products . As a consequence , there has been an increasing interest on leveraging social media and social networking sites to sense and forecast opinions , as well as understand opinion dynamics . For example , political parties routinely use social media to sense people ’ s opinion about their political discourse ; quantitative investment firms measure investor sentiment and trade using social media [ 18 ]; and , corporations leverage brand sentiment , estimated from users ’ posts , likes and shares in social media and social networking sites , to design their marketing campaigns _CITE_ . In this context , multiple methods for sensing opinions , typically based on sentiment analysis [ 21 ], have been proposed in recent years . However , methods for accurately forecasting opinions are still scarce [ 7 , 8 , 19 ], despite the extensive literature on theoretical models of opinion dynamics [ 6 , 9 ].__label__Supplement|Document|Introduce
The NELL - 1 and NELL - 2 datasets are from [ 8 ] and consists of ( noun phrase 1 , context , noun phrase 2 ) triples from the “ Read the Web ” project [ 6 ]. NELL - 2 is a version of NELL - 1 , which is obtained by removing entries whose values are below a threshold . The Yelp Phoenix dataset is from the Yelp Data Challenge _CITE_ , while Cellartracker , Ratebeer , Beeradvocate and Amazon . com are from the Stanford Network Analysis Project ( SNAP ) home page . All these datasets consist of product or business reviews . We converted them into a users x items x words tensor by first splitting the text into words , removing stop words , using Porter stemming [ 12 ], and then removing user - item pairs which did not have any words associated with them .__label__Material|Data|Use
We now show the benefits of our approach on large - scale datasets , since we exploit the efficiency of random features with the performance of kernel - learning techniques . We perform experiments on three distinct types of datasets , tracking training / test error rates as well as total ( training + test ) time . For the adult dataset we employ the Gaussian kernel with a logistic regression model , and for the reuters _CITE_ dataset we employ a linear kernel with a ridge regression model . For the buzz dataset we employ ride regression with an arc - cosine kernel of order 2 , i . e . P0 = N ( 0 , I ) and φ ( x , w ) = H ( wTx ) ( w x ) , where H (·) is the Heavyside step function [ 7 ].__label__Material|Data|Use
Sentiment : Product reviews to be classified as positive or negative . We used each Amazon product review domain as a sentiment classification task ( 6 datasets ). Spam : We selected three task A users from the ECML / PKDD Challenge _CITE_ , using bag - ofwords to classify each email as spam or ham ( 3 datasets ). For OCR data we binarized two well known digit recognition datasets , MNIST and USPS , into 45 all - pairs problems . We also created ten one vs . all datasets from the MNIST data ( 100 datasets total ).__label__Supplement|Website|Use
Furthermore , we test the robustness of our methods by randomly reassigning the labels of 10 % of the images , and the results indicate that the SGD - WPV improves the performance of SGD - Scan even more while SGD - SD overfits the data seriously . We test a simple multi - class logistic regression on CIFAR 10 [ 24 ]. _CITE_ Images are down - sampled significantly to 32 × 32 × 3 , so many examples are difficult , even for humans . SGD - SPV and SGD - SE perform significantly better than SGD - Uni here , consistent with the idea that avoiding difficult examples increases robustness to outliers . For CIFAR 100 [ 24 ], we demonstrate that the proposed approaches can also work in very deep residual networks [ 16 ].__label__Material|Data|Use
When we optimize over supergradients , all possible tight sets are considered . Similarly , the subgradients are optimized over B ( F ), and for any X ⊆ V there exists some sX ∈ B ( F ) tight at X . Our experiments _CITE_ aim to address four main questions : ( 1 ) How large is the gap between the upperand lower - bounds for the log - partition function and the marginals ? ( 2 ) How accurate are the factorized approximations obtained from a single MAP - like optimization problem ? ( 3 ) How does the accuracy depend on the amount of evidence ( i . e ., concentration of the posterior ), the curvature of the function , and the type of Bayesian submodular model considered ?__label__Method|Algorithm|Introduce
The predicted accuracy of HMM / NN is about three percent less than that of CNF . By seamlessly integrating neural networks and CRF , CNF outperforms all other thestate - of - art prediction methods on this dataset . We also tried Max - Margin Markov Network [ 8 ] and SVM - struct _CITE_ with RBF kernel for this dataset . However , because the dataset is large and the feature space is of high dimension , it is impossible for these kernel - based methods to finish training within a reasonable amount of time . Both of them failed to converge within 120 hours .__label__Method|Tool|Use
Therefore the correct ranking on the top positions is critically important . For example , modern web search engines only return top 1 , 000 results and 10 results in each page . According to a user study _CITE_ , 62 % of search engine users only click on the results within the first page , and 90 % of users click on the results within the first three pages . It means that two ranked lists of documents will likely provide the same experience to the users ( and thus suffer the same loss ), if they have the same ranking results for the top positions . This , however , cannot be reflected in the permutation - level 0 - 1 loss in Eq .( 2 ).__label__Supplement|Paper|Extent
Nesterov , Yurii . A method for unconstrained conv dient descentusing predictive variance reduction . In The experiments are performed on four benchmark data ses : a9a , covertype , mnist and dna _CITE_ . The mization problem with the rate of convergence c . Conf . Advances in Neural Informaion Processing details of the dataset and the mini - batch size that we use ina SADMM are shown in Table .__label__Material|Data|Use
The results show that the first and second CDBN representations both outperform baseline features ( RAW and MFCC ). The numbers compare MFCC and CDBN features with as many of the same factors ( such as preprocessing and classification algorithms ) as possible . Further , to make a fair comparison between CDBN features and MFCC , we used the best performing implementation _CITE_ among several standard implementations for MFCC . Our results suggest that without special preprocessing or postprocess ing ( besides the summary statistics which were needed to reduce the number of features ), the CDBN features outperform MFCC features , especially in a setting with a very limited number of labeled examples . We further experimented to determine if the CDBN features can achieve competitive performance in comparison to other more sophisticated , state - of - the - art methods .__label__Method|Code|Use
The objective function under amortization is given by We maximize this objective with respect to αv , ρ ( 0 ) v , and φ ( s ) using stochastic gradient ascent . We implement the hierarchical and amortized S - EFE models in TensorFlow ( Abadi et al ., 2015 ), which allows us to leverage automatic differentiation . _CITE_ Example : structured Bernoulli embeddings for grouped text data . Here , we consider a set of documents broken down into groups , such as political affiliations or scientific disciplines . We can represent the data as a binary matrix X and a set of group indicators si .__label__Method|Code|Produce
For more details , see Rahmani & Goldman [ 8 ]. We modified the collection by manually annotating the instance segments that belong to the labeled object for each image using a graphical interface we developed . We also created a semi - synthetic MI data set for text classification , using the 20 Newsgroups _CITE_ corpus as a base . This corpus was chosen because it is an established benchmark for text classification , and because the source texts — newsnet posts from the early 1990s — are relatively short ( in the MI setting , instances are usually paragraphs or short passages [ 1 , 9 ]). For each of the 20 news categories , we generate artificial bags of approximately 50 posts ( instances ) each by randomly sampling from the target class ( i . e ., newsgroup category ) at a rate of 3 % for positive bags , with remaining instances ( and all instances for negative bags ) drawn uniformly from the other classes .__label__Material|Data|Use
The most negatively related ( exclusive ) pairs ( the i and j entries with highest positive entries in 4 )) are circular and square legs which conforms fully to the generation process , since only one of them is chosen for any given location . Accordingly , the most positively related pairs are a body shape and one of its associated legs since every bug has a body and four legs with fixed positions . In this section , we apply SLFA to the NIPS corpus _CITE_ which contains 1740 abstracts from the NIPS Conferences 1 − 12 for the purpose of topic / content modeling . SLFA is used to organize and visualize the relationship between the structured topics . SLFA is applied on the 13649 dimensional tf - idf feature vector which is normalized to have the unit norm .__label__Material|Data|Use
Lemma 3 . 2 If we set ⌧ = 1K in ( 5 ), the constraint rank ( W ) < K will be automatically satisfied . � Finally , we construct Z * = U D as instructed in corollary 3 . 1 . 1 . Note that in the intermediate iterations , we do not need to compute Z ; we need to construct the matrix Z * to find the overlapping blocks after the learning algorithm will converge _CITE_ . Here , we show that GRAB algorithm generalizes the K - way graph cut algorithm in two ways : 1 ) GRAB allows each variable to be in multiple blocks with soft membership ; and 2 ) GRAB updates a network structure ⇥, used as a similarity matrix , in each iteration . The proof is in the Appendix .__label__Method|Algorithm|Introduce
We first focus on the AUC measure of a linear classifier θ as defined in ( 3 ). We use the SMVguide3 binary classification dataset which contains n = 1260 points in d = 23 dimensions . _CITE_ We set θ to the difference between the class means . For each generated network , we perform 50 runs of GoSta - sync ( Algorithm 1 ) and U2 - gossip . The top row of Figure 2 shows the evolution over time of the average relative error and the associated standard deviation across nodes for both algorithms on each type of network .__label__Material|Data|Use
At the end of the pruning step , we denote the resulting tree by T . The sketch . For each point xi E X the sketch stores the index of the leaf vi that contains it . In addition it stores the structure of the tree T , encoded using the Eulerian Tour Technique _CITE_ . Specifically , starting at the root , we traverse T in the Depth First Search ( DFS ) order . In each step , DFS either explores the child of the current node ( downward step ), or returns to the parent node ( upward step ).__label__Method|Algorithm|Use
The model captures fine - grained semantic structure and performs better when small semantic distinctions are important . CCGs map documents on a probabilistic simplex ( e . g ., ✓) and for W > [ 1 x 1 ] can be thought as an LDA model whose topics , hi , are much finer as computed from overlapping windows ( see also Eq . 10 ); a comparison is therefore natural . As first dataset we considered the CMU newsgroup dataset _CITE_ . Following previous work [ 2 , 3 , 6 ] we reduced the dataset into subsets with varying similarities among the news groups ; news20 - different , with posts from rec . sport . baseball , sci . space and alt . atheism , news - 20 - similar , with posts from rec . talk . baseball , talk . politics . gun and talk . politics . misc and news - 20 - same , with posts from comp . os . ms - windows , comp . windows . x and comp . graphics . For the news - 20 - same subset ( the hardest ), in Fig . 3a we show the accuracies of CCGs and LDA across the complexities .__label__Material|Data|Use
The conventional HMM has a large number of covariance parameters because it has a 6 - D output variable ; whereas the CHMM architecture has two 3 - D output variables . In consequence , due to their larger dimensionality HMMs need much more training data than equivalent CHMMs before yielding good generalization results . Our second experiment was with a pedestrian video surveillance task _CITE_ ; the goal was first to recognize typical pedestrian behaviors in an open plaza ( e . g ., walk from A to 13 , run from C to D ), and second to recognize interactions between the pedestrians ( e . g ., person X greets person Y ). The task is to reliably and robustly detect and track the pedestrians in the scene . We use in this case 2 - D blob features for modeling each pedestrian .__label__Method|Algorithm|Use
We initialize with an all zeros depth ( corresponding to a flat surface ) and the light is initialized to the mean light from the “ natural ” dataset in [ 3 ]. We perform the estimation in multiple scales using V - sweeps - solving at a coarse scale , upscaling , solving at a finer scale then downsampling the result , repeating the process 3 times . The same parameter settings were used in all cases _CITE_ . We use the same error measures as in [ 3 ]. The error for the normals is measured using Median Angular Error ( MAE ) in radians .__label__Supplement|Document|Produce
Consistent with observed timing of the cell ’ s response to cAMP stimulation , we find that the directional signal converges quickly enough for the cell to make a decision about which direction to move within the first two seconds following stimulus onset . Using MCell and DReAMM [ 10 , 11 ] we construct a spherical cell ( radius R = 7 . 5µm , [ 12 ]) centered in a cubic volume ( side length L = 30µm ). N = 980 triangular tiles partition the surface ( mesh generated by DOME _CITE_ ); each contained one cell surface receptor for cAMP with binding rate k + = 4 . 4 x 10 sec − _CITE_M − _CITE_ , first - order cAMP unbinding rate k − = 1 . 1 sec − _CITE_ [ 12 ] and Keq = k −/ k + = 25nMol cAMP . We established a baseline concentration of approximately 1nMol by releasing a cAMP bolus at time 0 inside the cube with zero - flux boundary conditions imposed on each wall . At t = 2 seconds we introduced a steady flux at the x = − L / 2 wall of 1 molecule of cAMP per square micron per msec , adding signaling molecules from the left .__label__Method|Tool|Use
( 2 ) NYT : Consists of a random subset of 30 , 000 documents from the New York Times dataset , vocabulary of 5 , 000 words and mean document length 238 . ( 3 ) Pubmed : Consists of a random subset of 30 , 000 documents from the Pubmed abstracts dataset , vocabulary of 5 , 030 words and mean document length 58 . ( 4 ) 20NewsGroup _CITE_ ( 20NG ): Consist of 13 , 389 documents , vocabulary of 7 , 118 words and mean document length 160 . To check the dominant topic and catchwords assumptions , we first run 1000 iterations of Gibbs sampling on the real corpus and learn the posterior document - topic distribution ({ W ., j }) for each document in the corpus ( by averaging over 10 saved - states separated by 50 iterations after the 500 burn - in iterations ). We will use this posterior document - topic distribution as the document generating distribution to check the two assumptions .__label__Material|Data|Use
In our experiments , we did not restrict the setting to a given small lexicon and instead used a general , large English lexicon . SVT does not contain symbols other than letters . Training Generative Shape Model To ensure sufficient coverage of fonts , we obtained 492 fonts from Google Fonts _CITE_ . Manual font selection is biased and inaccurate , and it is not feasible to train on all fonts ( 492 fonts times 52 letters gives 25584 training images ). After the proposed greedy font selection process for all letters , we retained 776 unique training images in total ( equivalent to a compression rate of 3 % if we would have trained on all fonts for all letters ).__label__Supplement|Website|Use
Each instance is drawn from a gender - specific Gaussian . In this example , we can find the optimal classifier by weighting the “ males ” and “ females ” components of the source to match the target . Domain adaptation is a widely - studied area , and we cannot hope to cover every aspect and application of it here _CITE_ . Instead , in this section we focus on other theoretical approaches to domain adaptation . While we do not explicitly address the relationship in this paper , we note that domain adaptation is closely related to the setting of covariate shift , which has been studied in statistics .__label__Supplement|Document|Produce
Therefore we have chosen to use the simple hand - tuned potentials that were used in previous work : the edge - based costs [ 1 ] and the count - based costs defined by [ 29 , 30 ]. Specifically , we used the following clique potentials in our experiments , all of which are submodular : We used five image segmentation instances to evaluate the algorithms . _CITE_ The experiments were carried out on a single computer with a 3 . 3 GHz Intel Core i5 processor and 8 GB of memory ; we reported averaged times over 10 trials . We performed several experiments with various combinations of potentials and parameters . In the minimum cut experiments , we evaluated the algorithms on instances containing only unary and are very similar and are deferred to the full version of the paper .__label__Material|Data|Use
We can see that the estimation errors grow linearly with the theoretical rate , which validates our theoretical guarantee on the minimax optimal statistical rate . In this subsection , we apply our method to TCGA breast cancer gene expression data to infer regulatory network . We downloaded the gene expression data from cBioPortal _CITE_ . Here we focused on 299 breast cancer related transcription factors ( TFs ) and estimated the regulatory relationships for each pair of TFs over two breast cancer subtypes : luminal and basal . We compared our method AltGD To demonstrate the performances of different methods on recovering the overall transcriptional regulatory network , we randomly selected 10 TFs in the benchmark network and plotted the subnetwork in Figure 2 which has 70 edges with nonzero regulatory potential scores .__label__Material|Data|Use
This naive approach to rule extraction from refined advice is shown here only to illustrate that it is possible to produce very useful domain - expert - interpretable rules from refinement . More efficient and accurate rule extraction techniques inspired by SVM - based rule extraction ( for example , [ 7 ]) are currently under investigation . Wargus _CITE_ is a real - time strategy game in which two or more players gather resources , build bases and control units in order to conquer opposing players . It has been widely used to study and evaluate various machine learning and planning algorithms . We evaluate our algorithms on a classification task in the Wargus domain developed by Walker et al .__label__Supplement|Media|Introduce
The basic idea is to treat the learning algorithm ’ s generalization performance as a sample from a Gaussian process and select the next parameter configuration to test based on the expected improvement . The authors showed that this way , the number of experiment runs to minimize a given objective can be significantly reduced while surpassing the performance of parameters chosen by human experts . We implemented _CITE_ our experiments using Theano [ 23 ] and pylearn2 [ 24 ]. The computations were run on a dedicated 12 - core workstation with two Nvidia graphics cards – a Tesla C2075 and a Quadro 2000 . We followed the common practice to optimize the performance on the validation set .__label__Method|Code|Produce
Evaluation Metrics and Baseline Algorithms : We select the receiver operating characteristic ( ROC ) curve as our performance metric because the discrimination thresholds of diseases vary . We first compare the accuracy and efficiency of VISKM with Gibbs sampling ( Gibbs ) and particle filtering ( PF ) on the Social Evolution data set [ 7 , 8 ]. _CITE_ Both Gibbs sampling and particle filtering iteratively sample the infectious and susceptible latent state sequences and the infection and recovery events conditioned on these state sequences . Gibbs - Prediction - 10000 indicates 10 , 000 iterations of Gibbs sampling with 1000 burn - in iterations for the prediction task . PF - Smoothing - 1000 similarly refers to 1000 iterations of particle filtering for the smoothing task .__label__Material|Data|Use
As a result , we can approximate any sufficiently smooth bounded function of u arbitrarily well , given sufficiently many training classification problems . To validate our method , we evaluated its ability to learn parameter functions on a variety of email and webpage classification tasks in which the number of classes , K , was large ( K = 10 ), and the number of number of training examples per class , m / K , was small ( m / K = 2 ). We used the dmoz Open Directory Project hierarchy , _CITE_ the 20 Newsgroups dataset , the Reuters - 21578 dataset , and the Industry Sector dataset . classifications using non - discriminative methods : the learned g , Naive Bayes , and TFIDF , respectively . Columns 5 - 7 give the corresponding values for the discriminative methods : softmax regression , 1 - vs - all SVMs , and multiclass SVMs .__label__Material|Data|Use
There are 357 positive samples and 212 negative samples . Each sample has 32 attributes . ORL face database _CITE_ contains 10 images for each of the 40 human subjects , which were taken at different times , varying the lighting , facial expressions and facial details . The original images ( with 256 gray levels ) have size 92 x 112 , which are resized to 32 x 32 for efficiency . Isolet was first used in [ 11 ].__label__Material|Data|Introduce
We compare our algorithm to [ 29 ] and [ 14 ], the former is an example of robust motion estimation and the latter is a representative of the approaches described in Sect . 1 . 1 . In our implementation _CITE_ , we first solve ( 14 ) with standard relaxation ( W is the identity ) and then with reweighted -` 1 . To handle large motion , we use a pyramid with scale factor 0 . 5 and up to 4 levels ; λ and µ are fixed at 0 . 002 and 0 . 001 ( Flower Garden ) and 0 . 0006 and 0 . 0003 ( Middlebury ) respectively . To make comparison with [ 29 ] fair , we modify the code provided online to include anisotropic regularization ( Fig .__label__Method|Code|Produce
The optimizer used is Adam [ 16 ]. For k - SBN , we show results for three values of k : 5 , 10 , and 20 , and the aggregation is done using the median estimator with T = 3 . We trained a generative model for silhouette images of 28 × 28 dimensions from the Caltech 101 Silhouettes dataset _CITE_ . The dataset consists of 4 , 100 train images , 2 , 264 validation images and 2 , 307 test images . This is a particularly hard dataset due to the asymmetry in silhouettes compared to other commonly used structured datasets .__label__Material|Data|Use
We hypothesize that the Poisson assumption made by the IBP is not appropriate for text data , as the statistics of word use in natural language tends to follow a heavier tailed distribution [ 22 ]. To test this hypothesis , we modeled a collection of corpora using both an IBP , and an IBP restricted to have a negative Binomial distribution over the number of words . Our corpora were 20 collections of newsgroup postings on various topics ( for example , comp . graphics , rec . autos , rec . sport . hockey ) _CITE_ . No pre - processing of the documents was performed . Since the vocabulary ( and hence the feature space ) is finite , we truncated both models to the vocabulary size .__label__Material|Data|Use
Sen . Shelby ( D - AL ) votes with the Republicans on Economic , with the Democrats on Education + Domestic and with a small group of maverick Republicans on Foreign and Social Security + Medicare . Sen . Shelby , together with Sen . Heflin , is a Democrat from a fairly conservative state ( Alabama ) and are found to side with the Republicans on many issues . The second dataset involves the voting record of the UN General Assembly _CITE_ . We focus on the resolutions discussed from 1990 - 2003 , which contain votes of 192 countries on 931 resolutions . If a country is present during the roll call , it may choose to vote Yes , No or the corresponding groups for each topic ( column ).__label__Material|Data|Introduce
In addition , we compared with the HSM result reported in [ 6 ], which used 1024 dimensions for word embedding , but still has 40x more parameters than our model . For further comparisons , we also ensembled LightRNN with the KN 5 - gram model . We utilized the KenLM Language Model Toolkit _CITE_ to get the probability distribution from the KN model with the same vocabulary setting . The results on BillionW are shown in Table 4 . It is easy to see that LightRNN achieves the lowest perplexity whilst significantly reducing the model size .__label__Method|Tool|Use
The total word likelihood could be used , but it would allow senses with longer entries to dominate . We evaluated the outlined algorithms on three datasets : the five - word MIT - ISD dataset [ 17 ], the three - word UIUC - ISD dataset [ 14 ], and OFFICE dataset of ten common office objects that we collected for the classification experiment . _CITE_ All datasets had been collected automatically by issuing queries to the Yahoo Image SearchTM engine and downloading the returned images and corresponding HTML web pages . For the MIT - ISD dataset , the query terms used were : BASS , FACE , MOUSE , SPEAKER and WATCH . For the UIUC - ISD dataset , three basic query terms were used : BASS , CRANE and SQUASH .__label__Material|Data|Use
It contains monthly observations of 17 variables such as Carbon dioxide and temperature spanning from 1990 to 2001 . The observations were interpolated on a 2 . 5 × 2 . 5 degree grid , with 125 observation locations . Yelp The Yelp dataset _CITE_ contains the user rating records for 22 categories of businesses on Yelp over ten years . The processed dataset includes the rating values ( 1 - 5 ) binned into 500 time intervals and the corresponding social graph for 137 active users . The dataset is used for the spatio - temporal recommendation task to predict the missing user ratings across all business categories .__label__Material|Data|Introduce
In contrast , the proposed compilation to PSDDs does not rely on an intermediate representation or additional boxes , such as d - DNNF or SDD compilers . The benchmarks in Table 1 are from the UAI - 14 Inference Competition . _CITE_ We selected all networks over binary variables in the MAR track , and report a network only if at least one approach successfully compiled it ( given time and space limits of 30 minutes and 16GB ). We report the size ( the number of edges ) and time spent for each compilation . First , we note that for all benchmarks that compiled to both PSDD and AC2 ( based on SDDs ), the PSDD size is always smaller .__label__Material|Data|Use
The interval m was set to 2n ( convex ) and 5n ( nonconvex ). The weights for SVRG were initialized by performing 1 iteration ( convex ) or 10 iterations ( nonconvex ) of SGD ; therefore , the line for SVRG starts after x = 1 ( convex ) or x = 10 ( nonconvex ) in the respective figures . First , we performed L2 - regularized multiclass logistic regression ( convex optimization ) on MNIST _CITE_ with regularization parameter λ = 1e - 4 . Fig . 2 ( a ) shows training loss ( i . e ., the optimization objective P ( w )) in comparison with SGD with fixed learning rates .__label__Material|Data|Use
This method is implemented in MATLAB . We used the modules for LBP , made available with UGM package . The LDA models are learnt using the lda package _CITE_ . Performance Evaluation : We evaluate performance based on the test perplexity [ 20 ] given by where n is the number of test samples and p is the number of observed variables ( i . e ., words ). Thus the perplexity is monotonically decreasing in the test likelihood and a lower perplexity indicates a better generalization performance .__label__Method|Tool|Use
All models were derived using a window of 2 tokens to each side of the focus word , ignoring words that appeared less than 100 times in the corpus , resulting in vocabularies of 189 , 533 terms for both words and contexts . To train the SGNS models , we used a modified version of word2vec which receives a sequence of pre - extracted word - context pairs [ 18 ]. _CITE_ We experimented with three values of k ( number of negative samples in SGNS , shift parameter in PMI - based methods ): 1 , 5 , 15 . For SVD , we take W = Ud · ✓ Ed as explained in Section 4 . Now that we have an analytical solution for the objective , we can measure how well each algorithm optimizes this objective in practice .__label__Method|Tool|Use
5 . 3 ) of the persistence diagrams in combination with a linear SVM . Implementation . All experiments were implemented in PyTorch , using DIPHA _CITE_ and Perseus [ 23 ]. Source code is publicly - available at https :// github . com / c - hofer / nips2017 . We apply persistent homology combined with our proposed input layer to two different datasets of binary 2D object shapes : ( 1 ) the Animal dataset , introduced in [ 3 ] which consists of 20 different animal classes , 100 samples each ; ( 2 ) the MPEG - 7 dataset which consists of 70 classes of different object / animal contours , 20 samples each ( see [ 21 ] for more details ).__label__Method|Tool|Use
Then , to estimate viewpoint , we assign the view label of the z - th output yz to x , such that yz is the most similar image to x . The above procedure is formulated as below . If v is discrete , the problem is , ar min k p ( v • = 1 | x hv ) − p ( v • = 1 | Yz ' hz ) k2 = argmin � 11 ' P ( U2j ∗ x + V2j ∗ hz ) Several experiments are designed for evaluation and comparison _CITE_ . In Sec . 3 . 1 , MVP is evaluated on a large face recognition dataset to demonstrate the effectiveness of the identity representation .__label__Method|Algorithm|Compare
For higher values of β the information I ( X ; T ) would continue to grow while I ( Y ; T ) would reach its limit leading to horizontal lines , but such high beta values lead to numerical instability . Since GIB suffers from a model mismatch problem when the margins are not Gaussian , the curves saturate for smaller values of I ( Y ; T ). We further applied MGIB to the Communities and Crime data set from the UCI repository _CITE_ . The data set contains observations of predictive and target variables . After removing missing values we retained n = 2195 observations .__label__Material|Data|Use
Our implementation is 1385 lines of code , excluding comments and whitespace . Our implementation is open - source and publicly available . _CITE_ Our experimental results show that , for large p and D , YGGDRASIL outperforms PLANET by an order of magnitude , corroborating our analysis in Section 3 . We benchmarked YGGDRASIL against two implementations of PLANET : Spark MLLIB v1 . 6 . 0 , and XGBOOST4J - SPARK v0 . 47 . These two implementations are slightly different from the algorithm from Panda et al .__label__Method|Code|Produce
More importantly , the latent variables sj in our model are automatically estimated from data , deciding how important each eigenvector is for the classification task in a principled Bayesian framework . We evaluated the new sparse Bayesian model , the EigenNet , on both synthetic and real data and compared it with three representative variable selection methods , the lasso , the elastic net , and an ARD approach ( Qi et al ., 2004 ). For the lasso and the elastic net , we used the Glmnet software package that uses cyclical coordinate descent in a pathwise fashion _CITE_ . Like the EigenNet , the ARD approach also uses EP to approximate the model marginal likelihood . For the lasso and the elastic net , we used cross - validation to tune the hyperparameters ; for the EigenNet , we estimated av from data and tuned as by cross - validation .__label__Method|Tool|Use
In the experiments below we compare running times of MDBNs to GIZA ++ on IBM Models 1 through 4 and the M - HMM model . GIZA ++ is a special - purpose optimized MT word alignment C ++ tool that is widely used in current state - of - the - art phrase - based MT systems [ 10 ] and at the time of this writing is the only publicly available software that implements all of the IBM Models . We test on French - English 107 hand - aligned sentences _CITE_ from a corpus of the European parliament proceedings ( Europarl [ 9 ]) and train on 10000 sentence pairs from the same corpus and of maximum number of words 40 . The Alignment Error Rate ( AER ) [ 13 ] evaluation metric quantifies how well the MPE assignment to the hidden alignment variables matches human - generated alignments . Several pruning and smoothing techniques are used by GIZA and MDBNs .__label__Material|Data|Use
We used the data sets evaluated in [ 27 ] ( plant , nonpl , psortPos , and psortNeg ), which consist of either 3 or 4 classes and use 69 biologically motivated sequence kernels . Furthermore , we also considered the proteinFold data set of [ 28 ], which consists of 27 classes and uses 12 biologically motivated base kernels . _CITE_ The results are summarized in Table 1 : they represent mean accuracy values with one standard deviation as computed over 10 random splits of the data into training and test folds . The fraction of the data used for training , as well as the total number of examples , is also shown . The optimal value for the parameter 0 E 12i , i = 0 , 1 , ... , 81 was determined by cross - validation .__label__Material|Data|Use
Common dense subgraph detection We evaluate our algorithm for finding large dense regions on the DIMACS Challenge graphs [ 15 ], which is a comprehensive benchmark for testing of clique finding and related algorithms . For the families of dense graphs ( brock , san , sanr ), we focus on finding large dense region in the complement of the original graphs . We run Algorithm 1 using SimpleMkl _CITE_ to find large common dense subgraph . In order to evaluate the performance of our algorithm , we compute a ¯ = maxm a ( m ) and a = minm a ( m ) where sity ); and nT / N is relative size of induced subgraph compared to original graph size . We want a high value of nT / N ; while a should not be lower than 1 .__label__Method|Tool|Use
10 ( b ) shows the optimal block size decreases as the mean shift increases , as expected . We test the performance of our M - statistics using real data . Our datasets include : ( 1 ) CENSREC1 - C : a real - world speech dataset in the Speech Resource Consortium ( SRC ) corpora provided by National Institute of Informatics ( NII ) _CITE_ ; ( 2 ) Human Activity Sensing Consortium ( HASC ) challenge 2011 data . We compare our M - statistic with a state - of - the - art algorithm , the relative densityratio ( RDR ) estimate [ 7 ] ( one limitation of the RDR algorithm , however , is that it is not suitable for high - dimensional data because estimating density ratio in the high - dimensional setting is illposed ). To achieve reasonable performance for the RDR algorithm , we adjust the bandwidth and the regularization parameter at each time step and , hence , the RDR algorithm is computationally more expensive than the M - statistics method .__label__Material|Data|Use
10 ( b ) shows the optimal block size decreases as the mean shift increases , as expected . We test the performance of our M - statistics using real data . Our datasets include : ( 1 ) CENSREC1 - C : a real - world speech dataset in the Speech Resource Consortium ( SRC ) corpora provided by National Institute of Informatics ( NII ) _CITE_ ; ( 2 ) Human Activity Sensing Consortium ( HASC ) challenge 2011 data . We compare our M - statistic with a state - of - the - art algorithm , the relative densityratio ( RDR ) estimate [ 7 ] ( one limitation of the RDR algorithm , however , is that it is not suitable for high - dimensional data because estimating density ratio in the high - dimensional setting is illposed ). To achieve reasonable performance for the RDR algorithm , we adjust the bandwidth and the regularization parameter at each time step and , hence , the RDR algorithm is computationally more expensive than the M - statistics method .__label__Supplement|Website|Use
We omit validation plots due to paucity of space . For our chunking experiments we use a similar base set of features as above : Since CoNLL 00 chunking data does not have a development set , we randomly sampled 1000 sentences from the training data ( 8936 sentences ) for development . So , we trained our chunking models on 7936 training sentences and evaluated their F1 score on the 1000 development sentences and used a CRF _CITE_ as the supervised classifier . We tuned the size of embedding and the magnitude of 22 regularization penalty in CRF on the development set and took log ( or - log of the magnitude ) of the value of the features . The regularization penalty that gave best performance on development set was 2 and here again the best size of LR - MVL embeddings ( state - space ) was k = 50 .__label__Method|Tool|Use
This also distinguishes DIS from methods such as adaptive importance sampling ( AIS ) [ 18 ]. We evaluate our approach ( DIS ) against AOBFS ( search , [ 16 ]) and WMB - IS ( sampling , [ 15 ]) on several benchmarks of real - world problem instances from recent UAI competitions . Our benchmarks include pedigree , 22 genetic linkage instances from the UAI ’ 08 inference challenge _CITE_ ; protein , 50 randomly selected instances made from the “ small ” protein side - chains of [ 22 ]; and BN , 50 randomly selected Bayesian networks from the UAI ’ 06 competition . These three sets are selected to illustrate different problem characteristics ; for example protein instances are relatively small ( M = 100 variables on average , and average induced width 11 . 2 ) but high cardinality ( average max | Xi |= 77 . 9 ), while pedigree and BN have more variables and higher induced width ( average M 917 . 1 and 838 . 6 , average width 25 . 5 and 32 . 8 ), but lower cardinality ( average max | Xi | 5 . 6 and 12 . 4 ). We alloted 1GB memory to all methods , first computing the largest ibound that fits the memory budget , and using the remaining memory for search .__label__Material|Data|Use
Remark on convergence of SGD with line searches : We note in passing that it is straightforward to ensure that SGD instances using the line search inherit the convergence guarantees of SGD : Putting even an extremely loose bound ¯ αi on the step sizes taken by the i - th line search , such that �° i ¯ αi = ∞ and �° i ¯ α i < ∞, ensures the line search - controlled SGD converges in probability [ 1 ]. Our experiments were performed on the well - worn problems of training a 2 - layer neural net with logistic nonlinearity on the MNIST and CIFAR - 10 datasets . _CITE_ In both cases , the network had 800 hidden units , giving optimization problems with 636 010 and 2 466 410 parameters , respectively . While this may be ‘ low - dimensional ’ by contemporary standards , it exhibits the stereotypical challenges of stochastic optimization for machine learning . Since the line search deals with only univariate subproblems , the extrinsic dimensionality of the optimization task is not particularly relevant for an empirical evaluation .__label__Material|Data|Use
We record the average running time over 100 trials in Figure 2 and from the results we can see that on the classification problems above , our proposed coordinate descent method is much faster than the CVX solver which demonstrates the efficiency of our proposed method . Here we study a multi - task regression problem to learn the inverse dynamics of a seven degree - offreedom SARCOS anthropomorphic robot arm . _CITE_ The objective is to predict seven joint torques based on 21 input features , corresponding to seven joint positions , seven joint velocities and seven joint accelerations . So each task corresponds to the prediction of one torque and can be formulated as a regression problem . Each task has 2000 data points .__label__Method|Algorithm|Use
They are also rarely predators . Unlike the land animals , many of the water animals in columns one and two do not breathe . Recipes The recipes data set consists of ingredients from recipes taken from the computer cooking contest _CITE_ . There are 56 recipes , with 147 total ingredients . The recipes fall into four categories : pasta , chili , brownies or punch .__label__Material|Data|Use
A particular problem , suffered also by the similar algorithm in [ 13 ], is that derivative calculations must be interleaved with site inclusions , and the latter operation tends to disrupt gradient information gained from the previous step . These complications are all sidestepped in our SPGP implementation . We conducted tests on a variety of data , including two small sets from [ 14 ] _CITE_ and the benchmark suite of R ¨ atsch . The dimensionality of these classification problems ranges from two to sixty , and the size of the training sets is of the order of 400 to 1000 . Results are presented in table 1 .__label__Material|Data|Use
We also apply uniform random search , which is known to outperform a grid or manual search ( Bergstra and Bengio , 2012 ). In the first experiment , we consider stochastic variational inference on latent Dirichlet allocation ( SVI - LDA ) applied to the 20 Newsgroups data . _CITE_ In the second , a deep latent Gaussian model ( DLGM ) on the Labeled Faces in the Wild data set ( Huang et al ., 2007 ). We find that EB - Hyp outperforms BayesOpt and random search as measured by predictive likelihood . For the performance model , we use the log Gaussian process in our experiments implemented in the GPy package ( GPy , 2012 ).__label__Material|Data|Use
The idea of working in the tangent space is both efficient and convenient , but comes with an element of approximation as the logarithmic map is only guarantied to preserve distances to the origin of the tangent and not between all pairs of data points . Practical experience , however , indicates that this is a good tradeoff ; see [ 19 ] for a more in - depth discussion of when the approximation is suitable . To illustrate the framework _CITE_ we consider an example in human body analysis , and then we analyze the scalability of the approach . But first , to build intuition , Fig . 3a show synthetically generated data samples from two classes .__label__Method|Algorithm|Introduce
Fig . 5 shows results on two 481 × 321 images taken from the Berkeley database . _CITE_ On these images the sampling process produced a sample with no more than 1000 pixels , and our current MATLAB implementation took only a few seconds to return a solution . Running the grouping algorithm on the whole images ( which contain more than 150 , 000 pixels ) would simply be unfeasible . In both cases , our approximation algorithm partitioned the images into meaningful and clean components .__label__Material|Data|Use
We use 4 different document sets in our experiments , as summarized in Table 1 . The NIPS and PATENTS document sets are used for perplexity experiments and the AP and FR data sets for retrieval experiments . The NIPS data set is available online _CITE_ and PATENTS , AP , and FR consist of documents from the U . S . Patents collection ( TREC Vol - 3 ), Associated Press news articles from 1998 ( TREC Vol - 2 ), and articles from the Federal Register ( TREC Vol - 1 , 2 ) respectively . To create the sampled AP and FR data sets , all documents relevant to queries were included first and the rest of the documents were chosen randomly .__label__Material|Data|Use
We now compare the different methods on several benchmark datasets . We first consider the Jester datasets [ 18 ]. The three datasets _CITE_ contain one hundred jokes , with user ratings between - 10 and + 10 . We randomly select two ratings per user as a test set , and two other ratings per user as a validation set to select the parameters A and a . The results are computed over four values a = 1000 , 100 , 10 , 1 .__label__Material|Data|Use
In this experiment we evaluated our method ’ s capability to reconstruct the evolutionary history of populations represented in the 1000 genomes dataset , which consists of whole genome sequences of 1 , 092 human individuals from 14 distinct populations . We used a trinary representation wherein each individual is represented as a vector of features corresponding to 0 , 1 or 2 . Every feature represents a known genetic variation ( with respect to the standard human reference genome _CITE_ ), where the number indicates the number of varied genome copies . We used data processed by the 1000 Genomes Consortium , which initially contained 2 . 25 million variations . To reduce dimensionality , we used the 1 , 000 features that had the highest information gain with respect to the populations .__label__Material|Data|Introduce
Implementation and Hardware All experiments were conducted on a computing cluster where each node has two 2 . 1 GHz 12 - core AMD 6172 processors with 48 GB physical memory per node . Our algorithms are implemented in C ++ using the Eigen library and compiled with the Intel Compiler . We downloaded Version 2 . 5 of the Tensor Toolbox , which is implemented in MATLAB _CITE_ . Since open source code for GigaTensor is not freely available , we developed our own version in C ++ following the description in [ 8 ]. Also , we used MPICH2 in order to distribute the tensor factorization computation to multiple machines .__label__Method|Code|Use
Learning was carried out using Contrastive Divergence by starting with one full Gibbs step and gradually increaing to five steps during the course of training , as described in [ 14 ]. For all three datasets , the total number of parameter updates was set to 100 , 000 , which took several hours to train . For the LDA model , we used the Gibbs sampling implementation of the Matlab Topic Modeling Toolbox _CITE_ [ 5 ]. The hyperparameters were optimized using stochastic EM as described by [ 15 ]. For the 20newsgroups and NIPS datasets , the number of Gibbs updates was set to 100 , 000 .__label__Method|Tool|Use
Under the NB processes , each word xji would be assigned to a topic k based on both F ( xji ; Wk ) and the topic weights { ajk } k = 1 , K ; each topic is drawn from a Dirichlet base measure as Wk ∼ Dir ( 77 , · · · , 77 ) E ] IRV , where V is the number of unique terms in the vocabulary and 77 is a smoothing parameter . Let vji denote the location of word xji in the vocabulary , then we have ( Wk |−) ∼ Dir ( 77 + Ej Ei 6 ( zji = k , vji = 1 ), · · · , 77 + Ej Ei 6 ( zji = k , vji = V )). We consider the Psychological Review _CITE_ corpus , restricting the vocabulary to terms that occur in five or more documents . The corpus includes 1281 abstracts from 1967 to 2003 , with 2 , 566 unique terms and 71 , 279 total word counts . We randomly select 20 %, 40 %, 60 % or 80 % of the words from each document to learn a document dependent probability for each term v as fjv = E 1 � K 1 EV 1 � K 1S = A /� ( s ) A ( s ) ` here Wvk is the probability of term vwvk jk s = 1v = k = wvk ik in topic k and S is the total number of collected samples .__label__Material|Data|Use
In all the experiments that c - relaxation is used , the value of c is 0 . 01 . Note that our empirical study is focused on whether the proposed boosting algorithm is able to effectively improve the accuracy of state - of - the - art boosting algorithms with the same weak learner space W , thus we restrict our comparison to boosting algorithms with the same weak learners , rather than a wide range of classification algorithms , such as SVMs and KNN . We first compare DirectBoost with AdaBoost , LogitBoost , soft margin LPBoost and BrownBoost on 10 UCI data sets _CITE_ from the UCI Machine Learning Repository [ 8 ]. We partition each UCI dataset into five parts with the same number of samples for five - fold cross validation . In each fold , we use three parts for training , one part for validation , and the remaining part for testing .__label__Material|Data|Use
The smoothness of the estimates is further indicating the viability of the approach , as the pitch estimates are frame - local . The algorithm was further evaluated on real room recordings that were also used in [ 17 ]. _CITE_ Two male speakers synchronously count in English and Spanish ( F3 = 16kHz ). The mixtures were degraded with noise ( SNR ∼ 20dB ). The filter length , the frame length , the order of the AR - process and the number of harmonics were set to L = 25 , T = 320 , p = 1 and K = 40 , respectively .__label__Method|Algorithm|Use
LMNN with energy - based classification obtains a test error rate of 3 . 7 %. The 20 - newsgroups data set consists of posted articles from 20 newsgroups , with roughly 1000 articles per newsgroup . We used the 18828 - version of the data set _CITE_ which has crosspostings removed and some headers stripped out . We tokenized the newsgroups using the rainbow package [ 10 ]. Each article was initially represented by the weighted word - counts of the 20 , 000 most common words .__label__Material|Data|Use
All points are averaged over 100 runs . We evaluate the performance of the proposed approach in two experiments , the first using a benchmark dataset for fairness , and the second on a real - world problem with churn and recall constraints . We compare training for fairness on the Adult dataset _CITE_ , the same dataset used by Zafar et al . [ 27 ]. The 32 561 training and 16 281 testing examples , derived from the 1994 Census , are 123 - dimensional and sparse .__label__Material|Data|Use
We subsampled the dataset to 1000 records . Bank . This dataset _CITE_ contains one record for each phone call in a marketing campaign ran by a Portuguese banking institution ( Moro et al . , 2014 )). Each record contains information about the client that was contacted by the institution .__label__Material|Data|Use
In the 22 - setting , for the sparse coding step we used a fast implementation of the LARS algorithm with positivity constraints [ 7 ] and the dictionary learning was done by solving a non - negative matrix factorization problem with additional sparsity constraints ( also known as the non - negative sparse coding problem [ 9 ]). A complete pseudo - code description is given in the full version of the paper [ 11 ]. _CITE_ Experimental Setup . All reported results are based on a Matlab implementation running on a quadcore 2 . 33 GHz Intel processor with 32GB RAM . The regularization parameter λ is set to 0 . 1 which yields reasonable sparsities in our experiments .__label__Supplement|Document|Produce
For example , for the quadratic loss function , in the standard WM and RWM algorithms , experts have no reason to misreport their beliefs ( see Proposition 8 ). This is not the case for other loss functions , such as the absolute loss function . _CITE_ The standard algorithm with the absolute loss function incentivizes extremal reporting , i . e . an expert reports 1 whenever b ( t ) i > 21 and 0 otherwise . This follows from a simple derivation or alternatively from results in the property elicitation literature .__label__Method|Algorithm|Introduce
In the case of / f Gaussian noise , any whitening transformation is equally likely and any value of beta is equally likely . Thus in this case , the algorithm cannot find the tree or the filters . We then ran experiments with a set of natural images [ 9 ] _CITE_ . These images contain natural scenes such as mountains , fields and lakes .. The data set was 50 , 000 patches , each 16 × 16 pixels large .__label__Material|Data|Use
3b and Fig . 3c ). We also implemented _CITE_ the distributed algorithms in Spark [ 9 ], an open - source cluster computing system . The DP - means and BP - means algorithms were initialized by pre - processing a small number of data points ( 1 / 16 of the first Pb points )— this reduces the number of data points sent to the master on the first epoch , while still preserving serializability of the algorithms . Our Spark implementations were tested on Amazon EC2 by processing a fixed data set on 1 , 2 , 4 , 8 m2 . 4xlarge instances .__label__Method|Code|Produce
For parameter selection , we use a = 10 − for A = aI and β = 0 . 15 for PageRank ( see Section 2 . 3 ) as suggested in [ 14 ]. The regularization parameter in Manifold Ranking is set to 0 . 99 , following [ 18 ]. The image benchmark USPS _CITE_ is used for this experiment , which contains 9298 images of handwritten digits from 0 to 9 of size 16 x 16 , with 1553 , 1269 , 929 , 824 , 852 , 716 , 834 , 792 , 708 , and 821 instances of each digit respectively . Each instance is used as a query and the mean average precision ( MAP ) is reported . The results are shown in Table 1 .__label__Material|Data|Use
Finally , the constraint ( 7 ) bounds the size of the vertex cover . We implemented both the combinatorial algorithm of Section 3 . 1 and the ILP formulation of Section 4 to benchmark the practical performance of the algorithms and test how good approximations bounded vertex cover DAGs provide . The combinatorial algorithm was implemented in Matlab and is available online _CITE_ . The ILPs were implemented using CPLEX Python API and solved using CPLEX 12 . The implementation is available as a part of TWILP software .__label__Method|Code|Produce
As O (| G |) supernodes need to be updated in each iteration of Algorithm 1 , this step ( which is the most expensive step in Algorithm 1 ) takes O (| G |· N log N ) time . The total time for Algorithm 1 is O ( k · | G |· N log N ). In this section , experiments are performed on a number of benchmark multilabel data sets _CITE_ , with both tree - and DAG - structured label hierarchies ( Table 1 ). As pre - processing , we remove examples that contain partial label paths and nodes with fewer than 10 positive examples . At each parent node , we then train a multitask lasso model with logistic loss using the MALSAR package [ 22 ].__label__Material|Data|Use
The T1 - weighted images were acquired for 176 sagittal slices with the following parameters : repetition time = 2300 ms , echo time = 2 . 98 ms , flip angle = 9 ◦, and voxel size = 1 × 1 × 1 mm . All the MR images were preprocessed by skull stripping [ 29 ], cerebellum removal , and then segmented into white matter ( WM ), gray matter ( GM ), and cerebrospinal fluid ( CSF ) tissues [ 20 ]. The anatomical automatic labeling atlas [ 27 ], parcellated with 90 predefined regions of interest ( ROI ), was registered using HAMMER _CITE_ [ 25 , 30 ] to each subject ’ s native space . We further added 8 more ROIs in basal ganglia and brainstem regions , which are clinically important ROIs for PD . We then computed WM , GM and CSF tissue volumes in each of the 98 ROIs as features .__label__Method|Tool|Use
On the small datasets , our approach is only competitive and does not dominate existing approaches , due to the lack of data to estimate scores well . However , we show that we can still use our PFS scheme as a pre - processing step to filter down the number of dimensions ; this step reduces the dimensionality , helps speed up existing FS methods from 3 - 5 times while keeps their accuracies . Large : TAC - KBP is a large data set with the number of samples and dimensions in the millions _CITE_ ; its domain is on relation extraction from natural language text . Medium : GISETTE and MADE ing function for the case of statistically dependent features . The top left point shows the scores for the 1st setting ; the middle points shows the scores for the 2nd setting ; and the bottom points shows the scores for the 3rd setting .__label__Material|Data|Use
We utilized several standard benchmark datasets from UCI datasets ( skin , winequality , census income , twitter , internet ad , energy heat , energy cool , communities ), libsvm datasets ( a1a , breast cancer ), and LIACC datasets ( abalone , kinematics , puma8NH , bank8FM ). Table 2 summarizes specifications for each dataset . For classification , we compared the global and local residual model ( Global / Local ) with L1 logistic regression ( Linear ), LSL - SP with linear discrimination analysis , LDKL supported by L2regularized hinge loss , FaLK - SVM with linear kernels _CITE_ , and C - SVM with RBF kernel . Note that C - SVM is neither a region - specific nor locally linear classification model ; it is , rather , non - linear . We compared it with ours as a reference with respect to a common non - linear classification model .__label__Method|Algorithm|Compare
Moreover , rotational invariance has been sought : ( i ) many data augmentation schemes have used rotated versions of images and ( ii ) models have been developed to learn this invariance , like the Spatial Transformer Networks [ 14 ]. Other explanations are the lack of experience on architecture design and the need to investigate better suited optimization or initialization strategies . The LeNet - 5 - like network architecture and the following hyper - parameters are borrowed from the TensorFlow MNIST tutorial _CITE_ : dropout probability of 0 . 5 , regularization weight of 5 x 10 − , initial learning rate of 0 . 03 , learning rate decay of 0 . 95 , momentum of 0 . 9 . Filters are of size 5 x 5 and graph filters have the same support of K = 25 . All models were trained for 20 epochs .__label__Supplement|Document|Use
We run experiments on various datasets and architectures , comparing COCOB with some popular stochastic gradient learning algorithms : AdaGrad [ Duchi et al ., 2011 ], RMSProp [ Tieleman and Hinton , 2012 ], Adadelta [ Zeiler , 2012 ], and Adam [ Kingma and Ba , 2015 ]. For all the algorithms , but COCOB , we select their learning rate as the one that gives the best training cost a posteriori using a very fine grid of values . We implemented _CITE_ COCOB ( following Algorithm 2 ) in Tensorflow [ Abadi et al ., 2015 ] and we used the implementations of the other algorithms provided by this deep learning framework . The best value of the learning rate for each algorithm and experiment is reported in the legend . We report both the training cost and the test error , but , as in previous work , e . g ., [ Kingma and Ba , 2015 ], we focus our empirical evaluation on the former .__label__Method|Code|Produce
A simple strategy to adopt is then to prune nodes in sequence : starting from the root node , the algorithm checks which children of a given node v should be pruned by creating the corresponding meta - instance and feeding the meta - classifier ; the child that maximizes the probability of the positive class is then pruned ; as the set of categories has changed , we recalculate which children of v can be pruned , prune the best one ( as above ) and iterate this process till no more children of v can be pruned ; we then proceed to the children of v and repeat the process . We start our discussion by presenting results on different hierarchical datasets with different characteristics using MLR and SVM classifiers . The datasets we used in these experiments are two large datasets extracted from the International Patent Classification ( IPC ) dataset _CITE_ and the publicly available DMOZ dataset from the second PASCAL large scale hierarchical text classification challenge ( LSHTC2 ) . Both datasets are multi - class ; IPC is single - label and LSHTC2 multi - label with an average of 1 . 02 categories per class . We created 4 datasets from LSHTC2 by splitting randomly the first layer nodes ( 11 in total ) of the original hierarchy in disjoint subsets .__label__Material|Data|Use
In this section , we will illustrate the ability of rKOPLS to discover relevant projections of the data . To do this , we compare the discriminative power of the features extracted by rKOPLS and KPLS2 in several multi - class classification problems . In particular , we include experiments on a benchmark of problems taken from the repository at the University of California Irvine ( UCI ) _CITE_ , and on a musical genre classification problem . This latter task is a good example of an application where rKOPLS can be specially useful , given the fact that the extraction of features from the raw audio data normally results in very large data sets of high dimensional data . We start by analyzing the performance of our method in six standard UCI multi - class classification problems .__label__Material|Data|Extent
This is naturally accomplished using our framework by computing [ a ∗ 1 ,. .. , a ∗ n ]& gt ; = arg maxαi & gt ; 0 f ( α ; K ) for fixed K = KLS ( G , σ , S ) and picking the sentences with the highest a ∗ i . We apply this method to the multi - document summarization task of DUC - 04 _CITE_ . We let Sij be the TF - IDF sentence similarity described by Lin & Bilmes [ 18 ], and let Qi = ( Ej Sij ) . State - of - theart systems , purpose - built for summarization , achieve around 0 . 39 in recall and F1 score [ 18 ].__label__Method|Algorithm|Use
The variance of the best - tuned SGD decreases , but this is due to the forced exponential decay of the learning rate and the variance of the gradients Vψi ( w ) ( the dotted line labeled as ‘ SGD - best / η ( t )’) stays high . Fig . 3 shows more convex - case results ( L2 - regularized logistic regression ) in terms of training loss residual ( top ) and test error rate ( bottom ) on rcv1 . binary and covtype . binary from the LIBSVM site , protein _CITE_ , and CIFAR - 10 . As protein and covtype do not come with labeled test data , we randomly split the training data into halves to make the training / test split . CIFAR was normalized into [ 0 , 1 ] by division with 255 ( which was also done with MNIST and CIFAR in the other figures ), and protein was standardized .__label__Material|Data|Use
Datasets . We apply the sparse gamma DEF on two different databases : ( i ) the Olivetti database at AT & T , which consists of 400 ( 320 for training and 80 for test ) 64 x 64 images of human faces in a 8 bit scale ( 0 — 255 ); and ( ii ) the collection of papers at the Neural Information Processing Systems ( nips ) 2011 conference , which consists of 305 documents and a vocabulary of 5715 effective words in a bag - of - words format ( 25 % of words from all documents are set aside to form the test set ). We apply the beta - gamma mf on : ( i ) the binarized mnist data , _CITE_ which consists of 28 x 28 images of hand - written digits ( we use 5000 training and 2000 test images ); and ( ii ) the Omniglot dataset ( Lake et al ., 2015 ), which consists of 105 x 105 images of hand - written characters from different alphabets ( we select 10 alphabets , with 4425 training images , 1475 test images , and 295 characters ). Evaluation . We apply mean - field vi and we compare g - rep with bbvi ( Ranganath et al ., 2014 ) and advi ( Kucukelbir et al ., 2016 ).__label__Material|Data|Use
This technology is useful because having a decomposition , into components for each device , of the total electricity usage in a household or building can be very informative to consumers and increase awareness of energy consumption which subsequently can lead to possibly energy savings . For full details regarding the energy disaggregation application see [ 7 , 10 , 11 ]. Next we consider a publicly available data set _CITE_ , called the Reference Energy Disaggregation Data Set ( REDD ) [ 11 ], to test the HB and BG sampling algorithms . The REDD data set contains several types of home electricity data for many different houses recorded during several weeks . Next , we will consider the main signal power of house_1 for seven days which is a temporal signal of length 604 , 800 since power was recorded every second .__label__Material|Data|Use
In order for coresets to be a worthwhile preprocessing step , it is critical that the time required to construct the coreset is small relative to the time needed to complete the inference procedure . We implemented the logistic regression coreset algorithm in Python . _CITE_ In Fig . 1a , we plot the relative time to construct the coreset for each type of dataset ( k = 6 ) versus the total inference time , including 10 , 000 iterations of the MCMC procedure described in Section 4 . 2 . Except for very small coreset sizes , the time to run MCMC dominates .__label__Method|Code|Produce
( ii ) D E Rp " p is a diagonal matrix whose entries are drawn independently from {− 1 , 11 . ( iii ) O E Rp " p is a unitary discrete Fourier tranansform ( DFT ) matrix . This formulations allows very fast implementations using the fast Fourier transform ( FFT ), for example using the popular FFTW package _CITE_ . Applying the FFT to a p − dimensional vector can be achieved in O ( p log T ) time . Similar structured random projections have gained popularity as a way to speed up [ 24 ] and robustify [ 27 ] large - scale linear regression and for distributed estimation [ 17 , 16 ].__label__Method|Code|Use
Our experiments suggest that the proposed template model is able to detect the meaningful parts and outperforms the previous work in terms of accuracy . We use kernel descriptors ( KDES ) to capture low - level image statistics : color , shape and texture [ 3 ]. In particular , we use four types of kernel descriptors : color - based , normalized color - based , gradientbased , and local - binary - pattern - based descriptors _CITE_ . Color and normalized color kernel descriptors are extracted over RGB images , and gradient and shape kernel descriptors are extracted over gray scale images transformed from the original RGB images . Following the standard parameter setting , we compute kernel descriptors on 16 x 16 image patches over dense regular grids with spacing of 8 pixels .__label__Method|Tool|Use
Indeed , by looking at the terms in Neurosynth , that are the closest to the one we use in this work , we find that motor is cited in 1090 papers , auditory 558 , word 660 , and the number goes as low as 55 and 31 for saccade and calculation respectively . Consequently , these databases may also yield inconsistent results . For instance , the reverse inference map corresponding to the term digits is empty , whereas the forward inference map is well defined _CITE_ . Neurosynth draws from almost 5K studies while our work is based on 19 studies ; however , unlike Neurosynth , we are able to benefit from the different contrasts and subjects in our studies , which provides us with 3 826 training samples . In this regard , our approach is particularly interesting and can hope to achieve competitive results with much less studies .__label__Material|Data|Introduce
We also performed additional experiments with 81 and 1024 dominant principal axes , corresponding to 18 % and 80 % coverage . Due to space constraints , we are unable to discuss the 1024 component model , other than to briefly mention that it conformed to the main results presented in this paper . To ensure replicable research , the source codes performing the experiments described in this paper have been made publicly available _CITE_ . In general , interpreting quadratic models can be difficult , and several strategies have been proposed in the literature ( see e . g . [ 12 ]).__label__Method|Code|Produce
[ 12 ], has size 17 , 702 x 19 and density 8 . 3 %. NOLA : This hidden matrix records the membership of 15 , 965 Facebook users from New Orleans across 92 different groups [ 22 ]. The density of this 0 – 1 matrix is 1 . 1 % _CITE_ . We start with an experiment that addresses the following question : “ Can entry - wise PDFs help us identify the values of the cells of the hidden matrix ?” To quantify this , we first look at the distribution of values of entry - wise PDFs per dataset , shown in Figure 2 ( a ) for the DBLP dataset ( the distribution of entry - wise PDFs is similar for the NOLA dataset ). The figure demonstrates that the overwhelming majority of the P ( i , j ) entries are small , smaller than 0 . 1 .__label__Material|Data|Introduce
The SFshop dataset contains 3 , 157 observations each consisting of a choice set of transportation alternatives available to individuals traveling to and returning from a shopping center , as well as a choice from that choice set . The SFwork dataset , meanwhile , contains 5 , 029 observations consisting of commuting options and the choice made on a given commute . Basic statistics describing the choice set sizes and the number of times each pair of alternatives appear in the same choice set appear in the Supplementary Materials _CITE_ . We train our model on observations Ttrain ~ C and evaluate on a test set Ttest ~ C via where ˆQ ( Ttrain ) is the estimate for Q obtained from the observations in Ttrain and ˜ piS ( Ttest ) = CiS ( Ttest )/ CS ( Ttest ) is the empirical probability of i was selected from S among observations in Ttest . Note that Error ( Ttrain ; Ttest ) is the expected ` 1 - norm of the difference between the empirical distribution and the inferred distribution on a choice set drawn uniformly at random from the observations in Ttest .__label__Supplement|Document|Produce
It might be because we can better estimate the statistics of each sample . We apply our method to a CNN [ 26 ] for MNIST using one of the Tensorflow tutorials . _CITE_ The dataset has high testing accuracy , so most of the examples are too easy for the model after a few epochs . Selecting more difficult instances can accelerate learning or improve testing accuracy [ 18 , 29 , 13 ]. The results from SGD - SD and SGD - WD confirm this finding while selecting uncertain examples can give us a similar or larger boost .__label__Supplement|Document|Use
The reads can be performed in parallel , and computing platforms such as MapReduce , Spark , distributed memory MPI , and GPUs can all achieve optimal parallel communication . For our implementation , we use Hadoop MapReduce for convenience . _CITE_ While all of the algorithms use sophisticated computation , these routines are only ever invoked with matrices of size n x n . Furthermore , the local memory requirements of these algorithms are only O ( n ). Thus , we get extremely scalable implementations . We note that , using MapReduce , computing GTX for the Gaussian projection technique is a simple variation of standard methods to compute XTX [ 4 ].__label__Method|Code|Produce
As a result , we can approximate any sufficiently smooth bounded function of u arbitrarily well , given sufficiently many training classification problems . To validate our method , we evaluated its ability to learn parameter functions on a variety of email and webpage classification tasks in which the number of classes , K , was large ( K = 10 ), and the number of number of training examples per class , m / K , was small ( m / K = 2 ). We used the dmoz Open Directory Project hierarchy , the 20 Newsgroups dataset , the Reuters - 21578 dataset , _CITE_ and the Industry Sector dataset . classifications using non - discriminative methods : the learned g , Naive Bayes , and TFIDF , respectively . Columns 5 - 7 give the corresponding values for the discriminative methods : softmax regression , 1 - vs - all SVMs , and multiclass SVMs .__label__Material|Data|Use
It essentially involves the stability of the 2 x 2 matrix ( 1 In this section we compare the performance of EASGD and with the parallel method DOWNPOUR and the sequential method SGD , as well as their averaging and momentum variants . All the parallel comparator methods are listed below3 : center variable as follows : zt + 1 = ( 1 + and = is a moving rate , and = t denotes the master clock , which is initialized to 0 and incremented every time the center variable is updated . where we compute the moving average of the center variable as follows : zt + 1 = ( 1 + and the moving rate was chosen to be constant , and = t denotes the master clock and is defined in the same way as have compared asynchronous ADMM [ 27 ] with EASGD in our setting as well , the performance is All the sequential comparator methods ( p = 1 ) are listed below : We perform experiments in a deep learning setting on two benchmark datasets : CIFAR - 10 ( we refer to it as CIFAR ) and ImageNet ILSVRC 2013 ( we refer to it as ImageNet ) _CITE_ . We focus on the image classification task with deep convolutional neural networks . We next explain the experimental setup .__label__Material|Data|Use
We now show the benefits of our approach on large - scale datasets , since we exploit the efficiency of random features with the performance of kernel - learning techniques . We perform experiments on three distinct types of datasets , tracking training / test error rates as well as total ( training + test ) time . For the adult _CITE_ dataset we employ the Gaussian kernel with a logistic regression model , and for the reuters dataset we employ a linear kernel with a ridge regression model . For the buzz dataset we employ ride regression with an arc - cosine kernel of order 2 , i . e . P0 = N ( 0 , I ) and φ ( x , w ) = H ( wTx ) ( w x ) , where H (·) is the Heavyside step function [ 7 ].__label__Material|Data|Use
j and k ). Implementation and Hardware All experiments were conducted on a computing cluster where each node has two 2 . 1 GHz 12 - core AMD 6172 processors with 48 GB physical memory per node . Our algorithms are implemented in C ++ using the Eigen library _CITE_ and compiled with the Intel Compiler . We downloaded Version 2 . 5 of the Tensor Toolbox , which is implemented in MATLAB . Since open source code for GigaTensor is not freely available , we developed our own version in C ++ following the description in [ 8 ].__label__Method|Code|Use
This is because each differentsized document can be represented as a separate RBM that has its own global normalizing constant . In this section we present experimental results on three three text datasets : NIPS proceedings papers , 20 - newsgroups , and Reuters Corpus Volume I ( RCV1 - v2 ) [ 10 ], and report generalization performance of Replicated Softmax and LDA models . The NIPS proceedings papers _CITE_ contains 1740 NIPS papers . We used the first 1690 documents as training data and the remaining 50 documents as test . The dataset was already preprocessed , where each document was represented as a vector containing 13 , 649 word counts .__label__Material|Data|Introduce
Spatial data . The second set of experiments consider the OS Terrain 50 dataset that contains spot heights of landscapes in Great Britain computed on a grid . _CITE_ A block of 200 × 200 points was split into 10 , 000 training examples and 30 , 000 interleaved testing examples . Mini - batches of data of size 750 and 1 , 000 arrive in spatial order . The first 1 , 000 examples were used as an initial training set .__label__Material|Data|Use
As mentioned in Sec . 1 , one important contribution of our method is its ability to use arbitrary deep learning generative and classification models . For the generative model , we use the C - GAN [ 22 ] _CITE_ , and for the classification model we rely on the ResNet18 [ 15 ] and ResNetpa [ 16 ]. The architectures of the generator and authenticator networks , which are kept unchanged for all three datasets , can be found in the supplementary material . For training , we use Adadelta ( with learning rate = 1 . 0 , decay rate = 0 . 95 and epsilon = 1e − 8 ) for the Classifier ( C ), Adam ( with learning rate 0 . 0002 , and exponential decay rate 0 . 5 ) for the Generator ( G ) and SDG ( with learning rate 0 . 01 ) for the Authenticator ( A ).__label__Method|Algorithm|Use
( 5 ) as well as the approximated true log - likelihood calculated using 5000 importance weighted samples , Eq . ( 6 ). The models were implemented using the Theano [ 20 ], Lasagne [ 5 ] and Parmesan _CITE_ frameworks . The source code is available at github For MNIST , we used a sigmoid output layer to predict the mean of a Bernoulli observation model and leaky rectifiers ( max ( x , 0 . 1x )) as nonlinearities in the MLP ’ s . The models were trained for 2000 epochs with a learning rate of 0 . 001 on the complete training set .__label__Method|Code|Use
It is well established that many web - related objects such as web directories and RSS directories admit a ( hierarchical ) categorization , and web directories aim to do this in a semi - automated fashion . For instance , it is desirable , when building a categorizer for the Yahoo ! directory , to take into account other web directories such as DMOZ _CITE_ . Although the tasks are clearly related , their label sets are not identical . For instance , some section heading and sub - headings may be named differently in the two directories .__label__Material|Data|Use
We subsampled the dataset to 1000 records . Census . This dataset _CITE_ contains the census records extracted from the 1994 US census ( Kohavi , 1996 ). Each record contains information about individuals including education , occupation , hours worked per week , etc .. We chose numeric attributes such as age , fnlwgt , education - num , capitalgain and hours - per - week to represents points in the Euclidean space and we aim to cluster the dataset so to balance gender . We subsampled the dataset to 600 records .__label__Material|Data|Use
Our solver was based on a combination of sub - gradient descent in both the matrices L and M , the latter used mainly to verify that we had reached the global minimum . We projected updates in M back onto the positive semidefinite cone after each step . Alternating projection algorithms provably converge [ 16 ], and in this case our implementation worked much faster than generic solvers _CITE_ . We evaluated the algorithm in the previous section on seven data sets of varying size and difficulty . Table 1 compares the different data sets .__label__Method|Code|Produce
Each computer ( input example ) is represented by 13 different computer characteristics ( RAM , cache , CPU , price etc .). Training and test sets were obtained by splitting the dataset into 75 % and 25 %, thus giving 15 examples for training and 5 examples for testing . School This dataset _CITE_ is from the Inner London Education Authority and consists of the examination scores of 15362 students from 139 schools in London . Here , each school corresponds to a task , thus a total of 139 tasks . The input consists of the year of the examination , 4 school - specific and 3 student - specific attributes .__label__Material|Data|Use
Different pollsters use different methodologies , reach different people , and may have sources of random errors , so generally the polls don ’ t fully agree with each other . Aggregators such as Nate Silver ’ s FiveThirtyEight , and The Upshot by the New York Times consolidate these different reports into a single prediction , and hopefully reduce random errors . _CITE_ FiveThirtyEight in particular has a solid track record for their predictions , and as they are transparent about their methodology we use them as a motivating example . To a first - order approximation , they operate as follows : first they take the predictions of all the different pollsters , then they assign a weight to each of the pollsters based on past performance ( and other factors ), and finally they use the weighted average of the pollsters to run simulations and make their own prediction . But could the presence of an institution that rates pollsters inadvertently create perverse incentives for pollsters ?__label__Method|Tool|Introduce
First , we consider a stochastic gradient descent scheme with mini - batches containing 100 triplets . Second , we use stepsizes of the form a /( 1 + k ) with k the iteration number and a a scalar ( common to all parameters ) optimized over a logarithmic grid on a validation set . _CITE_ Additionally , we cannot treat the NLP application ( see Sec . 8 ) as a standard tensor factorization problem . Indeed , in that case , we only have access to the positively labeled triplets P . Following [ 2 ], we generate elements in Af by considering triplets of the form {( i , j ', k )}, j ' =� j for each ( i , j , k ) E P . In practice , for each positive triplet , we sample a number of artificial negative triplets containing the same subject and object as our positive triplet but different verbs .__label__Material|Data|Use
More precisely , we extracted more advanced text features instead of simple term frequency ( TF ) features . For the images representation , we extracted SURF descriptors [ 21 ] and constructed a codebook of 100 visual words using the k - means clustering . For the text representation , we extracted 200 dimensional continuous word - vectors using a neural network skip - gram architecture [ 22 ] _CITE_ . To convert this word representation into a fixed - length sentence representation , we constructed a codebook of 100 word - vectors using again k - means clustering . We note that a more elaborate approach to transform word to sentence or document features has recently been developed [ 23 ], and we are planning to explore this in the future .__label__Method|Algorithm|Use
Here we use them for evaluating ranking performance . We compare our method against SVM for ranking ( e . g . [ 4 , 6 ]) using the SVM - light package 2 and an efficient Gaussian process method ( the informative vector machine ) _CITE_ [ 7 ]. These datasets were originally designed for regression , thus the continuous target values for each dataset were discretized into five equal size bins . We use these bins to define our ranking constraints : all the datapoints with target value falling in the same bin were grouped together .__label__Method|Algorithm|Use
The differences between inhomogenous , homogenous , and pairwise - relation based cuts are even more evident for large graphs and they may lead to significantly different partitioning performance in a number of important partitioning applications . The problem of inhomogeneous hypergraph clustering has not been previously studied in the literature . The main results of the paper are efficient algorithms for inhomogenous hypergraph partitioning with theoretical performance guarantees and extensive testing of inhomogeneous partitioning in applications such as hierarchical biological network studies , structure learning of rankings and subspace clustering _CITE_ ( All proofs and discussions of some applications are relegated to the Supplementary Material ). The algorithmic methods are based on transforming hypergraphs into graphs and subsequently performing spectral clustering based on the normalized Laplacian of the derived graph . A similar approach for homogenous clustering has been used under the name of Clique Expansion [ 14 ].__label__Method|Algorithm|Produce
Table 3 is a summary . AG ’ s news corpus . We obtained the AG ’ s corpus of news article on the web _CITE_ . It contains 496 , 835 categorized news articles from more than 2000 news sources . We choose the 4 largest classes from this corpus to construct our dataset , using only the title and description fields .__label__Material|Data|Use
The dataset is publicly available and has been extensively evaluated in various multi - task learning methods [ 4 , 7 , 30 ], where each task is defined as predicting the exam scores of students belonging to a specific school based on four student - dependent features ( year of the exam , gender , VR band and ethnic group ) and four school - dependent features ( percentage of students eligible for free school meals , percentage of students in VR band 1 , school gender and school denomination ). In order to compare with the above methods , we follow the same setup described in [ 3 , 4 ] and similarly we create dummy variables for those features that are categorical forming a total of 19 student - dependent features and 8 school - dependent features . We use the same 10 random splits _CITE_ of the data , so that 75 % of the examples from each school ( task ) belong to the training set and 25 % to the test set . On average , the training set includes about 80 students per school and the test set about 30 students per school . Scene and Yeast Data : We compare with the closely related nonparametric Bayesian methods [ 23 , 28 ], which were shown to outperform the independent Bayesian logistic regression and a singletask pooling approach [ 23 ], and a decoupled method MT - IBP + SVM that uses IBP factor analysis model to find shared latent features among multiple tasks and then builds separate SVM classifiers for different tasks .__label__Material|Data|Use
We then computed WM , GM and CSF tissue volumes in each of the 98 ROIs as features . 56 PD and 56 normal control ( NC ) subjects are used in our experiments . The second dataset is from Alzheimer ’ s disease neuroimaging initiative ( ADNI ) study _CITE_ , including MRI and FDG - PET data . For this experiment , we used 93 AD patients , 202 MCI patients and 101 NC subjects . To process the data , same tools employed in [ 29 ] and [ 32 ] are used , including spatial distortion , skull - stripping , and cerebellum removal .__label__Material|Data|Use
Our first example with real data is the sensor placements problem , where we try to select sensor locations to minimize the variance of observations . The dataset we used here is temperature measurements at discretized finite locations V obtained using the NIMS sensor node deployed at a lake near the University of California , Merced [ 9 , 12 ] (| V |= 86 ). _CITE_ As in [ 12 ], we evaluated the set of where Fs ( S ) = σ s − σ2s | S is the variance reduction and σ2s | S denote the predictive variance at location s E V after observing locations S C_ V . This function is monotone and submodular . The graphs in Fig .__label__Material|Data|Use
Medium : GISETTE and MADE ing function for the case of statistically dependent features . The top left point shows the scores for the 1st setting ; the middle points shows the scores for the 2nd setting ; and the bottom points shows the scores for the 3rd setting . LON are two largest data sets from the NIPS 2003 feature selection challenge _CITE_ , with the number of dimensions in the thousands . Small : Colon , Leukemia , Lymph , NCI9 , and Lung are chosen from the small Micro - array datasets [ 6 ], along with the UCI datasets . These sets typically have a few hundreds to a few thousands variables , with only tens of data samples .__label__Material|Data|Introduce
Datasets used are : ( 1 ) NIPS : Consists of 1 , 500 NIPS full papers , vocabulary of 2 , 000 words and mean document length 1023 . ( 2 ) NYT : Consists of a random subset of 30 , 000 documents from the New York Times dataset , vocabulary of 5 , 000 words and mean document length 238 . ( 3 ) Pubmed _CITE_ : Consists of a random subset of 30 , 000 documents from the Pubmed abstracts dataset , vocabulary of 5 , 030 words and mean document length 58 . ( 4 ) 20NewsGroup ( 20NG ): Consist of 13 , 389 documents , vocabulary of 7 , 118 words and mean document length 160 . To check the dominant topic and catchwords assumptions , we first run 1000 iterations of Gibbs sampling on the real corpus and learn the posterior document - topic distribution ({ W ., j }) for each document in the corpus ( by averaging over 10 saved - states separated by 50 iterations after the 500 burn - in iterations ).__label__Material|Data|Use
This is because the proposed method can capture the relationship between words and reflect the difference between documents across different domains by learning the latent vectors of the words . We performed experiments matching documents and tailgates , and matching images and tailgates with the datasets used in [ 3 ]. When matching documents and tailgates , we use datasets obtained from two social bookmarking sites , delicious _CITE_ and hatena , and patent dataset . The delicious and the hatena datasets include pairs consisting of a web page and a tag list labeled by users , and the patent dataset includes pairs consisting of a patent description and a tag list representing the category of the patent . Each web page and each patent description are represented as a bag - of - words as with the experiments using the Wikipedia dataset , and the tag list is represented as a set of tags .__label__Material|Data|Use
In this Section , we provide experimental results for our framework on data from remote sensing , and on a set of large text classification tasks with very many classes , the latter are hierarchical . We use the satimage remote sensing task from the statlog repository . _CITE_ This task has been used in the extensive SVM multi - class study of [ 5 ], where it is among the datasets on which the different methods show the most variance . It has n = 4435 training , m = 2000 test cases , and C = 6 classes . We use the isotropic Gaussian ( RBF ) kernel We compare the methods mc - sep ( ours with separate kernels for each class ; 12 hyperparameters ), mc - tied ( ours with a single shared kernel ; 2 hyperparameters ), ] rest ( one - against - rest : C binary classifiers are trained separately to discriminate c from the rest , they are voted by log probability upon prediction ; 12 hyperparameters ).__label__Material|Data|Extent
We split 5 , 000 training data of SVHN and CIFAR10 for validation if needed . On CIFAR10 , we follow [ 13 ] to perform ZCA for the input of C but still generate and estimate the raw images using G and D . We implement our method based on Theano [ 27 ] and here we briefly summarize our experimental settings . _CITE_ Though we have an additional network , the generator and classifier of Triple - GAN have comparable architectures to those of the baselines [ 26 , 25 ] ( See details in Appendix F ). The pseudo discriminative loss is not applied until the number of epochs reach a threshold that the generator could generate meaningful data . We only search the threshold in { 200 , 300 }, aP in { 0 . 1 , 0 . 03 } and the global learning rate in { 0 . 0003 , 0 . 001 } based on the validation performance on each dataset .__label__Method|Code|Produce
A simple strategy to adopt is then to prune nodes in sequence : starting from the root node , the algorithm checks which children of a given node v should be pruned by creating the corresponding meta - instance and feeding the meta - classifier ; the child that maximizes the probability of the positive class is then pruned ; as the set of categories has changed , we recalculate which children of v can be pruned , prune the best one ( as above ) and iterate this process till no more children of v can be pruned ; we then proceed to the children of v and repeat the process . We start our discussion by presenting results on different hierarchical datasets with different characteristics using MLR and SVM classifiers . The datasets we used in these experiments are two large datasets extracted from the International Patent Classification ( IPC ) dataset and the publicly available DMOZ dataset from the second PASCAL large scale hierarchical text classification challenge ( LSHTC2 ) _CITE_ . Both datasets are multi - class ; IPC is single - label and LSHTC2 multi - label with an average of 1 . 02 categories per class . We created 4 datasets from LSHTC2 by splitting randomly the first layer nodes ( 11 in total ) of the original hierarchy in disjoint subsets .__label__Material|Data|Use
We select the LPMRF models using all combinations of 20 log spaced A between 1 and 10 − , and 5 linearly spaced weighting function constants c between 1 and 2 for the weighting function described in Sec . 2 . 1 . In order to compare our algorithms with LDA , we also provide perplexity results using an LDA Gibbs sampler [ 13 ] for MATLAB _CITE_ to estimate the model parameters . For LDA , we used 2000 iterations and optimized the hyperparameters α and β using the likelihood of a tuning set . We do not seek to compare with many topic models because many of them use the Multinomial as a base distribution which could be replaced by a LPMRF but rather we simply focus on simple representative models .__label__Method|Tool|Use
On the other hand , LR is severely affected by the non - linearity of the level sets of the posterior probability distribution , and does not reach this limit ( best F1 - score of 48 . 9 %). Note also that even with this very large sample size , the SVM and LR classifiers are very different . The datasets we use are Adult ( binary classification , 32 , 561 / 16 , 281 train / test ex ., 123 features ), Letter ( single label multiclass , 26 classes , 20 , 000 ex ., 16 features ), and two text datasets : the 20 Newsgroups dataset News20 ( single label multiclass , 20 classes , 15 , 935 / 3 , 993 train / test ex ., 62 , 061 features , scaled version ) and Siam _CITE_ ( multilabel , 22 classes , 21 , 519 / 7 , 077 train / test ex ., 30 , 438 features ). All datasets except for News20 and Siam are obtained from the UCI repository . For each experiment , the training set was split at random , keeping 1 / 3 for the validation set used to select all hyper - parameters , based on the maximization of the F1 - measure on this set .__label__Material|Data|Use
Luckily , current GPUs , paired with a highly - optimized implementation of 2D convolution , are powerful enough to facilitate the training of interestingly - large CNNs , and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting . The specific contributions of this paper are as follows : we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC - 2010 and ILSVRC - 2012 competitions [ 2 ] and achieved by far the best results ever reported on these datasets . We wrote a highly - optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks , which we make available publicly _CITE_ . Our network contains a number of new and unusual features which improve its performance and reduce its training time , which are detailed in Section 3 . The size of our network made overfitting a significant problem , even with 1 . 2 million labeled training examples , so we used several effective techniques for preventing overfitting , which are described in Section 4 .__label__Method|Code|Produce
The goal of this task is to classify reviews for cellphones as positive or negative . 5 , 741 sentences were collected from an Web - BBS discussion about cellphones in which users were directed to submit positive reviews separately from negative reviews . Each sentence is represented in a word - based dependency tree using a Japanese dependency parser CaboCha _CITE_ . The task is to classify chemical compounds by carcinogenicity . We used the PTC data sets consisting of 417 compounds with 4 types of test animals : male mouse ( MM ), female mouse ( FM ), male rat ( MR ) and female rat ( FR ).__label__Method|Tool|Use
where each example may have more than one labels . Here , we maintained only a single label for each data point in order to apply standard multiclass classification . The maintained label was the first label appearing in each data entry in the repository files _CITE_ from which we obtained the data . Figure 3 displays convergence of the lower bounds ( and for the exact softmax cost ) for all methods . Recall , that the methods SOFT , OVE and BOUCHARD are non - stochastic and therefore their optimization can be carried out by standard gradient descent .__label__Supplement|Document|Use
We compare our MI Uncertainty ( MIU ) and Expected Gradient Length ( EGL ) selection strategies from Section 2 against two baselines : Uncertainty ( using only the instance - model ’ s uncertainty ), and instances chosen uniformly at Random from positive bags ( to evaluate the advantage of “ passively ” labeling instances ). The MILR model uses a = 2 . 5 for the softmax function and is trained by minimizing squared loss via L - BFGS [ 7 ]. The instance - labeled MI data sets and MI learning source code used in these experiments are available online _CITE_ . We evaluate our methods by constructing learning curves that plot the area under the ROC curve ( AUROC ) as a function of instances queried for each data set and selection strategy . The initial point in all experiments is the AUROC for a model trained on labeled bags from the training set without any instance queries .__label__Method|Code|Produce
We compare our MI Uncertainty ( MIU ) and Expected Gradient Length ( EGL ) selection strategies from Section 2 against two baselines : Uncertainty ( using only the instance - model ’ s uncertainty ), and instances chosen uniformly at Random from positive bags ( to evaluate the advantage of “ passively ” labeling instances ). The MILR model uses a = 2 . 5 for the softmax function and is trained by minimizing squared loss via L - BFGS [ 7 ]. The instance - labeled MI data sets and MI learning source code used in these experiments are available online _CITE_ . We evaluate our methods by constructing learning curves that plot the area under the ROC curve ( AUROC ) as a function of instances queried for each data set and selection strategy . The initial point in all experiments is the AUROC for a model trained on labeled bags from the training set without any instance queries .__label__Material|Data|Produce
G - RLSC uses the ” classical ” squared - loss as a classification loss criterion . The effectiveness of this criterion has been reported by the empirical results [ 13 ][ 14 ][ 15 ]. To evaluate the performance of G - RLS algorithm , empirical results are reported on text categorization tasks using the three datasets from CMU text mining group _CITE_ . The 7 - sectors dataset has 4 , 573 web pages belonging to seven economic sectors , with each sector containing pages varying from 300 to 1 , 099 . The 4 - universities dataset consists of 8 , 282 webpages collected mainly from four universities , in which the pages belong to seven classes and each class has 137 to 3 , 764 pages .__label__Material|Data|Use
We prevent gradients from ‘ exploding ’ as we backpropagate through time by truncating the length of gradients whose norm is above a threshold . For all models in this paper we consistently used hidden dimensionality of 200 and a mini - batch size of 100 . To facilitate research in DKTs we have published our code and relevant preprocessed data _CITE_ . The training objective for knowledge tracing is to predict a student ’ s future performance based on their past activity . This is directly useful – for instance formal testing is no longer necessary if a student ’ s ability undergoes continuous assessment .__label__Method|Code|Produce
We prevent gradients from ‘ exploding ’ as we backpropagate through time by truncating the length of gradients whose norm is above a threshold . For all models in this paper we consistently used hidden dimensionality of 200 and a mini - batch size of 100 . To facilitate research in DKTs we have published our code and relevant preprocessed data _CITE_ . The training objective for knowledge tracing is to predict a student ’ s future performance based on their past activity . This is directly useful – for instance formal testing is no longer necessary if a student ’ s ability undergoes continuous assessment .__label__Material|Data|Produce
For each image pair and keypoint detector the highest and second highest recognition rates are bolded with the second highest rate prefixed by an asterisk . We use a subset of the commonly used benchmarking dataset used in [ 17 ]. _CITE_ Our subset consists of the image pairs that do not undergo extreme affine warping since neither BRIEF nor LUCID are plotted as a function of the width of the descriptor patch and of the blur kernel applied . The best 100 of 300 FAST keypoints were detected in the first image of each pair . We found a blur width of 5 rarely hurts performance and often helps .__label__Material|Data|Use
SGD - SPV and SGD - SE perform significantly better than SGD - Uni here , consistent with the idea that avoiding difficult examples increases robustness to outliers . For CIFAR 100 [ 24 ], we demonstrate that the proposed approaches can also work in very deep residual networks [ 16 ]. _CITE_ To show the method is not sensitive to the network depth and the number of burn - in epochs , we present results from the network with 27 layers and 90 burn - in epochs as well as the network with 63 layers and 50 burn - in epochs . Without changing architectures , emphasizing uncertain or easy examples gains around 0 . 5 % in both settings , which is significant considering the fact that the much deeper network shows only 3 % improvement here . When training a neural network , gradually reducing the learning rate ( i . e ., the magnitude of gradients ) usually improves performance .__label__Method|Algorithm|Extent
Note that while the BNBP sampler in ( 12 ) is fully collapsed , the direct assignment sampler of the HDP - LDA in ( 14 ) is only partially collapsed as neither the globally shared Dirichlet process Ge nor the concentration parameter α are marginalized out . To derive a collapsed sampler for the HDP - LDA that marginalizes out Ge ( but still not α ), one has to use the Chinese restaurant franchise [ 6 ], which has cumbersome book - keeping as each word is indirectly linked to its topic via a latent table index . We consider the JACM _CITE_ , PsyReview , and NIPS12 corpora , restricting the vocabulary to terms that occur in five or more documents . The JACM corpus includes 536 documents , with V = 1539 unique terms and 68 , 055 total word counts . The PsyReview corpus includes 1281 documents , with V = 2566 and 71 , 279 total word counts .__label__Material|Data|Use
In other words , roughly 0 . 5 % entries out of the whole pairwise label matrix Z ∈ { 0 , 1 } 1000 × 1000 are observed . We show the ground - truth pairwise label matrix , the similarity matrix and the estimated label matrix in Figure 1 , which clearly demonstrates that the recovered label matrix is more accurate than the perturbed similarities . We further evaluate the performance of our algorithm on three real - world data sets : splice [ 24 ] 3 , gisette [ 12 ] 4 and citeseer [ 21 ] _CITE_ . The splice is a DNA sequence data set for recognizing the splice junctions . The gisette is a perturbed image data for handwritten digit recognition , which is originally constructed for feature selection .__label__Material|Data|Use
The description can be in terms of attributes [ 17 ], WordNet hierarchy [ 21 , 25 ], semantic class label graph [ 10 , 24 ], or text data [ 8 , 22 ]. These learning scenarios are also related to the open set one [ 2 ] where new classes grow continuously . For transfer learning , we follow [ 21 ] to train our feature embeddings and a Nearest Class Mean ( NCM ) classifier [ 21 ] on the large - scale ImageNet 2010 _CITE_ dataset , which contains 1 , 000 classes and more than 1 . 2 million images . Then we apply the NCM classifier to the larger ImageNet - 10K [ 5 ] dataset with 10 , 000 classes , thus do not use any auxiliary knowledge such as parts and attributes . We use the standard flat top - 1 accuracy as the classification evaluation metric .__label__Material|Data|Use
There were two groups of three individual rhythmic sequences for each cultural type of rhythm as shown in Table 1 . With three combinations within each group and two possible pitch assignments , this resulted in six rhythmic stimuli for each group , 12 per rhythm type and 24 in total . _CITE_ Finally , rhythmic stimuli could be played back at one of two tempi , having a minimum inter - onset interval of either 180 or 240ms . Furthermore , we also formed groups based on how these stimuli were created . These allowed a more coarse classification with fewer classes .__label__Supplement|Document|Produce
This representation enables us to train two logistic regression classifiers , both with small loss on the labeled data set , while satisfying two constraints to ensure feature decomposition and e - expandability . Our final classifier has the weight vector w = u + v . We refer to the resulting algorithm as CODA ( Cotraining for Domain Adaptation ), which can be stated concisely with the following optimization problem : The optimization is non - convex . However , as it is not particularly sensitive to initialization , we set u , v randomly and optimize with standard conjugate gradient descent _CITE_ . Due to space constraints we do not include a pseudo - code implementation of CODA . The implementation is essentially identical to that of SEDA ( Algorithm 1 ) where the above optimization problem is solved instead of eq .__label__Method|Algorithm|Use
For each article in the above data sets , we remove stop words and use tf - idf to choose the top 10 , 000 distinct words ( 14 , 000 for arXiv ) as the vocabulary . We implemented the batch and stochastic algorithms for CTPF in 4500 lines of C ++ code . _CITE_ Competing methods . We study the predictive performance of the following models . With the exception of the Poisson factorization [ 9 ], which does not model content , the topics and topic intensities ( or proportions ) in all CTPF models are initialized using LDA [ 2 ], and fit using batch variational inference .__label__Method|Code|Produce
Larger supports can be obtained by allowing the use of second and higher order neighbors of the parent for prediction . A variety of ways for optimizing auto - encoders are available , we refer the reader to the recent paper [ 15 ] and references therein . In our setting , due to the relatively small size of the training set and sparse inter - connectivity between the layers , an off - the - shelf L - BFGS _CITE_ unconstrained smooth optimization package works very well . In order to make our problem unconstrained , we avoid imposing the equation P ~ 1 = 0 ~ as a hard constraint , but in each row of P ( which corresponds to some active region ), the weight corresponding to the parent is eliminated . To obtain a smooth objective , we use L1 norm with soft absolute value s ( x ) = -,/ e + x ti | x |, where we set c = 10 − .__label__Method|Tool|Use
Note that while the BNBP sampler in ( 12 ) is fully collapsed , the direct assignment sampler of the HDP - LDA in ( 14 ) is only partially collapsed as neither the globally shared Dirichlet process Ge nor the concentration parameter α are marginalized out . To derive a collapsed sampler for the HDP - LDA that marginalizes out Ge ( but still not α ), one has to use the Chinese restaurant franchise [ 6 ], which has cumbersome book - keeping as each word is indirectly linked to its topic via a latent table index . We consider the JACM , PsyReview , and NIPS12 _CITE_ corpora , restricting the vocabulary to terms that occur in five or more documents . The JACM corpus includes 536 documents , with V = 1539 unique terms and 68 , 055 total word counts . The PsyReview corpus includes 1281 documents , with V = 2566 and 71 , 279 total word counts .__label__Material|Data|Use
See Appendix C . 5 for a discussion on the orders . Algorithm 1 Joint diagonalization ( JD ) algorithm for GP / DICA cumulants ( or LDA moments ) In this section , ( a ) we compare experimentally the GP / DICA cumulants with the LDA moments and ( b ) the spectral algorithm [ 3 ], the tensor power method [ 4 ] ( TPM ), the joint diagonalization ( JD ) algorithm from Algorithm 1 , and variational inference for LDA [ 1 ]. Real data : the associated press ( AP ) dataset , from D . Blei ’ s web page , _CITE_ with N = 2 , 243 documents and M = 10 , 473 vocabulary words and the average document length Lb = 194 ; the NIPS papers dataset [ 28 ] of 2 , 483 NIPS papers and 14 , 036 words , and Lb = 1 , 321 ; the KOS dataset , from the UCI Repository , with 3 , 430 documents and 6 , 906 words , and Lb = 136 . Semi - synthetic data are constructed by analogy with [ 29 ]: ( 1 ) the LDA parameters D and c are learned from the real datasets with variational inference and ( 2 ) toy data are sampled from a model of interest with the given parameters D and c . This provides the ground truth parameters D and c . For each setting , data are sampled 5 times and the results are averaged . We plot error bars that are the minimum and maximum values .__label__Material|Data|Use
See Appendix C . 5 for a discussion on the orders . Algorithm 1 Joint diagonalization ( JD ) algorithm for GP / DICA cumulants ( or LDA moments ) In this section , ( a ) we compare experimentally the GP / DICA cumulants with the LDA moments and ( b ) the spectral algorithm [ 3 ], the tensor power method [ 4 ] ( TPM ), the joint diagonalization ( JD ) algorithm from Algorithm 1 , and variational inference for LDA [ 1 ]. Real data : the associated press ( AP ) dataset , from D . Blei ’ s web page , _CITE_ with N = 2 , 243 documents and M = 10 , 473 vocabulary words and the average document length Lb = 194 ; the NIPS papers dataset [ 28 ] of 2 , 483 NIPS papers and 14 , 036 words , and Lb = 1 , 321 ; the KOS dataset , from the UCI Repository , with 3 , 430 documents and 6 , 906 words , and Lb = 136 . Semi - synthetic data are constructed by analogy with [ 29 ]: ( 1 ) the LDA parameters D and c are learned from the real datasets with variational inference and ( 2 ) toy data are sampled from a model of interest with the given parameters D and c . This provides the ground truth parameters D and c . For each setting , data are sampled 5 times and the results are averaged . We plot error bars that are the minimum and maximum values .__label__Supplement|Website|Use
Question Type ( QT ) [ 28 ]), which contains 1000 training examples and 500 testing examples . We use the CNN architecture proposed by Kim [ 22 ]. _CITE_ Like many other NLP tasks , the dataset is relatively small and this CNN classifier does not inject noise to inputs like the implementation of residual networks in CIFAR 100 , so this complicated model reaches 100 % training accuracy within a few epochs . To address this , we reduced the model complexity by ( i ) decreasing the number of filters from 128 to 64 , ( ii ) decreasing convolutional filter widths from 3 , 4 , 5 to 2 , 3 , 4 , ( iii ) adding L2 regularization with scale 0 . 01 , ( iv ) performing PCA to reduce the dimension of pre - trained word embedding from 300 to 50 and fixing the word embedding during training . Then , the proposed active bias methods perform better than other baselines in this smaller model .__label__Method|Algorithm|Use
Of the players that did not fold , the player with the highest ranked poker hand wins all of the bets . Full rules can be found on - line . _CITE_ We focus on the Limit Hold ’ em variant that fixes the bet sizes and the number of bets allowed per round . We denote the players ’ actions as f ( fold ), c ( check or call ), and r ( bet or raise ). Leduc Hold ’ em [ 10 ] ( or simply Leduc ) is a smaller version of Hold ’ em , played with a six card deck consisting of two Jacks , two Queens , and two Kings with only two betting rounds , pre - flop and flop .__label__Supplement|Document|Introduce
We evaluate our method on a series of challenging simulated robotics tasks described below . We would like to emphasize that the demonstrations consist of shuffled state - action pairs such that no temporal information or segmentation is used during learning . The performance of our method can be seen in our supplementary video _CITE_ . Reacher The Reacher environment is depicted in Fig . 2 ( left ).__label__Supplement|Media|Produce
Finally , section 6 discusses future work and states our conclusions . Tools for statically analyzing binary executables differ in the details of their workings but they all share the same high level logic , which is called recursive disassembly . _CITE_ The tool starts by obtaining the address of the first instruction from a specific location inside the executable . It then places this address on a stack and executes the following steps while the stack is non - empty . It takes the next address from the stack and disassembles ( i . e .__label__Method|Tool|Introduce
2 the clustering performance of each dataset under each normalization scheme under varying kernel setting ( Q and d values ). Generally , the performance of the Frobenius normalization behaves in a smoother manner and is more stable under varying kernel settings than the other normalization schemes . Our next set of experiments was over some well studied cancer data - sets _CITE_ . The data - sets are listed in Table 2 together with some of their characteristics . The column ”# PC ” refers to the number of principal components used in a PCA pre - processing for the purpose of dimensionality reduction prior to clustering .__label__Material|Data|Use
We can formulate the program in a standard form by introducing the index sets Denoting the m - th row of Y by y — m and the m - th row of V by v — m , one can equivalently rewrite ( 11 ) in terms of u as where Aside from the variety of convex solvers that can be used to address ( 14 ), we are specifically interested in using the alternating direction method of multipliers ( ADMM ). In fact the main motivation to translate ( 11 ) into ( 14 ) is the availability of ADMM implementations for problems in the form of ( 14 ) that are reasonably fast and scalable ( e . g ., see [ 17 ]). The authors have made the implementation publicly available online _CITE_ . Aside from the major technical contribution of the paper in providing a theoretical understanding of the Net - Trim pruning process , in this section we present some experiments to highlight its performance against the state of the art techniques . where The first set of experiments associated with the example presented in the introduction ( classification of 2D points on nested spirals ) compares the Net - Trim pruning power against the standard pruning strategies of Bl regularization and Dropout .__label__Method|Code|Use
We then find the true path in the parsing graph ( i . e . a sequence of activated random variables ) by matching the detected characters to the ground truth segmentation . In total , we used 630 images for training the parser using PyStruct _CITE_ . Shape Model Invariance Study We studied the invariance of our model by testing on transformations of the training images . We considered scaling and rotation .__label__Method|Tool|Use
In this subsection , we foll ow Gong et al . [ 19 ] to consider Sparse LR with a We compare monotone APG ( mAPG ) and nonmonotone APG ( nmAPG ) with monotone GIST ( mGIST ), nonmonotone GIST ( nmGIST ) [ 19 ] and IFB [ 22 ]. We test the performance on the real - sim data set _CITE_ , which contains 72309 samples of 20958 dimensions . We follow [ 19 ] to set λ = 0 . 0001 , θ = 0 . 1λ and the starting point as zero vectors . In nmAPG we set η = 0 . 8 .__label__Material|Data|Use
We evaluate our approach by training a factor graph for solving the ontology alignment problem . Ontology alignment is the problem of mapping concepts from one ontology to semantically equivalent concepts from another ontology ; our treatment of the problem involves learning a first - order probabilistic model that clusters concepts into semantically equivalent sets . For our experiments , we use the the dataset provided by the Illinois Semantic Integration Archive ( ISIA ) _CITE_ . There are two ontology mappings : one between two course catalog hierarchies , and another between two company profile hierarchies . Each ontology is organized as a taxonomy tree .__label__Material|Data|Use
However , the stable region can be highly complex , and there may be several folds and boundaries of the stable region in the interpolated area . In our experiments ( not shown ), interpolating from the LB - 1 solution yielded worse results . For learning the dynamics matrix , we implemented _CITE_ least squares , constraint generation ( using quadprog ), LB - 1 [ 1 ] and LB - 2 [ 2 ] ( using CVX with SeDuMi ) in Matlab on a 3 . 2 GHz Pentium with 2 GB RAM . Note that these algorithms give a different result from the basic least - squares system identification algorithm only in situations where the least - squares model is unstable . However , least - squares LDSs trained in scarce - data scenarios are unstable for almost any domain , and some domains lead to unstable models up to the limit of available data ( e . g .__label__Method|Code|Produce
[ 11 ], but ours would be more correctly termed “ brightness normalization ”, since we do not subtract the mean activity . Response normalization reduces our top - 1 and top - 5 error rates by 1 . 4 % and 1 . 2 %, respectively . We also verified the effectiveness of this scheme on the CIFAR - 10 dataset : a four - layer CNN achieved a 13 % test error rate without normalization and 11 % with normalization _CITE_ . Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map . Traditionally , the neighborhoods summarized by adjacent pooling units do not overlap ( e . g ., [ 17 , 11 , 4 ]).__label__Material|Data|Use
Foursquare : the Foursquare data set [ 14 ] contains users ’ check - in records in Pittsburgh area categorized by different venue types such as Art & University . It records the number of check - ins by 121 users in each of the 15 category of venues over 1200 time intervals . Meteo - UK : The data set is collected from the meteorological office of the UK _CITE_ . It contains monthly measurements of 5 variables in 16 stations across the UK from 1960 to 2000 . The forecasting task consists in predicting all variables at times t + 1 ,... , t + k from their values at times t − 2 , t − 1 and t . The first two real data sets were used in [ 2 ] with k = 1 ( i . e .__label__Material|Data|Introduce
The artificial views were produced using PORTAGE , a statistical machine translation system developed at NRC [ 15 ]. Each document from the comparable corpus was thus translated to the other 4 languages . _CITE_ For each class , we set up a binary classification task by using all documents from that class as positive examples , and all others as negative . We first present experimental results obtained in supervised learning , using various amounts of labeled examples . We rely on linear SVM models as base classifiers , using the SVM - Perf package [ 8 ].__label__Material|Data|Use
Acute leukemias can be roughly divided into two groups , acute myeloid leukemia ( AML ) and acute lymphoblastic leukemia ( ALL ) where the latter can furthermore be subdivided into B - cell ALL and T - cell ALL . Golub et al . used a data set of 72 leukemia samples ( 25 AML , 47 ALL of which 38 are B - cell ALL samples ) _CITE_ . For each sample , gene expression was monitored using Affymetrix expression arrays . We apply the preprocessing steps as in Golub et al .__label__Material|Data|Use
For the supervised metric learning results we first reduce the dimensionality of each representation to 200 dimensions ( if necessary ) with PCA and then run either NCA , ITML , or LMNN on the projected data . We tune all free hyperparameters in all compared methods with Bayesian optimization ( BO ), using the implementation of Gardner et al . [ 13 ] _CITE_ . kNN classification . We show the kNN test error of all document representation and distance methods in Table 2 .__label__Method|Code|Use
We refer to our approach as SetInEq and normal forms as Normal . We also compared with PTP [ 11 ] available in Alchemy 2 and GCFVOE [ 25 ] system . _CITE_ Both our systems and GCFOVE are implemented in Java . PTP is implemented in C ++. We experimented on four benchmark MLN domains for calculating the partition function using exact as well as approximate inference .__label__Method|Tool|Compare
These experiments demonstrate the feasibility of our method – a sanity check that the reduction does in fact preserve learnability – and compare different compression and reconstruction options . Image data . _CITE_ The first data set was collected by the ESP Game [ 24 ], an online game in which players ultimately provide word tags for a diverse set of web images . The set contains nearly 68000 images , with about 22000 unique labels . We retained just the 1000 most frequent labels : the least frequent of these occurs 39 times in the data , and the most frequent occurs about 12000 times .__label__Material|Data|Use
These experiments demonstrate the feasibility of our method – a sanity check that the reduction does in fact preserve learnability – and compare different compression and reconstruction options . Image data . _CITE_ The first data set was collected by the ESP Game [ 24 ], an online game in which players ultimately provide word tags for a diverse set of web images . The set contains nearly 68000 images , with about 22000 unique labels . We retained just the 1000 most frequent labels : the least frequent of these occurs 39 times in the data , and the most frequent occurs about 12000 times .__label__Supplement|Media|Use
We also experimented with the reduced set of 48 phones used by [ 35 ]. The code for all of our experiments is publicly available . _CITE_ Figure 1a shows frame classification accuracy using features from different layers of the DeepSpeech2 model . The results are all above a majority baseline of 7 . 25 % ( the phone “ s ”). Input features ( spectrograms ) lead to fairly good performance , considering the 60 - wise classification task .__label__Method|Code|Produce
We set R to take 20 roundings under low budgets and 80 under high ones , and the remaining budget goes towards LRPk updates . Datasets Each dataset comprises a family of binary pairwise MRFs . The sets seg , dbn , and grid40 are from the PASCAL 2011 Probabilistic Inference Challenge _CITE_ — seg are small segmentation models ( 50 instances , average 230 variables , 622 edges ), dbn are deep belief networks ( 108 instances , average 920 variables , 54160 edges ), and grid40 are 40x40 grids ( 8 instances , 1600 variables , 6240 or 6400 edges ) whose edge weights outweigh their unaries by an order of magnitude . The chain set comprises 300 randomly generated 20 - node chain MRFs with no unary potentials and random unit - Gaussian edge weights – it is principally an extension of the coupling two - node example ( Figure 2 ), and serves as a structural obverse to grid40 in that it lacks cycles entirely . Among these , the dbn set comprises the largest and most edge - dense instances .__label__Material|Data|Use
5 . 3 ) of the persistence diagrams in combination with a linear SVM . Implementation . All experiments were implemented in PyTorch _CITE_ , using DIPHA and Perseus [ 23 ]. Source code is publicly - available at https :// github . com / c - hofer / nips2017 . We apply persistent homology combined with our proposed input layer to two different datasets of binary 2D object shapes : ( 1 ) the Animal dataset , introduced in [ 3 ] which consists of 20 different animal classes , 100 samples each ; ( 2 ) the MPEG - 7 dataset which consists of 70 classes of different object / animal contours , 20 samples each ( see [ 21 ] for more details ).__label__Method|Code|Use
The best global factor is determined by crossvalidation . These results suggest that VRM with adaptive kernel widths can outperform state of the art classifiers on small training sets . b ) MNIST " 1 " versus other digits — A second test was performed using the MN1ST handwritten digits _CITE_ . We considered the sub - problem of recognizing the ones versus all other digits . The testing set contains 10000 digits ( 5000 ones and 5000 non - ones ).__label__Material|Data|Use
The method is parameterized with : ( 1 ) cr the cost range , ( 2 ) g the number of desired groups as a percentage of the number of attributes , and ( 3 ) sc the group shared cost as a percentage of the maximal marginal cost in the group . Using this method we assigned costs to 25 datasets : 21 arbitrarily chosen UCI datasets and 4 datasets that represent hard concept and have been used in previous research . The online appendix _CITE_ gives detailed descriptions of these datasets . Two versions of each dataset have been created , both with cost range of 1 - 100 . In the first g and sc were set to 20 % and in the second they were set to 80 %.__label__Supplement|Document|Produce
We draw problem instances from each domain by generating pairs of random initial states and goal conditions . The goal conditions specify block configurations involving all blocks in blocks worlds , and destinations for all packages in logistics worlds . _CITE_ Throughout , we use the domain - independent FF heuristic [ 13 ]. Each experiment specifies a planning domain and an initial policy and then iterates API until “ no more progress ” is made . We evaluate each policy on 1000 random problem instances , recording the success We use discount factor 1 and select large enough horizons to accurately rank most policies : 4xn for BW ( n ) and SBW ( n ), 6xn for SPW ( n ), 12xp for LW ( l , t , p ) and SLW ( l , c , t , p ).__label__Method|Algorithm|Use
This simplifies inference and learning as we can now use the linear - time inference and learning procedures known for PSDDs [ Kisa et al ., 2014a ]. In our experiments , we considered a dataset consisting of GPS data collected from taxicab routes in San Francisco . _CITE_ We acquired public map data from http :// www . openstreetmap . org /, i . e ., the undirected graph representing the streets ( edges ) and intersections ( nodes ) of San Francisco . We projected the GPS data onto the San Francisco graph using the map - matching API of the graphhopper package . For more on map - matching , see , e . g ., [ Froehlich and Krumm , 2008 ].__label__Material|Data|Use
In each dataset , we randomly select 300 images and each image is resized to the scale of 12 × 12 . Eight settings of CS ratios are adopted with resents an entire image which generally has unique statistics , it is infeasible to find suitable training data in practice . Therefore , GMM - TP and KSVD - OMP are not compared to in this experiment _CITE_ . For PLE , Sparse - GMM and Sparse - GMM ( G ), the minimum - norm estimates from the measurements , � xi = arg min . { kxk 2 : Φix = yi } = Φ i ( ΦiΦ i )− yi , i = 1 , ... , N , are used to initialize the GMM .__label__Material|Data|Use
Finally , section 6 discusses future work and states our conclusions . Tools for statically analyzing binary executables differ in the details of their workings but they all share the same high level logic , which is called recursive disassembly . _CITE_ The tool starts by obtaining the address of the first instruction from a specific location inside the executable . It then places this address on a stack and executes the following steps while the stack is non - empty . It takes the next address from the stack and disassembles ( i . e .__label__Method|Tool|Introduce
The MKL experiments reported in Table 1 took less than 1 minute ( for each graph family ); while the algorithm in [ 14 ] aborts after several hours due to memory constraints . Planted clique recovery We generate 100 random graphs based on planted clique model G ( n , 1 / 2 , k ) where n = 30000 and hidden clique size k = 2t √ n for each choice of t . We evaluate the recovery algorithm discussed in Section 4 . 2 . The SVM problem is solved using Libsvm _CITE_ . For t ≥ 2 we find perfect recovery of the clique on all the graphs , which is agreement with Theorem 5 . 1 . It is worth noting that the approach takes 10 minutes to recover the clique in this graph of 30000 vertices which is far beyond the scope of SDP based procedures .__label__Method|Tool|Use
It is worth noting that the number of iterations to reach a target precision of ε means that − f ( αk ) − minαEQ1 − f ( α ) = maxαEQ1 f ( α ) − f ( αk ) ≤ ε . However , this does not mean the dual gap as used in [ 15 ] is less than ε . In [ 15 ], the objective function is smoothed by adding a quadratic term and then they further proposed a projected gradient algorithm and analytic center cutting plane method ( ACCPM ) _CITE_ . As proved in Theorem 3 , the number of iterations of the projected gradient method is usually O ( L / e ). In each iteration , the main complexity cost O ( n ) is from the eigen - decomposition .__label__Method|Algorithm|Introduce
Some of the models were fine - tuned by continuing training for 2000 epochs while multiplying the learning rate with 0 . 75 after every 200 epochs and increase the number of Monte Carlo and importance weighted samples to 10 to reduce the variance in the approximation of the expectations in Eq . ( 4 ) and improve the inference model , respectively . Models trained on the OMNIGLOT dataset _CITE_ , consisting of 28x28 binary images images were trained similar to above except that the number of training epochs was 1500 . Models trained on the NORB dataset , consisting of 32x32 grays - scale images with color - coding rescaled to [ 0 , 1 ], used a Gaussian observation model with mean and variance predicted using a linear and a softplus output layer respectively . The settings were similar to the models above except that hyperbolic tangent was used as nonlinearities in the MLP ’ s and the number of training epochs was 2000 .__label__Material|Data|Use
We used each Amazon product review domain as a sentiment classification task ( 6 datasets ). Spam : We selected three task A users from the ECML / PKDD Challenge , using bag - ofwords to classify each email as spam or ham ( 3 datasets ). For OCR data we binarized two well known digit recognition datasets , MNIST _CITE_ and USPS , into 45 all - pairs problems . We also created ten one vs . all datasets from the MNIST data ( 100 datasets total ). Each result for the text datasets was averaged over 10 - fold cross - validation .__label__Material|Data|Use
Implementation Details . The proposed AMH - Net is implemented under the deep learning framework Caffe [ 18 ]. The implementation code is available on Github _CITE_ . The training and testing phase are carried out on an Nvidia Titan X GPU with 12GB memory . The ResNet50 network pretrained on ImageNet [ 8 ] is used to initialize the front - end CNN of AMH - Net .__label__Method|Code|Produce
Social media and social networking sites are increasingly used by people to express their opinions , give their “ hot takes ”, on the latest breaking news , political issues , sports events , and new products . As a consequence , there has been an increasing interest on leveraging social media and social networking sites to sense and forecast opinions , as well as understand opinion dynamics . For example , political parties routinely use social media to sense people ’ s opinion about their political discourse _CITE_ ; quantitative investment firms measure investor sentiment and trade using social media [ 18 ]; and , corporations leverage brand sentiment , estimated from users ’ posts , likes and shares in social media and social networking sites , to design their marketing campaigns . In this context , multiple methods for sensing opinions , typically based on sentiment analysis [ 21 ], have been proposed in recent years . However , methods for accurately forecasting opinions are still scarce [ 7 , 8 , 19 ], despite the extensive literature on theoretical models of opinion dynamics [ 6 , 9 ].__label__Supplement|Document|Introduce
When using a linear kernel and using e ( x ) = R ( x ) = 11x11 2 , the optimization problem ( 27 ) becomes a linearly constrained quadratic optimization problem for which a unique solution exists due to the convexity of the objective function : Unlike other SVM - like methods for ranking that need a O ( m ) number of slack variables y our formulation only require one slack variable for example , only m slack variables are used , giving our formulation computational advantage over ranking methods . Next , we demonstrate the effectiveness of our algorithm by comparing it to two state - of - the - art algorithms . We test tested our approach in a set of nine publicly available datasets _CITE_ shown in Tab . 1 ( several large datasets are not reported since only the algorithm presented in this paper was able to run them ). These datasets have been frequently used as a benchmark for ordinal regression methods ( e . g .__label__Material|Data|Use
The VGG network , which contains 11 - 19 weight layers depending on the typical architecture [ 3 ], takes 2 to 3 weeks on a system equipped with 4 NVIDIA Titan Black GPUs for training a single net . The residual network ResNet , which achieved state - of - the - art results in image classification and detection in 2015 [ 4 ], takes 3 . 5 days for the 18 - layer model and 14 days for the 101 - layer model using 4 NVIDIA Kepler GPU . _CITE_ Could we evaluate a network structure without taking a long time to train it ? There are some prior works to deal with this issue but they deal with much shallow networks [ 21 ]. In future work , we will address this issue by utilizing the untrained network to attempt to compare networks quickly without having to train them .__label__Method|Algorithm|Introduce
The second experiments considers variational auto - encoders for unsupervised learning . We mainly compare three approaches : VAE ( a = 1 . 0 ), IWAE ( a = 0 ), and VR - max ( a = − oo ), which are implemented upon the publicly available code . _CITE_ Four datasets are considered : Frey Face ( with 10 - fold cross validation ), Caltech 101 Silhouettes , MNIST and OMNIGLOT . The VAE model has L = 1 , 2 stochastic layers with deterministic layers stacked between , and the network architecture is detailed in the appendix . We reproduce the IWAE experiments to obtain a fair comparison , since the results in the original publication [ 17 ] mismatches those evaluated on the publicly available code .__label__Method|Code|Use
Fig . 3 ( a ) and Fig . 3 ( b ) are the results of fitting GMMs to the ‘ magic telescope ’ and ‘ year prediction ’ datasets _CITE_ . Fig . 3 ( c ) is the result for the natural image data of Table 5 .__label__Material|Data|Use
Typically , these Bayesian approaches aim to explicitly represent the unknown objective function of ( 1 ) by entertaining a posterior distribution over the space of objective functions . In contrast , we aim to model directly the distribution of the maximum of ( 2 ) conditioned on observations . Our model is intuitively straightforward and easy to implement _CITE_ . Let h ( x ) : X → R be an estimate of the mean ¯ f ( x ) constructed from data Dt := {( xi , yi )} ti = 1 ( Figure 1a , left ). This estimate can easily be converted into a posterior pdf over the location of the maximum by first multiplying it with a precision parameter α > 0 and then taking the normalized exponential ( Figure 1a , right ) In this transformation , the precision parameter α controls the certainty we have over our estimate of the maximizing argument : α ≈ 0 expresses almost no certainty , while α → ∞ expresses certainty .__label__Method|Algorithm|Produce
For both conv and ufo we experiment both with 21 and 22 regularization and report the best performance achieved in each case . We used the data sets evaluated in [ 27 ] ( plant , nonpl , psortPos , and psortNeg ), which consist of either 3 or 4 classes and use 69 biologically motivated sequence kernels . _CITE_ Furthermore , we also considered the proteinFold data set of [ 28 ], which consists of 27 classes and uses 12 biologically motivated base kernels . The results are summarized in Table 1 : they represent mean accuracy values with one standard deviation as computed over 10 random splits of the data into training and test folds . The fraction of the data used for training , as well as the total number of examples , is also shown .__label__Material|Data|Use
We utilized several standard benchmark datasets from UCI datasets ( skin , winequality , census income , twitter , internet ad , energy heat , energy cool , communities ), libsvm datasets ( a1a , breast cancer ), and LIACC datasets ( abalone , kinematics , puma8NH , bank8FM ). Table 2 summarizes specifications for each dataset . For classification , we compared the global and local residual model ( Global / Local ) with L1 logistic regression ( Linear ), LSL - SP with linear discrimination analysis , LDKL supported by L2regularized hinge loss _CITE_ , FaLK - SVM with linear kernels , and C - SVM with RBF kernel . Note that C - SVM is neither a region - specific nor locally linear classification model ; it is , rather , non - linear . We compared it with ours as a reference with respect to a common non - linear classification model .__label__Method|Algorithm|Compare
where x * is an optimal solution to ( 4 ). This section investigates behavior of Algorithm 2 on the basis of the simulation model used in [ 9 ], and we compare the proposed method with state - of - the - art methods : the SDP relaxation method [ 9 ] and the QPBO and QBPOI methods [ 11 ]. We use SDPA 7 . 3 . 8 to solve SDP problems _CITE_ and use the implementation of QPBO and QPBOI written by Kolmogolov . QPBO methods computes partial labeling , i . e ., there might remain unlabeled variables , and we set unlabeled variables to 0 in our experiments . For computing a minimum s - t cut , we use Dinic ’ s algorithm [ 6 ].__label__Method|Tool|Use
Here we approximated each MAP with its LP relaxation ( as in STRIPES ), so that both STRIPES and Nilsson come with certificates of optimality when their LP solutions are integral . BMMF relies on loopy BP to approximate the M best solutions . _CITE_ We used M = 50 in all experiments . To compare the algorithms , we pooled all their solutions , noting the 50 top probabilities , and then counted the fraction of these that any particular algorithm found ( its solution rank ). For run - time comparisons , we normalized the times by the longest - running algorithm for each example .__label__Method|Algorithm|Extent
It can be seen that even for condition numbers as high as 200 , n — 1500 measurements suffices for IHT to exactly recovery w *, whereas GOMP with the same setting is not able to recover w * even once . Tumor Classification , Breast Cancer Dataset We next compare the aforementioned methods on a gene selection problem for breast cancer tumor classification . We use the data used in [ 8 ] _CITE_ . We ran a 5 - fold cross validation scheme to choose parameters , where we varied 77 E { 2 - , 2 - , ... , 2 } k E { 2 , 5 , 10 , 15 , 20 , 50 , 100 } T E { 2 , 2 , ... , 213 }. Figure 2 ( Right ) shows that the vanilla hard thresholding method is competitive despite performing approximate projections , and the method with full corrections obtains the best performance among the methods considered .__label__Material|Data|Use
We have set J = 4 and the algorithm gives an “ empty class ,” thus discovering that three classes , with correct dimensionality and density , is enough for a good representation . The only errors are in the borders , as expected . In order to test the algorithm with real data , we first work with the MNIST database of handwritten digits , _CITE_ which has a test set of 10 . 000 examples . Each digit is an image of 28 × 28 pixels and we treat the data as 784 - dimensional vectors . We study the mixture of digits one and two and apply PMM and LD + IB with J = 2 and k = 10 .__label__Material|Data|Use
Here , we show that one of the reasons for this discrepancy is that the higher - level features learned by object - centric versus scene - centric CNNs are different : iconic images of objects do not contain the richness and diversity of visual information that pictures of scenes and environments provide for learning to recognize them . Here we introduce Places , a scene - centric image dataset 60 times larger than the SUN database [ 24 ]. With this database and a standard CNN architecture , we establish new baselines of accuracies on various scene datasets ( Scene15 [ 17 , 13 ], MIT Indoor67 [ 19 ], SUN database [ 24 ], and SUN Attribute Database [ 18 ]), significantly outperforming the results obtained by the deep features from the same network architecture trained with ImageNet _CITE_ . The paper is organized as follows : in Section 2 we introduce the Places database and describe the collection procedure . In Section 3 we compare Places with the other two large image datasets : SUN [ 24 ] and ImageNet [ 3 ].__label__Method|Algorithm|Compare
All the other three baselines , i . e ., CP - APR , Rubik , and BPTF , are tensor - based methods that can consider time utility when making recommendations . We refer to the proposed recommendation algorithm as Demand - Aware Recommender for One - Sided Sampling , or DAROSS for short . Our testbeds are two real - world datasets Tmall and Amazon Review _CITE_ . Since some of the baseline algorithms are not scalable enough , we first conduct experiments on their subsets and then on the full set of Amazon Review . In order to generate the subsets , we randomly sample 80 item categories for Tmall dataset and select the users who have purchased at least 3 items within these categories , leading to the purchase records of 377 users and 572 items .__label__Material|Data|Use
In all experiments , we use the low rank property of the sample covariance matrix and do not assume any other special structures . Our algorithm is implemented in a shared - memory architecture using OpenMP ( http :// openmp . org / wp /) and a distributed - memory architecture using OpenMPI ( http :// www . open - mpi . org ) and ScaLAPACK [ 15 ] ( http :// www . netlib . org / scalapack /). We compare CLIME - ADMM with three other methods for estimating the inverse covariance matrix , including CLIME , Tiger in package flare _CITE_ and divide and conquer QUIC ( DC - QUIC ) [ 13 ]. The comparisons are run on an Intel Zeon E5540 2 . 83GHz CPU with 32GB main memory . We test the efficiency of the above methods on both synthetic and real datasets .__label__Method|Code|Compare
We consider 3 datasets from the UCI repository Lichman ( 2013 ) for experimentation . Diabetes . This dataset _CITE_ represents the outcomes of patients pertaining to diabetes . We chose numeric attributes such as age , time in hospital , to represent points in the Euclidean space and gender as the sensitive dimension , i . e ., we aim to balance gender . We subsampled the dataset to 1000 records .__label__Material|Data|Use
When assessing the performance on each of the tasks , we notice that the advantage of learning jointly is particularly significant for those tasks with smaller number of observations . News Ontologies In this experiment , we consider multiclass learning in a 2 - task problem . We use the Reuters1 - v2 news article dataset [ 18 ] which has been pre - processed _CITE_ . In the pre - processing stage , the label hierarchy is reorganized by mapping the data set to the second level of topic hierarchy . The documents that only have labels of the third or fourth levels are mapped to their parent category of the second level .__label__Material|Data|Use
From the latter standpoint , we show that the standard deep generator architecture is powerful at modelling complex escorts of any deformed exponential family , factorising a number of escorts in order of the total inner layers ’ dimensions , and this factorization happens for an especially compact design . This hints on a simple sufficient condition on the activation function to guarantee the escort modelling , and it turns out that this condition is satisfied , exactly or in a limit sense , by most popular activation functions ( ELU , ReLU , Softplus , ...). We also provide experiments _CITE_ that display the uplift that can be obtained through a principled design of the activation function ( generator ), or tuning of the link function ( discriminator ). Due to the lack of space , a supplement ( SM ) provides the proof of the results in the main file and additional experiments . A longer version with a more exhaustive treatment of related results is available [ 27 ].__label__Method|Code|Produce
Neither of worked well , the former being to be too sensitive to the value of e which is in agreement with the observations made by [ 11 ] and the latter constraining the model by using a single a and b . We do not discuss this any further due to lack of space . Throughout our experiements , we used 4 popular benchmark datasets ( Table 1 ) with the recommended train - test splits - CLEF [ 8 ], NEWS20 , LSHTC -{ small , large } , IPC _CITE_ . First , to evaluate the speed advantage of the variational inference , we compare the full variational { M1 , M2 , M3 }- var and partial MAP { M1 , M2 , M3 - map } inference 5 for the three variants of HBLR to the MCMC sampling based inference of CorrMNL [ 18 ]. For CorrMNL , we used the implementation as provided by the authors .__label__Material|Data|Use
Thus on - the - fly feature calculation within FACTORIE is employed to remain tractable . The joint segmentation and coreference model described above is applied to the Cora dataset [ 25 ]. _CITE_ The dataset contains 1295 total mentions in 134 clusters , with a total of 36487 tokens . Isolated training consists of 5 loops of 100 , 000 samples each , and 300 , 000 samples for inference . For the joint task we run training for 5 loops of 250 , 000 samples each , with 750 , 000 samples for inference .__label__Material|Data|Use
The baseline algorithms we considered include Gibbs sampling ( Gibbs - LDA ) [ 17 ], logistic / linear regression on bag - of - words , supervised - LDA ( sLDA ) [ 4 ], and MedLDA [ 26 ], which are implemented either in C ++ or Java . And our proposed algorithms are implemented in C #. _CITE_ For BP - LDA and Gibbs - LDA , we first train the models in an unsupervised manner , and then generate per - document topic proportion Bd as their features in the inference steps , on top of which we train a linear ( logistic ) regression model on the regression ( classification ) tasks . We first evaluate the prediction performance of our models and compare them with the traditional ( supervised ) topic models . Since the training of the baseline topic models takes much longer time than BP - sLDA and BP - LDA ( see Figure 5 ), we compare their performance on two smaller datasets , namely a subset ( 79K documents ) of AMR ( randomly sampled from the 7 . 9 million reviews ) and the MultiSent dataset ( 342K documents ), which are all evaluated with 5 - fold cross validation .__label__Method|Code|Produce
Nematode biology abstracts . To demonstrate the strength of the nonparametric approach as exemplified by the HDP mixture , we compared it against latent Dirichlet allocation ( LDA ), which is a parametric model similar in structure to the HDP [ 1 ]. In particular , we applied both models to a corpus of nematode biology abstracts _CITE_ , evaluating the perplexity of both models on held out abstracts . Here abstracts correspond to groups , words correspond to observations , and topics correspond to mixture components , and exchangeability correspond to the typical bag - of - words assumption . In order to study specifically the nonparametric nature of the HDP , we used the same experimental setup for both models , except that in LDA we had to vary the number of topics used between 10 and 120 , while the HDP obtained posterior samples over this automatically .__label__Material|Data|Use
We use 5 - fold cross validation to determine the optimal A1 and A2 whose candidate values are chosen from n × { 0 . 01 , 0 . 1 , 0 . 5 , 1 , 5 , 10 , 100 } and the optimal number of nearest neighbors from { 5 , 10 , 15 , 20 }. The classification error is used as the performance measure . We compare our method , which is denoted as MT - KNN , with the KNN classifier which is a single - task learning method , the multi - task large margin nearest neighbor ( mtLMNN ) method [ 14 ] _CITE_ which is a multi - task local learning method based on the homogeneous neighborhood , and the multi - task feature learning ( MTFL ) method [ 2 ] which is a global method for multi - task learning . By utilizing hinge and square losses , we also consider two variants of our MT - KNN method . To mimic the real - world situation where the training data are usually limited , we randomly select 20 % of the whole data as training data and the rest to form the test set .__label__Method|Algorithm|Compare
Expected improvement [ 23 ] is the expected amount , under the GP surrogate , by which the function f ( t ) might be smaller than a ‘ current best ’ value η ( we set η = mini = 0 ,..., N { µ ( ti )}, where ti are observed locations ), The next evaluation point is chosen as the candidate maximizing this utility , multiplied by the probability for the Wolfe conditions to be fulfilled , which is derived in the following section . The key observation for a probabilistic extension of W - I and W - II is that they are positivity constraints on two variables at , bt that are both linear projections of the ( jointly Gaussian ) variables f and f : with correlation coefficient ρt = Ct b / pq aCtb . It can be computed efficiently [ 29 ], using readily available code _CITE_ ( on a laptop , one evaluation of pWolfe t cost about 100 microseconds , each line search requires < 50 such calls ). The line search computes this probability for all evaluation nodes , after each evaluation . If any of the nodes fulfills the Wolfe conditions with pWolfe t & gt ; cW , greater than some threshold 0 < cW ≤ 1 , it is accepted and returned .__label__Method|Code|Use
We downloaded Version 2 . 5 of the Tensor Toolbox , which is implemented in MATLAB . Since open source code for GigaTensor is not freely available , we developed our own version in C ++ following the description in [ 8 ]. Also , we used MPICH2 _CITE_ in order to distribute the tensor factorization computation to multiple machines . All our codes are available for download under an open source license from http :// www . joonheechoi . com / research . Scaling on Real - World Datasets Both CPALS and our implementation of GigaTensor are uniprocessor codes .__label__Method|Tool|Use
10 ( b ) shows the optimal block size decreases as the mean shift increases , as expected . We test the performance of our M - statistics using real data . Our datasets include : ( 1 ) CENSREC1 - C : a real - world speech dataset in the Speech Resource Consortium ( SRC ) corpora provided by National Institute of Informatics ( NII ) ; ( 2 ) Human Activity Sensing Consortium ( HASC ) challenge 2011 data _CITE_ . We compare our M - statistic with a state - of - the - art algorithm , the relative densityratio ( RDR ) estimate [ 7 ] ( one limitation of the RDR algorithm , however , is that it is not suitable for high - dimensional data because estimating density ratio in the high - dimensional setting is illposed ). To achieve reasonable performance for the RDR algorithm , we adjust the bandwidth and the regularization parameter at each time step and , hence , the RDR algorithm is computationally more expensive than the M - statistics method .__label__Material|Data|Use
These approaches all belong to the family of channel features detectors , and as the improvements proposed in this work are orthogonal , the methods could potentially be combined . Caltech Results : We present our main result on the Caltech Pedestrian Dataset [ 10 ], see Fig . 5 ( b ), generated using the official evaluation code available online _CITE_ . The Caltech dataset has become the standard for evaluating pedestrian detectors and the latest methods based on deep learning ( JointDeep ) [ 25 ], multi - resolution models ( MT - DPM ) [ 36 ] and motion features ( ACF + SDt ) [ 27 ] achieve under 40 % log - average MR . For a complete comparison , we first present results for an augmented capacity ACF model which uses more ( 4096 ) and deeper ( depth - 5 ) trees trained with RealBoost using dense sampling of the training data ( every 4th image ). See preceding note on model capacity for details and motivation .__label__Method|Code|Use
The results from SGD - SD and SGD - WD confirm this finding while selecting uncertain examples can give us a similar or larger boost . Furthermore , we test the robustness of our methods by randomly reassigning the labels of 10 % of the images , and the results indicate that the SGD - WPV improves the performance of SGD - Scan even more while SGD - SD overfits the data seriously . We test a simple multi - class logistic regression _CITE_ on CIFAR 10 [ 24 ]. Images are down - sampled significantly to 32 × 32 × 3 , so many examples are difficult , even for humans . SGD - SPV and SGD - SE perform significantly better than SGD - Uni here , consistent with the idea that avoiding difficult examples increases robustness to outliers .__label__Method|Algorithm|Use
with results obtained by applying different algorithms to learn sparse representations of this data set ( e . g ., [ 2 , 5 ]). We also applied the algorithm to a training set a set of 14 - by - 14 natural image patches , taken from a dataset compiled by van Hateren . _CITE_ We learned a sparse RBM model with 196 visible units and 400 hidden units . The learned bases are shown in Figure 3 ; they are oriented , gabor - like bases and resemble the receptive fields of V1 simple cells . We further learned a two - layer network by stacking one sparse RBM on top of another ( see Section 3 . 2 for details .__label__Material|Data|Use
As we see , incorporating energy saturation reduces the time to convergence ( while achieving the same level of accuracy ), and using focused random walk moves further decreases the convergence time , especially as n increases . We compare FocusedFlatSAT against several state - of - the - art methods for computing an estimate of or bound on the partition function . _CITE_ An evaluation such as this is inherently challenging as the ground truth is very hard to obtain and computational bounds can be orders of magnitude off from the truth , making a comparison of estimates not very meaningful . We therefore propose to evaluate the methods on either small instances whose ground truth can be evaluated by “ brute force ,” or larger instances whose ground truth ( or bounds on it ) can be computed analytically or through other tools such as efficient model counters . We also consider planar cases for which a specialized polynomial time exact algorithm is available .__label__Method|Algorithm|Compare
In this way , the input of the score function is dynamically adapted to the start - end behavior , but a classification feedback is given to each function for each decision taken . The functions of the system were actually modeled as Voted Perceptrons [ 11 ], which compute a prediction as an average of all vectors generated during training . For the batch classification setting , we modeled the functions as Voted Perceptrons and also as SVMs _CITE_ . In all cases , a function can be expressed in dual form as a combination of training instances , which allows the use of kernel functions . We work with polynomial kernels of degree 2 .__label__Method|Tool|Extent
We trained the linear multiclass model of Section 3 with the following alternative methods : exact softmax training ( SOFT ), the onevs - each bound ( OVE ), the stochastically optimized one - vs - each bound ( OVE - SGD ) and Bouchard ’ s bound ( BOUCHARD ). For all approaches , the associated cost function was maximized together with an added regularization penalty term , − 2A || w || , which ensures that the global maximum of the cost function is achieved for finite w . Since we want to investigate how well we surrogate exact softmax training , we used the same fixed value A = 1 in all experiments . We considered three small scale multiclass classification datasets : MNIST _CITE_ , 20NEWS and BIBTEX [ 12 ]; see Table 1 for details . Notice that BIBTEX is originally a multi - label classification dataset [ 2 ]. where each example may have more than one labels .__label__Material|Data|Use
The hyperparameters of the methods and the validation procedures are described below and in more detail in Appendix D . If necessary , the raw outputs of the learners were turned into probability estimates , i . e ., they were rescaled to [ 0 , 1 ] using logistic transform . We used in the experiments nine datasets taken from the LibSVM repository of binary classification tasks . _CITE_ Many of these datasets are commonly used as benchmarks in information retrieval where the F - score is routinely applied for model selection . In addition , we also used the textual data released in the Replab challenge of identifying relevant tweets [ 1 ]. We generated the features used by the winner team [ 8 ].__label__Material|Data|Use
To this end , we applied a Bayesian optimization technique for hyper - parameter selection in machine learning algorithms , which has recently been described by Snoek et al . [ 22 ] and has been implemented in Spearmint library . _CITE_ The basic idea is to treat the learning algorithm ’ s generalization performance as a sample from a Gaussian process and select the next parameter configuration to test based on the expected improvement . The authors showed that this way , the number of experiment runs to minimize a given objective can be significantly reduced while surpassing the performance of parameters chosen by human experts . We implemented our experiments using Theano [ 23 ] and pylearn2 [ 24 ].__label__Method|Code|Use
[ 9 ] we used an averaged perceptron trained for 10 epochs , for all the experiments . Test - time classification : Use the classifier trained in the previous step on the Reuters test set for language Y , using the word representations WY to represent the documents . We trained the following autoencoders _CITE_ : BAE - cr which uses reconstruction error based decoder training ( see Section 2 . 1 ) and BAE - tr which uses tree - based decoder training ( see Section 2 . 2 ). Models were trained for up to 20 epochs using the same data as described earlier . BAE - cr used mini - batch ( of size 20 ) stochastic gradient descent , while BAE - tr used regular stochastic gradient .__label__Method|Algorithm|Use
Sec . 2 for more implementation details . We evaluate our method using both simulated data and two real - world datasets _CITE_ . The implementation details are in Supp . Sec .__label__Material|Data|Use
The potentials were (− θi , θi ) for nodes and ( θij , − θij ; − θij , θij ) for edges . ( 3 ) Restricted Boltzmann Machines ( RBMs ): From the Probabilistic Inference Challenge 2011 . _CITE_ ( 4 ) Horses : Large ( N ≈ 12000 ) MRFs representing images from the Weizmann Horse Data ( Borenstein and Ullman , 2002 ) with potentials learned by Domke ( 2013 ). ( 5 ) Chinese Characters : An image completion task from the KAIST Hanja2 database , compiled in OpenGM by Andres et al . ( 2012 ).__label__Method|Algorithm|Introduce
As we show , fixed - k estimators can also exhibit superior rates of convergence . As shown in Table 1 , several authors have derived bias corrections necessary for fixed - k estimators of entropies and divergences , including , most famously , the Shannon entropy estimator of [ 20 ]. _CITE_ The estimators in Table 1 estimators are known to be weakly consistent , but , except for Shannon entropy , expectations are over X ∼ P . r ( t ) = f ∞ xt − 1e − x dx is the gamma function , and ψ ( x ) = ddx log ( r ( x )) is the digamma function . a E R \{ 1 } is a free parameter . ∗ For KL divergence , bias corrections for p and q cancel .__label__Method|Algorithm|Introduce
We use the linear SVM trained with instances labeled so far to judge the uncertainty . We call these methods pool uncertainty , stream uncertainty , secretary uncertainty respectively , and use them as baselines . We conducted experiments on two benchmark datasets , WDBC and MNIST _CITE_ . The WDBC dataset contains 569 instances , each of which consists of 32 - dimensional features of cells and their diagnosis Of particular relevance to our work is the one presented by Sabato and Hess [ 24 ]. They devised general methods for constructing stream - based algorithms satisfying a budget based on pool - based algorithms , but their theoretical guarantees are bounding the length of the stream needed to emulate the pool - based algorithm , which is a large difference from our work .__label__Material|Data|Use
We place a gamma prior with shape 0 : 1 and rate 0 : over the weights wkd , a uniform prior over the variables znk , and we use K = 100 latent components . Datasets . We apply the sparse gamma DEF on two different databases : ( i ) the Olivetti database at AT & T , _CITE_ which consists of 400 ( 320 for training and 80 for test ) 64 x 64 images of human faces in a 8 bit scale ( 0 — 255 ); and ( ii ) the collection of papers at the Neural Information Processing Systems ( nips ) 2011 conference , which consists of 305 documents and a vocabulary of 5715 effective words in a bag - of - words format ( 25 % of words from all documents are set aside to form the test set ). We apply the beta - gamma mf on : ( i ) the binarized mnist data , which consists of 28 x 28 images of hand - written digits ( we use 5000 training and 2000 test images ); and ( ii ) the Omniglot dataset ( Lake et al ., 2015 ), which consists of 105 x 105 images of hand - written characters from different alphabets ( we select 10 alphabets , with 4425 training images , 1475 test images , and 295 characters ). Evaluation .__label__Material|Data|Use
When these expressions are substituted into Eq . 4 , we find the following expressions : Computing the gradient requires only O ( NKD ) operations since the terms Pc pci log pci ˆpc may be computed once and reused in each partial derivative expression . The above gradients are used in the L - BFGS [ 6 ] quasi - Newton optimization algorithm _CITE_ . We find empirically that the optimization usually converges within a few hundred iterations . When specialized to multilogit regression , the objective function F ( W ; x , λ ) is non - concave .__label__Method|Algorithm|Extent
PCA is a good preprocessor in the current context since we have previously shown that in PCA - space strong correlations exist between man and machine [ 1 ]. Second , there is evidence that the PCA representation may be biologically - plausible [ 8 ]. The face stimuli were taken from the gender - balanced Max Planck Institute ( MPI ) face database _CITE_ composed of 200 greyscale 256 x 256 - pixel frontal views of human faces , yielding a data matrix X E R200X2562 . For the gender discrimination task we adhere to the following convention for the class labels : y = − 1 for females and y = + 1 for males . We consider no dimensionality reduction and keep all 200 components of the PCA .__label__Material|Data|Extent
In both of the pre - training and fine - tuning stages , the loss functions are optimized with L - BFGS algorithm ( a Quasi - Newton method ) which , according to [ 22 ], can achieve fastest convergence in our settings . We narrow our focus down to denoising and inpainting of grey - scale images , but there is no difficulty in generalizing to colored images . We use a set of natural images collected from the web _CITE_ as our training set and standard testing images as the testing set . We create noisy images from clean training and testing images by applying the function ( 1 ) to them . Image patches are then extracted from both clean and noisy images to train SSDAs .__label__Material|Data|Use
activation function . For the kernel MVA algorithms we used a Gaussian kernel k ( xi , xj ) = exp (− IIxi − xjII 2 / 2u ) using 10 - fold cross - validation ( 10 - CV ) on the training set to estimate u . To obtain some reference accuracy rates , we also trained a v - SVM with Gaussian kernel , using the LIBSVM implementation _CITE_ and 10 - CV was carried out for both the kernel width and v . Accuracy error rates for rKOPLS and different values of R are displayed in the first rows and first columns of Table 3 . Comparing these results with SVM ( under the rbf - SVM column ), we can dard deviation of the estimation are given for 10 different runs of rKOPLS and KPLS2 , both when using the pseudoinverse of the projected data together with the “ winner - takes - all ” activation function ( first rows ), and when using a v - SVM linear classifier ( last rows ). The results achieved by an SVM with linear classifier are also provided in the bottom right corner .__label__Method|Tool|Use
The Catchwords help isolate the nearly pure - topic documents and hence find the topic vectors . The proofs are complicated by the fact that each step of the algorithm induces conditioning on the data – for example , after clustering , the document vectors in one cluster are not independent anymore . We compare the thresholded SVD based k - means ( TSVD _CITE_ ) algorithm 3 . 3 with the algorithms of [ 5 ], Recover - KL and Recover - L2 , using the code made available by the authors . We observed the results of Recover - KL to be better than Recover - L2 , and report here the results of Recover - KL ( abbreviated R - KL ), full set of results can be found in supplementary section 5 . We first provide empirical support for the algorithm assumptions in Section 3 . 1 , namely the dominant topic and the catchwords assumption .__label__Method|Algorithm|Compare
Then , we produce the estimates for all variables of each timestamp . We repeat the procedure for 10 times and report the average prediction RMSE for all timestamps and 10 random sets of missing locations . We use the MATLAB Kriging Toolbox _CITE_ for the classical cokriging algorithms and the MTGP code provided by [ 4 ]. Table 1 shows the results for the cokriging task . The greedy algorithm with orthogonal projections is significantly more accurate in all three datasets .__label__Method|Tool|Use
We further pre - computed the scores of candidate parent sets , which were fed as input into each system evaluated . Finally , we used the EVASOLVER partial MaxSAT solver , for inferring ordering constraints . _CITE_ In our first set of experiments , we compared our approach with the ILP - based system of GOBNILP , where we encoded ancestral constraints using linear constraints , based on [ Cussens , 2008 ]; note again that both are exact approaches for structure learning . In Table 1 , we supplied both systems with decomposable constraints inferred via projection ( which empowers the oracle for searching the EC tree , and provides redundant constraints for the ILP ). In Table 2 , we withheld the projected constraints .__label__Method|Tool|Use
[ 25 ] in preparing the data . We use their splits to divide the classes into 100 training , 50 validation , and 50 test . For images we use 1 , 024dimensional features extracted by applying GoogLeNet [ 31 ] to middle , upper left , upper right , lower left , and lower right crops of the original and horizontally - flipped image _CITE_ . At test time we use only the middle crop of the original image . For class meta - data we use the 312 - dimensional continuous attribute vectors provided with the CUB dataset .__label__Material|Data|Use
A key factor to making such a collection effective is to filter it so that descriptions are likely to refer to visual content . Some small collections of captioned images have been created by hand in the past . The UIUC Pascal Sentence data set _CITE_ contains 1k images each of which is associated with 5 human generated descriptions . The ImageClef image retrieval challenge contains 10k images with associated human descriptions . However neither of these collections is large enough to facilitate reasonable image based matching necessary for our goals , as demonstrated by our experiments on captioning with varying collection size ( Sec 3 ).__label__Material|Data|Introduce
Finally experiment results are given . We conduct our experiments on 30 datasets consisting of 9 synthetic datasets , 20 Reuter datasets and 1 real dataset . To generate synthetic data , we randomly choose some datasets from different domains including economy and biology , etc _CITE_ whose scales vary from 690 to 3 , 196 . They only have one feature space at first . We artificially map the original datasets into another feature space by random Gaussian matrices , then we have data both from feature space S1 and S2 .__label__Material|Data|Use
For each of these versions , step size is tuned for each dataset to give the best convergence progress . All the algorithms were implemented in C ++ . We run our experiments on datasets from LIBSVM website _CITE_ . Similar to [ 29 ], we normalize each example in the dataset so that IIzi112 = 1 for all i E [ n ]. Such a normalization leads to an upper bound of 0 . 25 on the Lipschitz constant of the gradient of fi .__label__Material|Data|Use
For each of these versions , step size is tuned for each dataset to give the best convergence progress . All the algorithms were implemented in C ++ . We run our experiments on datasets from LIBSVM website _CITE_ . Similar to [ 29 ], we normalize each example in the dataset so that IIzi112 = 1 for all i E [ n ]. Such a normalization leads to an upper bound of 0 . 25 on the Lipschitz constant of the gradient of fi .__label__Supplement|Website|Use
We compare the proposed method with the LDA model . This method is implemented in MATLAB . We used the modules for LBP , made available with UGM _CITE_ package . The LDA models are learnt using the lda package . Performance Evaluation : We evaluate performance based on the test perplexity [ 20 ] given by where n is the number of test samples and p is the number of observed variables ( i . e ., words ).__label__Method|Tool|Use
This paper focuses on additional information in the form of multi - labeled data , where each document is tagged with a set of labels . These data are ubiquitous . Web pages are tagged with multiple directories , _CITE_ books are labeled with different categories or political speeches are annotated with multiple issues . Previous topic models on multi - labeled data focus on a small set of relatively independent labels [ 25 , 36 , 46 ]. Unfortunately , in many real - world examples , the number of labelsfrom hundreds to thousands — is incompatible with the independence assumptions of these models .__label__Material|Data|Use
We first describe a set of experiments on synthetic data appearing in previous work to illustrate the use of the PSS feature map ( D , and the universal persistence scale - space kernel on two different tasks . We then present two applications on real - world data , where we assess differences in the persistent homology of functions on 3D surfaces of lateral ventricles and corpora callosa with respect to different group assignments ( i . e ., age , demented / non - demented ). In all experiments , filtrations and the persistence diagrams are obtained using DrPaA _CITE_ , which can directly handle our types of input data . Source code to reproduce the experiments is available at https :// goo . gl / KouBPT . Computation of the mean PSS function .__label__Method|Tool|Use
For the UIUC - ISD dataset , the labels for each concrete sense were similarly core , related and unrelated . In addition , a people label was used for unrelated images depicting faces or a crowd . _CITE_ The OFFICE dataset was only labeled with core and unrelated labels . We evaluated our models on two retrieval tasks : retrieval of only core images of each sense , and retrieval of both core and related images . In the former case , core labels were used as positive labels for each sense , with related , unrelated and people images labeled as negative .__label__Material|Data|Use
The piecewise - constant shape of the test error curves ( see Fig . 1 ) results in log - log plots with unstable slopes . Table 1 shows a comparative evaluation on the MNIST handwritten digits database _CITE_ and a face video . The MNIST database contains 70 , 000 images at resolution 28x28 ( D = 784 ), and the face video has 1965 frames at resolution 28 x 20 ( D = 560 ). For each of the resulting 11 datasets ( taking each digit separately ), we used half the samples for training and half for testing .__label__Material|Data|Compare
We then provide a quantitative model comparison , in which the models are evaluated in terms of their ability to predict held out data . These results highlight the promise of GC - LDA and this type of modeling for jointly extracting the spatial extent and cognitive functions of neuroanatomical brain regions . Neurosynth Database : Neurosynth [ 12 ] is a publicly available database consisting of data automatically extracted from a large collection of functional magnetic resonance imaging ( fMRI ) publications _CITE_ . For each publication , the database contains the abstract text and all reported 3 - dimensional peak activation coordinates ( in MNI space ) in the study . The text was pre - processed to remove common stop - words .__label__Material|Data|Use
Historical Climatology Network Monthly ( USHCN ) dataset consists of monthly climatological data of 108 stations spanning from year 1915 to 2000 . It has three climate variables : ( 1 ) daily maximum , ( 2 ) minimum temperature averaged over month , and ( 3 ) total monthly precipitation . CCDS The Comprehensive Climate Dataset ( CCDS ) _CITE_ is a collection of climate records of North America from [ 18 ]. The dataset was collected and pre - processed by five federal agencies . It contains monthly observations of 17 variables such as Carbon dioxide and temperature spanning from 1990 to 2001 .__label__Material|Data|Introduce
Theorem 4 Given a maximum set of ancestral constraints A , and let O be a closed set of ordering constraints . The set O is entailed by A if O satisfies the following two statements : We now empirically evaluate the effectiveness of our approach to learning with ancestral constraints . We simulated different structure learning problems from standard Bayesian network benchmarks _CITE_ ALARM , ANDES , CHILD , CPCS54 , and HEPAR2 , by ( 1 ) taking a random sub - networkAf of a given size ( 2 ) simulating a training dataset from Af of varying sizes ( 3 ) simulating a set of ancestral constraints of a given size , by randomly selecting ordered pairs whose ground - truth ancestral relations in Af were used as constraints . In our experiments , we varied the number of variables in the learning problem ( n ), the size of the training dataset ( N ), and the percentage of the n ( n − 1 )/ 2 total ancestral relations that were given as constraints ( p ). We report results that were averaged over 50 different datasets : 5 datasets were simulated from each of 2 different sub - networks , which were taken from each of the 5 original networks mentioned above .__label__Material|Data|Use
We solve the following : Note , for one - to - one matching the objective coincides with the IQP for binary x since xTx = n . Computational Solution We can formulate ( 3 ) as maximization of a Rayleigh quotient under affine constraint . While the case of linear constraints has been addressed previously [ 6 ], imposing affine constraints is novel . We fully address this class of problem in the supplementary material _CITE_ and give a brief summary here . The solution to ( 3 ) is given by the leading eigenpair of PCWPC x = Ax , ( 4 ) where x is scaled so that Cx = b exactly . We introduced PC = Inn ′ − CTeq ( CeqCTeq )− Ceq and Ceq = [ Ik − 1 , 0 ] ( C − ( 1 / bk ) bCk ), where Ck , bk denote the last row of C , b and k = # constraints .__label__Supplement|Document|Produce
Clauses in a sentence form a hierarchical structure which constitutes the skeleton of the full syntactic tree . In the following example , the clauses are annotated with brackets : ( ( When ( you don ’ t have any other option )), it is easy ( to fight ) .) We followed the setting of the CoNLL - 2001 competition _CITE_ . The problem consists of recognizing the set of clauses on the basis of words , part - of - speech tags ( PoS ), and syntactic base phrases ( or chunks ). There is only one category of phrases to be considered , namely the clauses .__label__Method|Algorithm|Extent
Furthermore , the 8 data points selected by Bound are uniformly distributed on the two circles , four from the inner circle , and the other four from the outer circle , which can better represent the original data . In the following , we use three real - world benchmark datasets to evaluate the compared methods . wdbc is the Wisconsin Diagnostic Breast Cancer data set , which is from UCI machine learning repository _CITE_ . It aims at predicting the breast cancer as benign or malignant based on the digitalized images . There are 357 positive samples and 212 negative samples .__label__Material|Data|Use
In our implementation , we first solve ( 14 ) with standard relaxation ( W is the identity ) and then with reweighted -` 1 . To handle large motion , we use a pyramid with scale factor 0 . 5 and up to 4 levels ; λ and µ are fixed at 0 . 002 and 0 . 001 ( Flower Garden ) and 0 . 0006 and 0 . 0003 ( Middlebury ) respectively . To make comparison with [ 29 ] fair , we modify the code provided online _CITE_ to include anisotropic regularization ( Fig . 1 ). Note that no occlusion is present in the residual of the motion field computed by TV - L1 , and subsequently the motion estimates are less precise around occluding boundaries ( top - left corner of the Flower Garden , plane in the left in Venus ).__label__Method|Code|Extent
In this section we present the experimental results of LR - MVL on Named Entity Recognition ( NER ) and Syntactic Chunking tasks . We compare LR - MVL to state - of - the - art semi - supervised approaches like [ 1 ] ( Alternating Structures Optimization ( ASO )) and [ 2 ] ( Semi - supervised extension of CRFs ) as well as embeddings like C & W , HLBL and Brown Clustering . For the NER experiments we used the data from CoNLL 2003 shared task and for Chunking experiments we used the CoNLL 2000 shared task data _CITE_ with standard training , development and testing set splits . The CoNLL ’ 03 and the CoNLL ’ 00 datasets had ∼ 204K / 51K / 46K and ∼ 212K /−/ 47K tokens respectively for Train / Dev ./ Test sets . We use the same set of baseline features as used by [ 15 , 16 ] in their experiments .__label__Material|Data|Use
For the other experiments , re - weighting examples ( SGD - W *) generally gives us better performance than changing the sampling distribution ( SGD - S *). It might be because we can better estimate the statistics of each sample . We apply our method to a CNN [ 26 ] for MNIST _CITE_ using one of the Tensorflow tutorials . The dataset has high testing accuracy , so most of the examples are too easy for the model after a few epochs . Selecting more difficult instances can accelerate learning or improve testing accuracy [ 18 , 29 , 13 ].__label__Material|Data|Use
The task was to construct a network of I & F neurons that could recognize each of the 10 spoken words . Each of the 500 input files had been encoded in the form of 40 spike trains , with at most one spike per spike train 6 signaling onset , peak , or offset of activity in a particular frequency band . A network was presented in [ 8 ] that could solve this task with an error _CITE_ of 0 . 15 for recognizing the pattern “ one ”. No better result had been achieved by any competing networks constructed during a widely publicized internet competition [ 7 ]. The network constructed in [ 8 ] transformed the 40 input spike trains into linearly decaying input currents from 800 pools , each consisting of a “ large set of closely similar unsynchronized neurons ” [ 8 ].__label__Method|Algorithm|Use
Landmine data is collected from two different terrains : tasks 1 - 10 are from highly foliated regions and tasks 11 - 19 are from desert regions , therefore tasks naturally form two clusters . Any hypothesis learned from a task should be able to utilize the information available from other tasks belonging to the same cluster . Spam Detection _CITE_ We use the dataset obtained from ECML PAKDD 2006 Discovery challenge for the spam detection task . We used the task B challenge dataset which consists of labeled training data from the inboxes of 15 users . We consider each user as a single task and the goal is to build a personalized spam filter for each user .__label__Material|Data|Use
As predicted by the theory , √ DFEG has a constant regret , while Kernel GD has a regret of the form O ( rl T ). Hence , it can have a constant regret only when rl is set to zero , and this can be done only with prior knowledge of kuk , that is impossible in practical applications . For the second experiment , I analyzed the behavior of DFEG on two real word regression datasets , cadata and cpusmall _CITE_ . I used the Gaussian kernel with variance equal to the average distance between training input vectors . I have plotted in Figure 1 ( central ) the final cumulative loss of DFEG and the ones of GD with varying values of rl .__label__Material|Data|Use
Color and normalized color kernel descriptors are extracted over RGB images , and gradient and shape kernel descriptors are extracted over gray scale images transformed from the original RGB images . Following the standard parameter setting , we compute kernel descriptors on 16 x 16 image patches over dense regular grids with spacing of 8 pixels . For template relation learning , we use a publicly available L1 regularization solver _CITE_ . All images are resized to be no larger than 300 x 300 with the height / width ratio preserved . To learn the template model , we use 34 templates with different sizes .__label__Method|Tool|Use
Furthermore , it is straightforward to learn also the thresholds ( they appear as variables in the primal , and correspond to constraints in the dual )— either a single set of thresholds for the entire matrix , or a separate threshold vector for each row of the matrix ( each “ user ”). Doing the latter allows users to “ use ratings differently ” and alleviates the need to normalize the data . Experiments We conducted preliminary experiments on a subset of the 100K MovieLens Dataset _CITE_ , consisting of the 100 users and 100 movies with the most ratings . We used CSDP [ 11 ] to solve the resulting SDPs . The ratings are on a discrete scale of one through five , and we experimented with both generalizations of the hinge loss above , allowing per - user thresholds .__label__Material|Data|Use
FR first selects X2 since R2 Z , X2 is the largest , then selects X1 since R2Z ,{ X2 , X1 } > R2Z ,{ X2 , X3 }; thus produces a local optimal solution { X1 , X2 }. It is also easy to verify that other two previous methods OMP [ 19 ] and FoBa [ 26 ] cannot find the optimal solution for this example , due to their greedy nature . We conducted experiments on 12 data sets _CITE_ in Table 1 to compare POSS with the following methods : For POSS , we use I (·) = 0 since it is generally good , and the number of iterations T is set to be b2ek nc as suggested by Theorem 1 . To evaluate how far these methods are from the optimum , we also compute the optimal subset by exhaustive enumeration , denoted as OPT . To assess each method on each data set , we repeat the following process 100 times .__label__Material|Data|Use
All experiments were conducted on a Linux machine with AMD Opteron 2GHz CPU and 2GB RAM . First , we evaluated the feature - sign search algorithm for learning coefficients with the L1 sparsity function . We compared the running time and accuracy to previous state - of - the - art algorithms : a generic QP solver , _CITE_ a modified version of LARS [ 12 ] with early stopping , grafting [ 13 ], and Chen et al .’ s interior point method [ 11 ]; all the algorithms were implemented in MATLAB . For each dataset , we used a test set of 100 input vectors and measured the running time10 and the objective function at convergence . Table 1 shows both the running time and accuracy ( measured by the relative error in the final objective value ) of different coefficient learning algorithms .__label__Method|Tool|Compare
As for the objective function , the results are even more impressive as the distance from the exact value ( i . e ., 0 . 989 ) rapidly goes to zero starting from 0 . 00025 , at less than 10 % rate . Also , note how the CPU time increases linearly as the sampling rate approaches 100 %. Next , we tested our algorithm over the Johns Hopkins University ionosphere database _CITE_ which contains 351 labeled instances from two different classes . As in the previous experiment , similarities were computed using a Gaussian kernel . Our goal was to test how the solutions obtained on the sampled graph compare with those of the original , dense problem and to study how the performance of the algorithm scales w . r . t .__label__Material|Data|Use
Our best result for LMMN on this data set at 13 . 0 % test error rate improved significantly on kNN classification using Euclidean distances . LMNN also performed comparably to our best multiclass SVM [ 4 ], which obtained a 12 . 4 % test error rate using a linear kernel and 20000 dimensional inputs . Handwritten digit recognition The MNIST data set of handwritten digits _CITE_ has been extensively benchmarked [ 9 ]. We deskewed the original 28 × 28 grayscale images , then reduced their dimensionality by retaining only the first 164 principal components ( enough to capture 95 % of the data ’ s overall variance ). Energy - based LMNN classification yielded a test error rate at 1 . 3 %, cutting the baseline kNN error rate by over one - third .__label__Material|Data|Compare
Others have commented that VFE has the tendency to underfit [ 3 ]. Here we investigate the underfitting claim and relate it to optimisation behaviour . As this behaviour is not observable in our 1D dataset , we illustrate it on the pumadyn32nm dataset _CITE_ ( 32 dimensions , 7168 training , 1024 test ), see Table 1 for the results of a representative run with random initial conditions and M = 40 inducing inputs . divided by number of training points , the optimised noise variance Q2 ,,,, the ten most dominant inverse lengthscales and the RMSE on test data . Methods are full GP on 2048 training samples , FITC , VFE , VFE with initially frozen hyperparameters , VFE initialised with the solution obtained by FITC .__label__Material|Data|Use
All experiments were implemented using MATLAB . Source code can be found at : https :// github . com / bianan / non - monotone - dr - submodular . As a state - of - the - art global solver , QUADPROGIP _CITE_ [ 39 ] can find the global optimum ( possibly in exponential time ), which were used to calculate the approximation ratios . Our problem instances are synthetic DR - submodular quadratic objectives with down - closed polytope constraints , i . e ., f ( x ) = 2xTHx + hTx + c and P = { x E Rn 1 + | Ax & lt ; b , x & lt ; ¯ u , A E Rmxn ++, b E Rm + }. Both objective and constraints were randomly generated , in the following two manners : In both the above two cases , we set b = 1m , and u ¯ to be the tightest upper bound of P by ¯ uj = miniE [ m ] Aij , dj E [ n ].__label__Method|Tool|Introduce
The symbol * is used to indicate the cases significantly worse than the winning entry ; a p - value threshold of 0 . 01 in Wilcoxon rank sum test was used to decide this . We selected seven large regression datasets . _CITE_ Each of them is randomly partitioned into training / test splits . For the purpose of analyzing statistical significance , the partition was repeated 20 times independently . Test set performances ( NMSE and NLPD ) of the three methods on the seven datasets are presented in Table 1 .__label__Material|Data|Use
In this section we present the experimental results of LR - MVL on Named Entity Recognition ( NER ) and Syntactic Chunking tasks . We compare LR - MVL to state - of - the - art semi - supervised approaches like [ 1 ] ( Alternating Structures Optimization ( ASO )) and [ 2 ] ( Semi - supervised extension of CRFs ) as well as embeddings like C & W , HLBL and Brown Clustering . For the NER experiments we used the data from CoNLL 2003 shared task and for Chunking experiments we used the CoNLL 2000 shared task data _CITE_ with standard training , development and testing set splits . The CoNLL ’ 03 and the CoNLL ’ 00 datasets had ∼ 204K / 51K / 46K and ∼ 212K /−/ 47K tokens respectively for Train / Dev ./ Test sets . We use the same set of baseline features as used by [ 15 , 16 ] in their experiments .__label__Material|Data|Use
MLLE can recover the generating parameter perfectly up to an affine transformation . Next , we consider a data set containing N = 4400 handwritten digits (’ 2 ’-’ 5 ’) with 1100 examples of each class . The gray scale images of handwritten numerals are at 16 × 16 resolution and converted m = 256 dimensional vectors _CITE_ . The data points are mapped into a 2 - dimensional space using LLE and MLLE respectively . These experiments are shown in Figure 5 .__label__Material|Data|Use
We run regression experiments on 2008 US flight data with 2 million records and perform classification tests on MNIST using the latent variable model . We show that GPs perform better than many common models which are often used for big data . The proposed inference was implemented in Python using the Map - Reduce framework [ Dean and Ghemawat , 2008 ] to work on multi - core architectures , and is available as an open - source package _CITE_ . The full derivation of the inference is given in the supplementary material as well as additional experimental results ( such as robustness tests to node failure by dropping out nodes at random ). The open source software package contains an extensively documented implementation of the derivations , with references to the equations presented in the supplementary material for explanation .__label__Method|Code|Produce
by varying the value of the regularization parameter A . The curves obtained on the Jester 2 dataset are hardly distinguishable from ( a ) and hence are not displayed due to space limitation . Second , we conducted the same comparison on two MovieLens datasets _CITE_ , which contain ratings of movies by users . We randomly select 20 % of the entries as a test set , and the remaining entries are split between a training set ( 80 %) and a validation set ( 20 %). For all the methods , we stop the regularization path as soon as the estimated rank exceeds rmax = 100 .__label__Material|Data|Compare
These figures show that the improvement in recall is consistent over the full range of false positive rates . For further comparisons , our data and code are available online . _CITE_ Compactness of our kernels . In many applications of feature matching , the compactness of the descriptor is important . In Table 3 , we compare to other methods by grouping them according to their memory footprint .__label__Method|Code|Produce
These figures show that the improvement in recall is consistent over the full range of false positive rates . For further comparisons , our data and code are available online . _CITE_ Compactness of our kernels . In many applications of feature matching , the compactness of the descriptor is important . In Table 3 , we compare to other methods by grouping them according to their memory footprint .__label__Material|Data|Produce
However it is still slower because their local search is conducted on a constrained manifold . In contrast , our local search objective is entirely unconstrained and smooth , which we manage to solve efficiently by L - BFGS . _CITE_ JS , though applied indirectly , is faster than DHM in reducing the loss . We observed that DHM kept running coordinate descent with a constant step size , while the totally corrective update was rarely taken . We tried accelerating it by using a smaller value of the estimate of the Lipschitz constant of the gradient of L , but it leads to divergence after a rapid decrease of the objective for the first few iterations .__label__Method|Tool|Use
A collaborative filtering dataset can be interpreted as the incomplete observation of a ratings matrix with columns corresponding to users and rows corresponding to items . The goal is to infer the unobserved entries of this ratings matrix . We evaluate DFC on two of the largest publicly available collaborative filtering datasets : MovieLens 10M _CITE_ ( m = 4K , n = 6K , s > 10M ) and the Netflix Prize dataset ( m = 18K , n = 480K , s > 100M ). To generate test sets drawn from the training distribution , for each dataset , we aggregated all available rating data into a single training set and withheld test entries uniformly at random , while ensuring that at least one training observation remained in each row and column . The algorithms were then run on the remaining training portions and evaluated on the test portions of each split .__label__Material|Data|Use
The UIUC dataset contains 314 cluttered indoor images , of which the ground - truth is two label maps of background layout with / without foreground objects . Our dataset contains 220 images which cover six indoor scene categories : bedroom , living room , kitchen , classroom , office room , and corridor . The dataset is available on the project webpage _CITE_ . The ground - truths are hand labeled segments for scene components for each image . Our algorithm usually takes 20s in clustering , 40s in sampling , and 1m in preparing input features .__label__Material|Data|Produce
The UIUC dataset contains 314 cluttered indoor images , of which the ground - truth is two label maps of background layout with / without foreground objects . Our dataset contains 220 images which cover six indoor scene categories : bedroom , living room , kitchen , classroom , office room , and corridor . The dataset is available on the project webpage _CITE_ . The ground - truths are hand labeled segments for scene components for each image . Our algorithm usually takes 20s in clustering , 40s in sampling , and 1m in preparing input features .__label__Supplement|Website|Produce
The combinatorial algorithm was implemented in Matlab and is available online . The ILPs were implemented using CPLEX Python API and solved using CPLEX 12 . The implementation is available as a part of TWILP software _CITE_ . Combinatorial algorithm . As the worst - and best - case running time of the combinatorial algorithm are the same , we tested it with synthetic data sets varying the number of nodes n and the vertex cover bound k , limiting each run to at most 24 hours .__label__Method|Code|Produce
There are many samples generated by the models with MinVI objective that look clearly better than those generated by the model with ML objective . In this section , we evaluate our methods on MIR - Flickr database [ 11 ], which is composed of 1 million examples of image and their user tags collected from the social photo - sharing website Flickr . _CITE_ Among those , 25000 examples are annotated with 24 potential topics and 14 regular topics , which leads to 38 classes in total with distributed class membership . The topics include object categories such as dog , flower , and people , or scenic concepts such as sky , sea , and night . We used the same visual and text features as in [ 27 ].__label__Material|Data|Use
There are many samples generated by the models with MinVI objective that look clearly better than those generated by the model with ML objective . In this section , we evaluate our methods on MIR - Flickr database [ 11 ], which is composed of 1 million examples of image and their user tags collected from the social photo - sharing website Flickr . _CITE_ Among those , 25000 examples are annotated with 24 potential topics and 14 regular topics , which leads to 38 classes in total with distributed class membership . The topics include object categories such as dog , flower , and people , or scenic concepts such as sky , sea , and night . We used the same visual and text features as in [ 27 ].__label__Supplement|Website|Use
The overall home ranges estimated with a - LoCoHs and T - LoCoHs were very similar ( Fig 2 ), which was expected as T - LoCoH was developed as an extension of the location - based a - LoCoH [ 30 ]. The area estimates were also smaller than the GCM and BRB , which is supported by simulated LoCoH studies showing the hulls created essentially ‘ hug ’ the data [ 25 , 30 ]. However , this also means that the LoCoH PLOS ONE | _CITE_ March31 , 2017 14 / 23 Evaluating home range estimators using GPS collars methods are not as strong at modelling spatial uncertainty associated with GPS fixes [ 30 ]. They both perform most effectively with large data sets [ 76 ]; a - LoCoH has been shown to converge on the true range as sample size increases [ 25 ]. BRB appeared to show the best overall performance , producing high and robust AUC values , while not showing as much sensitivity to sample size or fix frequency as GCM , which had similarly high AUC values .__label__Supplement|Paper|Introduce
We used data on individuals (≥ 16 years ) of self - reported European ancestry from 29 studies from the CARTA consortium ( _CITE_ ): the 1958 Birth Cohort ( 1958BC ), the Avon Longitudinal Study of Parents and Children ( ALSPAC , including both mothers and children ), the British Regional Heart Study ( BRHS ), the British Women ’ s Heart and Health Study ( BWHHS ), the Caerphilly Prospective Study ( CaPS ), the Christchurch Health and Development Study ( CHDS ), CoLaus , the Danish Monica study ( Dan - MONICA ), the Exeter Family Study of Child Health ( EFSOCH ), the English Longitudinal Study of Ageing ( ELSA ), the National FINRISK studies , GEMINAKAR , GS : SFHS ( Generation Scotland : Scottish Family Health Study ), the Genomics of Overweight Young Adults ( GOYA ) females , GOYA males , the Helsinki Birth Cohort Study ( HBCS ), Health2006 , Health2008 , the Nord - Trfandelag Health Study ( HUNT ), Inter99 , MIDSPAN , the Northern Finland Birth Cohorts ( NFBC 1966 and NFBC 1986 ), the National Health and Nutrition Examination Survey ( NHANES ), the MRC National Survey of Health & Development ( NSHD ), the Netherlands Twin Register ( NTR ), the PROspective Study of Pravastatin in the Elderly at Risk ( PROSPER ) and Whitehall II . All studies received ethics approval from the local research ethics committees . Further details of these studies are provided in online supplementary material .__label__Material|Data|Use
We used data on individuals (≥ 16 years ) of self - reported European ancestry from 29 studies from the CARTA consortium ( _CITE_ ): the 1958 Birth Cohort ( 1958BC ), the Avon Longitudinal Study of Parents and Children ( ALSPAC , including both mothers and children ), the British Regional Heart Study ( BRHS ), the British Women ’ s Heart and Health Study ( BWHHS ), the Caerphilly Prospective Study ( CaPS ), the Christchurch Health and Development Study ( CHDS ), CoLaus , the Danish Monica study ( Dan - MONICA ), the Exeter Family Study of Child Health ( EFSOCH ), the English Longitudinal Study of Ageing ( ELSA ), the National FINRISK studies , GEMINAKAR , GS : SFHS ( Generation Scotland : Scottish Family Health Study ), the Genomics of Overweight Young Adults ( GOYA ) females , GOYA males , the Helsinki Birth Cohort Study ( HBCS ), Health2006 , Health2008 , the Nord - Trfandelag Health Study ( HUNT ), Inter99 , MIDSPAN , the Northern Finland Birth Cohorts ( NFBC 1966 and NFBC 1986 ), the National Health and Nutrition Examination Survey ( NHANES ), the MRC National Survey of Health & Development ( NSHD ), the Netherlands Twin Register ( NTR ), the PROspective Study of Pravastatin in the Elderly at Risk ( PROSPER ) and Whitehall II . All studies received ethics approval from the local research ethics committees . Further details of these studies are provided in online supplementary material .__label__Supplement|Website|Use
During data reduction , the SAS intensity data also should be placed on an absolute scale in units of cm - 1 by comparison with the incident beam flux or the scattering from pure H2O ( Orthaber et al ., 2000 ; Jacrot & Zaccai , 1981 ). Pure H2O is a readily accessible , universal standard whose scattering has been well characterized over a wide range of temperatures . Secondary standards are also available , such as glassy carbon ( see the new NIST Standard Reference Material 3600 ; _CITE_ Allen et al ., 2017 ). Absolute scaling enables the direct comparison of SAS data from different instruments , including X - ray and neutron sources , without arbitrary scaling and also enables the determination of M or V from I ( 0 ) without reference to the scattering from a reference protein . In the case of SANS , it has been routine to place the data on an absolute scale .__label__Supplement|Document|Use
This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http :// creativecommons . org / licenses / by / 2 . 0 ), which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited . Hofer er al . BMC Genomics 2011 , 12 : 262 Page 2 of 6 _CITE_ [ 11 ], the Atlas of Gene Expression in Mouse Aging Project ( AGEMAP ) [ 12 ], and the NetAge database [ 13 ]. However , to the best of our knowledge , there is no database which contains microarray gene expression data together with orthologous genes , ageing - related microarray miRNA expression data as well as data of follow - up experiments . We have therefore initiated the development of a database GiSAO . db ( Genes involved in senescence , apoptosis and oxidative stress ) to support ongoing and future studies in experimental ageing research .__label__Supplement|Paper|Introduce
Horev - Azaria et al . Particle and Fibre Toxicology 2013 , 10 : 32 Page 12 of 17 _CITE_ streptomycin ( Gibco , Invitrogen Corporation , Italy ). Cell preparations were maintained in standard cell culture conditions ( 37 ° C , 5 %, CO2 and 95 % humidity , HERAEUS incubator , Germany ) [ 25 ]. TK6 : TK6 cells are human lymphoblastoid cell line ( purchased from ATCC ).__label__Supplement|Paper|Introduce
Murine hepatoma Hepa1 - 6 cells ( ACC 175 ) were obtained from the Leibnitz Institute DSMZ ( German Collection of Microorganisms and Cell Cultures , Braunschweig , Germany ). Cells were cultured in Dulbecco ’ s MEM with 4 . 5 g / l glucose ( Life Technologies ) complemented with 10 % heat - inactivated fetal calf serum ( PAN Biotech , Aidenbach , Germany ) at 37 ° C in a humidified atmosphere containing 95 % air and 5 % CO2 . PLOS Neglected Tropical Diseases | _CITE_ January 12 , 2018 4 / 25 B . pseudomallei benefits from changing host iron balance Generation and cultivation of primary murine macrophages Bone marrow - derived macrophages ( BMM ) were generated and cultivated in a serum - free cell culture system as recently described [ 34 ].__label__Supplement|Paper|Introduce
Database search for potential anti - AD targets We searched the NHGRI - EBI GWAS Catalog ( _CITE_ ) to extract AD - associated genetic variations ; and the Human Metabolome Database ( HMDB ) to extract ADrelated metabolites . To shortlist AD - related proteins and epigenetic changes , we searched the PubMed database up to June 2016 using the keywords : “ Alzheimer ’ s disease and proteomics ”, “ Alzheimer ’ s disease and protein / proteomics ”, “ Alzheimer ’ s disease and DNA methylation ”, “ Alzheimer ’ s disease and epigenetics ”. We incorporated this literature in our study according to the following criteria : 1 ) all samples ( e . g ., serum , plasma , urine or tissue ) had to be human ; 2 ) the disease diagnosis had to be “ Alzheimer ’ s disease ” or “ Late - onset Alzheimer ’ s disease ”; and 3 ) for proteins , all samples had to be CSF .__label__Material|Data|Use
Following kidney transplantation , the risk of death is highest in the period immediately after surgery , but decreases sharply and then changes direction when the risk of death starts to gradually increase over time . While a number of standard parametric models ( such as the exponential , Weibull or loglogistic ) are available and © 2016 Li et al . Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Li et al .__label__Supplement|License|Other
DBpedia _CITE_ Ontology of wikipedia entries DBpedia : A Nucleus for a Web of Open Data SO [ sequence http :// www . Ontology of genetic sequences The Sequence Ontology : a tool for the unification of genome ontology ] sequenceontology . org / annotations GO [ gene http :// www . Ontology of biological Gene ontology : tool for the unification of biology .__label__Material|Data|Introduce
SIFTS is the authoritative source of up - to - date residue - level annotation of structures in the PDB with data available in UniProt , CATH ( Greene et al ., 2007 ), SCOP ( Andreeva et al ., 2008 ), GOA ( Barrell et al ., 2009 ), InterPro ( Hunter et al ., 2009 ) and Pfam . SIFTS itself is used by major resources such as Pfam , CATH , RCSB , DAS server providers ( http :// www . dasregistry . org /) and many research and service groups around the world . In the future , SIFTS might be extended to link PDB data to other resources such as ChEMBL ( _CITE_ protein – ligand interaction data ), IntAct ( Aranda et al ., 2010 ; macromolecular interaction data ), Reactome ( Matthews et al ., 2009 ; biological pathway data ), ChEBI ( De Matos et al ., 2010 ; ligand chemistry and function data ) and EnsEMBL ( Flicek et al ., 2010 ; SNPs and genetic variation data ). Although data - integration efforts have resulted in an infrastructure that allows the easy transfer of information and__label__Material|Data|Extent
Authors : Erlend I . Fossen , Torbjørn Ekrem , Anders N . Nilsson , Johannes Bergsten Data type : Word file Explanation note : Word file containing a list and descriptions of additional morphological characters that were measured . Contains 11 external body and 8 male genital characters . Copyright notice : This dataset is made available under the Open Database License ( _CITE_ ). The Open Database License ( ODbL ) is a license agreement intended to allow users to freely share , modify , and use this Dataset while maintaining this same freedom for others , provided that the original source and author ( s ) are credited . 120 Erlend I . Fossen et al .__label__Supplement|License|Other
gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = ee1233f263f94268ac62dc4cd358cd12 , http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = 00851eb8fb2c4050b581ab898b9d228e , http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = 1f905afc070241a58c74672b40333ac0 , http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = e6663cf00af64a928def7a70108a2b19 , http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = f2f4d89b7dbc4fc0ae78591b36f71585 , http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = c26168effc244aedb95fbea7de289aaf , http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = 7e45eb1bd3c34cc9a8a27bf20c182f4d ; NIST library subnetwork with cosine score & lt ; 0 . 7 http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = 8b0b5a467da9416b81ab4f925a4f4b43 ; for Fecal , Euphorbia dendroides extracts and Fungal dataset http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = f0cabc92247d44789900944a69874e8a , http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = ce2a564dbd704c0595494e04798b0233 , http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = b753797b0dad4f1e84142dd59c84615b ; and finally for CASMI negative mode http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = a3f02b1b648 a43b6a210063a4ee2f787 and positive mode http :// gnps . ucsd . edu / ProteoSAFe / status . jsp ? task = 231902c6d75f41df8403e454c96e8d4a . The parameters can be accessed by cloning the job or at the link “ Networking Parameters and Written Network Description ”. The corresponding NAP jobs can also be accessed through the web interface with the following job IDs : for NIST library - _CITE_ task = 29d517e67067476bae97a32f2d4977e0 , http :// proteomics2 . ucsd . edu / ProteoSAFe / status . jsp ? task = d270e79876cb48deb6aabd52a4fc647e , http :// proteomics2 . ucsd . edu / ProteoSAFe / status .__label__Supplement|Website|Use
YouTube is open access and also commonly used , yet automating data extraction and content analysis from videos is challenging . As a result , this digital data stream has primarily been evaluated with respect to its potential to spread misinformation or clinically useful material rather than as an epidemic surveillance or prediction tool [ 57 ]. Nevertheless , PLOS Neglected Tropical Diseases | _CITE_ November 30 , 2017 7 / 13 particular mention should be made of the study by Alasaad , who sought to evaluate the surveillance potential of Facebook and YouTube posts concerning probable leishmaniasis cases in Syria [ 12 ]. While this study had a very limited validation and the reference data set was difficult to reproduce ( Skype - calling individual clinicians in conflict zones with deteriorating public health infrastructure to confirm whether leishmaniasis had been seen in regions where YouTube and Facebook posts indicated disease ), the findings were nevertheless valuable for exploring a possible public health application of social media in a conflict zone . Limitations , challenges and future directions in the application of internet - based VBD biosurveillance The putative advantages of internet data for communicable disease surveillance are clear ; namely , these data are free , fast , and may offer valuable surveillance signals in regions with limited conventional surveillance infrastructure and rising internet access .__label__Supplement|Paper|Introduce
Inverse predicates do not exist in the OpenLifeData SPARQL endpoints , but rather are simply defined in the OWL logic that defines the entities and relationships in those endpoints . As such , we rely on logical reasoning to determine that an inverse invocation can be solved equally well by a ‘ forward ’ query ; thus the González et al . Journal of Biomedical Semantics 2014 , 5 : 46 Page 6 of 12 _CITE_ query that serves both forward and inverse services is identical . SADI service implementation To serve the OpenLifeData data , a single Perl script using the standard SADI :: Simple code libraries act as the SADI Service Daemon for all services . The script listens for HTTP calls to URLs of the form : In this URL , SADI is the name of the OpenLifeData2SADI Service script , while the additional path information ( namespace and service name ) are used as keys to access the configuration file and SPARQL query file appropriate for that service , as described above .__label__Supplement|Paper|Other
The skull from Gypsum Cave ( GCS ) can be distinguished from that of the francisci holotype ( fHS ) by its slightly larger size , and markedly longer and more slender rostrum , both Heintzman et al . eLife 2017 ; 6 : e29944 . DOI : _CITE_ 41 of 43 Research article Genomics and Evolutionary Biology absolutely and as a percentage of the skull length . The rostrum of the GCS is also absolutely narrower ; the fHS , despite being the smaller skull , is transversely broader at the i / 3 . The palatine foramina are positioned medial to the middle of the M2 in the GCS , whereas they are medial to the M2 - M3 junction in the fHS .__label__Supplement|Paper|Introduce
Because this test does not test independence of the samples , we include in the test statistic the Spearman ’ s rank correlation [ 38 ] of the residuals of the fit , ZS ( also distributed as a χi ) because if the residuals are correlated , the data are not independent . The p - value is thus computed by measuring how extreme K2 = Z2s + Z2k + Z2S is in the χ23 distribution ( with 3 degrees of freedom ). The implementation of this is available at _CITE_ In the population model , the calculation of the p - value must be different , because the variance is not being left as a free parameter , so we take a more classical approach . The p - value is computed by measuring how extreme is the difference between the data and its fit with respect to the difference between a sample from the model and its fit .__label__Method|Code|Produce
the phylogenetic distance between genomes selected for comparison based on the 16S alignment derived from the SILVA database ( 29 ). For genes whose sequence is not included in the alignment the closest match is used , if the identify of it to the 16S gene of the IMG taxon is > 97 %. The distance tree is displayed using the Archaeopteryx tool ( _CITE_ ), which uses phyloXML for data exchange ( 30 ). Each node in the tree hyperlinked to the IMG genome page for that node . The ‘ Radial Phylogenetic Tree ’ tool originally developed for MG - RAST ( 31 ), allows comparing the BLAST hits of the genes of up to 5 user selected genomes to the genes of all the genomes in the database using a colour - coded hierarchical circular tree viewer .__label__Method|Tool|Use
Supporting information : this article has supporting information at journals . iucr . org / d Acta Cryst . ( 2018 ). D74 , 441 – 449 _CITE_ 441 research papers images can be collected via the rotation method , where integrated Bragg intensities are experimentally obtained . At cryogenic temperatures , small - wedge ( 5 – 10 °) data collection can be a good compromise to obtain strong diffraction signals under tolerable doses , and tens to hundreds of data sets are usually sufficient to obtain a high - resolution structure . Efficient data collection can be achieved and easily automated when multiple microcrystals are held in a sample holder .__label__Supplement|Document|Introduce
Supporting information : this article has supporting information at journals . iucr . org / d Acta Cryst . ( 2018 ). D74 , 441 – 449 _CITE_ 441 research papers images can be collected via the rotation method , where integrated Bragg intensities are experimentally obtained . At cryogenic temperatures , small - wedge ( 5 – 10 °) data collection can be a good compromise to obtain strong diffraction signals under tolerable doses , and tens to hundreds of data sets are usually sufficient to obtain a high - resolution structure . Efficient data collection can be achieved and easily automated when multiple microcrystals are held in a sample holder .__label__Supplement|Paper|Introduce
Then we evaluated each edge for a set of conditions . Edges were considered to be added to the autonomic dysfunction network ( absent for control but present for the autonomic dysfunction PLOS Computational Biology | _CITE_ July21 , 2017 12 / 39 Data - driven modeling of multi - organ networks driving physiological dysregulation phenotype ) if the following conditions were satisfied :__label__Supplement|Paper|Extent
It may be noted that though both ReadDepth and CBS in iCopyDAV uses the same segmentation approach , the performance of CBS is better than ReadDepth for sequencing coverage & lt ; 40 × due to the difference in data pre - treatment approaches in two cases . From the comparison of recall and precision values of CBS and TVM in iCopyDAV with the three DoC - based tools , we observe higher recall with both TVM and CBS approaches for the detection of CNVs & lt ; 1 Kb , while for the detection of CNVs > 1 Kb all the DoC - based methods showed recall values approaching ‘ 1 ’ with increase in sequencing coverage ( S3 ( A ) Fig ). The precision ~ 1 with both CBS and TVM in the detection of small and large CNVs PLOS ONE | _CITE_ April 5 , 2018 16 / 37 iCopyDAV : Integrated platform for detection , annotation and visualization of copy number variations ( even at 5x sequencing coverage for TVM ), while precision values ( combined of small and large CNVs ) of ReadDepth , CNVnator , and Control - FREEC are — 0 . 71 , — 0 . 51 and — 0 . 26 , respectively , at 30x coverage ( S4 ( B ) Fig ). Also , it is observed that in case of ReadDepth and Control - FREEC , a reduction in precision is observed with increase in sequencing depth due to increase in the number of false positives . This clearly indicates the importance of appropriate normalization of read depth signals .__label__Supplement|Paper|Introduce
The capability of the software is now such that it can stand in for an expert crystallographer even in challenging cases . It is of particular use when driving command - line programs that do not have their own built - in graphical interfaces and can be intimidating or tedious to operate for many users . Although xia2 is itself primarily a command - line program , it has a structured interface for optional parameters using the Phil syntax of cctbx ( _CITE_ ). This interface is rich enough to describe the basic components of a GUI , including parameter types , tooltips , help strings and expert levels . It is possible to map most elements of the Phil interface onto a def file .__label__Method|Tool|Use
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ Supplementary Information accompanies this paper on European Journal of Human Genetics website ( http :// www . nature . com / ejhg )__label__Supplement|License|Other
Supplementary Information accompanies this paper on the Oncogene website ( _CITE_ )__label__Supplement|Website|Produce
Supplementary Information accompanies this paper on the Oncogene website ( _CITE_ )__label__Supplement|Document|Produce
Figure 2 ( d )). Summaries of elicited priors and PDFs were generated by user - written R [ 27 ] code calling the ‘ Shiny ’ package [ 28 ] to create a user - friendly interface . R routines implementing the methods described in this paper are available at _CITE_ ). html . Upon receiving feedback , the experts were allowed to revise their answers to ( i )–( iv ) until they were satisfied with the fitted PDFs as representations of their prior opinion . Redundant questions ( v ) and ( vi ) formed part of the conversations between experts and the statistical facilitators .__label__Method|Code|Produce
hT is study was partially funded by the FP7 project ViBRANT ( Virtual Biodiversity Research and Access Network for Taxonomy , _CITE_ ) ( to DR , LP , PS and TG ) and by NSF ( PBI Solanum : a worldwide treatment , DEB - 0316614 , to SK ). Work in Paraguay was supported by the Darwin Initiative of the UK Government ’ s Department of Environment , Food and Rural Affairs ( Defra 12011 – 2003 - 2006 ). We thank G . Hagedorn and D . Roberts for very helpful reviews .__label__Supplement|Website|Introduce
( E ) Exemplar motifs of a tutor and three of his 65d pupils , each of which was injected with a different viral construct at 30d . These examples illustrate the percent similarity depicted in panel D . ( F ) Summary of the learning and variability phenotypes observed after virus injection . DOI : _CITE_ The following source data and figure supplement are available for figure 2 : Source data 1 . Contains the effect sizes for each syllable that are presented in ( C ). DOI : https :// doi . org / 10 . 7554 / eLife . 30649 . 010 Source data 2 .__label__Supplement|Paper|Introduce
The datasets generated during and / or analysed during the current study are available in the documentation webpage ( _CITE_ ) of the software that is deposited to the CRAN ( http :// cran . r - project . org / package = XGR ).__label__Material|Data|Produce
The datasets generated during and / or analysed during the current study are available in the documentation webpage ( _CITE_ ) of the software that is deposited to the CRAN ( http :// cran . r - project . org / package = XGR ).__label__Supplement|Website|Produce
I don ' t imagine the use cases of these type of scientists ' behaviour in other countries are that different , but I would expect some pointers to this wider context . The referenced literature is good and covers many of the key sources I would refer to . However my own organisation in the UK has been advising on data documentation , including use of Excel and conversion issues , for some years , so it would be good to cite some examples of other efforts to address these issues on the non - ecology field and offer examples of non US resources that provide extensive data management advice ( _CITE_ ). On page 6 the checklist of issues is very clear and useful and great to alert researchers to these issues upfront . In terms of platforms for the tool , I think a Mac version will be important .__label__Supplement|Document|Produce
The use of a restricted database is expected to result in largely the same positive read classification regarding taxa that are included in the restricted reference . However , short - and / or low - complexity reads may exhibit higher misclassification rates , particularly when restricted reference data sets are used ( Schmieder and Edwards , 2011 ; Breitwieser et al ., 2017 ). To avoid issues with spurious classifications of short - and low - complexity reads , all metagenomic data analyzed here were first pre - processed with tools implemented in String Graph Assembler ( Simpson and Durbin , 2012 ) to remove duplicate and low - complexity reads using the subprograms ‘ preprocess ’ and ‘ filter ’ ( _CITE_ ). A second and possibly likelier cause of differing results , Centrifuge performs a rapid gapped alignment ( Kim et al ., 2016 ), whereas HOLI does not allow gaps in read alignments under 50 bp ( Pedersen et al ., 2016 ). Gapped alignment may allow reads to map more ambiguously to similar sequences across broader taxonomic scales , causing fewer genera to be unambiguously classified by Centrifuge versus HOLI and overall more conservative inferences made by Centrifuge ( Fig .__label__Method|Tool|Use
It would be reasonable to assume that samples one , two and three all contain a similar number of transcripts for this gene . Patrick et al . BMCBioinformatics 2013 , 14 : 31 Page 3 of 10 _CITE_ were mapped ) [ 21 ] would be appropriate if isoforms were mutually exclusive . Unfortunately there is often evidence of multiple isoforms for a gene being present . If the abundance of these isoforms could be accurately estimated [ 21 ] it may be possible to estimate the rate of transcription by summing the FPKM of all isoforms of a gene .__label__Supplement|Paper|Introduce
Clomial 1 . 3 . 0 available at R - bioconductor . SciClone available from https :// github . com / genome / sciclone . PhyloWGS available from _CITE___label__Method|Tool|Use
Supplemental information for this article can be found online at _CITE_ Saarela et al . ( 2018 ), PeerJ , DOI 10 . 7717 / peerj . 4299 55 / 71__label__Supplement|Document|Produce
Annotated disease causing variants ( DM ) were mapped to 2220 different genes . These genes were then used to test for enrichment of essential genes by 2x2 contingency table analysis using Fisher ’ s exact test . Gene expression data To address gene expression patterns of essential genes , we obtained a list of 13 , 629 genes ranked by the coefficient of variation of their gene expression [ 22 ] derived from the Gene Expression Omnibus ( _CITE_ ) database . This ranking places genes along the spectrum from ubiquitously expressed to tissue - specific . The distribution of 2 , 003 out of 2 , 472 EG genes showed that essential genes fall across the continuum of gene expression ( Figure S15 ).__label__Material|Data|Extent
We downloaded the combined annotation dependent depletion ( CADD ) score precalculated on 1000 Genome phase 3 variants from http :// cadd . gs . washington . edu / download / and used the scaled version ( PHRED - like score ). We downloaded the fitness consequence of functional annotation ( fitCons ) score integrated across the three ENCODE cell types from http :// compgen . cshl . edu / fitCons / 0downloads / tracks / current / i6 / scores /, and we used the highly significant scores ( P & lt ; 0 . 003 ) as defined by the authors . We obtained the contextual analysis of transcription factor occupancy ( CATO ) scores precalculated at 13 . 4 million single nucleotide polymorphisms ( SNPs ) overlapping with DNase Hypersensitivity Site from _CITE_ We mapped all scores to the 200 - bp windows used in this study and obtained the maximum score ( 0 if not available ) within each window . In the regression analysis , we used log (( x + 1e - 4 )/( 1 − x + 1e - 4 )) transformed scores as the response and we used the epigenetic states in the corresponding windows as dummy predictors .__label__Supplement|Document|Use
Many large funding organizations , including the National Institutes of Health ( NIH ), encourage researchers to make their data available in public databases . Policies like the NIH ’ s Genomic Data Sharing policy ( _CITE_ ) and other incentives around data sharing have promoted the development of several public data repositories . However , in spite of the availability of data , it can still be challenging to harness the power of these public databases , and researchers are faced with a variety of barriers in accessing shared data ( van Schaik et al ., 2014 ). A major obstacle to data discovery is the disconnectedness of various data sharing resources .__label__Material|Data|Introduce
alization window showing also a graphical representation of the gene domain composition ( Figure 3 ). For NRPS / PKS cluster types , the predicted peptide monomer composition and its corresponding SMILES formula are specified . The predicted chemical structure is displayed as well using the SMILES Depictor web service ( _CITE_ ). Below the graphical representation of the predicted antiSMASH cluster , a summary of MIBiG Clusters similarities , BGC gene composition as well as tailoring cluster similarities are given . This last item relies on a knowledge database provided with antiSMASH about tailoring clusters already described in known BGCs and associated with publications .__label__Supplement|Website|Use
Using binomially distributed maximum - likelihood estimation , we calculated the corrected Akaike Information Criterion ( AICc ) values that correct for bias associated with small sample sizes [ 511 to test power and exponential functions with these data . We analysed the data using R Statistical Software [ 521 v . 3 . 2 . 5 and packages bbmle [ 531 , car [ 541 , Hmisc [ 551 , lattice [ 561 and Rcpp [ 571 . Data and R code are available in electronic supplementary materials and on the Dryad Data Repository ( _CITE_ ) [ 581 .__label__Method|Tool|Use
CRAM [ 17 ], working with BAM - based input , records the variances of reads and a reference genome with Huffman coding . Quip [ 11 ] with ‘- r ’ © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Huang et al .__label__Supplement|License|Other
To characterize the storm , we downloaded wind and mean sea level pressure ( SLP ) data from the ERA - Interim global atmospheric reanalysis dataset ( _CITE_ interim_full_daily ) produced by the European Centre for Medium Range Weather Forecast ( ECMWF ), for which data is available from 1979 to the present . The data assimilation system used to produce the ERA - Interim data is based on an updated atmospheric model , including a four - dimensional variational analysis ( 4D - VAR ) with a 12 - h analysis window .__label__Material|Data|Use
All used R libraries are freely available and their licensing conditions are specified on the download page of each specific library in the appropriate R repository , such as CRAN or BioConductor . GenePattern server software is freely available under the GenePattern License Agreement . The text of this license agreement is available at _CITE___label__Supplement|License|Other
The code used in this work is provided at _CITE_ There are two main scripts in this folder - 1 ) joptb88vdw . py and 2 ) master . py . The joptb88vdw . py script heavily utilizes the Pymatgen8 and ASE42 codes for file and data management .__label__Method|Code|Produce
The experimental setup is at Fukuoka University where electrophysiological measurements ( Figure 1A ), electrophysiological analyses ( Figure 1D ), and imaging ( Figure 1B ) are performed . The image stacks are used at the University of Hyogo for neuronal segmentation ( Figure 1C ). The resulting 3D neuronal segmentations are then normalized by registering them to the Honeybee standard brain ( HSB ; _CITE_ ), which is done at Fukuoka University . Finally , morphological analyses , simulations , and further analyses are done at Ludwig - Maximilians - Universität München ( LMU ) ( Figure 1E ).__label__Method|Tool|Use
The data is deposited at Dryad ( Staab et al . 2018 , http :// dx . doi . org / 10 . 5061 / dryad . h6j0g4p ) and can be freely accessed as virtual representation of the type . In addition to the cybertype data at Dryad , we also provide a freely accessible 3D surface model of the holotype at Sketchfab ( _CITE_ ). Diagnosis . Proceratium kepingmai differs from the other members of the P . itoi clade by the following character combination : large species ( TL 4 . 39 – 4 . 54 ); sides of head weakly convex , broadest at level of eyes and gently narrowing anteriorly and stronger posteriorly ; vertex almost straight ; very reduced eyes ( OI 2 – 3 ) consisting of a single minute ommatidium ; frontal carinae well developed , with large lamellae that extend laterally above the antennal insertions ; frontal furrow darker than the surrounding anterior cephalic dorsum ; posterodorsal corners of the propodeum broadly angular ; propodeal declivity densely punctured , mostly opaque ; posterior face of petiolar node in profile steeper than anterior face and about half as long as anterior face ; apex of petiolar node distinctly broader than long in dorsal view ; in addition to dense pubescence , erect hairs present on scapes and dorsal surface of body , longest of those hairs at most as long as the maximum dorsoventral diameter of metafemur .__label__Supplement|Website|Use
Although the current TAIR annotation files may be accessed through the PO SVN repository site , they are not displayed on the PO browser until the next release . The Sol Genomics Network ( SGN ). The SGN ( _CITE_ ) database hosts genomic , phenotypic and taxonomic information on Solanaceae and related species , mostly from the asterid clade . As a clade - oriented database , SGN ’ s main focus is to exploit the high level of genome conservation in the Solanaceae family for comparative querying of phenotype and genotype data . For this purpose , PO is extensively used for annotating functional genes , gene models and phenotyped germplasm , such as mutants and mapping populations .__label__Material|Data|Introduce
Although hemaClass . org is still separated from the clinic we believe that a web based tool and suggestion for a clinical reference sample will bring cancer classification closer to the clinic . Hopefully , this work can also spawn interesting discussions on the clinical requirements of GEP based diagnostic and prognostic tools . All material for reproducing this paper and its results is found at _CITE_ Comments , suggestions , bug reports , and other issues are warmly welcome at https :// github . com / oncoclass / hemaclass / issues or by mail to the corresponding author .__label__Supplement|Document|Produce
VectorBase also assists the community with a helpdesk system at info @ vectorbase . org or via our ‘ Contact Us ’ link ( _CITE_ ). Helpdesk inquiries can be of any type , and we supplement our availability with online documentation such as FAQs , a Glossary of relevant terms and data policies . Constantly updated VectorBase tutorials provide training material for both novice and advanced users , and include practice exercises and sample files .__label__Supplement|Website|Produce
The interpreter would frequently switch back and forth between them to examine possible features in various data representations and the aerial photography . The LRM visualization was created with the LiVT software , developed by Ralf Hesse ( http :// sourceforge . net / projects / livt /), all other visualizations with the RVT 1 . 1 toolbox developed by Zakšek et al . ( 2011 ) ( _CITE_ ).__label__Method|Tool|Use
Thus we thought it important to look at the specification of qualifications from different exam boards to see how the term ‘ research ’ is used within this documentation . We focussed the investigation onto GCSE and GCE qualifications in biology as an example as the schools in this study all offer these courses . At GCSE level the pupils are expected to consider evidence from different areas of scientific research , as shown by statements that include “ explain how new evidence from DNA research and the emergence of resistant organisms supports Darwin ’ s theory ” ( p . 20 ) ( _CITE_ ) as well as to think about the “ the social and ethical issues concerning the use of stem cells from embryos in medical research and treatments ” ( p . 39 ) ( http :// filestore . aqa . org . uk / subjects / AQA - BIOLW - SP - 14 . PDF ). The OCR specification also clearly links the term ‘ research ’ to fact - finding , e . g . “ research diabetes and how it can be managed ’ ( p . 24 ) and ‘ research the work of John Ray and Carl Linnaeus in developing a modern classification system ” ( p . 30 ) ( http :// www . ocr . org . uk / Images / 82545 - specification . pdf ).__label__Supplement|Document|Produce
A total of 786 glycan array files for plant lectins were downloaded using a custom made script from Consortium for Functional Glycomics ( CFG ) as of Dec 2013 . CFG provides extensive glycomics resources so that one can explore functions of glycans and glycan - binding proteins that play important roles in human health and disease [ _CITE_ All of these 786 files were further processed into a single input file , which consists of rows of protein - carbohydrate pairs . Three datasets were generated by filtering the protein - carbohydrate pairs using the cutoff values of relative fluorescence units ( RFU ) 5000 , 10000 and 20000 .__label__Material|Data|Use
Two final learning blocks , two 30 min memory blocks and one novel pairs block were missing from experiment 1 . All error bars in the figures are standard error of the mean ( SEM ). MATLAB ( RRID : SCR_001622 ) was used for data processing ( code available at _CITE_ Grogan , 2017 ), and SPSS ( RRID : SCR_002865 ) for statistical tests . A copy of the code is available at https :// github . com / elifesciences - publications / Effects - of - dopamine - on - RL - consolidation - in - PD .__label__Method|Code|Use
The source code for OABrowser is available on GitHub at https :// github . com / mhalle / oabrowser /. The SPL brain atlas is available at https :// github . com / mhalle / splbrain - atlas . OABrowser and the SPL atlases are available under terms of the open - source 3D Slicer license : _CITE___label__Supplement|License|Other
The methods presented in this study will be used as a standardised analytical approach for assessing potential resistance mutations in the Relational Sequencing TB Data Sharing Platform currently available at _CITE_ The ReSeqTB platform serves as a globally harmonised knowledge base for the curation , validation and interpretation of existing and newly created genotypic and phenotypic data for TB drug resistance correlations [ 18 ]. In this context , the grading system presented here will be refined further by taking the following criteria into consideration : phylogenetic information , laboratory evidence ( e . g .__label__Material|Data|Use
For the influenza A internal protein Page 2 of 19 Figure 1 Flow chart of the PhyloMap algorithm . Zhang et al . BMC Bioinformatics 2011 , 12 : 248 Page 3 of 19 _CITE_ sequences analyzed here , the Jones - Taylor - Thornton [ 26 ] model is used to infer the distances .__label__Supplement|Paper|Introduce
Individual patient - level data may be available on request — either direct from the trial investigators and / or sponsors — or via a closed website from which data are released only after proposals for secondary analysis have been approved by a peer review process or panel ( e . g ., through the GlaxoSmithKline clinical study data initiative https :// clinicalstudydata . gsk . com /). The panel discussion at MCP 2013 focused , however , on open access to clinical trial data . To be truly open , a dataset should be deposited and shared with a license that allows text mining and other forms of unrestricted reuse ( _CITE_ ). And , as well as the anonymized patient - level data , the statistical code ; details of the extent , nature , and handling of any missing data ; and other supporting information should be available . There are many concerns about confidentiality when sharing study data , but these should be surmountable with respect to both commercial confidence ( EMA , 2013 ) and patient privacy ( Hrynaszkiewicz et al ., 2010 ; EMA , 2013 ).__label__Supplement|License|Other
The 3 , 000 articles from this search were reviewed and one project was identified that fit selection criteria , a USAID - funded child survival project implemented by World Relief in Mozambique from 1999 - 2003 . [ 8 ] A search for similar projects not published in the peer - reviewed literature was then run on USAID ’ s Development Experience Clearinghouse database ( http :// dec . usaid . gov ) and Child Survival and Health Grants database ( _CITE_ ). Five additional candidate projects were identified . Project documentation was reviewed and knowledgeable staff interviewed .__label__Material|Data|Use
We then fit a beta distribution for each of the cell types and used these distributions to calculate the probability that the score of the corresponding cell type is present in the mixture by random ( Additional file 2 : Figure S7 ). Applying this procedure to the test simulated mixtures enabled detection of about half of the non - expected nonnegligible scores as non - significant ( 46 . 9 % change — from 56 . 4 % non - negligible scores to 28 . 8 % with p value & gt ; 0 . 2 ), while detecting as non - significant only 15 . 3 % of nonnegligible scores for cell types used for generating the mixture ( from 88 . 6 % non - negligible scores to 75 . 1 %) ( Additional file 4 ). This pipeline for generating adjusted cell type enrichment scores from gene expression profiles , which we named xCell , is available as an R package and a simple web tool ( _CITE_ ). Validation of enrichment scores in simulated expression profiles We next compared the ability of xCell scores to infer the underlying cell type enrichments in simulated mixtures with a set of 53 previously published signatures corresponding to 26 cell types [ 6 , 12 , 27 , 28 ] ( Additional file 5 ). Our analyses showed that xCell outperformed the previously published signatures in recapitulating the underlying abundances , in mixtures generated using the training samples ( Additional file 2 : Figure S5 ) and the test samples ( Additional file 2 : Figure S6 ) and an independent data source ( GSE60424 [ 26 ]) ( Fig .__label__Method|Tool|Produce
Modular data processing pipelines were built using BIOVIA Pipeline Pilot v16 . 2 ( _CITE_ ). Various components and scripts were designed to parse data and metadata from databases , via APIs , and data files located in the SFTP server . The collection of pipelines performs field mappings , data joining and merging , basic validation and contains several checkpoints for manual data review .__label__Method|Tool|Use
PCR products from cell lines were subject to Sanger sequencing ( Quintara Biosciences ). For subclones with multiple KRAS mutant alleles , PCR products were cloned into a TOPO vector ( Life Technologies ) and at least 10 – 20 bacterial colonies were sequenced per cell line . The most probable offtarget genes for sgRNAs were identified using CRISPR Design ( _CITE_ edu ) and no gene had fewer than three exonic mismatches . All sequences and chromatograms were analyzed using MacVector software .__label__Method|Tool|Use
On the other hand , Dmrt1 is essential to maintain mammalian testis determination , and competing regulatory networks are involved in the maintenance of © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Cai et al . BMC Genomics ( 2017 ) 18 : 573 Page 2 of 16 gonadal sex long after the fetal choice between male and female [ 4 ].__label__Supplement|License|Other
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ © The Author ( s ) 2017 Supplementary Information accompanies the paper on the npj Genomic Medicine website ( doi : 10 . 1038 / s41525 - 017 - 0021 - 8 ). npj Genomic Medicine ( 2017 ) 19 Published in partnership with the Center of Excellence in Genomic Medicine Research__label__Supplement|License|Other
( XLSX ) S10 Table . Patient W . Contingency table formed using the average of all the training sessions . S10 Table data is located at _CITE_ ( XLSX ) S11 Table . Patient W . Contingency table formed using the average of all the feedback sessions .__label__Material|Data|Use
Since the vast majority of bioinformatics methods developed within the RNAseq and microarray data analysis end up as R packages [ 5 ], Guide makes some commonly used packages such as limma [ 6 ] readily accessible to the user without having to understand the details of the package , or having to use R directly . In addition , Guide provides the user with annotations on genes , orthologue lookups , and various other functions © 2013 Choi ; licensee BioMed Central Ltd . This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited . Choi BMCGenomics 2013 , 14 : 688 Page 2 of 7 http :// www . biomedcentral . com / 1471 - 2164 / 14 / 688 where data integration from different sources is required . This eliminates the often tedious task of gathering the appropriate pieces of information , transforming them into the correct formats and integrating them into the current analysis .__label__Supplement|License|Other
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : Data cannot be openly shared due to legal restrictions imposed by the Ministry of Health , Labour and Welfare ( URL : _CITE_ ). Applications to request data access may be sent as follows : Healthcare__label__Material|Data|Introduce
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : Data cannot be openly shared due to legal restrictions imposed by the Ministry of Health , Labour and Welfare ( URL : _CITE_ ). Applications to request data access may be sent as follows : Healthcare__label__Supplement|Website|Introduce
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : Data cannot be openly shared due to legal restrictions imposed by the Ministry of Health , Labour and Welfare ( URL : _CITE_ ). Applications to request data access may be sent as follows : Healthcare__label__Supplement|License|Introduce
For the Alzheimer ’ s Disease Neuroimaging Initiative -- A subset of SNP data used in the preparation of this article were obtained from the Alzheimer ’ s Disease Neuroimaging Initiative ( ADNI ) database ( adni . loni . ucla . edu ). As such , the investigators within the ADNI contributed to the design and implementation of ADNI and / or provided data but did not participate in analysis or writing of this report . A complete listing of ADNI investigators can be found in Supplementary Information and at _CITE___label__Supplement|Document|Produce
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ Metadata associated with this Data Descriptor is available at http :// www . nature . com / sdata / and is released under the CC0 waiver to maximize reuse . SCIENTIFIC DATA 1 2 : 150040 1 DOI : 10 . 1038 / sdata . 2015 . 40 6__label__Supplement|License|Other
Use of the Web Ontology Language ( OWL ) OBI is developed using the OWL 2 Web Ontology Language ( http :// www . w3 . org / TR / owl2 - overview /) as this provides richer semantic support than OBO format ( http :// www . geneontology . org / GO . format . obo - 1_2 . shtml )— the other commonly used alternative in the biomedical domain . The metadata scheme is implemented as OWL annotation properties . The OWL 2 Web Ontology Language ( _CITE_ ) is a W3C standard for the representation of ontologies within the larger framework of the semantic web . OWL builds on the Resource Description Framework ( RDF ; http :// www . w3 . org / TR / rdf - primer /) standard in which data is represented by sets of subject - predicate - object statements (“ triples ”) that form a directed graph . Subjects and predicates are named using Internationalized Resource Identifiers ( IRIs ; https :// tools . ietf . org / html / rfc3987 ), while the object position can be filled by an IRI or a literal value ( e . g .__label__Supplement|Document|Introduce
We constructed MELODI using the Django web framework [ _CITE_ with the following additional plugins and features . Authentication is handled via the Django Social Authentication plugin [ https :// django - social - auth . readthedocs . org / en / latest /], providing a method to make all data , jobs and results both user - specific and retrievable . As some of the database queries were proving too intensive for a responsive user experience , we integrated a task management system .__label__Method|Tool|Use
The DC elements ‘ dc : coverage ’ and ‘ dc : subject ’ have a high topical overlap ( _CITE_ ); for instance , subject elements often contain location names such as countries . The usage of the ‘ coverage ’ element – intended to denote spatial and temporal applicability – is very diverse and ranges from standardized dates with milliseconds granularity to relative - time indications such as ‘ Early Middle Ages ’, and can contain both instants and time ranges . We addressed this semantic problem by introducing a set of experimental , non - validated fields whose content is the result of a named entity recognition and geocoding .__label__Supplement|Document|Introduce
These records contain the date of birth , age , gender , residential address , the date of admission and discharge , primary discharge diagnosis , and up to 15 secondary discharge diagnoses . The criteria for data extraction in our study include : ( 1 ) a primary diagnosis of COPD ( International Classification of Diseases , 10th Revision codes : J41 – J44 ); ( 2 ) residential addresses of the patients are in urban districts of Chengdu ; ( 3 ) HAs data from tertiary hospitals and secondary hospitals . Ambient air quality data were derived from the web platform of the China National Environmental Monitoring Center ( _CITE_ ) managed by the Ministry of Environmental Protection of the People ’ s Republic of China . Data of hourly air pollution concentrations of PM2 . 5 , PM10 , SO2 , NO2 , CO and O3 from 6 air quality monitoring stations interspersed in five urban districts of Chengdu city were obtained . All areas of the five urban districts were located within 40 km radius of the monitoring stations .__label__Supplement|Website|Use
The ChemAgora portal is also a long - term strategic development , to which the European Commission ’ s Joint Research Centre is fully committed . ChemAgora has already caught the attention of other initiatives , e . g . IPCheM ( _CITE_ ), a European Commission project , which will take advantage of the search service provided by ChemAgora .__label__Method|Tool|Introduce
We confirmed association of RAC1 or CDC42 with RIT1 by co - IPs and myc - trap assays in HEK293T cells . Wild - type RIT1 co - IPed with ectopically expressed RAC1 or CDC42 , and the p . G95A substitution strengthened co - immunoprecipitation efficiency ( S6A and S6B Fig ). To demonstrate specificity of RAC1 and CDC42 binding to RIT1 , we used RHOA , another member of the RHO family of PLOS Genetics | _CITE_ May 7 , 2018 7 / 28 RIT1 and actin dynamics Fig 3 . NS - associated RIT1 amino acid changes enhance binding of RIT1 to PAK1 . ( A ) HEK293T cells were transfected with empty vector ( EV ) and RIT1 expression constructs ( WT , p . K23N , p . G31R , p . A57G , p . F82L , p . M90V , and p . G95A ) as indicated and cultured under serum - starved conditions ( 0 . 1 % serum ).__label__Supplement|Paper|Extent
The other search parameters were kept the same as in [ 16 ] ( 10 ppm precursor window , up to two missed cleavages , up to two oxidations of methionine per peptide , variable acetylation of N - termini ), except that we did not include variable modifications for the cyclization of N - terminal glutamine . For the Wu data set , we searched the spectra against the IPI Human database ver . 3 . 74 ( _CITE_ , accessed : May 22 , 2014 ) using the Tide search engine through the Crux interface . We used Tide ’ s default fragment tolerance , and the other search parameters were kept the same as in [ 17 ] ( 10 ppm precursor window , up to two missed cleavages , up to two oxidations of methionine per peptide , variable TMT labeling ( 229 . 16293 Da ) of lysine and N - terminal amino acids ). The target protein sequences were reversed to construct a decoy protein database , and separate searches were done on the target and decoy protein database for input to Percolator 3 . 0 .__label__Material|Data|Compare
All other authors not mentioned above and C . D . M ., K . M . J ., A . D . R ., A . S . F ., Z . W . and A . M . contributed to the crowdsourcing signature extraction process by submitting signatures to the database . Additional information Supplementary Information accompanies this paper at _CITE_ naturecommunications Competing financial interests : The authors declare no competing financial interests . Reprints and permission information is available online at http :// npg . nature . com / reprintsandpermissions / How to cite this article : Wang , Z . et al . Extraction and analysis of signatures from the Gene Expression Omnibus by the crowd .__label__Supplement|Document|Produce
The proposed measures are accompanied by computational time , and can be used in any other comparison study . Until now , there have been few studies where ranking metrics in Gene Set Enrichment Analysis were tested [ 29 – 31 ], however here we use a variety of ranks with powerful phenotype permutation , a large collection of data sets and statistical quality measures of gene set analysis . Finally , we have implemented GSEA method in MATLAB and named it MrGSEA ( MATLAB metric GSEA - _CITE_ ). The implementation includes all tested ranking metrics and leaves the possibility to implement new ranking metrics with the most powerful phenotype permutation . Additionally , the implementation has parallel computing capabilities .__label__Method|Code|Produce
All data used here have been released to the public and are available through the Planetary Data System ( _CITE_ ) and other sources . 2 . 2 . 1 . CheMin Diffraction patterns for the Windjana powder , as two - dimensional images from the CheMin CCD [ Blake et al ., 2012 ], were acquired for a total of 23 h over sols 623 – 632 .__label__Method|Tool|Use
All data used here have been released to the public and are available through the Planetary Data System ( _CITE_ ) and other sources . 2 . 2 . 1 . CheMin Diffraction patterns for the Windjana powder , as two - dimensional images from the CheMin CCD [ Blake et al ., 2012 ], were acquired for a total of 23 h over sols 623 – 632 .__label__Material|Data|Use
These include search services for various aspects of structural similarity ( Sequence Navigator , Structure Navigator , GIRAF , eF - seek ), function prediction and annotation ( SeSAW , SFAS ), structure prediction and modeling services ( CRNPRED , Spanner ), sequence and structure alignment ( ASH , MAFFTash ), derived databases ( EM Navigator , eF - site , ProMode ) and educational resources ( Protein Globe , eProtS , Japanese translation of the Molecule of the Month ). Among them , we describe below two recently developed services , Yorodumi and EM Navigator . Yorodumi and EM Navigator : integration with electron microscopy structures In addition to the simple summary page for each entry , PDBj also provides a more feature - rich entry - wise interface called Yorodumi ( Figure 3A , _CITE_ ). Yorodumi is an interactive and integrated interface for browsing 3D structure data not only in the PDB but also in the EMDB ( Electron Microscopy Data Bank , http :// www . ebi . ac . uk / pdbe / emdb /) ( 38 ). The user can select either Jmol ( 39 ) or jV ( 28 ) Java applets for interactive molecular graphics .__label__Supplement|Website|Produce
This reduction is likely attributable to an intensification of malaria control and eradication efforts over the period . Underpinning effective implementation of point - of - care ( PoC ), testing with RDT , is the programmatic paradigm shift from presumptive diagnosis and treatment of all fevers as malaria to a parasite - based diagnosis of malaria with microscopy © The Author ( s ) 2018 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Adah et al .__label__Supplement|License|Other
CEBS works with pathologists and ontologists on the consolidation of ontology terms . Currently , CEBS is working on a project toward linked data ( structured and computer - readable data that can be interlinked ). The diXa data infrastructure ( Ugis Sarkans , EBML - EBI ) The diXa infrastructure ( _CITE_ ) consists of a central data warehouse containing data from toxicogenomics project repositories and public databases . Apart from data from European projects , diXa also includes the public data from the National Project of Toxicogenomics in Japan , named Open TG - GATES ( Toxicogenomics ProjectGenomics Assisted Evaluation System , see below for more details ). The diXa data warehouse connects multi - omics studies , deposited in EBI resources [ ArrayExpress for transcriptomics ( http :// www . ebi . ac . uk / arrayexpress /), PRIDE for proteomics ( http :// www . ebi . ac . uk / pride / archive /), MetaboLights for metabolomics ( http :// www . ebi . ac . uk / metabolights /)] and provides a single interface for search and retrieval of “ omics ” data .__label__Material|Data|Introduce
Bioinformatics analysis and identification of somatic mutations in the Penn and TCGA tumor set . Quality control measures were determined with Picard Tools ( https :// broadinstitute . github . io / picard /). Penn tumors and matched germline were aligned to the hg19 assembly of the human genome using Burrows – Wheeler Aligner for short - read alignment66 ( _CITE_ ). Variants underwent initial quality control filtering according to Genome Analysis ToolKit ( GATK ) 67 best practices . Downloaded TCGA BAM files had been aligned to the hg38 assembly of the human genome .__label__Method|Tool|Use
family and kept the best likelihood value of each run for the LRT ( supplementary table S19 , Supplementary Material online ). The likelihood ratios were compared to a chisquare distribution with 1 degree of freedom as recommended in PAML user ’ s guide ( _CITE_ , last accessed April 24 , 2014 ).__label__Supplement|Document|Use
The funding agency had no role in the design of the study , the collection , analysis , and interpretation of data , or in writing the manuscript . Availability of data and materials All data used are available in the Genome Database for Rosaceae [ 111 ]. Phenotypic data can be found by filtering for “ Sweet Cherry ( RosBREED )” at _CITE_ Genotypic data can be found by filtering for “ Sweet_Cherry_CRS_SNP_genotyping ” at https :// www . rosaceae . org / search / snp_genotype . Authors ’ contributions JP conducted the bulk of the data analysis and results interpretation .__label__Material|Data|Use
We therefore choose the KRAS mutant non – small - cell lung cancer ( NSCLC ) cell line ( A549 cell line ) as our experimental model system . Using our simplified oncogenic KRAS signaling pathway ( Fig 2A , Mathematical modeling section for an explanation of how we obtained this simplified pathway ) as a guide , we pharmacologically inhibited individual proteins in the MAPK pathway ( MET , EGFR , MEK , ERK inhibitors ) and PI3K - AKT ( AKT inhibitor ) pathway in A549 cells in the both absence and presence of HGF . Drug - induced changes in the phosphorylation of pathway proteins , surrogates for protein PLOS Biology | _CITE_ March 9 , 2018 9 / 29 Impact of heterogeneity on targeted therapy activity , were measured by western blotting ( Fig 2B ). Cell viability was also assessed after 72 hours of drug treatment ( Fig 2C ). These experimental data were quantified using ImageJ ( Fig 2D ), and all data were normalized to the control experimental condition ( treatment - naïve condition ).__label__Supplement|Paper|Introduce
Quantifying geographic access to care is the most common use of geographical information systems ( GIS ) in maternal health research and practice [ 9 , 10 ]. The © The Author ( s ) 2017 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Makanga et al .__label__Supplement|License|Other
The DPI peak calling was performed using our own software , detailed in Forrest et al . 16 . It can be freely downloaded at this URL ( https :// github . com / hkawaji / dpi1 /). The liftOver tool was downloaded from the UCSC software repository38 ( _CITE_ ). The samtools ( v1 . 3 ) and bedtools ( v2 . 23 . 0 ) packages29 , 30 were downloaded from ( https :// sourceforge . net / projects / samtools / and https :// github . com / arq5x / bedtools2 ). The HISAT package33 ( v2 . 0 . 5 ) was downloaded from ( https :// github . com / infphilo / hisat ).__label__Method|Tool|Use
The DPI peak calling was performed using our own software , detailed in Forrest et al . 16 . It can be freely downloaded at this URL ( https :// github . com / hkawaji / dpi1 /). The liftOver tool was downloaded from the UCSC software repository38 ( _CITE_ ). The samtools ( v1 . 3 ) and bedtools ( v2 . 23 . 0 ) packages29 , 30 were downloaded from ( https :// sourceforge . net / projects / samtools / and https :// github . com / arq5x / bedtools2 ). The HISAT package33 ( v2 . 0 . 5 ) was downloaded from ( https :// github . com / infphilo / hisat ).__label__Method|Code|Use
The dataset can be found in S6 Data . Flavonoid ions were targeted for MS / MS fragmentation as [ M - H +]- electrospray derivatives with a window size of ± 4 m / z in Q1 . Fragmentation of the precursor ion was performed by collision - induced dissociation at 0 , 10 , 20 , and 40 eV collision energy , and fragment - ion spectra were recorded in scanning mode by high - resolution time - of - flight MS . Spectra were interpreted using MetFrag [ 80 ], and spectral cosine similarity scores were calculated between reference spectra that were obtained in - house or library spectra from MassBank of North America ( MoNA , _CITE_ ). For further details , see S5 Text . Untargeted metabolomics data analysis All steps of the downstream data analysis were performed in R ( R Foundation for Statistical Computing , Vienna , Austria ).__label__Supplement|Website|Use
The assay data used in this publication are listed in the Biostudies archive ( _CITE_ ) with accession identifier S - BSMS5 . All data can be accessed via the HipSci data portal ( http :// www . hipsci . org ), which references to EMBL - EBI archives that are used to store the HipSci data . Managed access data from all assays are accessible via EGA under the study EGAS00001001465 .__label__Material|Data|Use
CTCF Chip - seq data used in the Shh region was adquired from Encode _CITE_ and painted in the representative model ( mm9 data ) with a black - to - white gradient , from high to low score .__label__Material|Data|Use
CTCF Chip - seq data used in the Shh region was adquired from Encode _CITE_ and painted in the representative model ( mm9 data ) with a black - to - white gradient , from high to low score .__label__Supplement|Website|Use
We calculated the read distribution in different regions including exons , introns , untranslated regions , promoters and intergenic regions derived from the genomic annotations in Ensembl ( hg19 ), and found the majority of reads fell within exons and untranslated regions , whereas only a very small proportion of reads mapped to introns . Further , these data were consistent across the samples and there were no significant differences in these overall QC values between replicate groups ( Supplementary Table 1 ). Transcript models were derived for each sample independently using Cufflinks ( v2 . 2 . 0 ; with default parameters , except to specify strand specificity ; _CITE_ ). Resultant models were then merged using Cuffmerge to provide a global model and to classify transcripts as novel , or known , when they mapped to ENSEMBL ( v74 ; http :// www . ensembl . org ). In order to minimise false positives , a deliberately stringent filtration was used in order to call novel transcripts : Gene models were first filtered to keep transcripts only when an exon junction was supported by at least 2 reads in at least two samples .__label__Method|Tool|Use
Cognitive therapy approaches have been used effectively in treating PTSD following sexual or interpersonal violence [ 897 - 901 ], civilian trauma [ 902 - 908 ], and military trauma [ 909 - 914 ]. Katzman et al . BMC Psychiatry 2014 , 14 ( Suppl 1 ): S1 Page 34 of 83 _CITE_ Cognitive processing therapy ( CPT ) is an effective protocol that combines cognitive therapy and written accounts [ 899 - 901 , 910 - 913 ]; however , an analysis of the components found no differences in outcomes with either component alone or the combined protocol [ 899 ]. Prolonged exposure ( PE ) is a widely studied CBT approach . A meta - analysis of 13 RCTs concluded that PE therapy was more effective than wait - list or psychological placebo control conditions , and as effective as other active treatments ( e . g ., CBT , CPT , EMDR ) [ 69 ].__label__Supplement|Paper|Introduce
It may greatly enhance the utility of exome sequencing in disease studies . An R package incorporating the functionality of CANOES is available for download . The R package , documentation and source code may be obtained from _CITE___label__Method|Tool|Produce
It may greatly enhance the utility of exome sequencing in disease studies . An R package incorporating the functionality of CANOES is available for download . The R package , documentation and source code may be obtained from _CITE___label__Method|Code|Produce
It may greatly enhance the utility of exome sequencing in disease studies . An R package incorporating the functionality of CANOES is available for download . The R package , documentation and source code may be obtained from _CITE___label__Supplement|Document|Produce
That ’ s clearly not a robust way ...( and the Medication Safety ) Thermometer for our organisation is brilliant ... it has slightly made our data collection better , I ’ d argue . P13 ( Nurse , EA ) Participants reported an initial lack of understanding about how data could be used for improvement , particularly in LA organisations , where data collection was initiated prior to gaining a full understanding of how it should be used . Some participants felt strongly that the PLOS ONE | _CITE_ February28 , 2018 6 / 19 An evaluation of the implementation of the Medication Safety Thermometer MedsST should not be used for pin - pointing individuals and for staff to have “ the finger pointed at them .” ( P3 , Clinical Auditor , LA ). Nonetheless , one pharmacist ( P2 , LA ) stated that MedsST data had been used for pin - pointing poor practice of nurses and subsequent performance management . Most nurses agreed with participant 2 that MedsST data should be used for monitoring performance of nurses and that practice cannot be completely “ blame - free ”, as certain “ stupid ” individuals could cause errors ; however , they also believed that errors are actually caused by system problems that need to be addressed by supporting individuals .__label__Supplement|Paper|Introduce
Importantly this analysis confirms the previous observations of Hmga binding to major satellites ( S8C Fig ). In summary , these findings reinforce our observation that Hmga proteins bind the genome preferentially at regions of higher AT content , which , as a consequence of genome evolution , tend to overlap with large , heterochromatic domains . As we do not detect a dependence of binding on sequence composition of the surrounding regions at the kilobase scale , Hmga1 - 2 PLOS Genetics | _CITE_ December 21 , 2017 11 / 36 Genome wide binding of HMGA proteins Fig 4 . Genomic distribution of Hmga - enriched regions and AT - rich DNA . ( A ) Average profiles of log2 enrichment values over DBD - mutant at LMR regulatory regions .__label__Supplement|Paper|Introduce
When the operon information is available , the algorithm searches for genes in the same operon as the target gene and links these genes to the regulators of the target gene . This integration of operon information improves the detection sensitivity of regulatory links . GTRNetwork algorithm The GTRNetwork algorithm is implemented using Matlab and the source code is available at : _CITE_ Inputs : a ) Log 2 ratio transcriptome data in matrix [ Err ]__label__Method|Code|Produce
The analytical software was prompted to provide 1 hr ( on the hour ) windows ( hr1 - hr24 ). In addition other windows of interest ( determined by scrutinizing the literature and obtaining feedback from partners ) were also provided , including before or after school periods , morning and afternoon commute , and lunch and recess time ( see Additional file 2 , Table S3 ). A data dictionary which provides a definition of all accelerometer variables in the ICAD database can be found at _CITE___label__Material|Data|Use
The phenotype database is accessible from the menu items ( Fig 4A ). DNApod has been collecting public phenotypic data , and distributing the table of linked information between DNApod IDs ( SRA sample IDs ) and phenotypic data . As of April 2016 , DNApod genotypic data linked to phenotypic information included 29 rice samples linked to 44k SNP set , 29 rice samples to 1536 SNP set , 13 rice samples to Panicle Architecture , and 22 rice samples to High Density Rice Array of the Rice Diversity Project ( _CITE_ ). Moreover , DNApod contains the phenotypic information for 28 rice samples , 26 maize samples , and 6 sorghum samples linked to National Institute of Agrobiological Sciences ( NIAS ) Genebank ( Fig 4B ). Link information can be downloaded ( Fig 4C ).__label__Method|Tool|Use
These results were corroborated by Guna and colleagues [ 15 ] in a study that evaluated the LMC system using a plastic arm model to simulate human arm motion . They showed that LMC ’ s accuracy is < 0 . 5 mm when testing at locations ± 20 cm along the azimuth , up to 30 cm in depth and elevation . In contrast to these promising PLOS ONE | _CITE_ March 12 , 2018 2 / 25 Efficacy of the LMC for assessment of upper limb movements results using simulated human movement , more recent studies reported significantly greater spatial error for actual pointing movements performed to targets presented on a computer monitor . For example , Bachmann and colleagues [ 16 ] used a Fitts tapping task and found significantly greater error rates , defined as the percentage of out - of - target selections , for the LMC ( 7 . 2 %) compared to a standard mouse device ( 2 . 3 %). They also found that movement time was significantly longer with the LMC compared with the mouse movements ( 945 vs . 565 ms ).__label__Supplement|Paper|Compare
It is gridded at a spatial resolution of 1 / 16 degrees ( 3 . 75 ′ or approximately 6 km ), based on interpolated daily temperature and precipitation observations from ~ 20 000 NOAA Cooperative Observer stations . MACAv2 - LIVNEH dataset includes the same ensemble of GCMs and RCPs as MACAv2 - METDATA , and the data have the same spatial extent and temporal resolution and extent as MACAv2 - METDATA ; however , MACAv2 - LIVNEH offers a more limited number of climate variables ( Table 3 ). WorldClim WorldClim ( _CITE_ ) covers global lands surfaces except for Antarctica at 30 arcsecond resolution . WorldClim ’ s 1km resolution climate surfaces were created by interpolating weather station data from Global Historical Climatology Network ( GHCN ) 49 , World Meteorological Organization ( WMO ) Climatological normals ( CLINO ) 50 , Food and Agriculture Organization of the United Nations Agroclimatic Database ( FAOCLIM ) 51 , plus several regional databases . WorldClim uses 14 , 835 sites for maximum and minimum temperatures and 47 , 554 sites for precipitation , some of which may be used also in PRISM , METDATA and LIVNEH .__label__Supplement|Website|Introduce
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : WorldClim data are available from http :// www . worldclim . org / version1 , and the remote sensing climate data are available from https :// vdeblauwe . wordpress . com . Bamboo species presence data can be obtained by contacting the State Forestry Administration ( see _CITE_ ). Funding : The project was funded by the National Science Foundation under BIO - 1340812 and Michigan AgBioResearch . Any opinions , findings , conclusions or recommendations expressed in this__label__Material|Data|Use
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : WorldClim data are available from http :// www . worldclim . org / version1 , and the remote sensing climate data are available from https :// vdeblauwe . wordpress . com . Bamboo species presence data can be obtained by contacting the State Forestry Administration ( see _CITE_ ). Funding : The project was funded by the National Science Foundation under BIO - 1340812 and Michigan AgBioResearch . Any opinions , findings , conclusions or recommendations expressed in this__label__Supplement|Website|Use
Predicting DBP coding genes from global gene expression derived features Compiling gene expression profiles . Gene expression omnibus ( GEO ) contains about 1 . 2 million gene expression profiles , accumulated over some 13 , 000 different ‘ platforms ’. Of these , > 2 , 000 platforms correspond to human samples ( _CITE_ ) ( 57 ). Despite continuing efforts ( 58 – 60 ), cross - platform data comparison poses serious problems , including batch effects , normalization and scaling . To avoid these issues , we decided to use__label__Material|Data|Introduce
However , when there are no a priori reasons to spatially segment an assemblage , a method is needed to examine whether there is , nevertheless , spatial patterning in the data . To address this , a method for calculating and plotting nearest neighbor samples and , in effect , moving averages across an assemblage from a particular stratigraphic unit is presented . This method allows spatial patterning across an assemblage to be visualized in Benn PLOS ONE | _CITE_ January 2 , 2018 2 / 21 Statistical and graphical methods for analyzing site formation processes using artifact orientations space . When patterns are detected , the assemblage can then be spatially segmented and tested following the above mentioned techniques . Additionally , to further advance the analysis of artifact orientations , all of the software used in this paper to make the standard [ 1 ] and the newly presented statistical and graphical techniques are available in the Supplemental Information .__label__Supplement|Paper|Introduce
The sample was centrifuged for 15 min at 12 , 000 x g , at 4 ° C to separate the homogentate into a clear upper aqueous layer ( containing RNA ), an interphase and red lower organic layers ( containing the DNA and proteins ), for three min . DNA and trace phenol was removed using the RNeasy Mini Kit ( 74106 ; Qiagen Hilden , Germany ) column purification , following the manufacturer ’ s instructions ( RNeasy Mini Kit Protocol : Purification of Total RNA from Animal Tissues , from step 5 onwards ). RNA quantity was measured using a Qubit RNA BR Assay kit ( Q10210 ; Thermo PLOS Genetics | _CITE_ September 15 , 2017 24 / 38 The sheep gene expression atlas Fisher Scientific ) and RNA integrity estimated on an Agilent 2200 Tapestation System ( Agilent Genomics , Santa Clara , USA ) using the RNA Screentape ( 5067 – 5576 ; Agilent Genomics ) to ensure RNA quality was of RINe > 7 . RNA - Seq libraries were prepared by Edinburgh Genomics ( Edinburgh Genomics , Edinburgh , UK ) and run on the Illumina HiSeq 2500 sequencing platform ( Illumina , San Diego , USA ). Details of the libraries generated can be found in S2 Table .__label__Supplement|Paper|Use
In the second part of the study , the effects of the type of procedure on estimates of the functional status of patients was examined . It appears that cold deck imputation leads to significantly different estimates of the mean functional status in a group of patients than either hot deck imputation or treating the missing responses as if these items had never been offered . Differences between estimates Page 9 of 11 ( page number not for citation purposes ) Health and Quality of Life Outcomes 2004 , 2 _CITE_ tk obtained using the latter two methods were not significant . These results confirm that , in clinical studies , it is necessary to consider the method for dealing with responses in a & apos ; not applicable & apos ; category in the context of the data .__label__Supplement|Paper|Compare
The number of PubMed publication IDs associated with each gene was downloaded from NCBI at http :// www . ncbi . nlm . nih . gov / gene on September 18 , 2013 . External databases . Lists of multitasking and moonlighting proteins were obtained from the MultitaskProtDB ( _CITE_ ) [ 23 ] and MoonProt ( http :// www . moonlightingproteins . org /) [ 24 ] databases on April 7 , 2015 . Both databases curate the lists of proteins experimentally verified to have multiple biological functions .__label__Material|Data|Use
_CITE_ study and reported from previous studies corresponded with minimum ASM estimates of 6 . 5 to 8 . 4 yr and mean ASM estimates ranging from 9 . 5 to 17 . 1 yr ( Table 2 ). Comparison of growth data back - calculated for Kemp ’ s ridleys stranded in the GOM ( n = 535 ) vs . the Atlantic [ 13 ] prior to 2000 , the time frame for which data from both regions were available , indicated that overall rates were similar , with the exception that growth was much faster in the GOM for turtles in the 20 cm SCL size class ( Table 3 ; Fig 3A ). Fitting Faben ’ s modified von Bertalanffy growth curve to bootstrapped pre - 2000 GOM data yielded k = 0 . 26 and L ,,, = 65 . 3 , contrasting with Atlantic values of k = 0 . 115 and L ,,, = 74 . 9 cm SCL for the same time period and emphasizing divergence in early growth rates ( Fig 3B ).__label__Supplement|Paper|Introduce
The prevalence curves of each concomitant co - morbidity were modelled stratified for diabetes with fractional polynomials ( Software r Version 2 . 15 . 1 http :// cran . r - project . org , Paket mfp ) [ 29 , 30 ]. For each model the fit ( R2 ) was assessed . The intersection of the semi - maximum values and the prevalence curves were determined with the Ridders method ( Software r Version 2 . 15 . 1 _CITE_ , Paket pracma ) [ 31 ]. The interpolated age at the respective intersections was subsequently compared between persons with and without type 2 diabetes by calculating the difference .__label__Method|Tool|Use
The prevalence curves of each concomitant co - morbidity were modelled stratified for diabetes with fractional polynomials ( Software r Version 2 . 15 . 1 http :// cran . r - project . org , Paket mfp ) [ 29 , 30 ]. For each model the fit ( R2 ) was assessed . The intersection of the semi - maximum values and the prevalence curves were determined with the Ridders method ( Software r Version 2 . 15 . 1 _CITE_ , Paket pracma ) [ 31 ]. The interpolated age at the respective intersections was subsequently compared between persons with and without type 2 diabetes by calculating the difference .__label__Method|Algorithm|Use
Data from PSG , NET - PD , and the PPMI studies are managed by the Center for Human Experimental Therapeutics ( CHET ) at the University of Rochester . The CHET coordinating center currently houses data from over 40 PD clinical studies enrolling 7000 PD participants as well as from observational studies , including data from physician - rated clinical scales such as the UPDRS , Mini - Mental State Examination ( MMSE ) and the Beck Depression Inventory , as well as patient - reported outcomes data , imaging , laboratory and biomarkers , genetics , and demographics . The PSG hosts a list of data on the website , a short narrative about what the study covers , and guidance on how to access the data ( _CITE_ ). The review process is coordinated by the Michael J Fox Foundation and any researcher can apply . There have been over 200 publications resulting from the use of these data to date and future use is encouraged , especially for modeling disease progression Data used in modeling is only about 20 % of the data available through the PSG , but additional data sources are relevant for this purpose ( Table 1 ).__label__Material|Data|Produce
Data from PSG , NET - PD , and the PPMI studies are managed by the Center for Human Experimental Therapeutics ( CHET ) at the University of Rochester . The CHET coordinating center currently houses data from over 40 PD clinical studies enrolling 7000 PD participants as well as from observational studies , including data from physician - rated clinical scales such as the UPDRS , Mini - Mental State Examination ( MMSE ) and the Beck Depression Inventory , as well as patient - reported outcomes data , imaging , laboratory and biomarkers , genetics , and demographics . The PSG hosts a list of data on the website , a short narrative about what the study covers , and guidance on how to access the data ( _CITE_ ). The review process is coordinated by the Michael J Fox Foundation and any researcher can apply . There have been over 200 publications resulting from the use of these data to date and future use is encouraged , especially for modeling disease progression Data used in modeling is only about 20 % of the data available through the PSG , but additional data sources are relevant for this purpose ( Table 1 ).__label__Supplement|Document|Produce
For 2013 , SID files for 28 states were available directly from AHRQ ; files for the remaining states can potentially be obtained from the state - level organizations . 4 Nationally representative databases based on aggregated SID data include the annual Nationwide / National Inpatient Sample ( NIS ) and the triennial Kids ’ Inpatient Sample ( KID ). Other HCUP databases that capture CHD care are listed in Table 1 . Copies of the HCUP databases can be purchased ; aggregated data from select HCUP databases are freely available online at the HCUPnet site ( _CITE_ ). Several health service research studies have used HCUP data to assess data on incidence , outcomes , facility costs , and factors related to hospitalization for individuals with CHDs . 5 - 10 Health insurance claims databases include public insurers and proprietary insurance databases , such as Truven Health ’ s MarketScan ® suite of databases . The MarketScan ® research databases include commercial databases of employer - sponsored insurance , a Medicare database , and a Medicaid database representing claims from anonymized states that contract with Truven .__label__Material|Data|Use
For 2013 , SID files for 28 states were available directly from AHRQ ; files for the remaining states can potentially be obtained from the state - level organizations . 4 Nationally representative databases based on aggregated SID data include the annual Nationwide / National Inpatient Sample ( NIS ) and the triennial Kids ’ Inpatient Sample ( KID ). Other HCUP databases that capture CHD care are listed in Table 1 . Copies of the HCUP databases can be purchased ; aggregated data from select HCUP databases are freely available online at the HCUPnet site ( _CITE_ ). Several health service research studies have used HCUP data to assess data on incidence , outcomes , facility costs , and factors related to hospitalization for individuals with CHDs . 5 - 10 Health insurance claims databases include public insurers and proprietary insurance databases , such as Truven Health ’ s MarketScan ® suite of databases . The MarketScan ® research databases include commercial databases of employer - sponsored insurance , a Medicare database , and a Medicaid database representing claims from anonymized states that contract with Truven .__label__Supplement|Website|Use
Metagenomic datasets are freely available on the MG - RAST web - server ( _CITE_ ). The MG - RAST sample IDs are listed in the Table S1 .__label__Material|Data|Use
Metagenomic datasets are freely available on the MG - RAST web - server ( _CITE_ ). The MG - RAST sample IDs are listed in the Table S1 .__label__Supplement|Website|Use
IDAAPM is freely accessible via an online graphical user interface at ( _CITE_ ). The database browser was organized using a flask framework , java script , cascading style sheet and Jinja2 applications ( Additional file 11A ). The IDAAPM server was built with gunicorn , nginx and PostgreSQL 9 . 3 . 12 , installed on Ubuntu 14 . 04 . 4 .__label__Supplement|Website|Produce
AH was supported by SystemsX . ch IPhD Grant SXPHI0_142005 and JB by SystemsX . ch RTD Grant 2013 / 150 ( _CITE_ ). JK and JS were supported by ERC Synergy Grant 609883 ( http :// erc . europa . eu /). NB is part of WP11 of the SOUND project with Grant Agreement no 633974 ( http :// www . sound - biomed . eu / work - packages / wp11 /).__label__Method|Tool|Use
Approval by an ethics committee was not required for this study because all minipig tissue samples were obtained as catalogue item from Ellegaard Göttingen Minipigs A / S , Dalmose , Denmark ( _CITE_ ). All tissues came from six naïve female and 6 male in accordance with current animal welfare standards ( http :// minipigs . dk / the - goettingen - minipig / animal - welfare /). Details ( gender , weight , age , family relationship ) of all animals are on record and are included in our microarray data submission to GEOS .__label__Material|Data|Use
CROSS is freely available at _CITE_ new submission / cross .__label__Method|Tool|Produce
For CV analyses , models were fitted using 80 , 000 iterations collected after discarding the first 15 , 000 samples ; furthermore , samples were thinned at an interval of five . For all case studies , we report the average and SD ( across 200 CVs ) of the CV - AUC and the proportion of times that a model had a CV - AUC greater than other models , also computed using results from 200 CVs . Code to implement the models described herein is provided in File S2 and on the following website : _CITE___label__Method|Code|Produce
For CV analyses , models were fitted using 80 , 000 iterations collected after discarding the first 15 , 000 samples ; furthermore , samples were thinned at an interval of five . For all case studies , we report the average and SD ( across 200 CVs ) of the CV - AUC and the proportion of times that a model had a CV - AUC greater than other models , also computed using results from 200 CVs . Code to implement the models described herein is provided in File S2 and on the following website : _CITE___label__Supplement|Website|Produce
ProtTest , version 2 . 4 ([ 44 ]; _CITE_ ) was used with completed multiple sequence alignments to assess the optimal amino acid substitution model to use for subsequent phylogenetic tree inference . In all instances , the Le and Gascuel ( LG ) substitution model [ 45 ] with Gamma - distributed between sites rate variation ( LG + G ) was optimal , by both the AIC ( Akaike Information Criterion ) and BIC ( Bayesian Information Criterion ). Multiple sequence alignments were subjected to phylogenetic tree inference at both the CIPRES Science Gateway ([ 46 ]; http :// www . phylo . org / index . php / portal /) and locally .__label__Method|Tool|Use
( f ) Compiled predicted species data displaying sum of species occurrence across the region . ( g ) Modelled oil spill risk . ( Figure created in ArcGIS 10 . 2 _CITE_ ). Data available on the value of the petroleum resources within the region were limited . The layer used in the analysis was produced by Geoscience Australia and it provided broad scale relative prospectivity for the north and north - west of Australia ( Fig .__label__Method|Tool|Use
Some data types cannot be fully shared ( e . g ., EHR data — see rule 5 ), but most algorithms and summary results / statistics are shareable . Each of these types of open data necessitates a different platform for data sharing . Figshare ( _CITE_ ) allows users to share data involving published figures . Github ( https :// github . com /) allows users to share code that is in development or published . For code that is well developed , open - source packages can be created , for example , an R library , which can be deposited in CRAN or bioconductor .__label__Method|Tool|Introduce
In the “ Atlas of Abyssal Megafauna Morphotypes of the Clarion - Clipperton Fracture Zone ” created for the ISA ( _CITE_ ), this morphospecies is listed as " Mesothuria morphotype ".__label__Material|Data|Introduce
The Census dataset represents an interesting source of information that can be linked to the data described in this paper to , for example , understand and predict the socio - economic well - being of a given territorial area . For each information point I referring to a geographical area v ( contained in the shapefile ), we can calculate the proportion of data which belongs to each GRID & apos ; s square g : Ig 1 / 4 Iv CAAv gl where Ap is the area of a polygon p . After this process , the ISTAT data is correctly linked to the GRID . News Since the text of the news articles is not provided , a service like diffbot ( _CITE_ ) or any other similar service ( e . g ., Apache Tika ) could be used to extract the text from a given url . For instance , given the article http :// www . milanotoday . it / eventi / concerti / eventi - capodanno - 2014 - milano . html diffbot will output : { ‘ text ’: ‘ Tutti invitati al gran concerto di Capodanno in piazza [...]’ ‘ title ’: ‘ Concerto Capodanno in piazza Duomo :’ Lasciate a casa i botti ”, ‘ type ’: ‘ article ’, ‘ url ’: http :// www . milanotoday . it / eventi / concerti / eventi - capodanno - 2014 - milano . html }__label__Supplement|Website|Use
The GridCAT is openly available at the Neuroimaging Informatics Tools and Resources Clearinghouse ( NITRC ) and can be downloaded from : _CITE_ The GridCAT is free software and can be redistributed and / or modified under the terms of the GNU General Public License as published by the Free Software Foundation , either version 3 of the License , or ( at your option ) any later version . A copy of the GNU General Public License is__label__Method|Tool|Produce
T1 - weighted magnetization - prepared rapid - acquisition gradient - echo ( MP - RAGE ) fine structural images of the whole head were acquired for each participant ( 208 sagittal slices ; PLOS ONE | https :// doi . org / 10 . 1371 / journal . pone . 0198806 June 18 , 2018 3 / 28 Information spreading by MEG source estimation TR , 2250 ms ; TE , 3 . 06 ms ; TI , 900 ms ; flip angle , 9 ˚; field of view , 256 x 256 mm ; voxel size , 1 . 0 x 1 . 0 x 1 . 0 mm ; the same parameters were used on the Trio and Prisma scanners ). Extraction of cortical surfaces . The cortical surface was defined as a polygon model of the gray matter surface extracted from each participant ’ s MRI data using the FreeSurfer software suite ( _CITE_ ), with about 300 , 000 vertices . The polygonized cortical surface models were then imported into Brainstorm [ 25 ], and the number of vertices was downsampled to 15 , 002 to reduce computational load .__label__Method|Tool|Use
We see the difficulty of both data access and data sharing as an indicator of a general problem with current data sharing mechanisms : we believe that the ease of use , discoverability , availability and accessibility of data resources are crucial for promoting and facilitating data sharing for the genomics research community . The challenges of data sharing have been discussed extensively in the human genomics research community , since sharing of samples , data and results is an essential building block for progressing the body of knowledge in the field ( Callier et al ., 2014 ). Several organisations including the Research Data Alliance ( RDA ; _CITE_ ) and the Human Variome Project ( HVP ; http :// www . humanvariomeproject . org ) have made great efforts to promote capturing and sharing of research data , but hurdles in sharing genomic data still remain . In 2013 , the Global Alliance for Genomics and Health ( GA4GH ; http :// genomicsandhealth . org ) was established and has since engaged more than 180 institutions and organisations in working groups to address the challenges of regulatory restrictions , ethics , clinical demands , data representation , storage , analysis , and security related to genomic data sharing .__label__Supplement|Website|Introduce
( 2000 ) and Evanno et al . ( 2005 ). To visualize the STRUCTURE output , we used Structure Harvester ( _CITE_ Earl and von Holdt , 2012 ). To cross - validate the results of STRUCTURE , we also conducted a Principal Coordinates Analysis ( PCoA ) on the nSSR data using GenAlEx version 6 . 5 ( Peakall and Smouse , 2012 ). Having identified separate groupings , here termed evolutionary lineages ( i . e ., potential species ), using STRUCTURE , we conducted a series analyses .__label__Method|Tool|Use
The pre - publication history for this paper can be accessed here : _CITE_ pub Publish with OioMed Central and every scientist can read your work free of charge & quot ; BioMed Central will be the most significant development for disseminating the results of biomedical research in our lifetime .& quot ; Sir Paul Nurse , Cancer Research UK Your research papers will be : available free of charge to the entire biomedical community peer reviewed and published immediately upon acceptance cited in PubMed and archived on PubMed Central yours — you keep the copyright Submit your manuscript here : http :// www . biomedcentral . com / info / publishing_adv . asp BioMedcentral Page 5 of 5 ( page number not for citation purposes )__label__Supplement|Document|Produce
To gain insight on the functions of both the putative positively selected and genes under accelerated evolution , a functional annotation was performed using the euKaryotic Orthologous Group ( KOG ) terminology , according to the eggNOG 4 . 0 [ 48 ] database and using its BLASTbased online search tool ( _CITE_ ). Additionally , a Gene Ontology enrichment analysis was performed to determine if any category was overrepresented for genes under positive selection and faster evolutionary rate . Statistically significant enrichment was tested against a reference of all genes analysed using the Fisher ' s exact test and a p - value for the independence__label__Method|Tool|Use
Our prototype was implemented and integrated with a clinical data management system as a Plug - in module using a CDMS . A clinical data management system ( e . g SlimViangteeravat et al . Journal of Clinical Bioinformatics 2011 , 1 : 32 Page 3 of 10 _CITE_ Prim [ 21 , 22 ]) is used in administering and managing patient medical records and making the records available to health - care providers so that they can be used in their research and translational health care practice . We have tested the prototype system with some use case scenarios for distributed clinical data sources across several legacy clinical database management systems ( CDMS ) and database management systems ( DBMS ) at the University of Tennessee Health Science Center ( UTHSC ). These disparate systems were built on different underlying database technologies such as Oracle , MySQL and MS Access .__label__Supplement|Paper|Use
Each point in color represents the mean peak rate within the experimental group , i . e ., control , Ctx , Atx , and EGTA - AM . Paired ttest ( S3 Data ). Throughout the figure : concentration of a PLOS Biology | _CITE_ August 22 , 2017 6 / 32 Neuronal activity and oligodendrocyte precursor cells given drug is the same . The shown traces during drug application are taken from the time period when the drug effect on the amplitude of the evoked currents or rate of delayed currents was on the steady state . The numerical data used in D – O are included in S4 Data .__label__Supplement|Paper|Introduce
Cellomics Discovery The methods focus on visualizing simple quantitative http :// www . cellomics . com and [ 20 ] ToolBox and readouts of markers instead of images and especially the visualization method relationships among images that convey profound information closely related to effects of chemical compounds , gene functions , and biological processes . PhotoFinder and Those methods focus on image database visualization [ 21 , 22 ] Personal Photo Libraries targeted at personal photo albums , which are much smaller than HCS image databases and did not consider computational needs specific to HCS image analyses . ImCellPhen — interactive This is a method and a tool for interactive mining of cellular [ 19 ] _CITE_ mining of cellular phenotypes which provides intelligent interfaces for imcellphen / phenotypes visualizing large - scale RNAi - HCS image databases and interactive mining of cellular phenotypes . However , this method does not provide easy - to - use ( with one click access ) filtering functionality for image properties and image processing results . The Open Microscopy OME provides an open - source browser to navigate HCS Environment ( OME ) image databases that are described as a quasi - hierarchical structure representing the relationship between projects and datasets .__label__Method|Tool|Introduce
Bihrmann and Ersbøll InternationalJournal of Health Geographics 2015 , 14 : 1 Page 7 of 13 _CITE_ The variance parameter estimates were all reasonably similar , but with a slight tendency to either increase ( especially MAR0 OR = 1 / 3 and MAR1 OR = 1 / 3 ) or decrease ( especially MAR0 OR = 3 ) with more than 50 % missing data . Both the standard deviation of each parameter estimate , and the Root Median Squared Error ( RMeSE ) increased when the number of missing observations was increased , regardless of scenario . The median of the estimated range of influence within each simulation scenario ( Figure 1 ) ranged from 9 . 4 km ( SD 4 . 0 ) ( MAR1 OR = 1 / 3 , 75 %) to 14 . 8 km ( SD 19 . 3 ) ( MNAR OR = 3 , 75 %).__label__Supplement|Paper|Introduce
Data were analyzed by patient environment ( inpatient , outpatient , or homecare / hospice ), with homecare / hospice being applicable only to the clinical laboratory category because transfusions are not performed within that environment . Chi - square and t - tests were used to compare Whitehurst et al . Journal of Biomedical Semantics 2012 , 3 : 4 Page 6 of 10 _CITE_ categorical and continuous data , respectively . The Wilcoxon rank sum test was used to compare nonparametric data . All statistical analysis was performed using JMP Pro 9 . 0 ( SAS Corporation , Cary , NC , USA ).__label__Supplement|Paper|Introduce
aegypti [ 15 ]. Although wMel - infected females receive a frequency - dependent relative fitness advantage from CI , they also suffer from frequency - independent fitness costs , including decreases in fecundity and larval competitive ability [ 16 , 17 , 18 , 19 ]. Thus , CI does not produce a net fitness advantage while wMel is rare , resulting in dynamics analogous to those produced by an Allee PLOS Biology | _CITE_ May 30 , 2017 2 / 28 Spread of dengue - suppressing Wolbachia in Aedes aegypti effect in ecology [ 20 , 21 ] and by natural selection on a locus ( or alternative karyotypes ) in which heterozygotes are less fit than either homozygotes ( i . e ., underdominance , [ 22 , 23 , 24 ]). The interaction of the frequency - dependent advantage associated with CI and the frequencyindependent cost ( s ) produces “ bistable dynamics ” with a threshold frequency of infection ( denoted ^ p ) below which the infection will be locally eliminated and above which frequencies systematically increase [ 25 , 26 , 27 ]. Curtis [ 23 ] first proposed transforming pest populations by introducing translocations that are expected to show bistable dynamics ( cf .__label__Supplement|Paper|Compare
Funding We would like to acknowledge NIH grants # P30AG050911 and # P20GM103636 for supporting this work . Publication charges were paid for by # P20GM103636 . Availability of data and materials The code for label extraction , along with the database of extracted labels , is available at _CITE___label__Method|Code|Produce
Funding We would like to acknowledge NIH grants # P30AG050911 and # P20GM103636 for supporting this work . Publication charges were paid for by # P20GM103636 . Availability of data and materials The code for label extraction , along with the database of extracted labels , is available at _CITE___label__Material|Data|Produce
Workflow Analysis ( WFA ) Runs showed that the bead preparation had titration metrics above 74 % and Noiseto - Signal ratio below 5 %. The libraries were sequenced to 35 bp read length in a 1 - well deposition chamber using the Multiplex Fragment Sequencing reagents and the SOLiDTM 4 Analyzer . Analysis of raw sequencing data Quality control The raw data quality was analyzed with FastQC ( _CITE_ ), which determines various sequence characteristics to identify biases in the data . First , the raw sequence color space data was converted to Fastq format , which comprises both the nucleotide sequence and the corresponding nucleotide quality scores . Subsequently , FastQC was used to determine the per base sequence quality ( average quality at each nucleotide position in the reads ) and the per base sequence content ( proportion of each base at each nucleotide position for all reads ; should be around 0 . 25 ).__label__Method|Tool|Use
RNA - seq atlas of Glycine max Glycine max http :// soybase . org / soyseq / Severin et al . ( 2010 ) Tomato expression database Solanum lycopersicum http :// ted . bti . cornell . edu / Fei et al . ( 2006 ) Transcriptome atlas of Glycine max Glycine max _CITE_ Libault et al . ( 2010 ) structures for the display of varying experimental conditions or a set of different plant lines . This is achieved by concatenating the images to one montage image of , e . g ., different stages or conditions and repeating the process of segmentation for all segments of different stages .__label__Method|Tool|Introduce
The STROBE statement ( https :// www . strobestatement . org ), outlines the issues that should be considered in the reporting of epidemiology studies . Software is available ( e . g . Covidence _CITE_ , EPPI - Reviewer , http :// community . cochrane . org / tools / review - productiontools / eppi - reviewer / about or Rayyan — https :// rayyan . qcri . org /) which keeps all data secure remotely , and allows collaboration between authors in different locations : this can facilitate independent assessment , allow accountability and is a method to keep track of the process . It is not a substitute for adequate training in review methodology and statistical literacy . Issues to consider in studies of older people Geriatricians are well aware of the complexity of dealing with older patients , and the paucity of evidence to guide their practice .__label__Method|Tool|Use
between methylation / expression data of selected genes with clinical parameters . Integrative analysis and cross - study validation All probes differentially methylated (| Δβ | 0 . 15 and adjusted p & lt ; 0 . 05 ) and expressed ( FDR & lt ; 5 % and fold change & gt ; 2 ) were subjected to an integrative analysis , using a Pearson correlation test ( 34 PTC evaluated by both analysis ), aiming to obtain negative and positive significant correlations ( p & lt ; 0 . 05 ). A cross - study validation was performed to confirm the results using DNA methylation microarray and RNA sequencing data from TCGA database ( _CITE_ ). Similar parameters of the internal analysis were adopted to compare all conditions in the external dataset ( t test p & lt ; 0 . 05 , FDR & lt ; 5 % and Pearson correlation test p & lt ; 0 . 05 ) ( details in Additional file 1 ). Figure 1 summarizes the strategy and results obtained in this analysis .__label__Material|Data|Use
In addition , h . ArtifactSelection contains a list of all trials , with 0 corresponding to a clean trial and 1 corresponding to a trial containing an artifact . SigViewer 0 . 2 ( or higher ) can be used to view and annotate GDF files . SigViewer is available at _CITE_ A . 2 . DATA SET 2B All files are listed in Table A3 .__label__Method|Tool|Use
The software application Computational Anatomy Works ( CAWorks , Figure 5 ), available at _CITE_ , was developed to support computational anatomy and shape analysis . CAWorks works seamlessly with the NUSDAST image , landmark and surface data and the Large Deformation Diffeomorphic Metric Mapping ( LDDMM ) mapping engine ( Beg et al ., 2005 ). The capabilities of CAWorks include :__label__Method|Tool|Introduce
The data we deal with it comes from DREAM CHALLENGE 9 ( _CITE_ ), which is a challenge where the biologists provide data and ask questions , and physician , mathematician and computer scientist communities try to response to their questions . The data consists of measurements of 191 patients diagnosed with AML who were treated at MD Anderson Cancer Center ( USA ), from their proteomics ( 231 measured proteins ) and bio - clinical data ( 40 clinical covariates ). These patients are classified into two classes CR and PR .__label__Material|Data|Use
Sample collection and genotyping . Genographic sample collection was conducted according to the ethical protocol of The Genographic Project ( _CITE_ Geno2 . 0_Ethical - Framework . pdf ), with oversight provided by the University of Pennsylvania and regional IRBs ( specified in the original reports from which data analyzed in this study were taken ). IRBs were obtained for new collections in Italy , UK , Denmark , Greece , Germany and Romania ( sample and data collection were undertaken with approval from the IRB , Comite ` E ` tic d ’ Investigacio ´ ClinicaInstitut Municipal d ’ Assiste ` ncia Sanitiria ( CEIC - IMAS ) in Barcelona ( 2006 / 2600 / I )); Peru ( sample and data collection were undertaken with approval from the local IRB at Universidad San Martin de Porres , Lima , Peru ; Federal Wide Assurance ( FWA ) for International Protection of Human Subject 0001532 ; US Health and Human Services ( HHS ) International Review Board IRB0000325 ); Puerto Rico ( sample and data collection were undertaken with approval from the University of Pennsylvania IRB # 8 and the support of Liga Guakia Taina - Ke ); Mexico ( sample and data collection were undertaken with approval from the University of Pennsylvania IRB # 8 , the Centro de Investigacio ´ n y de Estudios Avanzados del Instituto Polite ´ cnico Nacional ( CINVESTAV - IPN ), and the Comision Nacional para el Desarrollo de los Pueblas Indigenas ( CDI )); Egypt , Iran , Kuwait , Lebanon and Tunisia ( the sample and data collection protocol were originally approved by the IRB committee of the Lebanese American University ); North Eurasia ( North Eurasia sample and data collection were undertaken with approval from the Ethical Committee of the Research Centre for Medical Genetics RAMS and the Academic Council of the same Research Centre ); India ( the sample and data collection protocol were originally approved by IRB of Madurai Kamaraj University , Madurai , India ). Approval for further sampling , studies and collaborations have been obtained from IRB of Chettinad Academy of Research & Education , Kelampakka , India .__label__Method|Tool|Extent
All models that include environmental predictors also include density dependence ( the sockeye models with environmental effects allowed density dependence to vary by population ). For each species , the best model and all models within 1 log - likelihood unit are highlighted in bold ( the best PLOS ONE | DOI : 10 . 1371 / journal . pone . 0172898 March 15 , 2017 18 / 24 Evaluating signals of EVOS , climate , and species interactions in herring and salmon populations model only being defined for this particular table — all results are included in Table 1 ). Additional details included online , _CITE_ ( DOCX ) S4 Table . Detailed results for models that only include effects of juvenile competition .__label__Supplement|Document|Produce
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : Data and code from the current study is located here : _CITE_ master . Due to ethical restrictions , participant data including participant identifiers , are not available without restriction . However , all interested researchers can request and obtain the raw . qtm files by contacting the corresponding author ( cleveland . barnett @ ntu . ac . uk ).__label__Method|Code|Produce
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : Data and code from the current study is located here : _CITE_ master . Due to ethical restrictions , participant data including participant identifiers , are not available without restriction . However , all interested researchers can request and obtain the raw . qtm files by contacting the corresponding author ( cleveland . barnett @ ntu . ac . uk ).__label__Material|Data|Produce
Raw and converted data are deposed to MassIVE ( MSV000079811 ) and ProteomeXchange ( PXD004308 ). Source code of Diffacto is freely available at _CITE_ under Apache 2 . 0 license .__label__Method|Code|Produce
Data sharing statement No additional data are available . Open Access This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ( CC BY - NC 4 . 0 ) license , which permits others to distribute , remix , adapt , build upon this work noncommercially , and license their derivative works on different terms , provided the original work is properly cited and the use is non - commercial . See : _CITE___label__Supplement|License|Other
Python packages used for analysis include numpy ( Oliphant , 2007 ; Van der Walt , Colbert & Varoquaux , 2011 ), matplotlib ( Hunter , 2007 ), sqlalchemy ( Bayer , 2014 ), pandas ( McKinney , 2010 ), macroecotools ( Xiao et al ., 2016 ), and retriever ( Morris & White , 2013 ). R packages used for analysis include ggplot2 ( Wickham , 2009 ), magrittr ( Bache & Wickham , 2014 ), tidyr ( Wickham , 2016 ), and dplyr ( Wickham & Francois , 2016 ). All of the code and all of the publicly available data necessary to replicate these analyses is available at _CITE_ and archived on Zenodo ( Baldridge et al ., 2016 ). The CBC datasets and NABA datasets are not publicly available and therefore are not included .__label__Method|Code|Produce
Python packages used for analysis include numpy ( Oliphant , 2007 ; Van der Walt , Colbert & Varoquaux , 2011 ), matplotlib ( Hunter , 2007 ), sqlalchemy ( Bayer , 2014 ), pandas ( McKinney , 2010 ), macroecotools ( Xiao et al ., 2016 ), and retriever ( Morris & White , 2013 ). R packages used for analysis include ggplot2 ( Wickham , 2009 ), magrittr ( Bache & Wickham , 2014 ), tidyr ( Wickham , 2016 ), and dplyr ( Wickham & Francois , 2016 ). All of the code and all of the publicly available data necessary to replicate these analyses is available at _CITE_ and archived on Zenodo ( Baldridge et al ., 2016 ). The CBC datasets and NABA datasets are not publicly available and therefore are not included .__label__Material|Data|Produce
Rankings of surgical innovation across specialties by cascade size and structural virality ( structural depth and width ) were found to correlate closely with the ranking by innovation value ( Spearman ’ s rank correlation coefficient = 0 . 758 ( p = 0 . 01 ), 0 . 782 ( p = 0 . 008 ), 0 . 624 PLOS ONE | _CITE_ August 25 , 2017 1 / 14 Networks of surgical innovation 2017 ). The funders had no role in the study design , ( p = 0 . 05 ), respectively ) which in turn matches the ranking based on real world big data from data collection and analysis , decision to publish , or the NIS ® ( Spearman ’ s coefficient = 0 . 673 ; p = 0 . 033 ). preparation of the manuscript .__label__Supplement|Paper|Introduce
The present study demonstrates high intra - platform precision for all platforms . The majority of replicate CV median measurements for nCounter & apos ;, BioMark HDTM and OpenArray & apos ; were in the range between 5 - 15 % which was slightly higher than the CV values observed on the Affymetrix & apos ; Forreryd et al . BMC Genomics 2014 , 15 : 379 Page 12 of 17 _CITE_ platform . In contrast to the CV values observed on the Affymetrix ® platform , the distribution of replicate CV values varies between stimulations on evaluated platforms . The distribution of replicate CV values for a certain stimulation is however comparable across the evaluated platforms .__label__Supplement|Paper|Introduce
To make the Characteristic Direction method accessible , we implemented it in Python , R , MATLAB and Mathematica . Readers that are interested in applying the method to their own data should refer to the open source scripts and examples available at : http :// www . maayanlab . net / CD . Availability and requirements Implementations of the method are provided in Python , R , MATLAB , and Mathematica freely available at : _CITE___label__Method|Code|Produce
Directional information ( up - or downregulation ) for each gene also was incorporated in the analysis to assess the similarities between datasets . To enable comparison across different arrays , orthologs were identified for each pair of organisms [ 99 ]. Ortholog identification was based on information obtained from Mouse Genome Informatics ( MGI ) at Jackson Lab ( _CITE_ ), HomoloGene at NCBI ( http :// www . ncbi . nlm . nih . gov ), and Ensembl ( http :// www . ensembl . org ). The gene overlap P - value calculated by NextBio indicates a statistically significant association between two given gene sets . The detailed methods for comparison of data sets are given in Additional file 8 .__label__Supplement|Website|Extent
The pre - publication history for this paper can be accessed here : _CITE_ pub Publish with BioMed Central and every scientist can read your work free of charge " BioMed Central will be the most significant development for disseminating the results of biomedical research in our lifetime ." Sir Paul Nurse , Cancer Research UK Your research papers will be : available free of charge to the entire biomedical community peer reviewed and published immediately upon acceptance cited in PubMed and archived on PubMed Central yours — you keep the copyright Submit your manuscript here : http :// www . biomedcentral . com / info / publishing_adv . asp BioMedcentral Page 10 of 10 ( page number not for citation purposes )__label__Supplement|License|Other
Aggregation was performed by arithmetic addition of all the counts belonging to a given group . Taxonomic and block - level distributions were visualized with the phylogram function in AMOR [ 58 ], and constrained ordination was performed with vegan ’ s capscale function [ 43 ] and visualized with ggplot2 [ 59 ]. The code and data to perform this analysis are bundled in the R package ( wheelP ) [ _CITE___label__Method|Code|Produce
Environmentally relevant peer - reviewed and non - reviewed („ grey ‟) literature for TMP was searched for using dedicated search engines on the internet ( ACS SciFinder , Google Scholar , chemical data collections like OECD Chemicals Portal _CITE_ or the European Union Chemical Substances Information System http :// esis . jrc . ec . europa . eu / as well as safety data sheet search engines such as https :// www . eusdb . de ), beside company - internal substance documentation and archives . The information was sighted , ordered and collated . Reference lists in the retrieved documents often allowed to supplement the literature dataset with further , mostly older publications and also online sources for MECs .__label__Supplement|Website|Introduce
To prevent positional effects on plant growth , all plates were randomized every 2 days . We set out to grow all genotypes simultaneously in three repeats , but due to germination issues with some seed batches , a total of 5 experiments have been performed to obtain three repeats for each genotype , with the exception of the cross ANTOE - da1 - 1 for which we could obtain results in one repeat . At 21 DAS , individual leaves ( cotyledons and rosette leaves ) were dissected at the base of the petiole and their area was measured with ImageJ v1 . 45 ( NIH ; _CITE_ ).__label__Method|Algorithm|Use
Supplementary Information accompanies this paper at _CITE_ Competing interests : The authors declare no competing financial interests . Reprints and permission information is available online at http :// npg . nature . com / reprintsandpermissions / Publisher ' s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations .__label__Supplement|Document|Introduce
For the deeply conserved miRNA sites , site conservation is defined by 9 out of these 12 species ( D . melanogaster , D . simulans , D . sechellia , D . yakuba , D . erecta , D . ananassae , D . pseudoobscura , D . persimilis , D . willistoni , D . virilis , D . mojavensis , D . grimshawi ), at least one of which had to be a virilis - group species ( D . virilis , D . grimshawi , D . mojavensis ). Only the strong seeds 7mer - m8 and 8mer were considered . Position weight matrices ( PWMs ) for RBPs were taken from [ 71 ], and the fimo program in The MEME Suite ( _CITE_ ) was used to scan RBP sites in the annotated 3 ′ UTRs . The RBP site conservation is defined by presence in at least 3 / 5 melanogaster - subgroup species ( D . melanogaster , D . simulans , D . sechellia , D . yakuba , and D . erecta ) along with at least one obscura - group species ( D . pseudoobscura , D . persimilis ) with one mismatch allowed .__label__Method|Tool|Use
Given that a prior model must be assumed to achieve satisfactory error estimation , an obvious course of action is to derive an optimal classifier based © 2014 Knight et al . ; licensee BioMed Central Ltd . This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly credited . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated .__label__Supplement|License|Other
The effect of filtering using the age - based threshold was evaluated by comparing HRV indices before and after filtering of the edited data set and the unedited data set , respectively . Karlsson et al . BioMedical Engineering OnLine 2012 , 11 : 2 Page 4 of 12 _CITE___label__Supplement|Paper|Introduce
These manually assigned labels were transferred back to the single cell level for further processing . Cells were visualized in phosphorylation space using random subsampling ( 2 , 000 cells were randomly chosen from PMA and Plasma treatments ) and K - means downsampling . Supporting code can be found at _CITE_ and the Jupyter notebook http :// nbviewer . jupyter . org / github / MaayanLab / Cytof_Plasma_PMA / blob / master / notebooks / Plasma_vs_PMA_Phosphorylation . ipynb .__label__Method|Code|Produce
Variants for analysis were restricted to the consensus coding sequence public transcripts ( CCDS release 14 ) plus 2 base pair intronic extensions . Variants were further required to have : i ) at least 10 - fold coverage , ii ) quality score ( QUAL ) of at least 30 , iii ) genotype quality ( GQ ) score of at least 20 , iv ) quality by depth ( QD ) score of at least 2 , v ) mapping quality ( MQ ) score of at least 40 , vi ) read position rank sum ( RPRS ) score greater than - 3 , vii ) mapping quality rank sum ( MQRS ) score greater than - 6 , viii ) indels were required to have a maximum Fisher ’ s strand bias ( FS ) of 200 , ix ) variants were screened according to VQSR tranche calculated using the known SNV sites from HapMap v3 . 3 , dbSNP , and the Omni chip array from the 1000 Genomes Project to “ PASS ” SNVs were required to achieve a tranche of 99 . 9 % for SNVs in genomes and exomes and 99 % for indels in genomes , x ) for heterozygous genotypes , the alternate allele ratio was required to be > 25 %. Finally , variants were excluded if they were among a predefined list of known sequencing artifacts or if they were marked by EVS ( _CITE_ ) or ExAC ( http :// exac . broadinstitute . org / about ) as being problematic variants . Variants were annotated to Ensembl 73 using SnpEff . All variants meeting these criteria were eligible to be qualifying variants in the gene - based collapsing analyses .__label__Method|Tool|Use
Special thanks to Fan Xu from the University of British Columbia for contributing several new and improved R apps to the framework and members of the Lorincz lab , Underhill lab and Rossi lab for providing valuable feedback during the development of the tool . This article has been published as part of BMC Bioinformatics Volume 16 Supplement 11 , 2015 : Proceedings of the 5th Symposium on Biological Data Visualization : Part 1 . The full contents of the supplement are available online at _CITE___label__Supplement|Document|Produce
Given the size of the datasets (~ 150 2 h imaging sessions ), we provide MATLAB data containing solely deconvolved images and labels for each stimulus ( concept or sentence ). The raw and processed NIFTI imaging datasets , as well as associated event files , will be shared via a repository ( http :// www . openfmri . org ), after re - processing . Any updates on the data and scripts will be posted on the paper website ( _CITE_ crwz7 ).__label__Method|Code|Produce
Given the size of the datasets (~ 150 2 h imaging sessions ), we provide MATLAB data containing solely deconvolved images and labels for each stimulus ( concept or sentence ). The raw and processed NIFTI imaging datasets , as well as associated event files , will be shared via a repository ( http :// www . openfmri . org ), after re - processing . Any updates on the data and scripts will be posted on the paper website ( _CITE_ crwz7 ).__label__Material|Data|Produce
Given the size of the datasets (~ 150 2 h imaging sessions ), we provide MATLAB data containing solely deconvolved images and labels for each stimulus ( concept or sentence ). The raw and processed NIFTI imaging datasets , as well as associated event files , will be shared via a repository ( http :// www . openfmri . org ), after re - processing . Any updates on the data and scripts will be posted on the paper website ( _CITE_ crwz7 ).__label__Supplement|Website|Produce
Finally , the BNT toolbox can be downloaded from https :// code . google . com / p / bnt /. Instructions for installations as well as how to use the package is available in the website . The material from [ 1 ] has been made available in the Google drive _CITE_ folderview ? id = 0B7Kkv8wlhPU - T05wTTNodWNydjA & usp = sharing . This contains the individual files , contents of which are used in this manuscript . The drive and its contents can be accessed via the URLs mentioned earlier in the abstract .__label__Supplement|Website|Use
Finally , the BNT toolbox can be downloaded from https :// code . google . com / p / bnt /. Instructions for installations as well as how to use the package is available in the website . The material from [ 1 ] has been made available in the Google drive _CITE_ folderview ? id = 0B7Kkv8wlhPU - T05wTTNodWNydjA & usp = sharing . This contains the individual files , contents of which are used in this manuscript . The drive and its contents can be accessed via the URLs mentioned earlier in the abstract .__label__Supplement|Document|Use
© The Author ( s ) 2017 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Orrell et al . AIDS Res Ther ( 2017 ) 14 : 20 Page 2 of 11 Background Adherence is critical to realising the clinical and prevention benefits of ART [ 1 – 3 ].__label__Supplement|License|Other
Intergenic regions in the mouse genome with no known transcriptional activity were selected using the UCSC genome browser ( _CITE_ ). In total , 30 assays targeting 10 different regions on 5 chromosomes were designed using PrimerBlast ( NCBI ). Amplification efficiencies were determined with a dilution series of gDNA ( 50 – 5000 haploid genome copies ).__label__Method|Tool|Use
The method is implemented in the add - on SpeciesNetwork for BEAST 2 ( Bouckaert et al . 2014 ), including the inference , simulation , and summary tools , and is hosted publicly on GitHub ( _CITE_ last accessed December 10 , 2017 ).__label__Method|Tool|Produce
The method is implemented in the add - on SpeciesNetwork for BEAST 2 ( Bouckaert et al . 2014 ), including the inference , simulation , and summary tools , and is hosted publicly on GitHub ( _CITE_ last accessed December 10 , 2017 ).__label__Method|Code|Produce
Supplementary information accompanies this paper at _CITE_ Competing financial interests : The authors declare no competing financial interests . How to cite this article : Duan , X . et al . Unsupervised Data Mining in nanoscale X - ray Spectro - Microscopic Study of NdFeB Magnet .__label__Supplement|Document|Produce
Separate tables are available for node - by - node estimates of the diffusion properties along the length of the fiber groups , and for the subject metadata , and these tables can be merged in an unambiguous manner through a shared subject ID variable . These files can be read using the standard data science tool - box : Software libraries such as the Python pandas library52 , or using the R statistical language53 . Once data are read into tables , data processing and visualization with tools such as Seaborn ( _CITE_ ) or ggplot ( http :// ggplot2 . org /) are also straightforward . Furthermore , very few steps are required to apply machine learning techniques to the data , using tools such as the scikit - learn library54 , and results such as classifier weights can be easily interpreted with respect to known brain anatomy . An example of such an analysis is presented in Fig .__label__Method|Tool|Use
The weighting methods described in this paper are implemented in the voomWithQualityWeights function in the open - source ‘ limma ’ package distributed as part of the Bioconductor project ( _CITE_ ). A Galaxy tool that includes the option to apply ‘ voom ’ with sample - specific weights in an RNA - seq differential expression analysis is available from the Galaxy Toolshed at https :// toolshed . g2 . bx . psu . edu / view / shians / voom rnaseq . The R code and plots of results for all simulation settings along with the R code to carry out the analyses of the ‘ Control ’ and ‘ Smchd1 ’ RNA - seq experiments are provided as ‘ Supplementary Materials ’ at http :// bioinf . wehi . edu . au / voomWithQualityWeights /.__label__Method|Tool|Use
MRS has been optimized for speed and ease of use . For example , a search for ‘ lysozyme ’ in 12 MRS - files , including EMBL , PDB ( 4 ), UniProt ( 5 ), etc ., typically takes 0 . 02 s on a single processor PC , whereas combined searches like ‘ chloride AND channel ’ typically take 0 . 15 s . Similar searches using the EBI search engines typically take several seconds . An MRS server is available at _CITE_ Scientists from academia and industry can freely use this server to search presently in 14 data banks . All materials needed to build one ’ s own MRS server are available at http :// mrs . cmbi . ru . nl / download /.__label__Supplement|Website|Produce
In situations where the version of a data record matters , advertise the corresponding permanent link ( permalink ) together with a statement about persistence . E . g . : “ The permanent link to this page , which will not change with the next release of Ensembl is : _CITE_ We aim to maintain all archives for at least five years ; some key releases may be maintained for longer ”__label__Supplement|Website|Produce
All OM data involved in this study are available on Zenodo [ 50 ]. The OMSV source code is available on GitHub ( https :// github . com / moziya / OMSV / tree / v1 . 0 ) and Zenodo ( http :// doi . org / 10 . 5281 / zenodo . 1035506 ). Our supplementary website ( _CITE_ ) provides a compiled OMSV package , detailed instructions for using the package , and links to the GitHub and Zenodo entries . The complex SV and large indel callers of OMSV were implemented in C ++ and Linux Bash , and the CNV caller was implemented in Matlab R2011b ( 7 . 13 . 0 . 564 ) 64 - bit ( glnxa64 ). The whole package requires at least 4 GB of physical memory and has been tested on both Debian GNU / Linux 9 . 0 ( stretch ) and CentOS Linux release 7 . 3 . 1611 ( Core ) platforms .__label__Supplement|Website|Produce
This work has implications for researchers and those who use meta - analyses to help inform clinical and policy decisions . ( i ) Investigators should ensure a comprehensive systematic literature search to avoid or at least attenuate the effect of dissemination bias . Such searches can be PLOS ONE | _CITE_ April 25 , 2017 12 / 16 Study data not published in full text articles have unclear impact on meta - analyses results resource - intensive particularly when unpublished and grey literature data need to be identified . If the available resources do not permit comprehensive searches to identify unpublished or grey literature data , we strongly recommend ( at least ) a search in trial registries ( such as the ICTRP and ClinicalTrials . gov ) and websites of regulatory authorities which is less resourceintensive than searching for conference proceedings or dissertations , contacting experts , the industry and authors . When including unpublished or grey literature data sensitivity analyses should be carried out taking into account that this research may provide only preliminary results , is usually not peer reviewed and / or at higher risk of bias .__label__Supplement|Paper|Introduce
Using Pathway Data Integration Portal v2 . 5 . 1 . 2 ( _CITE_ ), we performed comprehensive pathway enrichment analysis across 20 major pathway databases [ 27 ]. We considered literature curated gene : pathway memberships as well as those predicted according to experimentally detected proteinprotein interactions ( including interactions experimentally detected between orthologues plus FpClass interactions with minimum confidence level for predicted associations equal 0 . 95 ; for more details see pathDIP documentation ).__label__Material|Data|Use
BioData Mining ( 2018 ) 11 : 5 Page 9 of 22 package . For gradient boosting , we set the number of trees as 800 for main effect datasets and 15 , 000 trees for interaction effects datasets . We set the bag fraction as 0 . 5 and shrinkage as 0 . 01 , which have been suggested to result in the best performance based on the best practices from R package manual ( _CITE_ ). TuRF refers to Tuned ReliefF and it performs feature selection recursively . It is suggested to use TuRF along with ReliefF algorithms to get better performance when using a large number of variables [ 5 , 15 , 28 ].__label__Supplement|Document|Extent
The miRBase v . 13 ( corresponding to the human genome assembly that we analysed ) was used to determine the coverage of miRNAs lying in fragile sites and structural cluster regions ( 23 – 25 ). Several sets of deep - sequencing reads were used to predict structural clusters . Data sets were retrieved from the Gene Expression Omnibus database at NCBI ( _CITE_ ) and from the Sequence Read Archive at NCBI ( http :// www . ncbi . nlm . nih . gov / sra / ( see Supplementary Table S1 for accession numbers ). Among deep - sequencing data coming from the Sequence Read Archive , some were extracted from breast cancer cells ( SRR015446 , SRR015447 and SRR015448 ), and the others from 12 melanoma and pigment cells . Reads coming from Gene Expression Omnibus archive , originate from cell lines derived from cervical cancer cells ( GSE14362 and GSE10829 ), small RNAs from human embryonic stem cells , derived neural progenitors and neurons ( GSE13483 ), endogenous small RNAs associated to human Argonaute 1 and 2 ( GSE13370 ).__label__Material|Data|Use
Further details on this step of the analysis are provided in the Supporting Information ( S1 File ). Testing the statistical significance of conserved subgraphs . To evaluate the significance of the individual discriminative power of the obtained conserved subgraphs , we compute their PLOS ONE | _CITE_ October 10 , 2017 10 / 22 Learning about learning : Mining human brain sub - network biomarkers from fMRI data q - values [ 23 ] with respect to a random population of connected subgraphs of matching size . We choose the q - value as our significance measure as it reflects the false discovery rate ( FDR ), as opposed to the false positive rate ( FPR ) captured byp - values . The q - value measure of statistical significance has been employed for genome - wide studies and has significant advantages over alternative corrections for FDR [ 23 ].__label__Supplement|Paper|Extent
Additional information Supplementary Information accompanies this paper at https :// doi . org / 10 . 1038 / s41467018 - 04957 - 4 . Competing interests : The authors declare no competing interests . Reprints and permission information is available online at _CITE_ Publisher & apos ; s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations . Open Access This article is licensed under a Creative Commons Attribution 4 . 0 International License , which permits use , sharing , adaptation , distribution and reproduction in any medium or format , as long as you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material .__label__Supplement|License|Other
An initial API ( application programmer interface ) providing public access to the electrophysiological data is described at _CITE_ This RESTful API allows contents of the NeuroElectro database to be dynamically retrieved in JSON or XML format for utilization within external applications . For example , using the current API , a developer could build an application which dynamically queries NeuroElectro for all data__label__Method|Code|Use
PIT analysis was done using a bespoke bioinformatic pipeline ( PIT : Genome annotation_from mgf ; _CITE_ d1 ) available on the publically available proteomics resource GIO [ 7 ]. The default settings on each tool contained in the pipeline were used unless otherwise stated , as follows . The 20 . RAW files from the MS / MS analysis were first converted to mzML files using MSConvert whilst the de novo transcriptome produced by Trinity ( containing 73 , 881 sequences ) was translated in all 6 frames ( ORFs with a start codon > 200 nt ) using PIT : ORFall to produce 62 , 675 ORFs .__label__Method|Tool|Use
We would like to thank Lidia Mateo for technical support and all the members of the Petrov lab for their comments . We also would like to thank the ScaleGenomics ( _CITE_ ) and Proclus ( Stanford University ; http :// www . stanford . edu /) computational platforms that we used to investigate this study .__label__Supplement|Website|Other
Krishnan etal . Theoretical Biology and Medical Modelling 2014 , 11 : 28 Page 13 of 17 _CITE_ library and after necessary textual modifications in the structural model and data file , the modeling and simulation environment is ready for simulating the specified behavior . In this sense , the modeling and simulation environment is a plug and play system with no re - programing effort and hence reusable . The flexibility of the modeling and simulation environment was demonstrated by modeling and simulating the gastric emptying behavior in humans .__label__Supplement|Paper|Produce
We collected observed meteorological data ( including daily mean temperature , precipitation , solar radiation , wind speed , and vapor pressure ) for 1970 – 2010 from 117 stations within and around the Tibetan Plateau ( Table S8 ) from the China Meteorological Data Sharing Service System ( _CITE_ ). We calculated annual average temperature and annual precipitation for each station using the observed daily data . Since most stations have no directly observed evaporation data , we estimated the potential evapotranspiration of each station based on the Penman - Monteith model recommended by the Food and Agriculture Organization of the United Nations ( FAO ) [ 50 ].__label__Method|Tool|Use
We collected observed meteorological data ( including daily mean temperature , precipitation , solar radiation , wind speed , and vapor pressure ) for 1970 – 2010 from 117 stations within and around the Tibetan Plateau ( Table S8 ) from the China Meteorological Data Sharing Service System ( _CITE_ ). We calculated annual average temperature and annual precipitation for each station using the observed daily data . Since most stations have no directly observed evaporation data , we estimated the potential evapotranspiration of each station based on the Penman - Monteith model recommended by the Food and Agriculture Organization of the United Nations ( FAO ) [ 50 ].__label__Supplement|Website|Use
Ono et al . BMC Genomics 2014 , 15 : 1028 Page 7 of 15 _CITE_ ( See figure on previous page .) Figure 3 CCA results using the Gata3 dataset for the Th differentiation programmes . CCA was applied to the Gata3 dataset , using the microarray dataset that analysed Th1 , Th2 , Th17 , and iTreg ( the Th dataset ) as explanatory variables for the Th differentiation programmes .__label__Supplement|Paper|Introduce
Coseismic deformations associated to Tohoku - oki event were measured by the GPS stations distributed along the Honshu Island . Geospatial Information Authority ( GSI ) of Japan has provided all original GPS Earth Observation Network ( GEONET ) RINEX data . To extract the three components of the coseismic offsets , we process the data with GIPSY - OASIS software ( _CITE_ ), and Jet Propulsion Laboratory ( JPL ) flinnR orbit and clock products . We use the kinematic precise point positioning strategy72 with ambiguity resolution73 . Coseismic displacements are calculated as a simple difference of the position estimates averaged over 15 minutes before and after the mainshock excluding the first 5 minutes during the most intense ground shaking .__label__Method|Tool|Use
Agriculture and nutrition outcomes The last decade has witnessed a surge of interest in leveraging agricultural development for better nutrition . However , there is a dearth of rigorous evidence and policy - relevant research on agriculture - nutrition linkages ( Pinstrup - Andersen , 2013 ). As part of the Advancing Research on Nutrition and Agriculture ( AReNA ) initiative , HarvestChoice overlaid CELL5M indicators to an extensive series of georeferenced Demographic and Health Surveys ( DHS ; _CITE_ ). Figure 5 shows the location of 28 , 866 clusters in SSA . Combining such datasets allows for more advanced econometric analyses to explore , for example , the spatial relationships between farming systems , biophysical characteristics , agricultural performance , market access and rural diets .__label__Method|Tool|Introduce
Such discrepancy emphasizes the need for caution when working with model results , but also suggests that incorporating data of the benthic communities can substantially enhance the quality of ecosystem models . All these aspects add to the complexity of the system and the result ´ s interpretation . The outcome of our analysis reflects some general principles of how abiotic variables drive the high PLOS ONE | _CITE_ April19 , 2017 15 / 22 Is interpretation of long - term data in transitional waters more than speculation ? Fig 11 . Median anomaly of abundance , biomass and species number at station 109 .__label__Supplement|Paper|Extent
If I tell the doctor that I have fever he might give me the medicine ... then all that I am feeling will calm down [ Focus group discussion ( FGD ) 1 Tanzania respondent 5 ]. Allen et al . BMC Medical Research Methodology 2013 , 13 : 140 Page 7 of 13 _CITE_ Treatment use phraseology meanwhile revealed a hierarchy ; after ARVs or antimalarials , use of intermittent or over the counter substances ( such as painkillers or vitamins ) were mentioned , but qualified with ‘ only ’ or ‘ apart from ’. These perceptions of significance may intersect with the next factor shaping reporting behaviour : relevance to report . Relevance to report Participants appeared to delay reporting experiences that they perceived irrelevant , with the checklists helping them to decide what was necessary .__label__Supplement|Paper|Introduce
Another disadvantage of human odor observers relative to dispersion models is that models can be used for predictive [ 9 ] and hypothetical scenarios , though these are not relevant to our application , so we focused on thorough data gathering and analysis using human odor observation rather than splitting our resources between those techniques and dispersion modeling . Our human odor observers were first trained and tested for their olfactory sensitivity by the use of the ASTM E544 - 10 standardized olfactory perception practice for determining odor type and concentration [ 11 ]. The choice of ASTM E544 - 10 is supported quantitatively by a careful comparison of seven different well - established techniques for odor detection that showed , using least significant difference ( LSD ) multiple comparison results , that all of the common field techniques tested were statistically indistinguishable from one another in their session means , and only a laboratory - based dynamic triangular forced - choice olfactometry ( DTFCO ) approach ( that collected samples in Tedlar bags for later laboratory evaluation by a group of panelists ) produced significantly different results , with the latter difference probably PLOS ONE | _CITE_ January31 , 2018 3 / 30 Combining Ordinary Kriging with wind directions to identify sources of industrial odors in Portland due to the collection bags having had a detectable background odor [ 12 ]. The ASTM E544 - 10 is an American standard closely related to the German VDI 3940 [ 13 ] technique , and is used in our study in a grid technique [ 14 ] similar to that of VDI 3940 [ 15 ] with sampling locations appropriate to the unusual topography of our study ’ s location . Our odor observers were first trained in odor detection , as described in the Methods section below , and our sampling sites along the top of a ~ 40 m tall bluff were organized to avoid the “ vicinity of houses , high walls , fences , edge of forest , roads with heavy traffic , railways , bus stops and taxi ranks ” as recommended by So ´ wka [ 16 ].__label__Supplement|Paper|Compare
Such a map rendering method produces visual discontinuities and conflicts at the tile borders , which may break the spatial distribution characteristics of features [ 49 ] and reduce the readability and applicability of the role in study design , data collection and analysis , decision to publish , or preparation of the manuscript . Competing interests : The authors have declared that no competing interests exist . PLOS ONE | _CITE_ May 5 , 2017 2 / 26 Tiled vector data model map [ 48 , 50 ]. Therefore , a critical issue for tiled vector E - maps is to join neighboring features with their symbol representation to maintain visual continuity and avoid visual conflicts . To eliminate visual breakage , this study proposes a tiled vector data model for the geographical features that define the additivity of map features and geographical features , partition vector geographical features , and implement map symbolizations to graphically match joined symbolized partitioned features without causing graphic conflicts and losses .__label__Supplement|Paper|Introduce
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ © The Author ( s ) 2018__label__Supplement|License|Other
The Blender file allows the user to browse the virtual worm and learn more about the anatomy of C . elegans , for example by allowing users to select parts of the worm to display the names of individual cells and tissues . This Blender file also provides a variety of visualization options such as applying transparency , color , or hiding cells to make viewing easier . Video tutorials are available at _CITE_ Images from this file have been incorporated into both gene and expression pattern pages on the new WormBase website .__label__Supplement|Document|Produce
The hierarchical Dirichlet processes ( HDP ) topic model was used in this study because it automatically determines the number of the topics from the data . A C ++ implementation of HDP was downloaded from _CITE_ The parameters used were “– eta 0 . 1 – max_iter 2000 ”. Eta is the hyperparameter for the topic Dirichlet distribution .__label__Method|Code|Use
Other Python packages for working with time series data do not fill the niche that Neo does . The core function of Neo is data representation . The Pandas package ( _CITE_ ) is ageneraltoolkitforworkingwithheterogeneousdata . Neocontains functionality specific for neural data analysis , such as grouping of channels by anatomical location . Another package , nitime ( http :// nipy . org / nitime ), provides algorithms for time - series analysis that aretailoredtowardneuroimagingdata . Incontrast , theNeopackage intentionallydoesnotprovidealgorithmsfordataanalysissincethis will vary widely across users . Overall , Neo provides functionality that is specific to neuroscience data ( unlike Pandas ) but not specific to particular applications within neuroscience ( unlike nitime ).__label__Method|Code|Use
We then discuss in the final section how this system has improved the BMRP data collection and storage , where it still has limitations , how we plan to develop it in the future and how other long - term individual - based projects could potentially benefit from using a similar system . The source code needed to build the system is freely available under open - source GPL license . The current release ’ s source code ( version 2 . 5 at the time of writing ) is available from _CITE_ and further updates can be found on GitHub at https :// github . com / nebogeo / mongoose - web .__label__Method|Code|Use
_CITE_ section Materials and methods ) used in CePa , the most influential genes were the nodes with the highest betweenness centrality .__label__Method|Algorithm|Introduce
_CITE_ section Materials and methods ) used in CePa , the most influential genes were the nodes with the highest betweenness centrality .__label__Supplement|Document|Introduce
For each of the relation type dictionaries we define an active feature , if at least one keyword from the corresponding dictionary matches a word in the window size of 20 , i . e . - 10 and + 10 tokens away from the current token . Page 11 of 14 ( page number not for citation purposes ) BMC Bioinformatics 2008 , 9 : 207 _CITE___label__Supplement|Paper|Introduce
The MI score tables were generated using a python script that submits iRefIndex interaction records , one at the time , to the scoring servers [ 41 ] and receives and consolidates these scores in an iRefIndex MITAB format . The algorithm to compute the scores is explained in [ 42 ]. The difference between both methods is that the first one includes information from IntAct only while the PSICQUIC version includes interaction data from all Mora and Donaldson BMC Bioinformatics 2012 , 13 : 294 Page 15 of 17 _CITE_ PSICQUIC servers ( APID , ChEMBL , BioGrid , IntAct , DIP , InnateDB , MPIDB , iRefIndex , MatrixDB , MINT , Interoporc , Reactome , Reactome – FIs , STRING , BIND , DrugBank , I2D , I2D – IMEx , InnateDB – IMEx , and MolCon ). In order to select the cut - off values for each score type , 9 networks were generated for each score and the ROC test was applied to each of them . Values of 0 . 6 ( for MI score - Intact ) and 0 . 7 ( for the MI score - PSICQUIC ) had the highest AUC values and were chosen as cut - offs in this study .__label__Supplement|Paper|Compare
In experiments that include multiple QC types , mixnorm will estimate the mean difference in metabolite levels for different types of QCs . If different QC pools are reflective of different types of analytical samples of interest , these location shifts can be applied to analytical data if desired . Mixnorm functionality is available in the metabomxtr R package ( devel ) [ 23 ] at _CITE_ [ 26 ].__label__Method|Code|Use
In fact , their findings suggest that uncertainty introduced via natural climate variability [ 62 ] should be explicitly included in future climate change assessments in addition to that introduced by the choice of GCM . Both studies also found that the relative contributions of the different uncertainty sources varied by species , likely a function of differences in the complexity of the climatic environment in which individual species reside . PLOS ONE | _CITE_ January 10 , 2018 17 / 23 Future species distributions in mountainous regions An interesting finding of our analysis is that the choice of conversion thresholds to convert probabilities to species presence contributed to only a small portion of variance in the future projections . Nenze ´ n and Arau ´ jo [ 60 ] found that the conversion threshold can induce a 1 . 7 to 9 . 9 - fold difference in the proportions of species projected to become threatened by climate change . Our results instead suggest that the choice of baseline climate dataset and GCM introduces more uncertainty to the climate change assessment than the choice of conversion threshold , although the differences between the two studies need to be interpreted cautiously as Nenze ´ n and Arau ´ jo [ 60 ] considered an overlapping , but not duplicative , set of potential uncertainty sources .__label__Supplement|Paper|Introduce
Data Availability : The authors confirm that all data underlying the findings are fully available without restriction . All relevant data are within the paper and its Supporting Information files . Funding : This work was supported by Institut National de la Sante ´ et de la Recherche Me ´ dicale ( INSERM ), Centre National de la Recherche Scientifique ( CNRS ), Strasbourg University , Agence Nationale de la Recherche ( ANR - 11 - BSV1 - 026 ), Association Francaise contre les Myopathies ( 15352 ), Muscular Dystrophy Association ( 186985 ), and Myotubular Trust ( _CITE_ ). OAN was supported by a fellowship from CAPES Foundation , Ministry of Education of Brazil , process number 1286 / 51 - 2 ( http :// www . capes . gov . br /). The funders had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript .__label__Supplement|Website|Introduce
This approach has been remarkably successful , resulting in 44 % of new Drosophilarelated papers being skim curated by authors . We describe the pipeline we devised to e - mail the authors and we assess the effectiveness of author curation for triaging papers . Overview of the literature curation pipeline Publications of all typesthat maycontain Drosophila - related information are identified by a weekly semi - automated literature search of the PubMed database ( _CITE_ ) ( Figure 1 ). The citation data for each publication verified to contain Drosophila - related information are then uploaded into the bibliography of the FlyBase database . Prior to integrating community curation into the pipeline , each new primary research paper was subsequently quickly read (‘ skimmed ’) by FlyBase curators ( Figure 1a ).__label__Material|Data|Use
RNA - seq data from a panel of 20 diverse human tissues10 ( data available at _CITE_ ), and observed widespread tissue - specific differences in the mRNA stability profiles ( Supplementary Fig . 6 ). Comparison of these stability profiles with those obtained from a panel of mouse tissues suggests a high degree of conservation across species ( Supplementary Fig .__label__Material|Data|Use
These are crucial steps in the multistep process of metastasis [ 33 ]. Data used in this study contain miRNA expression profiles for the NCI - 60 panel of 60 cancer cell lines obtained from Sø et al . [ 34 ] ( available at [ _CITE_ ). The mRNA expression profiles for NCI - 60 are downloaded from ArrayExpress [ http :// www . ebi . ac . uk / arrayexpress ], accession number E - GEOD - 5720 . Cell lines categorised as epithelial ( 11 samples ) and mesenchymal ( 36 samples ) are used in this work .__label__Material|Data|Use
PLOS ONE | https :// doi . org / 10 . 1371 / journal . pone . 0198189 May 24 , 2018 13 / 24 A time series of urban extent in China using NTL data Fig 5 . Offset in the extracted urban extents of Beijing in 2008 and 2009 . ( a ) The result for 2008 is superimposed on the result for 2009 ; ( b ) The result for 2009 is superimposed on the result for 2008 . _CITE_ The spatial distribution of the urban sprawl pattern displays some aggregation , which is approximately distributed in the eastern , central and western areas ( Fig 8 ). De - urbanization and Constant Urban Activity do not exit or have not appeared in China . Only three patterns of urban growth are shown on the map .__label__Supplement|Document|Introduce
However , this approach may not fully reflect the effect emerging from the complex ensemble of multiple rare variants , because it only uses the information from the presence of rare variants within a specific genomic region . The combined multivariate and collapsing ( CMC ) method divides rare variants into multiple classes , based on their MAFs , by collapsing each group , using © The Author ( s ). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Lee et al .__label__Supplement|License|Other
Supplementary information accompanies this paper at _CITE_ Competing Interests : The authors declare no competing financial interests . How to cite this article : Fu , F . et al . Dueling biological and social contagions .__label__Supplement|Document|Produce
Additionally , because the interviewees were drawn from the large proportion of the ADF population who provided responses to the Phase 1 questionnaire , the potential for sampling error was further reduced . Furthermore , the use of diagnostic interviews reduces the bias in response validity associated with self - report surveys . Citation : European Journal of Psychotraumatology 2014 , 5 : 23950 - _CITE_ 9 ( page number not for citation purpose )__label__Supplement|Paper|Introduce
As we attempt to address the modern biodiversity crisis , assessing the conservation status of species has become an invaluable tool for biodiversity conservation . Evaluating threat based on the Red List Categories and Criteria of the International Union for Conservation of Nature ( IUCN , 2012 ) is an authoritative , comprehensive and widely used approach in conservation biology ( Rodrigues , Pilgrim , Lamoreux , Hoffmann , & Brooks , 2006 ). Indeed , many decisions made by governments , natural resource managers , and conservation planners ( Rodrigues et al ., 2006 ) rely ( often solely ) on the “ Red List ” published by IUCN ( _CITE_ ). For example , programs such as Important Bird Areas ( IBA ), Important Plant Areas ( IPA , Anderson , 2002 ) or Tropical Important Plant Areas ( TIPA , Darbyshire et al ., 2017 ) all rely directly on threat assessments based on IUCN criteria . In parallel , there is also an urgency in listing threatened species in the near future .__label__Supplement|Website|Introduce
Higher entropy indicates greater sequence variability . For the entropy calculation , a web tool provided by the HCV database at Los Alamos National Laboratory was used : http :// hcv . lanl . gov / content / sequence / ENTROPY / entropy_one . html . The tool takes an MSA as input , and the MSA ‘ Web alignment 2008 ’ provided by the HCV database was used : _CITE_ The MSA consists of 471 representative HCV polyproteins from strains belonging to genotypes 1 to 6 . Hydrophilicity values were calculated , utilizing a tool housed on the IEDB webpage that utilizes the amino acid scale by Parker et al .__label__Material|Data|Use
Previously reported mutational signatures were obtained from _CITE_ on 1 © 2018 The Authors . The Journal of Pathology published by John Wiley & Sons Ltd J Pathol 2018 ; 245 : 283 – 296 on behalf of Pathological Society of Great Britain and Ireland . www . pathsoc . org www . thejournalofpathology . com 286 D Temko et al June 2017 .__label__Supplement|Document|Use
Each cluster had 3 CHWs , resulting in a total of 42 CHWs , and thus a corresponding number of reporting units , distributed across the study area . Parasitological assessments were conducted continuously from January 2011 to March 2013 in Luangwa and from April 2011 to March Hamainza et al . Malaria Journal 2014 , 13 : 489 Page 4 of 13 _CITE_ 2013 in Nyimba district in all the selected clusters . All consenting households received monthly active visits from CHWs , which included parasitological surveys using RDTs detecting histidine - rich protein 2 antigen ( Malaria Pf cassette test , ICT Diagnostics ), coupled with registers designed in a pre - defined questionnaire format [ 10 ]. Consent for household participation was given by the head of the household and consent was obtained from individual study participants , or parents / guardians in the case of minors , for the RDT test .__label__Supplement|Paper|Introduce
In addition , for a direct comparison with the X - Ray based Kypho - Lordotic angle values presented in the literature [ 71 – 74 ] the sagittal plane spine angle values have also been assessed for the most reported anatomical regions ( T1 — T12 ) as well as T4 — T12 for kyphosis and T12 — L5 for lordosis . After computation , a graphical report summarizes and represents the 3D full skeleton reconstruction and the related computed quantitative parameters ( Fig 4 ). PLOS ONE | _CITE_ June 22 , 2017 10 / 31 Normative 3D posture and spine data in healthy young adults In this paper , the total number of computed quantitative biomechanical parameters has been limited to n = 25 as shown in Table 2 .__label__Supplement|Paper|Introduce
research , and the efficient dissemination of the guidance documents to this community . Collaboration with well - established scientific organizations will play an essential role . We are linked to the ISCB , but we intend to establish formal links with other international societies and research organizations , including the International Biometric Society ( IBS ), International Society for Pharmacoepidemiology ( ISPE ), International Epidemiological Association ( IEA ), American Epidemiological Association and other , ongoing initiatives that typically focus on more specialized issues or fields of study , many of which also involve STRATOS members ( for example , EQUATOR _CITE_ and PCORI http :// www . pcori . org /). One direct product of such collaborative links will take the form of educational sessions and minisymposia at the annual meetings of the respective societies . Indeed , the STRATOS initative was launched at a half - day mini - symposium on the last day of the ISCB meeting in Munich , in August 2013 .__label__Supplement|Website|Introduce
Publication of this article is funded by Delaware INBRE program , with grant from the National Institute of General Medical Sciences - NIGMS ( 8 P20 GM103446 - 12 ) from the National Institutes of Health . This article has been published as part of BMC Systems Biology Vol 10 Suppl 2 2016 : Selected articles from the IEEE International Conference on Bioinformatics and Biomedicine 2015 : systems biology . The full contents of the supplement are available online at _CITE___label__Supplement|Document|Produce
Modules are provided for each information concept found in such tomes – checklists , keys , taxonomic treatments and taxon descriptions with distribution maps and images . Linkages between the information content of each module provides for dynamic and user - driven data retrieval . Natural history specimen and ( where appropriate ) observation records are the core of any Symbiota portal , stored centrally in a relational schema based on the Darwin Core standard ( _CITE_ ). A standard specimen search engine with auto - completion and pick - list functionality facilitates taxonomic and geographic searches . Returned is a list of specimen records that match the criteria ( Fig .__label__Supplement|Document|Extent
Our method has been implemented in an R package seqlm , freely available through Github : _CITE_ The method is not specific to the Illumina 450k methylation array and can be used with many other arrays . For example the code can be easily used for analyzing tiling array data .__label__Method|Code|Produce
The very first step in RNA - Seq data analysis is to assess the quality of the sequenced data for further analysis . A number of pre - processing steps such as removal of the low - quality sequences , exclusion of the poor - quality reads with more than five unknown bases and trimming the sequencing adapters and primers should be taken into consideration to obtain a clean and ready to use RNA - Seq data for downstream analysis . Several tools / packages such as FASTQC ( _CITE_ ), HTSeq [ 14 ], R ShortRead package [ 15 ], PRINSEQ ( http :// edwards . sdsu . edu / cgi - bin / prinseq / prinseq . cgi ), FASTX Toolkit ( http :// hannonlab . cshl . edu / fastx_toolkit /) and QTrim [ 16 ] are available for quality__label__Method|Tool|Use
Primary reason for exclusion Number (%) of studies Does not capture a health or economic benefit of pneumococcal vaccination 107 ( 47 . 98 %) Is a news article , comment , editorial , or review 80 ( 35 . 87 %) Examines a target population that does not meet our criteria ( e . g ., children ) 29 ( 13 . 00 %) Is in a language other than English 5 ( 2 . 24 %) Could not access article 2 ( 0 . 90 %) _CITE_ in included papers , we did not include any such articles , as our search captured all articles that either experts or reference lists brought to our attention . S4 File contains the extracted information for all included articles . We could not to apply criteria to grade the quality of the evidence in the included studies because most studies captured in our review are modeling studies .__label__Supplement|Paper|Compare
Plotted are mean ± SD of mean values for each class of trait calculated across 100 posterior trees ( see S6 Data ). BM , Brownian motion ; MDI , morphological disparity index ; ML , maximum likelihood . _CITE_ PLOS Biology | https :// doi . org / 10 . 1371 / journal . pbio . 2003563 January 31 , 2018 8 / 23 Competition and evolution in a songbird radiation crypsis opposing the effects of sexual selection [ 31 ], S4 Table ). Although we did not find a direct negative relationship between evolutionary rate per se and evidence for competition ( S17 Fig ), there is a trend in the predicted direction , and together with estimates of disparity and divergence , our results are consistent with a model of evolution in which the impact of interspecific competition is negatively related to evolutionary rates : traits either evolve rapidly ( e . g ., under social selection ) and escape the effect of competition upon secondary contact , or they evolve slowly and thus are subject to the effect of interspecific competition in sympatry . We note that there are other possible reasons for why we did not detect a consistent effect of competition on traits involved in social interactions .__label__Supplement|Paper|Introduce
It is , of course , nevertheless the case that the validity and integrity of data are ultimately linked to the sum of the integrity and validity of all data processes in the lineage of data creation . Recommendation 16 : GBIF should investigate innovative mechanisms for discovery and publishing of Moritz et al . BMC Bioinformatics 2011 , 12 ( Suppl 15 ): S1 Page 7 of 10 _CITE_ primary biodiversity data in multiple languages . GBIF should commission a position paper detailing such mechanisms for potential uptake by the community . Recommendation 17 : GBIF must institutionalize the ‘ biodiversity informatics potential ’ ( BIP ) Index to demonstrate the potential and urgency for nations to implement biodiversity informatics [ 61 ].__label__Supplement|Paper|Introduce
SPR as a measure of transmission captures well the association between malaria transmission intensity and all - cause / malaria mortality . This offers a quick and efficient way to monitor PLOS ONE | _CITE_ July 13 , 2017 1 / 19 Mortality in relation to malaria incidence in Western Kenya malaria burden . Excess mortality estimates indicate that small changes in malaria incidence substantially reduce overall and malaria specific mortality .__label__Supplement|Paper|Extent
In the previous NAR manuscript update ( 5 ), we reported that we were working in the development of ‘ PRIDE - Q ’, a resource that would show the high - quality identification data coming from PRIDE . The name of this resource was later changed to ‘ PRIDE Proteomes ’. Here , we are just giving a brief update about this new PRIDE resource ( in beta , _CITE_ ). PRIDE Proteomes provides a quality - filtered , across - dataset view on the identification data available in PRIDE Archive . The PSMs reported in PRIDE Archive are first qualityfiltered using a spectrum clustering approach : all the identified spectra coming from the public experiments in PRIDE Archive are clustered using a refined version of the ‘ PRIDE Cluster ’ algorithm ( 47 ).__label__Material|Data|Produce
The dataset ( s ) supporting the conclusions of this article is ( are ) available in the project ’ site repository at _CITE___label__Material|Data|Produce
The dataset ( s ) supporting the conclusions of this article is ( are ) available in the project ’ site repository at _CITE___label__Supplement|Website|Produce
© The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Davidsson et al . BMC Plant Biology ( 2017 ) 17 : 19 Page 2 of 17 In addition to PAMPs , damage - associated molecular patterns ( DAMPs ) play a vital role in defense activation against bacterial and fungal necrotrophs ( i . e .__label__Supplement|License|Other
where G0 denotes equilibrium shear modulus , S network stiffness , Γ the Gamma function , and n is a network specific exponent defining the power law slope . Unfortunately , this model was not sufficient to describe our data . First , it predicts a ratio between storage and loss PLOS ONE | _CITE_ April6 , 2018 12 / 22 Ultrasoft PDMS elastomers for mechanobiology module that is far from our observation . Second , it predicts a constant exponent n over the full range of frequencies . However , as can be clearly seen from our data , e . g .__label__Supplement|Paper|Introduce
© The Author ( s ) 2017 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Yukich et al . Malar J ( 2017 ) 16 : 317 Page 2 of 13 Background In areas where malaria transmission has been suppressed by vector control interventions many malaria control programmes actively seek new interventions to further reduce malaria prevalence , incidence , and transmission .__label__Supplement|License|Other
If the tables contain motifs , the motif logos will always be included in a separate column . This is very useful , since rather than just listing numerous motif identifiers or names of transcription factors the user may or may not be familiar with , the logos enable users to immediately identify properties of the corresponding motifs and see similarities between them . Results from multiple analyses can be collated into “ meta - analyses ” by extracting selected columns from Klepper and Drabløs BMC Bioinformatics 2013 , 14 : 9 Page 7 of 14 _CITE_ individual analyses and combining them into larger tables . Information from different types of analyses can be combined in this way to produce more comprehensive reports , or results from the same analysis run multiple times with different parameter settings can be juxtaposed to assess the impact of varying these parameters .__label__Supplement|Paper|Use
Various IRB - approved recruitment methods are used for the RVR , including health fairs , direct advertisement , and referral from other OHSU IRB - approved studies . The RVR includes an IRB - approved consent form , completed in person or online via a brief , secure REDCap survey . REDCap is a widely deployed secure web - based data collection and management application for building and managing online surveys and databases , especially for research activities ( _CITE_ ) [ 5 ]. Consented participants are provided with a link to a REDCap enrollment survey , or an option to complete the survey on paper or via phone . The survey includes demographic information , medical diagnoses , medications , and contact preferences .__label__Material|Data|Produce
RH data recorded in 2007 were arranged in a matrix comprised by 432 observations ( time instants , in rows ) by 25 variables ( RH sensors , in columns ). This matrix was row - centered as described in [ 21 ]. Next , a principal components analysis ( PCA ) was carried out using the software SIMCA - P 10 . 0 ( _CITE_ ). The same analysis was repeated with sensor data recorded in 2008 with 409 , 312 observations and 2010 with 429 , 012 observations . Results from these three models were compared in order to check if the relationships among sensors were maintained year after year .__label__Method|Tool|Use
RH data recorded in 2007 were arranged in a matrix comprised by 432 observations ( time instants , in rows ) by 25 variables ( RH sensors , in columns ). This matrix was row - centered as described in [ 21 ]. Next , a principal components analysis ( PCA ) was carried out using the software SIMCA - P 10 . 0 ( _CITE_ ). The same analysis was repeated with sensor data recorded in 2008 with 409 , 312 observations and 2010 with 429 , 012 observations . Results from these three models were compared in order to check if the relationships among sensors were maintained year after year .__label__Method|Algorithm|Use
the Benjamini Hochberg FDR estimation method . The qvalues for the vector of p - values are computed via the R package provided at _CITE___label__Method|Code|Produce
Final libraries were pooled to equal molarity in Buffer EB ( 10 mM Tris - HCl , pH 8 . 5 , Qiagen ) containing 0 . 1 % TWEEN 20 ( Sigma Aldrich ). Sequencing was performed on an Illumina NextSeq 500 instrument at TATAA Biocenter ( Gothenburg , Sweden ) using 150 bp single - end reads . Raw FastQ files were subsequently processed as described [ 28 ] using Debarcer Version 0 . 3 . 0 ( _CITE_ ). Sequence reads with the same barcode were grouped into families for each amplicon . Barcode families with at least 20 reads , where & gt ; 90 % of the reads were identical , were required to compute consensus reads .__label__Method|Tool|Use
The score on this task was the mean response time in milliseconds across trials which contained matching pairs . Cronbach ’ s alpha for this task has previously been reported as 0 . 85 [ 11 ]. Visual memory : A visual memory test was administered , labelled ‘ pairs - matching ’ ( _CITE_ ), where participants were asked to memorize the positions of six card pairs , and then match them from memory while making as few errors as possible . Scores on the pairs - matching test are for the number of errors that each participant made ; therefore , higher scores reflect poorer cognitive function . The Pairs matching task had two versions : 3 - pair and 6 - pair .__label__Method|Algorithm|Use
This larger set of variations was further refined to a list of high confidence variations by identifying reciprocal genotype calls in both strains . The potential function of these mutations was characterized in more detail using a two - fold approach . First , the variants were annotated with respect to protein domain using SMART ( _CITE_ ) and Pfam ( http :// pfam . xfam . org /). Second , conservationbased evaluation of the impact of the observed amino acid substitutions were scored using SIFT ( http :// sift . jcvi . org /). The reference orthologous protein sequences for use in SIFT analysis were obtained from the Fungal Orthogroups Repository ( http :// www . broadinstitute . org / regev / orthogroups /).__label__Method|Tool|Use
Because of the ‘ digital ’ end - point measurement in dPCR , effects of PCR efficiencies are eliminated in the quantification . Moreover , with dPCR , it is possible to perform a direct absolute quantification of intact DNA copies in the PCR mix [ 49 , 53 ]. Similar as in the qPCR , several CF primers resulting in PLOS ONE | _CITE_ June 14 , 2018 11 / 17 Evaluation of bisulfite kit performance using dPCR amplicons of different lengths ( 88 to 414 bp ), were used to investigate the difference in fragmentation between the twelve kits . 2 µl DNA of all 60 bisulfite treated samples ( 1 : 2 diluted ) was added to the dPCR mix containing QX200 ddPCR EvaGreen Supermix ( 2x ) ( 186 – 4033 , Bio - Rad ) with 100 nM forward and reverse primers , in a final volume of 20 µl . PCR reactions consisted of initial denaturation at 95 ° C for 5 min , followed by 40 cycles of denaturation at 95 ° C for 30 sec and annealing / elongation at specific temperature ( S10 Table ) for 2 min , and ended with a signal stabilization at 4 ° C for 5 min and 95 ° C for 5 min .__label__Supplement|Paper|Introduce
For protein identification and quantification , MS raw data were processed using MaxQuant46 ( version 1 . 5 . 2 . 8 ) and its integrated search engine Andromeda47 . Data derived from experiments of different RNAi target proteins and cellular fractions were analysed separately . MS / MS data were searched against all entries for T . brucei TREU927 listed in the respective fasta file downloaded from the TriTryp database ( version 8 . 1 ; 11 , 067 ), to which the 18 mitochondrially encoded proteins ( _CITE_ ) were added . Database searches were performed with tryptic specificity ( with a maximum of two missed cleavages ) not allowing cleavage N terminal to proline ( Trypsin / P ), mass tolerances of 4 . 5 p . p . m . for precursor and 0 . 5 Da for fragment ions , carbamidomethylation of cysteine as fixed and N - terminal acetylation and oxidation of methionine as variable modifications .__label__Material|Data|Use
See _CITE_ for details . Data Documentation Initiative DDI DC is generic and its terms are broadly defined . For the domain of the social sciences , the DDI initiative has created a range of specific metadata standards for describing data produced by surveys and other methods in social and economic sciences , and that are used for the documentation , discovery and interpolation .__label__Supplement|Document|Produce
A full list of indexed fields and filters , such as assay name , description , protocol , target description , readout name and tested chemical name , are documented at the PubChem Help page ( http :// pubchem . ncbi . nlm . nih . gov / help . html # PubChemindex ). Entrez BioAssay search can be accessed at _CITE_ or at http :// pubchem . ncbi . nlm . nih . gov . Documentation for general use of the NCBI Entrez system is available at http :// www . ncbi . nlm . nih . gov / Database / index . html . Detailed description for making an effective Entrez bioassay query has been described previously ( 1 , 3 ) and will be further described below .__label__Supplement|Website|Introduce
Especially , SNF is demonstrated to outperform other integrative methods like iCluster [ 31 ] which is based on pre - selection of genes . Direct concatenation was implemented by the matrix concatenation operation in Matlab . The Matlab code of SNF was downloaded from _CITE_ Evaluating iBFE on the DNA methylation , mRNA expression and miRNA expression datasets of lung and kidney cancers produced by TCGA The DNA methylation , mRNA expression and miRNA expression datasets of lung squamous cell carcinoma__label__Method|Code|Produce
Three out of four tested elements displayed reproducible LacZ reporter staining at E11 . 5 , with patterns of activity specific to limbs and overlapping well - known subdomains of Hand2 expression [ 50 ]. Interestingly , the only element that tested negative also showed the lowest predicted combined score ( Fig 5C and S15 Table ). Finally , we made the genome - wide predictions available at _CITE_ These can be directly and systematically queried through a user - friendly interface . The website also provides two tutorials that leverage published datasets that were not used in the predictions .__label__Supplement|Document|Produce
percentage cropland ) to categorical ones by using a 50 % threshold . Table 1 also shows the spatial and temporal coverage of each input data set . Geo - Wiki reference data on abandoned land We collected reference data on abandoned land through the Geo - Wiki platform ( _CITE_ ), which allows users to classify Google Earth and Bing VHR imagery . An example of the interface is provided in Figure 2 . The blue box corresponds to a 10 - sec pixel ; in the top left corner is a time slider to view available historical imagery at this location while the user chooses the classes from the right hand panel .__label__Supplement|Website|Use
Since this is a mixed first / second - order system , we base our analysis on the first - order pseudodifferential reduction 14 Weak hyperbolicity of the system ( 4 . 20 , 4 . 21 ) with given shift and densitized lapse has also been pointed out in [ 254 ] based on a reduction , which is first order in time and space . However , there are several inequivalent such reductions , and so it is not sufficient to show that a particular one is weakly hyperbolic in order to infer that the second - order - in - space system ( 4 . 20 , 4 . 21 ) is weakly hyperbolic . Living Reviews in Relativity _CITE_ Continuum and Discrete Initial - Boundary Value Problems and Einstein ’ s Field Equations 43 discussed in Section 3 . 1 . 5 . After linearizing and localizing , we obtain the constant coefficient linear problem (𝜕 𝑡 − 𝛽 ˚ 𝑘 𝜕 𝑘 ) 𝛼 = − 𝛼 ˚ 2𝑓𝐾 , ( 4 . 29 ) (𝜕 𝑡 − �� 𝜕 𝑘 ) 𝛽 — 𝛼 𝛾 aj𝛼 + 𝛼 𝛾 𝛾 𝜕 𝑘𝛾𝑗𝑙 − 2 𝜕 𝑗𝛾𝑘𝑙 ( 4 . 30 ) (︂ 𝑘 𝑖 = ˚ ij 2 𝑖𝑗 𝑘𝑙 1 )︂ , (𝜕 𝑡 − 𝛽 ˚ 𝑘 𝜕 𝑘 ) 𝛾𝑖𝑗 = 2𝛾 ˚ 𝑘 ( 𝑖 𝜕 𝑗 ) 𝛽𝑘 − 2𝛼 ˚ 𝐾𝑖𝑗 , ( 4 . 31 ) ˚ (𝜕 𝑡 − ��˚ 𝑘 𝜕 𝑘 ) 𝐾𝑖𝑗 = −𝜕 𝑖 𝜕 𝑗𝛼 + 𝛼2 𝛾 ˚ 𝑘𝑙 (−𝜕 𝑘 𝜕 𝑙𝛾𝑖𝑗 − 𝜕 𝑖 𝜕 𝑗𝛾𝑘𝑙 + 𝜕 𝑖 𝜕 𝑘𝛾𝑙𝑗 + 𝜕 𝑗 𝜕 𝑘𝛾𝑙𝑖 ) , ( 4 . 32 ) where 𝛼 ˚, 𝛽 ˚ 𝑘 and 𝛾 ˚ 𝑖𝑗 refer to the quantities corresponding to 𝛼 , 𝛽𝑘 , 𝛾𝑖𝑗 of the background metric when frozen at a given point . In order to rewrite this in first - order form , we perform a Fourier ^ ( 𝑎 , 𝑏𝑖 , 𝑙𝑖𝑗 , 𝑝𝑖𝑗 ) with 2𝑖 ^ 𝐾𝑖𝑗 , ( 4 . 33 ) transformation in space and introduce the variables 𝑈 = := | 𝑘 |^ 𝛾𝑖𝑗 , 𝑝𝑖𝑗 := 𝑎 := | 𝑘 |^ 𝛼 / 𝛼 ˚, 𝑏𝑖 := | 𝑘 | 𝛾 ˚ 𝑖𝑗 ^ 𝛽𝑗 / 𝛼 ˚, 𝑙𝑖𝑗 √︁ where | 𝑘 |:= 𝛾 ˚ 𝑖𝑗𝑘𝑖𝑘𝑗 and the hatted quantities refer to their Fourier transform .__label__Material|Data|Use
cudaBayesreg [ 14 ] Package for Compute Unified Device Architecture ( CUDA ) based Bayesian multilevel analysis of fMRI data . We will use some of the packages above in the examples in this manuscript . In addition , a frequently updated more exhaustive list on CRAN of packages for medical image analysis can be found here : _CITE_ The remainder of the manuscript is organized as follows . Sections 1 , 2 and 3 describe the structure of the fMRI data , discuss ways of obtaining the data and give a brief overview of the preprocessing steps .__label__Supplement|Document|Produce
If each video is reviewed completely by two observers , nR nO will be 2 . 0 . Thus , the product nR nO reflects that observers may make repeated Trask et al . BMC Medical Research Methodology 2013 , 13 : 124 Page 4 of 14 _CITE_ observation of the same video frames , which is one way of improving precision of exposure estimations obtained by observation [ 7 ]. Substituting Eq . s ( 2 ) and ( 4 ) into Eq . ( 1 ) results in models specific to the observation method :__label__Supplement|Paper|Introduce
Purified RNA ( 5 µg ) was sent to the University of Missouri DNA Core , where RNA quality check , RNA - Seq library construction , and Illumina HiSeq 2000 runs were performed as 100 bp SE reads with four samples per lane ( one lane included two additional samples that were not included in this study ). We used SOAPdenovoTrans v1 . 01 for de novo assembly of the transcriptomes of each taxon using five k - mer values ( 13 , 23 , 33 , 43 , 63 ) following the additive multiple - k assembly method of Surget - Groba and Montoya - Burgos ( 2010 ). We combined redundant contigs from the multiple k - mer assembles with CD - HIT - EST ( Li and Godzik 2006 ) and removed all sequences below 100 bp with FASTX - Toolkit ( _CITE_ , last accessed November 4 , 2013 ). To identify orthologous genes between transcriptomes and ESTs , we used HaMStR v8b ( Ebersberger et al . 2009 ).__label__Method|Tool|Use
We further show that the effects of high larval population density persist through adulthood , as C . elegans larvae raised at high densities exhibit significantly reduced adult lifespan and respond differently to exogenous chemical signals compared to larvae raised at low densities , independent of density during adulthood . Our results demonstrate how inter - organismal signaling during development regulates reproductive maturation and longevity . PLOS Genetics | _CITE_ April 10 , 2017 1 / 21 Larval crowding accelerates C . elegans development and reduces lifespan__label__Supplement|Paper|Introduce
Phasing is a nontrivial process which is subject to error . By extending the tree estimation method and likelihood score to be computed on genotypic data , these methods will be more easily applied to real data sets . Advantages of the model include its ability to find different signals than previous statistical methods and its flexibility to be extended Thompson and Kubatko BMC Bioinformatics 2013 , 14 : 200 Page 10 of 10 _CITE_ to analyze different types of data . Although these extensions are under investigation , the proposed data analysis technique appears to be an impactful modification of the ideas presented in QBlossoc , especially in the presence of population structure .__label__Supplement|Paper|Extent
MEDUSA was implemented in python programming language and requires the numpy package ( http :// www . numpy . org /). MEDUSA makes use of standalone tools such as FASTX , bowtie2 [ 26 ] and GEM [ 27 ] that need to be callable from the Unix command line . The MEDUSA pipeline together with databases and results are available at _CITE_ medusa .__label__Method|Code|Produce
MEDUSA was implemented in python programming language and requires the numpy package ( http :// www . numpy . org /). MEDUSA makes use of standalone tools such as FASTX , bowtie2 [ 26 ] and GEM [ 27 ] that need to be callable from the Unix command line . The MEDUSA pipeline together with databases and results are available at _CITE_ medusa .__label__Material|Data|Produce
Tibetan participants were recruited from two districts during spring and summer of year 2012 : 23 individuals are from Tsum region in Gorkha district and 30 individuals are from Upper Mustang region in Mustang district . All participants were born and raised in high altitude regions (& gt ; 3 , 000 m ). These 53 individuals are a subset of a bigger cohort recruited at the same time , and selected for PLOS ONE | _CITE_ April 27 , 2017 6 / 12 The population structure of Tibetans this study based on harboring negligible level of South Asian ancestry . Saliva samples were collected using OG - 500 Oragene saliva collection kits ( DNA Genotek , Inc ., Ottawa , ON , Canada ) and genomic DNA was extracted using PT - L2P reagents ( DNA Genotek , Inc ., Ottawa , ON , Canada ) following manufacturer ’ s protocol . Genome - wide genotyping experiments were performed at the Genomics facility at the University of Chicago , using both Illumina HumanCore v1 - 0 ( 298 , 931 markers ) and HumanOmniExpress - 24 v1 . 0 ( 716 , 503 markers ) arrays .__label__Supplement|Paper|Introduce
Incorporating negative evidence allows Omics Integrator to avoid unexpressed genes and avoid being biased toward highly - studied hub proteins , except when they are strongly implicated by the data . The software is comprised of two individual tools , Garnet and Forest , that can be run together or independently to allow a user to perform advanced integration of multiple types of high - throughput data as well as create condition - specific subnetworks of protein interactions that best connect the observed changes in various datasets . It is available at _CITE_ and on GitHub at https :// github . com / fraenkel - lab / OmicsIntegrator . This is a PLOS Computational Biology Software paper .__label__Method|Tool|Produce
_CITE_ Based on the above property , the MD5 hash value can be referred to the image of a fingerprint . Therefore , the duplicated data detection of each chunk can be achieved by checking the fingerprint , which can be formally defined as follows : Definition 1 Duplicated data detection . Set A is defined as A = { ci | ci is the data chunk already written }.__label__Supplement|Document|Extent
A new © 2016 van der Kloet et al . Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . van der Kloet et al . BMC Bioinformatics 2016 , 17 ( Suppl 5 ): 195 Page 272 of 415 group of low level data fusion methods has recently been introduced that are able to separate the variation in all data - sets .__label__Supplement|License|Other
The background weighted PPI network was constructed using data from STRING database ( _CITE_ , version 10 ) [ 21 ]. It weights protein - protein interactions by calculating confidence scores . In this work , 70 % confidence score (& gt ; 700 ) has been used as a cut - off for further analysis .__label__Material|Data|Use
So far , all results report performance in terms of the mean Robinson – Foulds ( RF ) distance between reconstructed and reference tree . As alternative evaluation measures , we also computed the fraction of trees where the topology improved , remained constant , and worsened in terms of RF distance ( Supplementary Fig . 24 and 25 available on Dryad at _CITE_ ). In the majority of cases , the topology remained unchanged . When it changed , in 25 out of the 30 method data set combinations , changes were overwhelmingly for the worse .__label__Supplement|Document|Produce
With the aim to identify chronic disease events in the mid - term , the study covers a middle - aged range ( 40 – 65 years old ) corresponding to 30 % of the Catalan population . 22 In addition , participants are required to be able to understand at least one of the two official languages in Catalonia ( Catalan or Spanish ) to provide written informed consent , to possess an Individual Health System Identification Card and to be current residents of Catalonia . Potential participants are excluded if they have mental or health impairment disorders that impede giving written informed consent or efficient communication , or if they are planning to leave Catalonia during the following 5 years . Participants are invited to participate using multiple active strategies , such as phone call , mail , GCAT web page ( _CITE_ ) or in person . Then , an appointment is agreed on and participants are asked to attend a recruitment centre . There are 11 permanent recruitment centres ( figure 1 ).__label__Supplement|Website|Use
A few existing databases have information on susceptible genes for depression by literature mining or by review of prior publications , such as HuGE navigator , to serve as a convenient searching engine . However , without a proper weighting scheme for the strength of evidence provided from different studies and data sources , these databases are less informative for follow - up studies . For instance , in HuGE Navigator ( 8 Feb 2011 version ; _CITE_ ), we searched gene information for depression and found 690 depression candidate genes with scores ranged between 0 and 1 . 5 . Using a__label__Method|Tool|Use
Users of the system can download the publicly available datasets in the Data Browsing and Download & gt ; Data measured on plants section ( see _CITE_ ), using similar searching criteria to those described above to restrict the downloading to specific data of interest .__label__Material|Data|Use
The phosphopeptide data set from Rikova et al . (“ 20070918_spectrumtable . txt ”) was downloaded from PhosphoSitePlus ( http :// www . phosphosite . org / suppData / RikovaCell / 20070918_spectrumtable . xls ) [ 58 ]. Gene names were mapped to HUGO gene names ( _CITE_ ) using the R library “ org . Hs . eg . db ” and checked against UNIPROT and ENTREZ IDs . All peptide counts for all proteins were summed for each protein in each lung cancer sample . For graphing , “ total phosphorylation ” represents the sum of phosphopeptides detected for that protein in the entire data set .__label__Material|Data|Use
All data used in this study are available from the ENCODE project repository [ 13 , 23 ]. ( IDs : wgEncodeBroadHistoneK562H3k4me3StdAlnRep1 . bam , wgEncodeBroadHistoneK562H3k4me3StdAlnRep2 . bam , wgEncodeBroadHistoneK562H3k9acStdAlnRep1 . bam , wgEncodeBroadHistoneK562H3k9acStdAlnRep2 . bam , wgEncodeBroadHistoneK562ControlStdAlnRep1 . bam ). DGW is available as a open - source Python package on Github [ _CITE_ [ 19 ]. The manual illustrating the package is available from the same URL .__label__Method|Code|Produce
Data Availability Statement : All relevant data are within the paper and its Supporting Information files . Funding : The funders had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript . This research was partially funded by grant LP100200057 ( YP , MK ) funded by the Australian Research Council ( _CITE_ ), Victorian Health Promotion Foundation ( https :// www . vichealth . vic . gov . au / YP , MK , NP ) and the Australian Human Rights Commission ( https :// www . humanrights . gov . au / YP ). YP is supported by an Australian Research Council Future Despite a growing body of epidemiological evidence in recent years documenting the health impacts of racism , the cumulative evidence base has yet to be synthesized in a comprehensive meta - analysis focused specifically on racism as a determinant of health . This metaanalysis reviewed the literature focusing on the relationship between reported racism and mental and physical health outcomes .__label__Supplement|Website|Introduce
Nurses at the 39 sites were instructed to apply the case definition to all presenting patients and to record and report demographic characteristics ( name , age , sex , place of residence ), symptom ( fever , rash , mucosal bleeding anorexia , arthralgia , abdominal pain , persistent vomiting , lethargy , fluid retention , liver enlargement , tourniquet test ), date of symptom onset , rapid diagnostic test result , and hospitalisation status data for patients that met the definition on a provided form . The first new sentinel sites began reporting data in epidemiological week 33 / 2016 with others contributing by epidemiological week 42 / 2016 . PLOS ONE | _CITE_ June 7 , 2018 8 / 15 Enhanced surveillance during a large dengue outbreak in Solomon Islands , 2016 - 17 Data were transferred from surveillance sites to the Honiara - based MHMS surveillance unit on a weekly basis by various means , including collection of reporting forms by MHMS staff ( number of surveillance sites = 17 , 44 %), hand delivery of forms to MHMS staff ( number of surveillance sites = 12 , 31 %), by email ( number of surveillance sites = 4 , 10 %), and verbally by telephone ( number of surveillance sites = 6 , 15 %). MHMS surveillance staff manually entered data received into an Excel ® database for analysis . The resulting information was presented in a weekly report sent to ~ 120 recipients including staff of national and provincial health services , other government departments , development partners and relevant SI - based non - government organisations .__label__Supplement|Paper|Introduce
The model population adopted the mid - year population structure of the year , 2007 and 2008 respectively , and was classified into six age groups : 0 – 5 , 6 – 12 , 13 – 19 , 20 – 39 , 40 – 59 , and ≥ 60 years ( see Additional file 2 : Table S2 ). The mid - year age - structured population sizes were obtained from Department of Household Registration , Ministry of the Interior ( _CITE_ ). Births and deaths were neglected because of the relatively short time scale that the simulation spanned . The model further adopted the normalized age - specific contact rates estimated in Wallinga et al .__label__Supplement|Website|Use
OWL versions of the information models ; descriptions of interview protocols ; models of stakeholders and users ; competency questions ; and other supporting documents can be found at _CITE_ Raw data and other materials will be made available upon request to the corresponding author .__label__Method|Algorithm|Produce
OWL versions of the information models ; descriptions of interview protocols ; models of stakeholders and users ; competency questions ; and other supporting documents can be found at _CITE_ Raw data and other materials will be made available upon request to the corresponding author .__label__Supplement|Document|Produce
The most important parameters have been included in the database to describe the technical specifications of each sensor recording such as sensor type ( accelerometer , gyroscope , magnetometer ), device type , sample frequency and sensor range ( see Additional file 1 : Table S1 ). ( 2 ) Minimum clinical dataset : Almost every study uses an individual predefined set of clinical variables that is not modifiable for numerous reasons . Therefore , a minimum ( core ) dataset of participant characteristics was used including sex , age and a functional description based upon the International Classification of Functioning ( ICF ) multilingual coding system ( _CITE_ ) ( see Additional file 1 : Table S2 ). ( 3 ) Fall reports : The FARSEEING consortium agreed on the following fall definition based on the ProFaNE recommendation [ 12 ]: A fall is an unexpected event in which the person comes to rest on the ground , floor , or lower level . In case of a reported or measured fall , a standardised fall report ( based on an interview or oral confirmation ) describing the fall event should be completed within one day after the fall event .__label__Method|Tool|Extent
Raw electricity consumption data from full datasets for all 6 buildings ( building number shown on the right most side of each plot ). Building 5 shows the presence of outliers and mean - shifts ; all buildings exhibit missing data . _CITE_ baseload ( e . g . requiring setpoint changes at night or during periods of low occupancy ). For example , Building 1 from Table 3 exhibits a 15 - minute baseload consumption of 325kWh with 41kWh of daily variation , so only 11 % of the consumption may be probed for further analysis .__label__Material|Data|Use
Therefore , ADAPT is a very powerful approach to study longitudinal development of diseases and therapeutic interventions and uses experimental data to infer adaptations in the system [ 36 , 37 , 39 , 40 ]. In previous studies , ADAPT has been applied to study hepatic steatosis [ 37 , 39 ], and treatment of type 2 diabetes [ 40 ], but has not yet been applied to study the full metabolic complexity of MetS . PLOS Computational Biology | _CITE_ June 7 , 2018 2 / 19 In vivo and in silico dynamics of the development of Metabolic Syndrome Therefore , we aimed to design a computational , data - driven approach to study the longitudinal and progressive dynamics of the majority of metabolic alterations of MetS , i . e . obesity , glucose intolerance , insulin resistance and dyslipidemia . We employed a systems biology methodology that integrates three main concepts to infer metabolic adaptations during MetS development : i ) the long - term simulation method ADAPT , combined with ii ) a newly developed in silico MetS model that describes the metabolic processes involved in whole - body carbohydrate and lipid metabolism , and integrated with iii ) time - series data obtained from an in vivo MetS model .__label__Supplement|Paper|Introduce
OCP provides Web - services for individual tile requests that extract image planes from volume databases . To accelerate visualization when accessing nearby slices or when viewing data repeatedly , OCP manages a hierarchy of caches . First , an in - memory tile cache is maintained in the OCP data cluster using memcached ( _CITE_ ). The cache contains the most frequently accessed tiles , subject to the cluster ’ s memory capacity . Tile requests that miss the memory cache are rendered on demand by the Web server .__label__Method|Tool|Use
Strictly speaking , this means that almost all these studies focused on the effects of heavy – AD on © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Kurai et al . Genes and Environment ( 2017 ) 39 : 25 Page 2 of 8 health .__label__Supplement|License|Other
ftp . ncbi . nlm . nih . gov / blast / executables / blast +/ LATEST /) and organized in various databases by taxonomy and exchanger subtype . BLAST searches are ran on the server - side using PHP scripts and served asynchronously using Ajax ( http :// api . jquery . com / jquery . ajax /) and jQuery ( http :// jquery . com /). FASTA formatted sequences can be retrieved at the NCX - DB BLAST page by using the NCBI Entrez Programming Utilities ( _CITE_ ). Accession numbers can be entered as a comma separated list and retrieved remotely . The following parameters are implemented for BlastP searches using NCX - DB : initial word_ size match is 3 ; the minimum threshold score to add a word to the BLAST lookup table is 11 ; the compositionbased score adjustment is defined as per [ 55 ]; the heuristic value ( in bits ) for the final gapped alignment is 25 ; and the window size for multiple hits is set to 40 .__label__Method|Tool|Use
This article has been published as part of BMCGenomics Volume 19 Supplement 4 , 2018 : Selected original research articles from the Fourth International Workshop on Computational Network Biology : Modeling , Analysis , and Control ( CNB - MAC 2017 ): genomics . The full contents of the supplement are available online at _CITE___label__Supplement|Document|Produce
BMJ Open 2016 ; 6 : e011562 . doi : 10 . 1136 / bmjopen - 2016011562 ▸ Prepublication history and additional material is available . To view please visit the journal ( _CITE_ ).__label__Supplement|Document|Produce
BMJ Open 2016 ; 6 : e011562 . doi : 10 . 1136 / bmjopen - 2016011562 ▸ Prepublication history and additional material is available . To view please visit the journal ( _CITE_ ).__label__Supplement|Paper|Produce
The second version was used since 2011 and a total of 693 children used the new version . Although Zhou et al . Molecular Autism 2014 , 5 : 52 Page 3 of 13 _CITE_ there were small differences between the two versions , both versions consisted of the following five sections . The first section queried demographic information about the child such as gender , birth date , and current age , and information about both parents such as age when married , education level and occupation . The second section included questions about prenatal conditions such as the length of pregnancy , and complications , illnesses and medications taken during pregnancy .__label__Supplement|Paper|Compare
list of correct answers ( Supporting Information Table 1 ). The C - score will be available for testing within the Proteoform Characterization Tool , 34 hosted by the Consortium for Top Down Proteomics ( _CITE_ ).__label__Method|Tool|Use
To determine the functional relevance of the modules , we test whether the genes from the modules are enriched for specific biological functions or signaling pathways . We perform a pathway enrichment test using gene ontology ( GO ) biological process terms [ 27 ], KEGG pathways [ 28 ], and BioCarta pathways ( http :// www . biocarta . com ). First , we download these pathways from GSEA ( _CITE_ ) and apply a hypergeometric test to each module , obtaining the p - values . We exclude biological functions or signaling pathways containing more than 300 genes , as such functions are too general . Supplementary Fig .__label__Material|Data|Use
( PDF ) Table S1 Functional genomics features used in our analysis . This Excel spreadsheet lists the files used from ENCODE ( _CITE_ ) or GEO ( http :// www . ncbi . nlm . nih . gov / geo /). There is a sheet for each of the classifiers based on functional genomics data that lists all data files used . ENCODE data set names are UCSC track names .__label__Material|Data|Use
© 2016 by the authors ; licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ( CC - BY ) license ( _CITE_ ).__label__Supplement|License|Other
The Reactome graph database is freely available at : https :// reactome . org / dev / graph - database . The API for the ContentService is available at _CITE_ with documentation and tutorials available at : https :// reactome . org / dev / content - service . The source code , in Java , is freely available at : https :// github . com / reactome ( See the graph - core , graphimporter and content - service repositories ). Future development will focus on updating the version of SDN and integrating interaction data from IntAct ( http :// www . ebi . ac . uk / intact /) directly to the Reactome graph database .__label__Supplement|Website|Use
© The Author ( s ) 2017 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Sadarangani and Menon BioMed Eng OnLine ( 2017 ) 16 : 59 Page 2 of 19__label__Supplement|License|Other
Many other libraries are also available for generating charts . For instance , written in Java , GRAL ( GRAphing Library , _CITE_ ) is a free to use and lightweight library for displaying plots ( graphs , diagrams and charts ), implementing most of the charts included in JFreeChart . Other example is the Java library Jzy3d ( http :// www . jzy3d . org /), which allows drawing scientific data in 3D including surfaces , scatter plots , bar charts and a lot of other 3D primitives . The API provides support for rich interactive charts with colour bars , tooltips and overlays .__label__Method|Code|Introduce
The details of the complete conversion and importing process are out of the scope of this paper . In addition to the relational database , we use a dedicated cache server which stores the graph structure in a compact format in main memory , enabling significantly faster queries than would be possible by using only the relational database . A public query interface to the database is available at _CITE___label__Material|Data|Use
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ © The Author ( s ) 2017__label__Supplement|License|Other
For our analysis we used a slightly changed posterior parameter estimate Frontiers in Neuroscience | Brain Imaging Methods February 2015 | Volume 9 | Article 43 | 4 Arand et al . Parameter identifiability in DCM for fMRI θ ± 0 . 1 from model inversion as initial guesses . With real data , the easiest way to define start values is to use the connectivity priors as defined in SPM ( _CITE_ ). DCMs depend sensitively on their parameter values as models with circular connections start oscillating for certain parameter sets . Additionally , DCMs can diverge if the inhibitory self - connections are not strong enough to prevent run - away excitation ( Friston et al ., 2003 ).__label__Method|Algorithm|Use
PLOS Biology | _CITE_ October 18 , 2017 6 / 26 Robust bimodality in the HIV fate - specification circuit LTR . Nine Iso populations were exposed to 48 different doxycycline and Shield - 1 conditions ( S2 and S3 Figs and S1 – S23 Data ), and bimodality was tested for by the Hartigan Dip Test [ 39 ] ( the threshold for determining bimodality was p & lt ; 0 . 3 , agreeing with an independent test , S3 Fig and S24 Data ). Gray squares indicate populations that were determined to be unimodal , and black squares represent bimodal populations .__label__Supplement|Paper|Other
Three steps were necessary in applying the propensity score method to the Medicare HOS data . First , self - comPage 4 of 12 ( page number not for citation purposes ) Health and Quality of Life Outcomes 2003 , 1 _CITE___label__Supplement|Paper|Introduce
genotyping missingness rate , ( b ) p values of Hardy – Weinberg test , and ( c ) allele frequencies . Protein - truncating variant annotation . We annotated 784 , 257 autosomal variants extracted from the mapping bim files provided by the UK Biobank using VEP version 87 and the LOFTEE plugin ( _CITE_ ) and identified 27 , 057 putative PTVs77 . We first removed 8118 PTVs specific to the UK BiLEVE Axiom Array or with missingness > 1 % among the subjects genotyped on the UK Biobank Axiom Array . Despite a missingness rate of 28 % on the Axium Biobank Array , we kept rs141992399 ( CARDS ) in the analysis .__label__Method|Tool|Use
Furthermore , experimentaldata - constrained RNA secondary structures are not available for RBP binding sites in any current database . Finally , it is often desirable to know whether RBPs can interact with RNA molecules , especially novel lncRNAs ; however , only a few online tools ( 21 , 22 ) are available for predicting RBP binding sites on given RNA sequences . Previously , we developed CLIPdb ( 20 ), which simply provided RBP binding sites without further annotation and interpretation ( _CITE_ ). Here , we constructed a new platform for CLIPdb version 2 , POSTAR , which focuses on POST - trAnscriptional Regulation coordinated by RNA - binding proteins ( RBPs ), to facilitate searching , annotation , visualization , integration , connection , and interpretation of data regarding multiple posttranscriptional regulatory events in humans and mice . First and foremost , POSTAR provides a comprehensive repository of experimentally probed ( i . e .__label__Method|Tool|Produce
Many of these cell lines will grow as subcutaneous xenografts , thus cell lines sensitive to an agent in vitro were often subjected to further analyses in xenografts derived from those cell lines . The NCI - 60 panel has been extensively molecularly characterized , with data available for gene expression , DNA variation ( mutation and SNPs ), protein expression , DNA methylation , microRNA expression and metabolomics ( http :// dtp . cancer . gov / mtargets / mt_index . html ) [ 3 - 9 ]. The COMPARE algorithm ( _CITE_ compare . html ) allows investigators to correlate NCI - 60 drug activity profiles with all other open agents in the database and with molecular characteristics of the cells [ 10 ]. While the in vitro grown cells have been characterized , the corresponding subcutaneous xenografts had not . However , other studies have succeeded in molecular profiling of other xenografts [ 2 , 11 , 12 ].__label__Method|Algorithm|Introduce
In addition , we evaluated agreement between detected DMRs and changes in DNase I hypersensitivity as conducted in [ 6 ]. We collected data from [ 19 ], where human foreskin fibroblasts and embryonic stem cells are measured by bisulfite - seq . For these cell types , we obtained DNase I hypersensitivity data from the ENCODE project _CITE_ jan2011 / byDataType / openchrom / jan2011 / fdrPeaks /. The data for each cell type contain the set of 150 - bp regions that show the local maxima of DNase I hypersensitivity with false discovery rate ( FDR ) less than 1 %. We defined “ differentially sensitive sites ” ( DSSs ) as those 150 - bp regions present in either one of the two cell types .__label__Method|Tool|Use
In addition , we evaluated agreement between detected DMRs and changes in DNase I hypersensitivity as conducted in [ 6 ]. We collected data from [ 19 ], where human foreskin fibroblasts and embryonic stem cells are measured by bisulfite - seq . For these cell types , we obtained DNase I hypersensitivity data from the ENCODE project _CITE_ jan2011 / byDataType / openchrom / jan2011 / fdrPeaks /. The data for each cell type contain the set of 150 - bp regions that show the local maxima of DNase I hypersensitivity with false discovery rate ( FDR ) less than 1 %. We defined “ differentially sensitive sites ” ( DSSs ) as those 150 - bp regions present in either one of the two cell types .__label__Material|Data|Use
Ontologized MIABIS ( OMIABIS ) ( _CITE_ ) was created as an OWL implementation of the BBMRI & apos ; s Minimum Information About BIobank data Sharing ( MIABIS ). It is based on the BBMRI use cases , which are mostly population and cohort based . Due to juridical and ethical reasons searching individual specimens was out of scope for the initial implementation of OMIABIS [ 8 ].__label__Method|Tool|Produce
It is equal to Actual Mapped +( Expected Unmapped - Actual Unmapped ) while Reported correct is the total number of correctly mapped reads . Hatem et al . BMC Bioinformatics 2013 , 14 : 184 Page 22 of 25 _CITE_ the Human genome using ART . The maximum allowed error rate was 5 %, i . e ., 5 mismatches in that case . The results for this experiment are shown in Table 3 .__label__Supplement|Paper|Introduce
results in development of some ‘ PO : 0009006 ! shoot system PO and GO are working together to align these two ontologies systematically through an ongoing process of suggesting new terms and modifications of existing plant - specific GO terms through the GO SourceForge tracker ( https :// sourceforge . net / tracker /? func = add & group_id = 36855 & atid = 440764 ). In the future , the GO intends to use PO in combination with TermGenie ( _CITE_ ), a template - based , reasoner - assisted ontology term generation tool , for creation of new plant - related terms ( Chris Mungall , personal communication ). Arabidopsis annotations to GO terms are developed by the TAIR ( Berardini et al . 2004 , Lamesch et al .__label__Method|Tool|Use
In typical clinical microarray studies , clinical researchers spend time in this exploratory process . In this section , we propose a use case of application of Linked Aoki - Kinoshita et al . Journal of Biomedical Semantics 2014 , 6 : 3 Page 4 of 13 _CITE_ Data to microarray analysis in Alzheimer ’ s disease ( AD ) study as one of typical clinical microarray studies .__label__Supplement|Paper|Extent
This is an Open Access article under the terms of the Creative Commons Attribution License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited . The license is subject to the Beilstein Journal of Nanotechnology terms and conditions : ( http :// www . beilstein - journals . org / bjnano ) The definitive version of this article is the electronic one which can be found at : doi : 10 . 3762 / bjnano . 6 . 189__label__Supplement|License|Other
Session 4 : Private - public partnership for the development of new tools for arbovirus vector control The session aimed at discussing the challenge of insecticide resistance in the context of developing new effective tools for insect vector control from the insecticide manufacturer ’ s perspective . Representatives of the agrochemical sector ( 28 companies were represented ), Innovative vector Control Consortium ( IVCC ) and Insecticide Resistance Action Committee ( IRAC ) attended the workshop to present efficacy data and share their experience of vector control and resistance management . Mr . John Lucas ( Sumitomo Chemical Co ., UK ) provided an overview of the Insecticide Resistance Action Committee ( IRAC ) that was formed in 1984 to provide a coordinated industry approach to counter the development of resistance in pests and mites ( _CITE_ ). The challenge of insecticide resistance in insects that impact public health comes from the limited arsenal of new chemistries . This has been exacerbated by a major decline in the number of companies actively involved in insecticide development .__label__Supplement|Document|Introduce
( Notice that the gene annotations in the GPKB are not considered in such transitive relationship based annotation identification ; they are only used for comparison with the identification results .) Overall , we obtained a recall of 90 . 56 % ( 99 . 09 %, 48 . 65 %, 99 . 03 % and 99 . 97 % recall for the gene to pathway , gene to biological function , gene to transcript and gene Masseroli et al . BMC Genomics 2015 , 16 ( Suppl 6 ): S5 Page 7 of 16 _CITE_ Figure 3 GPKB Web interface : Search result page . The transferred new annotation of the Insulin - like growth factor 2 ( somatomedin A ) ( IGF2 ) human gene to the Insulin - like growth factor binding biological function is shown . ( Notice the external links on all IDs , the “ Show new transitive relationships only “ button and the “ Download “ icon .)__label__Supplement|Paper|Introduce
DOI : https :// doi . org / 10 . 7554 / eLife . 30766 . 002 The following source data and figure supplements are available for figure 1 : Source data 1 . Figure 1 — source data . DOI : _CITE_ Figure supplement 1 . Expression of Dnmts in various metabolic tissues . DOI : https :// doi . org / 10 . 7554 / eLife . 30766 . 003 Figure supplement 1 — source data 1 .__label__Supplement|Paper|Introduce
natal death . 12 Aflatoxin is a metabolite produced by the fungi Aspergillus flavus and Aspergillus parasiticus in maize and nuts , and is a known human liver carcinogen . 13 See Appendix A ( available at : _CITE_ imputation - appendix . pdf ) for further details regarding these two example data sets . The goal was to produce a complete set of incidence estimates for WHO Member States for the year 2005 . We compared the performance of various methods using leave - one - out cross - validation , because no external incidence estimates for the missing data were available .__label__Supplement|Document|Produce
We downloaded the Human Microbiome Project ( HMP ) data from the VAMPS website [ 28 ] at the Marine Biological Laboratory ( https :// vamps . mbl . edu ) under the name HMP_ST_v3v5 . We downloaded the Arumugam et al . [ 1 ] datasets from the authors ’ website ( _CITE_ ads . html ) under the “ individual ” subcategory of “ Genus and phylum abundance tables of the three datasets .” We downloaded the Tyakht et al . [ 12 ] study data from the Russian Metagenome Project website ( http :// www . metagenome . ru / files / rus_met /). The genera represented in the taxonomy tables were filtered to include only genera whose maximum relative abundance across all samples was greater than 0 . 0001 or 0 . 01 %.__label__Material|Data|Use
We downloaded the Human Microbiome Project ( HMP ) data from the VAMPS website [ 28 ] at the Marine Biological Laboratory ( https :// vamps . mbl . edu ) under the name HMP_ST_v3v5 . We downloaded the Arumugam et al . [ 1 ] datasets from the authors ’ website ( _CITE_ ads . html ) under the “ individual ” subcategory of “ Genus and phylum abundance tables of the three datasets .” We downloaded the Tyakht et al . [ 12 ] study data from the Russian Metagenome Project website ( http :// www . metagenome . ru / files / rus_met /). The genera represented in the taxonomy tables were filtered to include only genera whose maximum relative abundance across all samples was greater than 0 . 0001 or 0 . 01 %.__label__Supplement|Website|Use
Shorter and longer exposures are presented in S7 Fig . The table lists the description of all genes analyzed . _CITE_ from HU - block , but not 6 . 5h after release ( Fig 8B , S7 Fig ). No other genes in the cluster were activated to the same extent at either of the two time - points . In the chromosome 32 cluster that was analyzed CYC9 was transcriptionally activated only at 6 . 5h after release , and none of the other genes in the cluster were activated to the same extent at either time - point .__label__Supplement|Document|Introduce
Supplementary information accompanies this paper at _CITE_ Competing Interests : The authors declare no competing interests . Publisher ' s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations .__label__Supplement|Document|Produce
Funding : The authors wish to acknowledge the support of DARPA ( Prophecy Program , Defense Advanced Research Agency ( http :// www . darpa . mil /), Defense Sciences Office ( DSO ), Contract No . HR0011 - 11 - C - 0095 ) and the contributions of all the members of the ALiVE ( Algorithms to Limit Viral Epidemics ) working group . Additional funding came from grants from the Swiss National Science Foundation ( _CITE_ ), and a European Research Council Starting Grant ( ERC ; http :// erc . europa . eu /) to JDJ . ASM was funded by an Early Postdoc Mobility fellowship from the Swiss National Science Foundation . The funders had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript .__label__Supplement|Website|Other
Level 3 gene expression data derived from Aymetrix U133A platform , level 2 copy number data derived from Agilent CGH - 1x1M platform , and level 3 methylation data derived from Illumina HumanMethylation27 platform were chosen and downloaded . The gene expression data are constituted Hsu et al . BMC Genomics 2012 , 13 ( Suppl 6 ): S13 Page 3 of 15 _CITE_ of 12 , 042 normalized log2 values , and each value represents an expression level of a gene . Copy number data contain 962 , 434 normalized log2 ratios among which 358 , 119 ratios with gene annotations were utilized . To match these copy number ratios with gene expression levels , values corresponding to a same gene were averaged .__label__Material|Data|Extent
Level 3 gene expression data derived from Aymetrix U133A platform , level 2 copy number data derived from Agilent CGH - 1x1M platform , and level 3 methylation data derived from Illumina HumanMethylation27 platform were chosen and downloaded . The gene expression data are constituted Hsu et al . BMC Genomics 2012 , 13 ( Suppl 6 ): S13 Page 3 of 15 _CITE_ of 12 , 042 normalized log2 values , and each value represents an expression level of a gene . Copy number data contain 962 , 434 normalized log2 ratios among which 358 , 119 ratios with gene annotations were utilized . To match these copy number ratios with gene expression levels , values corresponding to a same gene were averaged .__label__Supplement|Paper|Extent
_CITE_ Figure 43 , Appendix 11 Opisthostoma kitteli Maassen , 2002 : 176 , figures 35 & 36 ( original description ). Type material . Holotype : RMNH 92942 ( 1 ) ( seen ).__label__Supplement|Document|Introduce
A change in the set of exporters of a product is thus necessary to cause the observed trajectories . The finding of Eq 3 suggests a scheme where most of the products are at their asymptotic market , with the logPRODY reflecting their Complexity . Their moving away from the asymptotic market is unpredictable and accompanied , on average , by a decrease of the competition PLOS ONE | _CITE_ May 17 , 2017 6 / 20 The complex dynamics of products and its asymptotic properties on the market for a given product . Their return to asymptotic market is instead much more regular and evident ( see Fig 1 panel a ) and corresponds to an increase in competition . This mechanism seems to be reliable enough that we can describe the motion on the plane with an equation that connects velocities to competition , and we can confidently say that vertical motion on the plane corresponds to shifts in the market composition .__label__Supplement|Paper|Introduce
For instance , the Cancer Genome Atlas ( TCGA ) [ 6 , 7 ], is the largest resource available for multi - assay cancer genomics data ; the 1000 Genome Project [ 8 , 9 ] aims to provide a comprehensive resource for human genetic variants and gene - expression across populations and ; the International Cancer Genome Consortium ( ICGC ) [ 10 , 11 ] coordinates 55 research projects to characterize the genome , transcriptome and epigenome © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Hernandez - Ferrer et al . BMC Bioinformatics ( 2017 ) 18 : 36 Page 2 of 7 of multiple tumors .__label__Supplement|License|Other
Then for each day of BC data we minimize the effects of rainfall by first calculating the residual : residual = yi — y Where yˆ is the predicted BC and yi is the observed BC for that day . We then subtracted the residual from our original BC data to get rainfall - adjusted BC concentration useful to compare variation between the biomass burning months and non - biomass burning months and general longer - term trends . PLOS ONE | _CITE_ May 8 , 2018 5 / 21 Hanoi , Vietnam air pollution__label__Supplement|Paper|Introduce
After getting all significant probes from SFC , we converted the probe level significance to gene level using an annotation file . Venn diagrams showed the significant genes with differential expression . Pathway and gene ontology ( GO ) enrichment analyses were performed by using the Database for Annotation , Visualization and Integrated Discovery ( DAVID ; _CITE_ ) with the Bonferroni correction - adjusted P - values < 0 . 05 [ 9 ]. Mouse transplantation data have been deposited in NCBI ’ s Gene Expression Omnibus [ 10 ] and are accessible through GEO Series accession no . GSE89340 .__label__Material|Data|Use
Carreira et al . [ 35 ] compared a group provided with a thumb splint for 90 days to a group who only used the splint for evaluation . While the group who used the splint for 90 days showed superior results in terms of pain reduction ( significant large ES : - 1 . 1 ( 95 % CI - 1 . 90 to - 0 . 30 )), non - significant PLOS ONE | _CITE_ March 14 , 2018 11 / 42 Prosthetic and orthotic interventions : A systematic review small effect sizes were calculated for the remaining function and dexterity measures ( grip and pinch strength , upper limb dexterity and DASH ). The first of two studies which examined knee orthoses in individuals with osteoarthritis compared a knee brace group to a control group [ 58 ]. The knee brace group had superior pain reduction ( significant medium ES : - 0 . 75 ( 95 % CI - 1 . 16 to - 0 . 34 )) but no differences in KOOS and patellofemoral bone marrow lesion volume results .__label__Supplement|Paper|Introduce
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ The Creative Commons Public Domain Dedication waiver http :// creativecommons . org / publicdomain / zero / 1 . 0 / applies to the metadata files made available in this article . © The Author ( s ) 2018 SCIENTIFIC DATA 1 5 : 180082 1 DOI : 10 . 1038 / sdata . 2018 . 82 12__label__Supplement|License|Other
Behavioral National Sciences and Engineering Council ( NSERC ) to PPO . Competing interests : The authors have declared that no competing interests exist . PLOS ONE | _CITE_ March 1 , 2018 2 / 21 Social consistency and plasticity across changes in population density variation associated with animal personality and plasticity may be important for individuals to adapt to changes in population density [ 28 ]. For instance , at low local density , selection favoured fast exploring great tits , Parus major , while at high density , selection favoured slow exploring birds , presumably because temporal variation in local density selects for a range of personality types [ 4 ]. Elk , Cervus canadensis , are gregarious ungulates that exhibit sexual segregation outside of the breeding season [ 29 ].__label__Supplement|Paper|Introduce
A larger number thus indicates better performance . For each given threshold , we colored the best method with red and the second best method with blue of the effect size distribution is a key to achieve accurate prediction performance24 , 34 , 36 , we expect our non - parametric model to perform robustly well across a range of polygenic architectures . Our method is implemented in the DPR software , freely available at _CITE_ Simulations . We first compare the performance of DPR with several other commonly used prediction methods using simulations .__label__Method|Tool|Produce
The SUBA4 search portal is a web - browser based GUI ( Graphical User Interface ) written in dynamic HTML using Asynchronous JavaScript + XML ( AJAX ) to interact with the SUBA4 server . The SUBA4 search portal can be used without prior knowledge of SQL and allows the construction of complex queries by selecting parameters in a point and click manner from the tabs in the search menu . The data can be accessed via _CITE_ and is compatible with most current browsers including Windows Explorer , Safari , Firefox and Chrome . The search window contains five query tabs that separate filter options into categories for ease of use . Each tab contains pre - formulated queries in full text with pull down menus and simple text boxes .__label__Material|Data|Produce
Chromatin occupies a major part of the nuclear space and requires a high level of organization . It is now evident that the higher level organization of the nucleus affects gene function ( Cremer et al ., 2006 ; Fraser and Bickmore , 2007 ; Meaburn et al ., 2007 ; de Wit and van Steensel , 2009 ; Nunez et al ., 2009 ). Chromosomes are positioned in preferred locations within the nucleus , so - called chromosome territories ( CTs ; Cremer and Cremer , 2001 ), which seem to correlate This article was published online ahead of print in MBoC in Press ( _CITE_ ) on September 8 , 2010 . Address correspondence to : Angus I . Lamond ( angus @ lifesci . dundee . ac . uk ).__label__Supplement|Paper|Introduce
Because the parameter values are not identical at different locations , estimation via the ordinary least squares ( OLS ) with all observations would likely distort the local © The Author ( s ) 2017 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Leong and Yue Int J Health Geogr ( 2017 ) 16 : 11 Page 2 of 18 distinctness . One possible solution is to include only the locations of data with similar attributes ( i . e ., homogeneity ).__label__Supplement|License|Other
The source code for the model is publicly available at _CITE_ Redistribution and modification of the code is permitted under the terms of the GNU General Public License http :// www . gnu . org / licenses / gpl . html For further information and availability on the electrophysiological data set contact Dr . Jonathan W . Peirce at jonathan . peirce @ nottingham . ac . uk . Acknowledgement This project is supported by Welcome Trust grant number 085444 / Z / 08 / Z .__label__Method|Code|Produce
The genomic targets can be given by their extended genomic sequence or genomic coordinates . ( 2 ) Given a specified nucleotide sequence , CRISTA identifies all potential targets within it ( i . e ., those followed by ‘ NGG ’) and ranks these according to the predicted cleavage score . ( 3 ) Given an sgRNA and a specified genome ( currently 230 genome assemblies PLOS Computational Biology | _CITE_ October 16 , 2017 5 / 24 A machine learning approach for predicting CRISPR - Cas9 cleavage efficiencies are supported encompassing vertebrates , plants , yeast , insects , and deuterostomes [ 49 , 50 ]), CRISTA detects possible off - targets throughout the genome . As opposed to most currently available alternatives , the off - targets detected by CRISTA also include DNA / RNA bulges . A comprehensive detection of off - targets using the pairwise alignment approach described above is computationally demanding .__label__Supplement|Paper|Introduce
Database containing somatic mutations from human cancers separated into expert curated data and genome - wide screen published in scientific literature . Database of genes and mechanisms that contribute to blood , lung , http :// evs . gs . washington . edu / EVS / and heart disorders through NGS data in various populations . Database of 60 , 706 unrelated individuals from disease and population _CITE_ [ 3 ] exome sequencing studies . Machine learning algorithm to score all possible 8 . 6 million substitutions in the human reference genome from 1 to 99 based on known and simulated functional variants . Uses the Likelihood Ratio statistical test to compare a variant to known variants and determine if they are predicted to be benign , deleterious , or unknown .__label__Material|Data|Introduce
In this paper we studied the application of compressionbased distance measures for the problem of sequence comparison , with a special focus on NGS short read data . Their key advantages are assembly - free , alignment - free , and parameter - free . We conducted extensive validation on various types of sequence data : NGS short reads , 16S rRNA sequences , mtDNA sequences , and whole genome Tran and Chen BMC Research Notes 2014 , 7 : 320 Page 12 of 13 _CITE_ sequences . The sequence data was obtained from several mammalian and bacteria genomes at different taxonomy levels , as well as from microbial metagenomic samples . The results show that the compression - based distance measures produced comparably accurate results as the kmer based methods , and both were in good agreement with the alignment - based approach and with existing benchmarks in the literature .__label__Material|Data|Use
In this paper we studied the application of compressionbased distance measures for the problem of sequence comparison , with a special focus on NGS short read data . Their key advantages are assembly - free , alignment - free , and parameter - free . We conducted extensive validation on various types of sequence data : NGS short reads , 16S rRNA sequences , mtDNA sequences , and whole genome Tran and Chen BMC Research Notes 2014 , 7 : 320 Page 12 of 13 _CITE_ sequences . The sequence data was obtained from several mammalian and bacteria genomes at different taxonomy levels , as well as from microbial metagenomic samples . The results show that the compression - based distance measures produced comparably accurate results as the kmer based methods , and both were in good agreement with the alignment - based approach and with existing benchmarks in the literature .__label__Supplement|Paper|Use
The theory guiding the interpretation of SAS data in terms of structure generally assumes an effective point source and a single wavelength . The instrument setup used for a SAS experiment may be an excellent approximation to a point source , or may differ significantly from it and thus require corrections to be made to data or to model scattering profiles for comparison with the experiment . The wavelength resolution ( AX / X ) for SAXS ( whether synchrotron or laboratory - based ) is generally a good approximation to a single wavelength , while for SANS it can be of the order of 10 – 15 % in order to optimize the neutron flux on the sample ( for examples , see _CITE_ ). Beam size and shape also play a key role in data smearing . Modern synchrotron beams and most laboratory - based instruments have sufficiently small beam dimensions ( in the range of tenths of a millimetre to millimetres at the detector ) such that smearing effects can be safely ignored for most applications .__label__Supplement|Document|Produce
location of the partitions with regards to this compromise will be dictated by the data ; it is easier to avoid misassignments where the FRET levels are widely spaced . These methods have been implemented in Matlab ( available online at _CITE_ ).__label__Method|Code|Produce
Table 1 presents the software we have embedded in the workflows released with GPCG . We evaluated the workflows with both real and simulated data ( Supplementary Table S2 ). Evaluation with simulated data We used dwgsim [ 84 ], a utility for whole - genome Illumina reads simulation , contained in DNAA v0 . 1 . 2 ( _CITE_ ), to generate Illumina - like short sequences , using the default empirical error model illustrated on DNAA & apos ; s Whole - Genome Simulation web - site ( http :// dnaa . sf . net ). In total we generated 30 million reads with 100 bp length , using the complete human genome ( hg18 ) as a reference and with default parameters . We developed also a module that allows the users to generate simulation datasets according to their needs ( see Table 2 and Supplementary Materials S2 ).__label__Method|Code|Use
Within the web server ( _CITE_ de ) we provide a video tutorial and sample data ( http :// deeptools . ie - freiburg . mpg . de / library ) to familiarize every user with the common workflows and various modules of deepTools . The functionality of each module is illustrated with detailed examples from real - life NGS analyses and can be seen once a tool is selected .__label__Material|Data|Produce
Within the web server ( _CITE_ de ) we provide a video tutorial and sample data ( http :// deeptools . ie - freiburg . mpg . de / library ) to familiarize every user with the common workflows and various modules of deepTools . The functionality of each module is illustrated with detailed examples from real - life NGS analyses and can be seen once a tool is selected .__label__Supplement|Website|Produce
Within the web server ( _CITE_ de ) we provide a video tutorial and sample data ( http :// deeptools . ie - freiburg . mpg . de / library ) to familiarize every user with the common workflows and various modules of deepTools . The functionality of each module is illustrated with detailed examples from real - life NGS analyses and can be seen once a tool is selected .__label__Supplement|Media|Produce
The Seqcrawler software is freely available for download with installation instructions at _CITE___label__Method|Tool|Introduce
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ © The Author ( s ) 2017__label__Supplement|License|Other
The workflow of KAMO is shown in Fig . 1 . KAMO functionality was implemented in the yamtbx package ( _CITE_ ), which depends on the cctbx library ( Grosse - Kunstleve et al ., 2002 ). The header information of several diffraction - image formats is obtained using a modified version of the XIO module originally included in the xdsme package ( P . Legrand ; https :// github . com / legrandp / xdsme ). Other dependent Python libraries include networkx for graph algorithms ( Hagberg et al ., 2008 ) and SciPy for optimization and clustering ( Jones et al ., 2001 ).__label__Method|Code|Produce
All datasets described in this article , for the 87 countries listed in Tables 1 and 2 , are publicly and freely available through both the WorldPop Dataverse Repository ( Data Citation 1 , Data Citation 2 , Data Citation 3 , Data Citation 4 ) and the WorldPop website ( _CITE_ ). While the datasets available through the WorldPop Dataverse Repository will be preserved in their published form while the ones available through the WorldPop website will be integrated with additional countries ( Middle Eastern countries and Japan ). Furthermore , additional gridded subnational dependency ratio datasets and high resolution gridded 5 - year age / sex group count datasets for all countries located in Latin America and the Caribbean will be soon available through the WorldPop website .__label__Material|Data|Use
All datasets described in this article , for the 87 countries listed in Tables 1 and 2 , are publicly and freely available through both the WorldPop Dataverse Repository ( Data Citation 1 , Data Citation 2 , Data Citation 3 , Data Citation 4 ) and the WorldPop website ( _CITE_ ). While the datasets available through the WorldPop Dataverse Repository will be preserved in their published form while the ones available through the WorldPop website will be integrated with additional countries ( Middle Eastern countries and Japan ). Furthermore , additional gridded subnational dependency ratio datasets and high resolution gridded 5 - year age / sex group count datasets for all countries located in Latin America and the Caribbean will be soon available through the WorldPop website .__label__Supplement|Website|Use
[ 14 ] utilized the propensity score methodology in a nonrandomized study of amiodarone and mortality among acute myocardial infarction patients with atrial fibrillation . Patient characteristics that were associated with prescriptive use of amiodarone were incorporated into a regression analysis through the use of the propensity score methodology . Assessing the difference between proxy - completed and self - completed responses in survey research is analogous to nonrandomized treatment studies such as the two disPage 3 of 12 ( page number not for citation purposes ) Health and Quality of Life Outcomes 2003 , 1 _CITE_ cussed above . Proxy - completed responses can be conceptualized partly as the results of selection bias , and partly the result of true differences between proxy and selfrespondents . Generally , proxy respondents who answer survey items for the respondent occupy a specific role in the respondent ' s life such as a family member ( spouse , child ), friend , or professional caregiver .__label__Supplement|Paper|Introduce
That is , the estimates of y ~ can be calculated considering ~ b to be a known value in Eq 2 . In fact , our proposed model provide better estimates of ~ b than the standard model . However , if examinees are free to choose an arbitrary number of items from a PLOS ONE | _CITE_ February 1 , 2018 18 / 23 A new item response theory model to adjust data allowing examinee choice Fig 6 . Scatter plot using data generated from scenario 2 with a weight value equal to 10 . The differences between estimated and true item difficulties are plotted against the true item difficulties .__label__Supplement|Paper|Use
Checks for data completeness and validity led to the removal of 5 , 103 ineligible records ( 0 . 16 % of all available records ). Of these , 4 , 568 ( 89 . 5 %) were coded as ‘ Dead on Arrival ’, while the remaining 535 records had missing , duplicated , or invalid time measures ( e . g ., discharge time PLOS ONE | _CITE_ March 14 , 2018 4 / 17 Impact of the Four - Hour Rule in Western Australian hospitals__label__Supplement|Paper|Compare
Following overnight hybridisation , the array was washed and then scanned using the BeadArray reader ( IlluminaTM ). Image processing and intensity data files were analysed using BeadStudio software . The probes used for the DASL assay were sourced from a cancer panel which consisted of 502 genes generated using 10 publically available data sets ( _CITE_ ). The selection was based on their frequency of citation in the lists and also their association with cancer . Allelic composition analysis : molecular inversion probe ( MIP ) assay DNA ( 2 . 35 µl ) was mixed with 1 . 1 µl of 53 K probe pool ( 200 amol / µl / probe ) and placed in a 96 - well plate in ice .__label__Material|Data|Use
Schmidt et al . BMC Bioinformatics 2013 , 14 : 302 Page 6 of 18 _CITE_ the anisotropy . While a 2D - SF plot provides the means to examine the orientational components of the anisotropy , it fails to provide any mechanism of confirming the rigidity assumption . A 3D - SF plot has been included within RED CAT to facilitate a complete comparison of order tensors by including the magnitude of individual order parameters along the z - axis .__label__Supplement|Paper|Compare
PubChem ( _CITE_ ) [ 1 – 6 ] is an open archive which contains information on a broad range of chemical entities , including small molecules , lipids , carbohydrates , and ( chemically modified ) amino acid and nucleic acid sequences ( including siRNA and miRNA ). Since it was launched in 2004 as a component of the Molecular Libraries Program ( MLP ) of the U . S . National Institutes of Health ( NIH ), PubChem has been serving as a chemical information resource for scientific__label__Material|Data|Introduce
The number of studies assessing safety of vaccination in pregnancy continues to increase ; however , inter - study variability makes comparisons and pooling of data challenging [ 8 ]. The failure to collect and consistently report critical data and the absence of guidance for data collection were identified at two international conferences , which concluded that data collection and presentation should be harmonized across different studies and settings [ 9 , 10 ]. The Global Alignment of Immunization Safety Assessment in Pregnancy ( GAIA ) project ( _CITE_ ), coordinated by the Brighton Collaboration Foundation ( https :// brightoncollaboration . org ), aims to improve data collection and create a shared understanding of maternal , fetal and neonatal outcomes__label__Method|Tool|Introduce
Open Access This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution ( CC BY 4 . 0 ) license , which permits others to distribute , remix , adapt and build upon this work , for commercial use , provided the original work is properly cited . See : _CITE___label__Supplement|License|Other
In addition , this research was funded in part by CELEST , an NSF Science of Learning Center ( SMA - 0835976 ). The authors thank the Scientific Computing and Visualization group at BU for providing computational resources . We would also like to acknowledge the Cognitive Rhythms Collaborative ( _CITE_ ), the Boston University Center for Computational Neuroscience and Neural Technology ( http :// compnet . bu . edu /), and the Boston University Graduate Program for Neuroscience ( http :// www . bu . edu / neuro / graduate /). The authors would also like to thank Dr . Jason Tourville , who performed the manual ROI parcellation for the ECoG dataset .__label__Supplement|Website|Other
To facilitate taking advantage of other genetic and epigenetic information ( such as genes , SNPs , CpG islands , DNA methylation and histone modifications ) in the relevant biological databases , it provides a directly linked server from the genomic methylation regions out to the bioinformatics secondary databases of UCSC ( 53 ), MethyCancer ( 68 ) and HHMD ( 69 ). Users may rapidly and efficiently obtain the relevant information for the genomic regions with different methylation patterns . The detailed information of identified genomic regions by the four modules of CpG_MPs from the five cell types can be downloaded from _CITE_ CpG_MPs .__label__Supplement|Document|Produce
Ji BMC Bioinformatics 2013 , 14 : 222 Page 9 of 14 _CITE_ P28 , respectively . The complete set of visualization results for all other ages are included in the Additional file 1 . We observe that t - SNE is better at visualizing the high - dimensional gene expression data than PCA .__label__Supplement|Paper|Other
MetNet AtGeneSearch http :// www . metnetdb . org / MetNet_atGeneSearch . htm indicated that among the 2 , 451 differentially expressed genes , several pathways were represented and fell into one of four categories : biosynthesis , respiration and related energetics , signaling transduction and degradation or assimilation . Particularly , represented pathways were : 1 ) biosynthesis of carbohydrates ( sucrose and starch ), lipids ( fatty acid biosynthesis and elongation , linoleate and sterol synthesis ), and amino acids ; 2 ) respiration , specifically , glycolysis and the TCA cycle ; 3 ) regulatory and signaling pathways including the AGRIS regulatory network , jasmonate ( JA ) biosynthesis , IAA / ethylene / gibberellin acid signaling , and regulation of gibberellin metabolism / ethylene signaling ; 4 ) catabolism of sucrose and some amino acids ( e . g ., phenylalanine , glutamate , and valine ) ( Additional file 2 : Table S2c1 ). MetNet ’ s “ Over - representation Search ” _CITE_ identifies over - represented pathways using Fisher ’ s exact test given a user - supplied list of genes . The linoleate biosynthesis , chlorophyllide a biosynthesis , fatty acid ( 3 - oxidation , gibberellic acid biosynthesis , and 4 - aminobutyrate degradation pathways are each over - represented from among all the 2 , 451 differentially expressed genes ( Fisher ’ s exact test p - value & lt ; 0 . 05 ) ( Figure 4A and Additional file 2 : Table S2c2 ). Analysis of the genes from each cluster ( Figure 3 ) shows that the pathways that most genes were involved in also belonged to one of the four groups mentioned above ( Additional file 2 : Table S2d ); these pathways were predominantly related to the biosynthesis of carbohydrates , lipids , and amino acids .__label__Method|Tool|Introduce
In this paper , we describe data acquisition of scores from subtests of the Movement ABC at 7 years . We also include data from models of associations with prenatal blood lead , cadmium and mercury concentrations to support the main analyses in our parallel paper [ 1 ]. The ALSPAC study website contains details of all the data that are available through a fully searchable data dictionary : _CITE_ Data can be obtained by bona fide researchers after application to the ALSPAC Executive Committee ( http :// www . bristol . ac . uk / alspac / researchers / access /).__label__Material|Data|Use
Supplementary data set S1 , figures S1 – S4 , and tables S1 and S2 are available at Genome Biology and Evolution online ( _CITE_ ).__label__Material|Data|Produce
Supplementary data set S1 , figures S1 – S4 , and tables S1 and S2 are available at Genome Biology and Evolution online ( _CITE_ ).__label__Supplement|Website|Produce
Acknowledgments We would like to acknowledge the MRC ( MR / J01107X / 1 ), the National Institute for Health Research ( NIHR ), the EPSRC ( EP / H046410 / 1 ) and the National Institute for Health Research University College London Hospitals Biomedical Research Centre ( NIHR BRC UCLH / UCL High Impact Initiative - BW . mn . BRC10269 ). This work is supported by the EPSRC - funded UCL Centre for Doctoral Training in Medical Imaging ( EP / L016478 / 1 ). Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made .__label__Supplement|License|Other
[ 49 ] studied an experimental system in which apoptosis was chemically induced in dividing cells both in the thymus and the periphery . In otherwise healthy mice , they found that the subsequent approximately 50 % drop in naive T - cell numbers was equivalent to that incurred by thymectomy , but the same treatment in Tx mice had no impact on naive T - cell numbers over a 2 - wk period . These observations are consistent with the consensus that the thymus is the dominant source of naive T - cell production in mice PLOS Biology | _CITE_ April 11 , 2018 15 / 20 Naive T cells acquire homeostatic fitness as they age and that there is little or no increase in peripheral division to compensate for thymectomy [ 52 ]. Thomas - Vaslin et al . found that in healthy mice , naive T - cell numbers returned to normal levels within 10 wk of treatment .__label__Supplement|Paper|Introduce
R commands to obtain normalized data are listed in Additional file 4 . Each method outputs normalized read counts , that were log2 - transformed ( setting genes to NA when having 0 read counts ). The CMS classification was performed using the “ CMSclassifier ” package ( _CITE_ ), using the single - sample prediction parameter . The Oncotype DX ® [ 14 ] recurrence score was performed as described for the RT - qPCR data , and using the RNA - seq normalized values as input for the algorithm . In short , expression data of 7 genes are used ; BGN , FAP , INHBA ( stromal panel ),__label__Method|Code|Use
Veterinarians in both countries were informed about the inclusion criteria , and selected the dogs accordingly . In both countries , pre - selection of dogs was done by trained veterinary technicians or veterinary students , before the veterinarian selected the dogs . To determine the BCS , the BCS chart from Hill & apos ; s Pet Nutrition ( 5 scales silhouette system ; _CITE_ ) was used and the BCS was noted at the beginning of the questionnaire . Scores on this scale determined by different operators have been shown to correlate well , although a degree of expertise is required ; which makes the scoring instrument less suitable for owners [ 2 , 8 ]. The questionnaire contained four sections : one about demographic variables , one about the dog ’ s food and__label__Method|Tool|Use
Using customized scripts allows querying the database for large lists of genes , diseases or variants , and including DisGeNET data in computational workflows . The DisGeNET Cytoscape App is especially suited to carry out network medicine analyses and visualize their results . Accessing the data using Semantic Web technologies enables to combine DisGeNET data with other types of biological information available in the Linked Open Data ( LOD ) cloud ( _CITE_ ). Finally , the disgenet2r R package facilitates exploring , analysing and visualizing the data using the powerful graphical and statistical capabilities of the R environment . The web interface The DisGeNET web interface allows searching by single gene , disease and variant , using different types of identifiers .__label__Supplement|Website|Use
Data access requests are handled by the Norwegian Data Inspectorate and Norwegian Patient Registry . All those who would like access to the individual patient data will need a confidentiality permission by the Ministry of Health . Interested researchers can request data access using the following link : _CITE___label__Material|Data|Produce
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ Metadata associated with this Data Descriptor is available at http :// www . nature . com / sdata / and is released under the CC0 waiver to maximize reuse . © The Author ( s ) 2016 SCIENTIFIC DATA 13 : 160094 1 DOI : 10 . 1038 / sdata . 2016 . 94 11__label__Supplement|License|Other
( And what do they even mean when indels are the main error ?) Characterize context - dependence of errors , e . g . homopolymers , CCXGG context ( _CITE_ ). Can rearrangement errors be characterized ? Long reads are promising for finding rearrangements ( e . g .__label__Material|Data|Introduce
The evolutionary processes underlying the origin and maintenance of species diversity in tropical forests — some of the oldest and most diverse ecosystems on Earth — have long been a source of fascination and debate among biologists [ 1 , 2 ]. Although tropical forests are known to be species rich , precise estimates of their diversity , and variation across space , are difficult to obtain due to limitations of sampling and our ability to accurately circumscribe species . PLOS ONE | _CITE_ June 15 , 2018 1 / 20 Reconciling species diversity in Canarium funders had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript . Competing interests : The authors have declared that no competing interests exist . Refining these estimates is important , however , as it directly impacts a variety of fields that rely on species as units of analysis , including conservation , macroecology , and macroevolution [ 3 , 4 ].__label__Supplement|Paper|Introduce
They used mapping techniques in GIS software ( https :// www . arcgis . com / home ) to identify where they had been most physically active , and to consider how and where they could be more active . Details of the core intervention components can be found in online supplementary appendix tables 1 – 3 . Further details concerning the precise nature of the peer - mentoring and PL approach can be found on the project website ( _CITE_ ).__label__Supplement|Website|Produce
They used mapping techniques in GIS software ( https :// www . arcgis . com / home ) to identify where they had been most physically active , and to consider how and where they could be more active . Details of the core intervention components can be found in online supplementary appendix tables 1 – 3 . Further details concerning the precise nature of the peer - mentoring and PL approach can be found on the project website ( _CITE_ ).__label__Supplement|Document|Produce
To address the data collection and storage challenges we outline above , the BMRP formed a collaboration with FoAM Kernow ( _CITE_ ), a non - profit organisation that develops links between computer technology , science and design . The aim of this collaboration__label__Supplement|Website|Introduce
The data used in Fig 12A are derived from nine lung SQCC cohorts . Eight cohort datasets can be downloaded from GEO ( _CITE_ ) with the accession numbers GSE14814 , GSE19188 , GSE29013 , GSE30219 , GSE3141 , GSE37745 , GSE4573 , and GSE50081 . The ninth cohort is from the TCGA and can be downloaded from the NIH Genomic Data Commons https :// gdc . cancer . gov /. The data used in Fig 12B are derived from three lung SQCC cohorts , which can be downloaded from GEO with the accession numbers GSE4573 , GSE50081 , and GSE29013 .__label__Material|Data|Use
The Paralympic Games are the world ’ s second largest sporting event , and athletes with 10 different eligible physical impairments [ 1 ] participated in 23 summer disciplines in Rio 2016 and will participate in 6 winter disciplines in Pyoengchang 2018 ( https :// www . paralympic . org / sports ). Of these , 16 of the summer sports and 5 of the winter sports disciplines have at least one sitting class . Depending on the eligibility criteria of each sitting sports discipline , athletes with impaired muscle power , impaired passive range of movement , limb deficiency , leg length difference , hypertonia , ataxia and athetosis are allowed to compete ( _CITE_ ). Even though performance in all Paralympic sitting sports disciplines is mainly dependent on the work done by the upper body , the physical demands vary within a spectrum from typical endurance sports requiring high aerobic energy delivery over sustained periods to those performed with relatively low levels of displacement and corresponding low aerobic demands [ 2 ]. As an indicator of the humans ’ maximal ability to deliver energy aerobically , the measurement of maximal oxygen uptake ( VO2max ) is regarded as the “ gold standard ” [ 3 ].__label__Supplement|Document|Extent
To allow full propagation of parametric uncertainty , we used an objective Bayesian approach , taking flat prior distributions in the absence of data and informative priors only when suitable external data were available . We used as many data as were available from these studies , including some which would not be available in other settings using only 1 source of data . Full details on the statistical methods used and the distributions of key parameters can be found in the Web Appendix ( _CITE_ ). Method 1 : paired serologic surveys . To estimate infection rates from paired serologic surveys , we defined overall seroconversion as a 4 - fold or greater rise in titer on hemagglutination inhibition ( HAI ) testing between baseline titers and subsequent samples for the same individual .__label__Supplement|Website|Produce
To allow full propagation of parametric uncertainty , we used an objective Bayesian approach , taking flat prior distributions in the absence of data and informative priors only when suitable external data were available . We used as many data as were available from these studies , including some which would not be available in other settings using only 1 source of data . Full details on the statistical methods used and the distributions of key parameters can be found in the Web Appendix ( _CITE_ ). Method 1 : paired serologic surveys . To estimate infection rates from paired serologic surveys , we defined overall seroconversion as a 4 - fold or greater rise in titer on hemagglutination inhibition ( HAI ) testing between baseline titers and subsequent samples for the same individual .__label__Supplement|Document|Produce
Portal installation and data population instructions are provided via the Symbiota software project website ( http :// symbiota . org ). The Symbiota code base is regularly updated and available through a subversion repository ( SVN ) hosted by Source Forge ( http :// sourceforge . net / projects / symbiota /). Although Sourceforge has been the repository of choice , as the user and contributor community grows the code may be moved to GitHub ( _CITE_ ) for easier access and communication among developers . The Symbiota CMS is written uniformly in the server - side PHP integrated with 10 Gries C et al . client - side JavaScript , with a MySQL back - end database .__label__Method|Code|Use
© 2017 by the authors . Licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ( CC BY ) license ( _CITE_ ).__label__Supplement|License|Other
It has been shown that hybrids of Arabidopsis ecotypes and rice © The Author ( s ). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Seifert et al . BMC Genomics ( 2018 ) 19 : 371 Page 2 of 14 subspecies showed substantial epigenetic variations at the level of DNA methylation , histone modifications and small RNAs ( sRNAs ) [ 15 , 16 ].__label__Supplement|License|Other
A recent study indicated a higher incidence of melioidosis in Malaysia among children with thalassemia that significantly decreased with intravenous iron chelation therapy [ 25 ]. Consistent with these observations during human melioidosis , we established in a murine experimental model that FAC - derived iron supplementation tends to stimulate both systemic and hepatic iron load leading to higher bacterial numbers in organs as well as release of cytokines and MPO . On the other hand , the DFOmediated limitation of iron availability improves outcome of B . pseudomallei - infected mice as shown by reduced bacterial burden and inflammatory response . PLOS Neglected Tropical Diseases | _CITE_ January 12 , 2018 18 / 25 B . pseudomallei benefits from changing host iron balance Thus , it is reasonable to propose that manipulation of host iron metabolism or direct targeting of iron may represent a new therapeutic approach during melioidosis . In this context , Nifedipine , a calcium channel blocker , can induce Fpn expression , thus mobilizing tissue iron and improving host resistance to Salmonella [ 76 ]. Recent evidence further suggest that an inverse agonist of estrogen related receptor gamma ( ERRγ ) is able to ameliorate Salmonellainduced hypoferremia by reduction of ERRγ - mediated hepcidin expression in hepatocytes leading to a better control of infection [ 55 ].__label__Supplement|Paper|Introduce
Reid et al . eLife 2018 ; 7 : e33105 . DOI : _CITE_ 19 of 29 Tools and resources Cell Biology Genomics and Evolutionary Biology__label__Supplement|Paper|Produce
Passatutto has been implemented as a Java v1 . 6 program . It reads and writes spectra in MassBank file format and fragmentation trees in the SIRIUS DOT file format . Source code is available from _CITE_ , Java executables ( JAR files ) are available from https :// bio . informatik . uni - jena . de / passatutto /. Passatutto contains modules for ( a ) generating a decoy database , ( b ) database searching in locally stored data sets , and ( c ) estimating q - values either by means of the target - decoy approach or by empirical Bayes estimation . For generating a decoy database using the fragmentation tree - based method , SIRIUS can be used for the computation of fragmentation trees , which is available from https :// bio . informatik . uni - jena . de / sirius /.__label__Method|Code|Use
All relevant data are available at the Gene Expression Omnibus ( GEO ) under accession numbers GSE2508 , GSE26637 , GSE27949 , GSE48964 , GSE62117 , GSE64567 , GSE33526 , GSE78958 , GSE65540 , GSE66306 , and GSE32575 ( see Supplementary Table 4 for details ). In addition , data for batch 11 was obtained from the Bgee Gene Expression Database35 and can be publicly accessed at _CITE_ Summary statistics from the Twins UK dataset used in Supplementary Figure 2 can be accessed at http :// expression . kcl . ac . uk / phenoexpress / 1 /.__label__Material|Data|Use
A second command line function , afqbrowser - run , launches a static web - server on the user ’ s computer with AFQ - Browser running for this dataset . Running a static web - server is required to access locally stored data files . Navigating a web browser to the returned URL ( defaulting to _CITE_ ) will open the visualization . Even though AFQ - Browser was designed specifically to interact with the AFQ software , there are many other approaches to deriving Tract Profiles of diffusion properties , and we have designed AFQ - Browser to be broadly compatible with other software packages . The afqbrowser - assemble function also accepts the output of a group analysis in TRACULA ( stats folder , see https :// surfer . nmr . mgh . harvard . edu / fswiki / FsTutorial / Tracula for TRACULA documentation ).__label__Supplement|Website|Use
Also , it is not surprising that haplotype frequencies estimated from the multiallelic data set are found to be less accurate than those estimated from SNPs , given the more complex nature of the data . The RCI is a relative measure , and illustrates not so much the accuracy of the algorithm , rather the effect of additional missing data . The results displayed in Tables 2 and 4 show that the algorithm handles the increase in the proportion of unknown alleles equally well for both SNPs and multiallelic data , although it should be pointed out that the RCI measure Page 9 of 13 ( page number not for citation purposes ) BMC Bioinformatics 2004 , 5 : 188 _CITE_ hˆ gives no indication of the accuracy of the point estimates , and should generally be considered in tandem with a measure such as D ( h , ). Interestingly , the results for the hˆ multiallelic data set were achieved despite departure from Hardy - Weinberg equilibrium ( HWE ) at two of the seven loci ( see Methods section ). Although this technique relies on the assumption of HWE , Niu et al .__label__Supplement|Paper|Introduce
Most columns on these summaries are sortable by clicking on arrows found in the headers . Export features have been added to the assay results and genes tabs to allow results to be downloaded in text or spreadsheet format . The genes list can be directly forwarded to the MGI batch query ( _CITE_ ) to search for additional__label__Method|Tool|Use
_CITE_ The resulting continuous datasets were clustered using the standard Euclidean distance , as a reference . As a second reference , completely binarized data ( cutting values at the median for each variable ) were clustered using the simple matching coefficient distance . Subsequently , a certain fraction of variables ( 10 %, 25 %, 50 %, 75 %, 100 %) was categorized in the same way as in the previous simulation studies , thus retaining the same underlying true sample and variable classifications .__label__Material|Data|Introduce
We thank Sonja Hopf from the Evolutionary and Functional Genomics group ( LMU Munich ) for valuable biological feedback . The work presented here was partially funded by the German Federal Ministry of Economy and Technology ( BMWi ) under the THESEUS project . Page 12 of 14 ( page number not for citation purposes ) BMC Bioinformatics 2008 , 9 : 207 _CITE___label__Supplement|Document|Introduce
Ideally , publication of data sets will encourage the communities to adopt standardized formats and ensure complete population of experimental metadata with adequate accuracy to support reprocessing . While the SBDG immediately serves the well - defined area of X - ray crystallography , our pilot project has demonstrated that our infrastructure can preserve additional data types , such as decoy data sets for NMR computations or MicroED data sets . SBDG will duplicate XFEL data sets that are currently accessible through the Coherent X - ray Imaging Data Bank ( _CITE_ ) and support their distribution by DAA . In addition , SBDG will collaborate with MicroED and XFEL collection curators who will moderate development of community driven efforts to automate data analysis pipelines to parallel automatic processing of X - ray diffraction data sets with packages like DIALS or xia2 . We envision that the tools and technologies that arise from this project will ultimately lead to the development of a fully featured , primary data publication system .__label__Material|Data|Use
b mRNA expression levels of protein transport related genes ( Napg , Rab5A and Arpc1b ). c Quantification of secreted IgG production when CHO cells were transfected with dCas9 - VPR / dCas9 and different sgRNAs © The Author ( s ). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . BMC Proceedings 2018 , 12 ( Suppl 1 ): 3 Page 2 of 78 O - 002 Degradation of recombinant proteins of diverse formats by CHO host cell proteases is circumvented via knock - out of CHO matriptase__label__Supplement|License|Other
Supplementary information accompanies this paper at _CITE_ Competing financial interests : The authors declare no competing financial interests . How to cite this article : Chen , Z . et al . Uncovering representations of sleep - associated hippocampal ensemble spike activity .__label__Supplement|Document|Produce
The compaction C can be defined as the number of base pairs per unit length © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Arbona et al . Genome Biology ( 2017 ) 18 : 81 Page 2 of 15 along the fiber ( bp / nm ).__label__Supplement|License|Other
Using network analysis , new information is incorporated in the model . The results are presented using three simulation scenarios . In the first scenario , the assumptions required to apply the standard Rasch model in the PLOS ONE | _CITE_ February 1 , 2018 19 / 23 A new item response theory model to adjust data allowing examinee choice__label__Supplement|Paper|Use
were not filtered in any way by the presence or absence of an actual structure − activity relationship , and so it can be expected that some proportion of the data sets are simply not suited to modeling . We consider it reasonable to operate under the assumption that the data sets extracted from ChEMBL are representative of the kinds of real world drug discovery scenarios for which this method will be used . The collection of structures and activities that were used for this validation exercise can be downloaded from _CITE_ composite - bayes . 32 For each data set , 10 % of the entries were set aside to use as the test set . These entries were selected using the greedy clustering algorithm described earlier , which means that in general the choice of test set is nondiabolical and falls within the same domain as the training set ( although it should be noted that when we repeated the experiment with a random selection of testing sets there was no bias in favor of preclustering ). Data sets for which the partition detection method was not able to detect at least three bins were left out of the results .__label__Supplement|Document|Produce
Other DNA sensor genes in the SLEmetaSig100 signature are key enzymes involved in breakdown of DNA including nucleases such as DNASE1 , DNASE1lL3 , TREX1 , and TREX2 . Importantly , a loss - of - function variant of DNASE1L3 causes a familial form of SLE . Mutations PLOS ONE | _CITE_ July 5 , 2018 11 / 16 Identification of a gene - expression predictor for diagnosis and personalized stratification of lupus patients in TREX1 are associated with familial chilblain lupus and are also associated with the inflammatory disorder Aicardi - Goutieres syndrome . The SLEmetasig100 emphasizes the importance of including DNA processing pathways , which may capture the contributions of proteostasis and ER stress to SLE pathogenesis . Lupus nephritis is a frequently seen complication in patients with SLE and is known to significantly reduce the survival of SLE patients .__label__Supplement|Paper|Introduce
VIPUR is currently available as an independent Python module requiring BLAST +, ROSETTA and PROBE ( all freely available for academic use ). Please see the VIPUR code for usage and analysis details , available at https :// osf . io / bd2h4 . The full predictions for all variants below , including structural models , are also available at _CITE_ Classifying ClinVar annotated single nucleotide variant phenotypes We demonstrate that VIPUR ’ s deleterious predictions are an accurate indication of variant pathogenicity by classifying variants in the ClinVar database ( 29 ). ClinVar is a collection of human variants with annotated phenotypic effects , including variants with causative ‘ pathogenic ’ effects and ‘ benign ’ variants with no known disease effect .__label__Method|Algorithm|Produce
VIPUR is currently available as an independent Python module requiring BLAST +, ROSETTA and PROBE ( all freely available for academic use ). Please see the VIPUR code for usage and analysis details , available at https :// osf . io / bd2h4 . The full predictions for all variants below , including structural models , are also available at _CITE_ Classifying ClinVar annotated single nucleotide variant phenotypes We demonstrate that VIPUR ’ s deleterious predictions are an accurate indication of variant pathogenicity by classifying variants in the ClinVar database ( 29 ). ClinVar is a collection of human variants with annotated phenotypic effects , including variants with causative ‘ pathogenic ’ effects and ‘ benign ’ variants with no known disease effect .__label__Supplement|Document|Produce
; licensee BioMed Central Ltd . This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http :// creativecommons . org / licenses / by / 2 . 0 ), which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Hutchins et al . Cell Regeneration 2014 , 3 : 1 Page 2 of 6 http :// www . cellregenerationjournal . com / content / 3 / 1 / 1 the major analytical frameworks for genomic analysis .__label__Supplement|License|Other
In contrast , we found no enrichment for genes with stage - specific mRNA expression in the glia DS - DM gene set . To follow up this finding we asked whether the genes with DS - DM in DS versus normal brain cells might be enriched in examples with brain - specific expression or brain - specific repression . Expression data across various human tissues were downloaded from the Allen Brain Atlas database ( _CITE_ ) and bioGPS . When we classified these genes based on their expression across the multiple tissues we found a modest but significant enrichment in genes with brain - specific expression in neurons ( 36 % of the DS - DM genes versus 28 % of the BeadChip gene content , p = 3 . 7 × 10 − 5 ; Figure S14 in Additional file 1 ). No such enrichment was observed in a parallel analysis of the glia DS - DM gene set .__label__Material|Data|Use
PLoS One 2011 ; 6 : e18398 . This work is licensed under a Creative Commons AttributionNonCommercial - ShareAlike 3 . 0 Unported License . To view a copy of this license , visit _CITE_ Supplementary Information accompanies the paper on the The Pharmacogenomics Journal website ( http :// www . nature . com / tpj ) The Pharmacogenomics Journal ( 2014 ), 208 – 216 © 2014 Macmillan Publishers Limited__label__Supplement|License|Other
Three assumptions or alternative input data are the most influential on the relative and absolute outcome of the health impact assessment : the sources of relative risks used in the blood pressure to health association , the dose - response between salt intake and blood pressure and the distribution of risk factors . Because of the importance of the effect of the relative risk on the outcome , it is important that the source is obtained from good quality , prospective studies . This study showed that using PLOS ONE | _CITE_ November28 , 2017 10 / 14 Identification of differences in health impact modelling of salt reduction categorical risk factor distributions seemed to reduce the sensitivity of the model to changes in salt intake . This is probably due to the fact that this modelling approach lowers blood pressure in all subjects lowering salt intake will decrease blood pressure in all subjects , but only a few subjects will shift to a lower blood pressure category and thereby will have a lower risk of developing CVD . In general , uncertainty analyses show how the health impact estimate depends on the underlying assumptions and ( demographic ) input data within a single HIA model and is therefore helpful to identify the range of the expected effect .__label__Supplement|Paper|Use
The mzML2ISA & nmrML2ISA packages have parsed and created ISA - Tab structures for all MetaboLights studies that have associated mzML , imzML , nmrML files . The resulting ISA - Tab structures have then been successfully validated using the ISAvalidator ( _CITE_ ) tool . See Supplementary Table S2 for details of the 21 studies tested . Using sample sets of 50 XML files ( either mzML , imzML or nmrML ) derived from MetaboLights , XML to ISA - Tab conversion is completed in less than 45 seconds .__label__Method|Tool|Use
Calculation of small RNA expression fold - changes upon Dicer silencing The C / D box , H / ACA box and Cajal - body - specific snoRNA sequences were obtained from snoRNABase at http :// www - snorna . biotoul . fr . The tRNA sequences were obtained from tRNAdb at http :// trnadb . bioinf . uni - leipzig . de / ( 19 ). The four consensus rRNA sequences ( 5S , 5 . 8S , 18S and 28S ) were obtained from NCBI at _CITE_ The initial set of 26241 control sequences consisted of all potential precursors excised by miRDeep2 but discarded before being assigned a miRDeep2 score . The set of 940 known human miRNA precursors were downloaded from miRBase version 16 at http :// www . mirbase . org / ( 20 ).__label__Supplement|Website|Use
doi : 10 . 1371 / journal . pone . 0128955 . g001 alignment using the web tool BLASTN ( _CITE_ ) to ensure that the P1and P2 regions were mapped onto human and pathogen sequences . MiRNA - seq data analysis pipeline for Data set 1 . MiRNA - seq is a type of RNA - seq , which uses next - generation sequencing technology to sequence MicroRNAs .__label__Method|Tool|Use
In particular , solving the moment equations by matrix exponentiation results in computation times independent of the duration of the simulated experiments . In contrast , the likelihood - based method requires solving a very large system of differential equation forward through time , and so the later the observation time , the longer it takes to compute the parameter estimate . Although this could PLOS Computational Biology | _CITE_ November20 , 2017 15 / 27 Efficient moments - based inference for within - host bacterial infection dynamics Fig 12 . Pairwise comparison of the mean absolute relative error ( MARE ) achieved by MLE ( x axis ) or MDE ( y axis ). Each dot shows the MARE values for one simulated dataset , grouped by parameter values ( colour scale ), observation time and sample size ( panels ).__label__Supplement|Paper|Compare
Each dataset is expressed by a matrix X = { xi , j }, where i = 1 , ..., T and j = 1 , ..., F are for time and frequency , respectively . The entries , xi , j , are intensities corresponding to the level of vocalization . PLOS ONE | _CITE_ May 9 , 2018 3 / 26 Functional clustering of mouse USV data__label__Supplement|Paper|Introduce
Later time points have fewer EBNA - LP mutant samples as some knockouts did not survive . Complete data are in S3 Data . _CITE_ EBER genes , which are not EBNA2 dependent ( Fig 7G and 7H ). Overall , we found that mutation of EBNA - LP led to a delayed transcription of all viral latency genes ( other than EBNA2 ), regardless of whether they were EBNA2 - dependent , but did not reduce the induction of host genes by EBNA2 . EBNA - LP facilitates EBNA2 recruitment to viral but not host promoters EBNA - LP has reportedly been detected by chromatin immunoprecipitation ( ChIP ) at various genomic loci , often in the presence of EBNA2 [ 42 ].__label__Material|Data|Use
Ozer and Sezerman BMC Genomics 2015 , 16 ( Suppl 12 ): S7 Page 7 of 19 _CITE___label__Supplement|Paper|Introduce
Accordingly , there is a wide range of prices for EEG devices , from brain — computer interface systems designed for a specific task to medical - grade devices with hundreds of high quality electrodes . These measurement devices are all based on the same principle , neurons communicate through chemical neurotransmitters and electrical impulses , giving rise to electromagnetic waves . Electrodes are then used in EEG to measure oscillatory signals related to action PLOS ONE | _CITE_ May 24 , 2018 1 / 21 Collective signal improvement in an EEG headset Competing interests : The authors have declared that no competing interests exist . potential across different regions of the brain . In EEG devices , it is generally believed that most of the measured signal is provided by pyramidal neurons of the cortex [ 6 , 7 ].__label__Supplement|Paper|Use
Merrill et al . Journal of Biomedical Semantics 2014 , 5 ( Suppl 1 ): S3 Page 6 of 12 _CITE_ public data , and whose SPARQL endpoint is publicly available ; the other which contains the entire data and is kept secure using an API key . The secure , administrative endpoint is used by R scripts ( described in the next section ) to access data for query and analysis by members who have access authorization . The other benefit of having decoupled stores is that we have the flexibility of optimizing the performance and scalability of each store independently from the other .__label__Supplement|Paper|Introduce
The post - processed proteomics data were also made freely available for queries through the PIQMIe server ( http :// piqmie . biotools . nl / results /< dataset >, where dataset refers to one of the six EMF exposures : ELF_human , ELF_mouse , UMTS_human , UMTS_mouse , WIFI_human or WIFI_mouse ). The TIFF images taken from the immunoblots and the intensities of the observed protein bands in these blots are provided in supplementary materials ( S1 File ). The source codes of the bioinformatics tools ( in R and Python languages ) developed to analyze semi - quantitative proteomics data , such as those from in vitro exposures to EMFs , can be found on GitHub and / or CERN ’ s Zenodo platform ( PIQMIe version 1 . 0 , _CITE_ EMF - DM version 1 . 0 . 1 , http :// dx . doi . org / 10 . 5281 / zenodo . 166705 ).__label__Method|Code|Use
Moreover , MultiDataSet can also perform other complex subsetting operations , such as selecting subjects with specific phenotypes or selecting features belonging to a gene , which can be of great help in candidate gene studies . In addition , MultiDataSet easily recovers the original dataset for the use with native Bioconductor functions . A Bioconductor group interested on combinable experiments has started a repository ( _CITE_ ). This group aims to use R / Bioconductor to define interfaces allowing efficient selection and combination of high - dimensional assays . However , it seems that the project focuses on TCGA .__label__Material|Data|Introduce
Supplementary Information accompanies the paper on the npj Microgravity ( _CITE_ ) Published in cooperation with the Biodesign Institute at Arizona State University , with the support of NASA npj Microgravity ( 2016 ) 16035__label__Supplement|Document|Produce
in vivo binding , in vitro binding , indirect binding and sequence analysis ). This collection of interactions is used to compare with the TF - gene binding data from Chip - chip experiments . The result yields 1017 TRIs between 87 TF genes and 400 target genes and can be downloaded at _CITE_ network . Among our four TRI yeast databases , we believe that the first two ( Chip - chip and Chip - chip / Sequence motif ) are of generally better quality . We also note that these first two databases ( in contrast to the other two ) cover almost the whole genome .__label__Supplement|Document|Produce
‘ Hoo ’ is the most similar timepoint in development in the Hoo et al . ( 2016 ) dataset . DOI : _CITE_ The following figure supplements are available for figure 4 :__label__Supplement|Paper|Produce
In addition to calculating the rates of trait evolution as the MLE of σ2 in BM models , we calculated two other indices of evolutionary rates : ( 1 ) “ felsens ,” a measurement of divergence between sister taxa scaled by their divergence time [ 74 ], and ( 2 ) BM fits to untransformed data , incorporating ME as the standard error of each individual trait measurement where available ( unknown values were then estimated directly as an additional parameter ), in geiger [ 62 ]. We then fit ANOVAs to the ln - transformed felsen value and the ln - transformed estimate of the BM rate parameter ( σ2 ), as above . PLOS Biology | _CITE_ January31 , 2018 13 / 23 Competition and evolution in a songbird radiation We further examined the tempo of trait evolution by analyzing how disparity accumulates through time for each trait [ 75 ]. Using the dtt function in geiger [ 62 ] with the average squared Euclidean distance among z - transformed trait values , we simulated 1 , 000 BM datasets to create a null distribution for the MCC tree and each of 100 posterior trees . We then calculated the MDI as the area between the null and observed disparity - through - time curves for each phylogeny , such that large MDI values indicate rapid and sustained accumulation of within - clade disparity greater than that expected under BM .__label__Supplement|Paper|Extent
Two additional data sets consisting of three samples each that were generated from CD11b + MACS - isolated peritoneal myeloid cells were downloaded from _CITE_ arrayexpress [ 25 ]— E - MEXP - 3623 ; [ 26 ]— E - GEOD - 25585 ) and used as peripheral controls . Two independent approaches were used for bioinformatic data analysis . For the first method the Affymetrix Expression Console Software Version 1 . 0 was used to create summarized expression values ( CHP - files ) from the expression array feature intensities ( CEL - files ).__label__Material|Data|Use
While no distinct policy was created to establish medicine prices in the RPI , the management applied minimal markups sufficient to cover their estimated operating costs . Retail mark - ups initially averaged approximately 30 - 50 % for most medicines . Surprisingly , as the rural pharmacy initiative emerged , the private pharmacies in the district center appeared to be changing their prices on key mediPage 2 of 15 ( page number not for citation purposes ) International Journal for Equity in Health 2009 , 8 : 43 _CITE___label__Supplement|Paper|Introduce
German Federal Ministry of Health was not involved in data collection , analysis and writing of the manuscript . Availability of data and materials The data sets generated and analysed during the current study are available in the ZENODO . ( _CITE_ ).__label__Material|Data|Produce
German Federal Ministry of Health was not involved in data collection , analysis and writing of the manuscript . Availability of data and materials The data sets generated and analysed during the current study are available in the ZENODO . ( _CITE_ ).__label__Supplement|Document|Produce
German Federal Ministry of Health was not involved in data collection , analysis and writing of the manuscript . Availability of data and materials The data sets generated and analysed during the current study are available in the ZENODO . ( _CITE_ ).__label__Supplement|Paper|Produce
The pathway gene set database ( _CITE_ , file Human_GOBP_AllPathways_no_GO_iea_April_01_2017_symbol . gmt ) 13 from the Bader lab dated April 1 , 2017 was used in all analyses . This database contains pathways from Reactome61 , NCI Pathway Interaction Database62 , GO ( Gene Ontology ) biological process63 , HumanCyc64 , MSigdb65 , NetPath66 and Panther67 . For GO , terms inferred from electronic annotation were excluded from our analyses .__label__Material|Data|Use
The addition of a 12 - hour component ( middle ) to the model ( right ) yields a better fit for which underlying assumptions are validated . © Halberg Chronobiology Center . Cornelissen Theoretical Biology and Medical Modelling 2014 , 11 : 16 Page 11 of 24 _CITE_ the environment . Counterparts in biology have been found , as discussed elsewhere [ 3 , 53 - 56 ]. There are several approaches available to analyze non - stationary time series , such as wavelets , short - term Fourier transforms , and gliding spectral windows complemented by chronobiologic serial sections , as discussed below .__label__Supplement|Paper|Produce
To facilitate the use of available data for scientists , several web - based or community resources have been established specifically for cyanobacteria . They allow users to custom analyze data repositories as part of their experimental design protocol , and to compare their data with other research results ( Table 2 ). The central and most widely - used resource in the field is CyanoBase ( _CITE_ Nakao et al ., 2010 ). Started in 1995 , this database includes currently sequenced and annotated genomes for 39 species of cyanobacteria . Although it contains only a very limited number of analysis tools ( i . e ., Blast2 for genes and genomes similarities searches , and KazusaMart to quickly convert between IDs in different formats , powered by the free software BioMart ), it allows the user to explore genomes , as well as to obtain gene annotations .__label__Material|Data|Introduce
Journal of Human Kinetics - volume 55 / 2017 _CITE_ by Isao Hayashi et al . 43 The feedback signal is then used to govern learning :__label__Supplement|Paper|Introduce
Sedimentation equilibrium for SPOP mutBTB at a rotor temperature of 20 ° C was attained at increasing rotor speeds of 5 , 900 g ( 9 , 000 rpm ) ( for 42 h ), 12 , 300 g ( 13 , 000 rpm ) ( for 32 h ), and 35 , 200 g ( 22 , 000 rpm ) ( for 20 h ) ( Zhao et al , 2013a ). Loading protein concentrations were between 1 . 60 and 16 . 57 lM ( 130 ll ), and absorbance distributions were recorded at 280 nm in 0 . 001 cm radial intervals with 20 replicates for each point . Global least squares modeling was performed at multiple rotor speeds with the software SEDPHAT ( _CITE_ default . aspx ) using the reversible monomer – dimer self - association model ( Zhao et al , 2013a ).__label__Method|Tool|Use
For the biospecimen - linked EHR to be maximally useful for research , investigators must be able to access , query , download , and analyze it while following the regulations set out by the Institutional Review Board . Data issues such as quality , timeliness , storage , acquisition , distribution , security , and interpretation must be addressed in the implementation . The Partners Biobank Portal is an open - source application based on the i2b2 infrastructure ( _CITE_ ) [ 1 ]. It was created to enable Partners researchers to query and download data about Biobank subjects and make requests for samples and genomic data , while addressing these issues . The Biobank Portal effectively links dispersed information about Biobank subjects , including :__label__Method|Tool|Extent
2014 , 32 , 903 – 914 . © 2014 by the authors ; licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license ( _CITE_ ).__label__Supplement|License|Other
The results shown in this manuscript are based upon the data generated by the TCGA Research Network : _CITE___label__Supplement|Website|Use
Different approaches have been proposed to evaluate the performance of automatic alignment systems and the output they produce . These include the manual analysis of correspondences in the alignment [ 7 ], comparing the alignment against a reference alignment [ 4 , 5 ], measuring the extent to which the alignment preserves the structural properties of the input ontologies [ 18 ], checking the coherence of the alignment with respect to the input ontologies [ 19 ], and evaluating the alignment within an application ( end - to - end evaluation ) [ 20 ], while the comparison against ( preferably manually created ) reference alignments is by far the most common evaluation approach that has been used in international evaluation campaigns for many years now [ 21 ]. However , although the manual creation of ontology alignments is known to be time consuming and expensive , and in the same time inherently error - prone ( Euzenat even states that “ humans are not usually very good at Beisswanger and Hahn Journal of Biomedical Semantics 2012 , 3 ( Suppl 1 ): S4 Page 13 of 14 _CITE_ matching ontologies manually ”, p . 202 in [ 1 ]), not much work has been published on quality assurance of existing manual alignments , yet . This particularly concerns technical aspects , which strongly affect the reusability of alignments ( target of our Checks 1 to 5 ). Regarding validity aspects ( target of our Checks 6 to 10 ), some of the evaluation approaches proposed for the analysis of automatically created alignments could be adopted , such as the structural [ 18 ] or alignment coherence analysis [ 19 ].__label__Supplement|Paper|Introduce
As a member of the worldwide Protein Data Bank ( wwPDB , _CITE_ ) ( 1 ), the Protein Data Bank Japan ( PDBj , http :// pdbj . org /) ( 2 ) accepts and processes PDB entries deposited mainly from Asia and Oceania regions and maintains a centralized archive of macromolecular structures , in collaboration with other wwPDB members , RCSB - PDB ( 3 ), BMRB ( 4 ) in the USA and PDBe ( 5 ) in Europe . The PDBj provides its own data viewer , integrated tools and derived databases in order to facilitate structural biology and bioinformatics research from different perspectives than other members of the wwPDB . PDBj has been engaged in validation of the PDB data description by developing a canonical XML description ( PDBML ), primarily in collaboration with RCSB - PDB ( 6 ).__label__Supplement|Website|Introduce
Measurement of lipid content . The sulfo - phospho - vanillin ( SPV ) method was used to determine the lipid content of C . sorokiniana UTEX 1230 as reported previously [ 25 , 26 ]. The PLOS ONE | _CITE_ July 3 , 2018 3 / 19 Impacts of monosaccharides to the growth of Chlorella sorokiniana OD530 of the culture was measured with a spectrophotometer ( 7200 Unico ). For construction of a standard curve , a stock solution was prepared by dissolving 100 mg triolein in 100 mL chloroform . After 0 , 10 , 20 , 30 , 40 , 50 , 60 , 70 and 80 µL aliquots of stock solution were blown dry in a fume hood , 100 µL of distilled water and 1 mL of sulfuric acid were added to the residues and mixed by pipetting .__label__Supplement|Paper|Introduce
In the “ Atlas of Abyssal Megafauna Morphotypes of the Clarion - Clipperton Fracture Zone ” created for the ISA ( _CITE_ ), this morphospecies is listed as " Hyocrinidae sp . 1 ".__label__Material|Data|Introduce
For instance , studies of the somatic mutation landscape of a wide range of different cancer types have unraveled a somatic mutational signature that is associated with smoking exposure [ 4 , 10 ]. Other studies comparing gene expression levels in the normal lung tissue adjacent to cancer in smokers vs non - smokers have © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Chen et al .__label__Supplement|License|Other
The Detecting Breakpoints and Estimating Segments in Trend ( DBEST ) method similarly segments time series to measure timing , type and magnitude of changes [ 30 ], but without considerations for seasonal variations as in BFAST . The Landsat - based detection of Trends in Disturbance and Recovery ( LandTrendR ) method segments annual or composited Landsat time series using a series of parameters describing segment length , inter - segment angle and other characteristics of time series trajectories [ 27 ]. While these and other time series segmentation algorithms have been proven to be useful in describing changes or other state variables using satellite time series , they have all been developed for regularly timed observations such as MODIS 16 - day composites [ 29 ], with Nature and Biodiversity Conservation Union ( NABU ), _CITE_ , http :// www . nabu . de . This work was also supported by German Federal Ministry for the Environment , Nature Conservation and Nuclear Safety ( BMU ) International Climate Initiative ( IKI ) through the project : & quot ; From Climate Research to Action under Multilevel Governance : Building Knowledge and Capacity at Landscape Scale & quot ;, in partnership with the Center for International Forestry Research ( CIFOR ), http :// www . international - climateinitiative . com / en /, http :// www . cifor . org . This work was also supported by Google Earth Engine Research Award : & quot ; Interactive forest monitoring using satellite data and community - observations in the Google platform & quot ;, https :// research . google . com / university / relations / ee_awards . html .__label__Supplement|Website|Introduce
For filtering the read dataset from known non - miRNA sRNA sequences , we used the Rfam ( http :// rfam . sanger . ac . uk /) database . We then used miRCat ( Stocks et al ., 2012 ) to predict mature miRNAs and their precursors from a sRNA dataset and a flax genome ( Wang et al ., 2012 ). Among the sRNA sequences that could not be annotated , novel potential miRNAs were identified using the mfold webserver ( _CITE_ ) that folds flanking sequences and predicts secondary structures of single stranded nucleic acids ( Zuker , 2003 ). Target predictions were performed using psRNATarget ( http :// plantgrn . noble . org / psRNATarget /; Moxon et al ., 2008 ; Dai and Zhao , 2011 ) with default parameters .__label__Supplement|Website|Use
Stimuli were presented to the participant via a coil attached mirror reflecting a projector situated at the bore of the magnet . Experimental tasks were presented using E - Prime software version 2 . 0 ( Psychology Software Tools , Inc ., Pittsburgh , PA ). Eye tracking was performed using an Avotec system ( _CITE_ ) and Viewpoint software ( http :// www . arringtonresearch . com /). Eye position was also monitored by the experimenter during the course of the session for saccades and vigilance , largely to confirm that participants did not use gaze location to assist spatial memory . Since experimenter monitoring confirmed that participants maintained central fixation and vigilance , eye movement data were not further analyzed .__label__Method|Tool|Use
We also determined Bonnet et al . eLife 2018 ; 7 : e32937 . DOI : _CITE_ 41 of 49 Research article Developmental Biology numerically the confidence regions for the distributions of fates that can yield fN f 2 . 5 %, fN f 5 % and f � f 10 %. In the end , we also report the distribution of fates that was actually measured in each condition , and check in which confidence interval it is ( Ventral zone : Appendix 4 — figure 2 , Appendix 4 — figure 3 , Appendix 4 — figure 4 , Dorsal zone : Appendix 4 — figure 5 , Appendix 4 — figure 6 , Appendix 4 — figure 7 ). Appendix 4 — figure 2 .__label__Supplement|Document|Introduce
These six images were downloaded from USGS ( http :// earthexplorer . usgs . gov /). Both Standard Terrain Correction ( Level 1T ) OLI data and Provisional Landsat 8 Surface reflectance product images ( LaSRC , version 2 . 2 ) were downloaded for further testing . The ground control points used for Level 1T correction are derived from the GLS2000 data set ( _CITE_ ). The bands analyzed to determine which OLI scene to use were OLI bands 2 – 7 and NDVI which had a pixel size of 30 m x 30 m . In the final modelling three OLI band ratios ( Band5 / Band4 , Band6 / Band4 , Band7 / Band4 ) were added and therefore the statistics obtained from this initial screening of suitable image acquisitions might differ slightly from those obtained for the final models . Based on studies of the images , including analysis of correlations , scatter plots and best subset regressions with the data to be modeled as dependent variables , one of the Landsat images was selected for further modelling of SOC and another image for modeling of AGB and BGB .__label__Material|Data|Use
The images contain thousands of spots , one spot for every cluster , with a cluster representing one read . Each of these files must be analyzed to designate one of McCormick et al . Silence 2011 , 2 : 2 Page 7 of 19 _CITE_ the four nucleotide bases ( Illumina ) or color space call ( SOLiD ) for each spot on the image , and then the data from each image for the same spot must be combined to give full sequence reads , one per spot . Each technology has its own specifications regarding the file formats used ; for example , Illumina recently changed its standard output format from . qseq , which uses ASCII - 64 encoding of Phred quality scores ( a widely accepted metric to characterize the quality of DNA sequences ), to . bcl , a binary format containing base call and quality for each tile in each cycle . SOLiD systems use . csfasta to encode color space calls and . qual files to record the quality values for each sequence call .__label__Supplement|Paper|Introduce
These data sets was chosen for a variety of reasons , including 1 ) the relevance of the experimental perturbation to modulating the types of cell proliferation that can occur in cells of the normal lung , 2 ) the availability of raw gene expression data , 3 ) the statistical soundness of the underlying experimental design , and 4 ) the availability of appropriate cell proliferation endpoint data associated with each transcriptomic data set . In addition , the perturbations used to modulate cell proliferation in these experiments covered mechanistically distinct areas of the Cell Proliferation Network , ensuring that robust coverage of distinct mechanistic pathways controlling lung cell proliferation were reflected in the network . Data for GSE11011 and GSE5913 were downloaded from Gene Expression Omnibus ( GEO ) _CITE_ , while data for E - MEXP - 861 was downloaded from ArrayExpress http :// www . ebi . ac . uk / microarray - as / ae /. The data from PMID15186480 was obtained from a link within the online version of the paper http :// jbiol . com / content / 3 / 3 / 11 . Raw RNA expression data for each data set were analyzed using the “ affy ” and “ limma ” packages of the Bioconductor suite of microarray analysis tools available for the R statistical__label__Material|Data|Use
The ideas presented in this paper closely reflect the intentions of the ENVIROFI FP7 project which is due to start in April 2011 . The Authors acknowledge the use of background literature , personal know - how and the visions generated by high profile research projects , funded by the Sensors 2011 , 11 3902 European Commission over the years . These include the FP6 flagship projects ORCHESTRA ( FP6 Contract number 511678 ; _CITE_ ) and SANY ( FP6 Contract number 033564 ; http :// sany - ip . eu /), as well as the following on - going FP7 projects :__label__Method|Tool|Use
They include samples from individuals X310763260 , X311245214 , X316192082 , X316701492 , and X317690558 [ 28 ], designated within this article as HF1 – 5 , respectively . Only samples labeled as “ Whole ” ( samples preserved by flash - freezing ) were selected for analysis [ 28 ]. The published wastewater sludge microbial community datasets ( MG and MT ) were obtained from NCBI Bioproject with the accession code PRJNA230567 ( _CITE_ ). These include samples A02 , D32 , D36 , and D49 , designated within this article as WW1 – 4 , respectively [ 43 ]. The published biogas reactor microbial community data set ( MG and MT ) was obtained from the European Nucleotide Archive ( ENA ) project PRJEB8813 ( http :// www . ebi . ac . uk / ena / data / view / PRJEB8813 ) and is designated within this article as BG [ 29 ].__label__Material|Data|Use
2015 ) and epiproteomes ( Waldrip et al . 2014 ) at specific loci ; but they have drawbacks , for example , because every zinc - finger domain must be custom evolved to target a specific sequence , and target motifs are size limited . One recent innovation in the field of target specific DNA methyla tion is the development of a suite of tools , based on the Piwi - interacting RNA ( piRNA ) system , to accurately induce DNA methylation of targeted loci in adult tissues ( work presently being done under NIH grant ES026877 ; _CITE_ ). The major strength in the piRNA approach is that induced changes in DNA methylation will be propagated by endogenous epigenetic maintenance pathways . Thus , piRNA treatment for both laboratory and clinical use will be acute and systemic , rather than chronic with potentially decreasing effectiveness .__label__Method|Tool|Use
Funding : Financial support for this study was received from Rentenbank ( _CITE_ ) and Monsanto ( http :// www . monsanto . com / global / de /). Additionally , the publication of this article was funded by the Open Access Fund of the Leibniz Association . The funders had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript .__label__Supplement|Website|Other
Another improvement will come with the use of absent phenotypes . For example , the lung phenotype of UDP_2700 mapped well to Fraser syndrome but the absence of syndactyly and severe neurological symptoms was not considered , and thus Fraser syndrome ranked inappropriately high . These improvements are currently being implemented in the OWLsim algorithm ( _CITE_ ) 25 and will be incorporated into the next version of Exomiser . Some patients likely have genetic disorders unsolvable by exome sequencing and Exomiser alone . Besides the possibility that the initial assumption of a germline genetic basis for the disease might be invalid , exome data only cover 2 % of the genome and are insensitive to certain types of mutations , including copy number variations and trinucleotide repeats .__label__Method|Algorithm|Introduce
provided expert advice . Additional information Supplementary Information accompanies this paper at http :// www . nature . com / naturecommunications Competing interests : The authors declare no competing financial interests . Reprints and permission information is available online at _CITE_ How to cite this article : Sinha , R . et al . Analysis of renal cancer cell lines from two major resources enables genomics - guided cell line selection . Nat .__label__Supplement|License|Other
© 2017 by the authors . Licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ( CC BY ) license ( _CITE_ ).__label__Supplement|License|Other
We conducted new analyses to inform future estimates . We found that acute watery diarrhoea was associated with 87 % ( 95 % CI 83 – 90 %) of U5 diarrhoea hospitalisations based on data from 84 hospital sites in 9 countries , and 65 % ( 95 % CI 57 – 74 %) of U5 diarrhoea deaths based on verbal autopsy reports from 9 country sites . We reanalysed data from the Global Enteric Multicenter Study ( GEMS ) and found 44 % ( 55 % in Asia , and 32 % in Africa ) rotavirus - positivity among U5 acute watery diarrhoea hospitalisations , and 28 % rotavirus - positivity among U5 acute PLOS ONE | _CITE_ September 11 , 2017 1 / 18 Estimating rotavirus deaths in children aged < 5 years watery diarrhoea deaths . 97 % ( 95 % CI 95 – 98 %) of the U5 diarrhoea hospitalisations that tested positive for rotavirus were entirely attributable to rotavirus . For all clinical syndromes combined the rotavirus attributable fraction was 34 % ( 95 % CI 31 – 36 %).__label__Supplement|Paper|Introduce
[ 20 ], profiling of N - glycosylated proteins in the serum of advanced breast cancer patients was performed in order to discover serum biomarkers for chemoresistance . Twenty three proteins were identified to be differentially expressed between patients defined as sensitive and patients defined as resistant to docetaxel and doxorubicin treatment . The expression pattern of several proteins was Abelson BMC Bioinformatics 2014 , 15 : 53 Page 7 of 10 _CITE_ later validated in independent samples . Interestingly , in our analysis we found transcripts encodes to 10 out of the 23 aforementioned proteins ( Additional file 1 ). Even though patients bearing different type of cancer and under different treatment regimen were investigated , it is very unlikely that these results are consequence of chance ( p = 9 . 29e - 20 by hypergeometric probability density test ); This is further supported by other studies , which demonstrate elevated expressions of different serum proteins found in our analysis and in patients with chemoresistant tumors [ 21 , 22 ].__label__Supplement|Paper|Introduce
Definition of initial TF list We constructed an initial TF gene list as follows . We first used the definition of human TFs , as defined by the Molecular Signatures Database from the Broad Institute ( _CITE_ index . jsp ), consisting of a total of 1385 TFs . The most relevant subset of TFs for development and differentiation processes are those which are bivalently or PRC2 marked in hESCs [ 10 , 11 ]. This resulted in a list of 458 bivalent / PRC2 - marked TFs , of which 403 were also present in the Stem Cell Matrix - 2 ( SCM2 ) compendium mRNA expression data set .__label__Material|Data|Use
Images were processed with Fiji ( Schindelin et al ., 2012 ) and Photoshop , and displayed using the ‘ Fire ’ look - up table . Fiber length and fiber cross - sectional area were measured with freehand drawing tools in Fiji based on rhodamine - phalloidin staining . Sarcomere length , myofibril width , and myofibril diameter were measured automatically using a custom Fiji plug - in , MyofibrilJ , available from _CITE_ and code from https :// github . com / giocard / MyofibrilJ ( Cardone , 2018 ; copy archived at https :// github . com / elifesciences - publications / MyofibrilJ ). All measurements are based on rhodamine - phalloidin staining , except 34 hr APF sarcomere lengths , which are based on both rhodaminephalloidin and Unc - 89 - GFP staining . ‘ Sarcomeres per fibril ’ was calculated as average individual fiber length divided by sarcomere length for fiber 3 or 4 .__label__Method|Tool|Use
Supplementary information accompanies this paper at _CITE_ Competing Interests : The authors declare no competing interests . Publisher & apos ; s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations .__label__Supplement|Document|Produce
The authors are grateful to Adrien Debry and Rosa Afonso for maintenance of the Pa . guttatus and E . macularius colonies . Most of the computations were performed at the Vital - IT ( _CITE_ ) Center for high - performance computing of the SIB ( Swiss Institute of Bioinformatics ) and the Baobab cluster ( https :// baobabmaster . unige . ch / iface /) of the University of Geneva ( Switzerland ). They thank the reviewers for constructive comments , which improved the manuscript . This work was supported by grants from the University of Geneva ( Switzerland ), the Swiss National Science Foundation ( FNSNF , grant 31003A_125060 ), and the SystemsX . ch initiative ( project EpiPhysX ).__label__Method|Tool|Use
_CITE_ Figure 44 , Appendix 14 Opisthostoma charasense Tomlin , 1948 : 225 , Plate 2 - figure 4 ( original description ). Opisthostoma charasense Tomlin , van Benthem Jutting ( 1952 : 42 ). Type material .__label__Supplement|Document|Introduce
p ( y — x , w ) = Žitnik et al . BMC Bioinformatics 2015 , 16 ( Suppl 16 ): S1 Page 6 of 16 _CITE_ when mentions that are arguments of a certain relationship appear on longer distances . For example , mentions spoVG and E sigma H should be related via the Interaction . Transcription relationship . However , this relationship cannot be extracted from representation that considers only consecutive mention pairs .__label__Supplement|Paper|Introduce
In this study , we examine some of the challenges we encountered when comparing sources of health care across countries using DHS data . Taking these limitations into account , we present the classification we used in our analysis and provide recommendations that might enable more detailed and accurate assessment of providers in future work . The DHS has a continuing process of user feedback through its Questionnaire Review Portal on the DHS user forum ( _CITE_ ), and the recommendations we present are intended to be part of such ongoing discussions . Challenges in using source of care data for crosscountry comparisons To compare sources of care , we used the most recent DHS surveys conducted between 2000 and 2012 ; 57 countries had data available on family planning and delivery care and 47 countries had data available on antenatal care ( Appendix 1 ). We analysed five questions , shown in Table 1 , which ask respondents where they received care , and , for antenatal and delivery care , who provided it .__label__Supplement|Website|Use
Additionally , it can assign predicted serovars to Salmonella isolates . It is used by Public Health England on clinical isolates and has strict , well - defined conservative criteria for calling STs to ensure accuracy . mlst ( _CITE_ ) takes de novo assemblies as input on the command line and uses BLASTN to align sequences to alleles . It is very fast and searches all databases on pubMLST to automatically detect the organism , then calculates the ST . Installation is very easy using brew .__label__Method|Code|Produce
The Markov clustering ( MCL ) algorithm [ 10 ] was used with an inflation value of 2 . 2 for identifying co - expression clusters . In order to identify the functional relevance of transcript clusters , we used a combination of bioinformatics tools , literature review , as well as similarity to previously defined co - expression clusters [ 17 , 18 ]. Each co - expression cluster was examined using a number of bioinformatics tools , including gene ontology ( GO ) annotation enrichment analysis ( _CITE_ Gene Ontology database release 2016 - 04 - 23 ), pathway inspection ( Reactome , http :// www . reactome . org ; KEGG , http :// www . genome . jp / kegg ), and protein localization ( Human Protein Atlas , http :// www . proteinatlas . org ). In addition , co - expression signatures from previous studies were manually compared with clusters derived from skin , allowing the naming of some of the signatures . Clusters without GO enrichment or without similarity to previously reported co - expression signatures were further investigated by checking individual genes against the literature and the phenotypes reported for knockout mice .__label__Method|Tool|Use
The Markov clustering ( MCL ) algorithm [ 10 ] was used with an inflation value of 2 . 2 for identifying co - expression clusters . In order to identify the functional relevance of transcript clusters , we used a combination of bioinformatics tools , literature review , as well as similarity to previously defined co - expression clusters [ 17 , 18 ]. Each co - expression cluster was examined using a number of bioinformatics tools , including gene ontology ( GO ) annotation enrichment analysis ( _CITE_ Gene Ontology database release 2016 - 04 - 23 ), pathway inspection ( Reactome , http :// www . reactome . org ; KEGG , http :// www . genome . jp / kegg ), and protein localization ( Human Protein Atlas , http :// www . proteinatlas . org ). In addition , co - expression signatures from previous studies were manually compared with clusters derived from skin , allowing the naming of some of the signatures . Clusters without GO enrichment or without similarity to previously reported co - expression signatures were further investigated by checking individual genes against the literature and the phenotypes reported for knockout mice .__label__Material|Data|Use
performance Integrated Virtual Environment ( HIVE ) server ( _CITE_ ) [ 30 ]. The results are searchable and are also available as tabdelimited files . Users can either browse the curated data or search for specific genes or proteins using RefSeq accession number as the query .__label__Supplement|Website|Introduce
[ 29 ] use the APG tree as the backbone for a supertree used to analyze wood traits in 608 species ; Walls [ 44 ] uses different versions of the APG tree ( and the tree from [ 47 ]) in an analysis of leaf vein patterns ; in an analysis of scaling relationships in phylogenetic diversity , Morlon , et al . [ 23 ] heavily supplement the APG Stoltzfus et al . BMC Research Notes 2012 , 5 : 574 Page 7 of 15 _CITE_ backbone with other phylogenetic results . A fifth study [ 48 ] uses the APG tree from Phylomatic as a standard of comparison to validate its own tree . These examples reflect the ready availability of a mega - tree covering plants .__label__Supplement|Paper|Introduce
To smooth each track , the next directional estimate of each voxel was weighted by 20 percent of the previous moving direction and 80 percent by the incoming direction of the fiber . The tracking was terminated when the relative quantitative anisotropy ( QA ) for the incoming direction dropped below a preset threshold of 0 . 2 or exceeded a turning angle of 75 °. The CMU - 30 template fiber pathways can be downloaded , along with a Python script illustrating level set tree estimation , at _CITE___label__Method|Code|Produce
The genotyping SNP and STR data for mitochondrial and Y chromosomal DNA generated during the current study are included in the published article and its Supplementary Information files . The complete mitochondrial genome sequences generated during the current study are available from GenBank ( https :// www . ncbi . nlm . nih . gov / genbank /) under the accession numbers MG244202 – MG244226 . The seven novel Y chromosome sequences are available from European Nucleotide Archive ( _CITE_ ) under the accession number PRJEB22729 . The Autosomal data produced from 30 Leeward Society Islanders is available from the corresponding author on reasonable request .__label__Material|Data|Use
To comply with the study ’ s ethical approval , access to the raw data is only available to qualified researchers upon request . All summary statistics and analysis scripts are available directly from the authors ( please contact Jonas Grauholm at JOGR @ ssi . dk ). R scripts used to perform the analyses reported in this manuscript are available on GitHub ( _CITE_ ) and have been archived in Zenado at https :// zenodo . org / badge / latestdoi / 116149862 .__label__Method|Code|Produce
CHDS : Data contributed for this submission are available on request from the CHDS ( john . horwood @ otago . ac . nz ). Colaus / PsyCoLaus : Data from the CoLaus / PsyCoLaus study can be requested according to the procedure described on the CoLaus website ( _CITE_ htm ). ELSA : ELSA data are made available through the ESDS website ( http :// www . elsa - project . ac . uk / availableData ).__label__Material|Data|Use
CHDS : Data contributed for this submission are available on request from the CHDS ( john . horwood @ otago . ac . nz ). Colaus / PsyCoLaus : Data from the CoLaus / PsyCoLaus study can be requested according to the procedure described on the CoLaus website ( _CITE_ htm ). ELSA : ELSA data are made available through the ESDS website ( http :// www . elsa - project . ac . uk / availableData ).__label__Supplement|Website|Use
Our study demonstrates the population prevalence of clinical signs / symptoms and EBOV CT values over time in a large , diverse cohort of patients with EVD , as well as associations between symptoms / EBOV CT values and mortality . These findings have implications on surveillance , operational planning , and clinical care for future EVD outbreaks . PLOS Neglected Tropical Diseases | _CITE_ July 19 , 2017 1 / 17 Natural history of Ebola Virus Disease decision to publish , or preparation of the Author summary manuscript . Previous studies of Ebola Virus Disease ( EVD ) have focused on clinical symptoms and Competing interests : The authors have declared viral load ( or its proxy of cycle threshold value ) in the blood of patients measured the day that no competing interests exist . they begin medical care .__label__Supplement|Paper|Introduce
Heterogeneity in phenotype development The clinical presentation of MetS in humans is highly heterogeneous and spans over decades . Male E3L . CETP mice fed a high - fat diet supplemented with cholesterol develop MetS within a time scale of several months . Although all of these animals have the same genetic background , received PLOS Computational Biology | _CITE_ June 7 , 2018 11 / 19 In vivo and in silico dynamics of the development of Metabolic Syndrome the same diet and were kept and monitored in a controlled , standardized environment , this in vivo model did show heterogeneity in phenotypic presentation . In addition , the manifestation of the full repertoire of metabolic alterations associated with MetS makes this a useful in vivo model , whereas other animal models only describe one or partial metabolic aspects of MetS [ 53 – 59 ]. Using a traditional statistical approach , both this heterogeneity and limited datasets comprising of low number of animals are problematic .__label__Supplement|Paper|Introduce
After purification of the PCR products and gel electrophoresis on a 2 % ( w / v ) low melting agarose gel , libraries with insert sizes of 270 – 370 bp were selected and quantified using the Quant - iTTM PicoGreen ® double stranded DNA ( dsDNA ) reagent and kits ( Invitrogen , Carlsbad , CA , USA ). Afterwards , the DNA was sequenced for 200 cycles on a next - generation sequencing instrument ( Illumina Hiseq 2000 with a TruSeq SBS Kit v3 - HS ) [ 18 ]. The raw MB - seq data were deposited in the Sequence Read Archive ( SRA ) database ( _CITE_ ) of the National Center for Biotechnology Information ( NCBI ) with accession number of SRP067471 .__label__Material|Data|Use
© 2015 by the authors ; licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license ( _CITE_ ).__label__Supplement|License|Other
[ 36 ]. All efforts were made to ensure the welfare , and reduce stress of the animals , with the addition of full personal protective equipment worn by all team members throughout the process to prevent human - primate disease transmission . A veterinarian specialised in the capture and anaesthesia of wildlife performed the darting , having previously conducted an evaluation of PLOS ONE | _CITE_ March31 , 2017 3 / 23 Evaluating home range estimators using GPS collars the area and target individual to minimise risk to the animals . Animals were anaesthetised using Zoletil 100 ( Tiletamine + Zolazepam ; 6 – 10 mg / kg ), and a prophylactic dose of Alamycine LA ( 20 mg / kg ) and Ivermectine ( 0 . 2 mg / kg ) was given as a preventative measure to assist in the post - anesthesia recovery . Anaesthesia and the vital signs were monitored throughout the procedure .__label__Supplement|Paper|Introduce
For supplementary material accompanying this paper visit _CITE___label__Supplement|Document|Produce
Comments are denoted by a #. Abbreviations for sequencing center and tissue source site are as follows : BCGSC — British Columbia Genome Sequencing Center ; Broad — The Broad Institute of MIT and Harvard ; Fox Chase — Fox Chase Cancer Center ; MSKCC — Memorial Sloan Kettering Cancer Center ; UCSF — University of California San Francisco ; UNC — University of North Carolina ; Wash Univ St . Louis — Washington University in St . Louis . This file is available at Dryad with the following link : _CITE_ ( GZ 259 kb ) Additional file 3 : Figure S2 . These histograms illustrate the variation in counts of bacterial taxa per sequencing run across cancer types .__label__Supplement|Website|Use
Comments are denoted by a #. Abbreviations for sequencing center and tissue source site are as follows : BCGSC — British Columbia Genome Sequencing Center ; Broad — The Broad Institute of MIT and Harvard ; Fox Chase — Fox Chase Cancer Center ; MSKCC — Memorial Sloan Kettering Cancer Center ; UCSF — University of California San Francisco ; UNC — University of North Carolina ; Wash Univ St . Louis — Washington University in St . Louis . This file is available at Dryad with the following link : _CITE_ ( GZ 259 kb ) Additional file 3 : Figure S2 . These histograms illustrate the variation in counts of bacterial taxa per sequencing run across cancer types .__label__Supplement|Document|Use
The occurrence of anthrax showed an affinity for low values of precipitation during the driest month ( 0 to 10 mm ) and then dropped off dramatically as precipitation increased from 10 to 40 mm . Furthermore , as average NDVI ( wd0114a0 ) increased from 0 . 3 to 0 . 6 the probability of anthrax occurrence decreased linearly , with a more suitable range of vegetation greenness identified in the lower ranges between 0 . 1 and 0 . 3 ( Fig 6 ). PLOS Neglected Tropical Diseases | _CITE_ October 13 , 2017 8 / 17 Modeling and mapping anthrax in Ghana Fig 4 . Annual livestock anthrax vaccine doses administered in Ghana during January 2005 - June 2016 .__label__Supplement|Paper|Compare
Some 406 ( 81 . 2 %) gray matter regions from the total of 503 have data about the reception of at least one axonal input . From data available in BAMS so far , regions that receive the most axonal inputs are mostly located in the cerebral nuclei and hypothalamus . The connectivity data used for constructing the second BAMS rat macroconnectome is available to the neuroscience community in interactive graphical format in the newest version of BAMS : _CITE_ Users can construct it online and export the data in XML or JSON formats , or as a flat image . A second XML version of the macroconnectome that includes the BAMS unique ID ’ s of brain regions is provided in the classic version of BAMS : http :// brancusi . usc . edu / bkms / brain / choose - connection . php .__label__Material|Data|Use
Data detection process can be detailed in algorithm 2 . When the DOMe is started , a purely in - memory hash table is initialized as the fingerprint index ( line 1 ). In a backup , the ID ( the PLOS ONE | _CITE_ October 19 , 2017 6 / 17 DOMe database name + backup timestamp ) is used as the file name to create a metadata file ( line 2 ). Then , the buffer is constantly checked to find any chunk exist ( line 3 ). When a data chunk is read ( line 4 ), its MD5 value is calculated as the fingerprint fp ( line 5 ).__label__Supplement|Paper|Extent
describing genes and their encoded products in terms of their molecular functions , biological processes or cellular components [ 1 ]. A GO enrichment analysis can be undertaken using one of the many publicly available tools ( _CITE_ ) and these analyses examine the gene list for the occurrence of GO terms that are more prevalent in the query gene list than expected by chance ( it is important to note that using an appropriate background or ‘ universe ’ to assess statistical significance is essential ) [ 2 ]. Such over - represented terms may highlight previously unrecognised biological processes ( as opposed to individual genes ) that are preferentially and differentially regulated in the condition of interest . A feature of GO that is both a strength and a limitation is its hierarchical structure .__label__Method|Tool|Use
downloaded from the USGS website ( URL = _CITE___label__Supplement|Website|Use
Mapping was undertaken as part of the study “ Epidemiology of Yaws in the Solomon Islands and the Impact of a Trachoma Control Programme .” The full methodology and results of this survey are described elsewhere ( 5 , 18 ). These data were collected in Western and Choiseul provinces of the Solomon Islands , September – October 2013 . Demographic information is given in Web Table 1 in Web Appendix 1 ( available at _CITE_ ). The trachoma survey enrolled all participants regardless of age or sex and in these two provinces . This included a total of 5 , 838 individuals in 98 villages .__label__Supplement|Document|Produce
Thus they support local zooming , highlighting and other controls . A user can readily traverse between a broad overview and a detailed inspection . For example , the daf - 16 graph is accessible here ( _CITE_ c elegans / gene / WBGene00000912 # b - c - 10 ).__label__Supplement|Document|Produce
Several mapping software packages have been developed to map short reads to the reference genome [ 23 ]. BWA is a robust and fast short - read aligner , and has been widely used to map ChIP - Seq reads [ 24 ]. Novoalign ( _CITE_ ) is slower than BWA but is known to have higher sensitivity [ 23 ]. To decide which one to be implemented into the pipeline , we compared mapping rate between Novoalign and BWA on both single - end and paired - end ChIP - Seq data , and further assessed how the mapping difference might impact peak calling .__label__Method|Tool|Introduce
The indicated p - values for pair - wise comparison are calculated using Mann - Whitney Utest , n = 60 alleles for each interrogated locus . The distances between the centres of the regions covered by the probes D and E ( located in the gene poor TADs 2 and 5 respectively ) are similar to the much closer regions covered by the probes B and C ( located in the adjacent gene - rich TAD3 and TAD4 ). _CITE_ Correction of the 5C data for non - biological biases associated with this technology was performed as described previously [ 17 , 43 ] ( see Materials and Methods for details ) ( S2 – S5 Figs ). The corrected data were binned ( bin size 150kb with the step size of 15kb ) to account for the differences in the 5C probe coverage in the different parts of the 5 . 3 Mbp genomic region ( Fig 2D , S2E Fig , S3E Fig , S4E Fig , S5E Fig and S6A Fig ). The heatmaps representing 5C data clearly showed several consecutive chromatin regions with high spatial self - associations ( visible as darker “ triangles ” above a black “ diagonal ”) corresponding to the distinct TADs in keratinocytes and thymocytes ( Fig 2D , S6A Fig ) [ 9 , 10 , 17 ].__label__Material|Data|Produce
Notably , the detected Ct values for a few selected genes were not proportionally coordinated with basemean values . One obvious outlier is MDP0000211280 , whose expression levels showed unexpected high Ct values . The higher than expected Ct values are more noticeable in PLOS ONE | _CITE_ September 21 , 2017 8 / 17 Reference genes for normalizing gene expression in apple roots the tissue types of “ 115 Rs ” and “# 75 Pu ” tissues . Both lines (# 115 and # 75 ) are known to be susceptible to these pathogens ( personal communication ). This observation may indicate its expression was suppressed in selected genotypes during pathogenesis .__label__Supplement|Paper|Introduce
aegypti has a substantially higher optimum and maximum temperature than Ae . albopictus ( Fig 2 ) due to its greater rates of adult survival at high temperatures ( see Supplementary Materials for sensitivity analyses ). PLOS Neglected Tropical Diseases | _CITE_ April 27 , 2017 4 / 18 Temperature predicts Zika , dengue , and chikungunya transmission__label__Supplement|Paper|Introduce
Genomes generated as part of the Human Microbiome Project ( HMP ) ( 25 ) and the Genome Encyclopedia of Bacterial and Archaea Genomes ( GEBA ) project ( 26 ) are of special interest . With the goal of characterizing microbial communities found at multiple human body sites , HMP has initially focused on the sequencing of reference genomes from both cultured and uncultured bacteria ( 25 ). Over 550 reference genomes sequenced as part of the HMP initiative , as well as over 1500 genomes associated with a human host and thus relevant to HMP , can be examined and analyzed using IMG / HMP ( _CITE_ ), which is provided as part of the HMP Data Analysis and Coordination Center ( DACC ). The aim of the GEBA is to fill systematically the sequencing gaps along the bacterial and archaeal branches of the tree of life . After a pilot project in 2009 that generated complete genomes for about 100 organisms ( 26 ), the number of sequenced GEBA genomes has steadily increased and stands at 205 as of August 2011 .__label__Method|Tool|Use
All data sets , analysis results , and supplementary material are available on FigShare Repository ( _CITE_ a8cea06c05465c939e15 ).__label__Material|Data|Produce
All data sets , analysis results , and supplementary material are available on FigShare Repository ( _CITE_ a8cea06c05465c939e15 ).__label__Supplement|Website|Produce
All data sets , analysis results , and supplementary material are available on FigShare Repository ( _CITE_ a8cea06c05465c939e15 ).__label__Supplement|Document|Produce
DSS does a much better job than metilene in this respect , although informME is clearly the best method among the three to accomplish this goal . For this reason , we provide in the following a further assessment of the performance of informME and DSS when applied on real data . We used gene ontology ( GO ) enrichment analysis ( _CITE_ ) [ 44 ] to compare performance by evaluating the potential of informME to that of DSS for addressing a specific problem of interest to epigenetic biology : identifying biological processes that are significantly enriched in epigenetically dysregulated genes . By using GO enrichment analysis on gene lists of equal size formed by selecting genes with the largest detected methylation discordance at their promoters , we can remove the issue of sensitivity and specificity and focus on the ability of each method to produce biologically relevant results . It is important to note that the gene selection method used in [ 16 ] selects a gene by checking whether a statistic T , which counts the number of the top 2000 differentially methylated CpG sites in the gene , is above a threshold t = 4 .__label__Method|Algorithm|Use
The integrated and permanently updated report frontend ( Figure 2 ) supports the interactive exploration of stored datasets . Beside functionalities for browsing , filtering and downloading of datasets , download and access statistics are provided for each DOI . Using the metadata search provided by DataCite ( Figure 3 ) ( _CITE_ ), users are enabled to explore and search specific PGP datasets of interest . A general keyword - based search functionality over all metadata , as well as a more advanced search , allows for filtering by parameters , such as authors , dates and file types are implemented . Besides , via the provided web interface , it is also Page 4 of 10 Database , Vol .__label__Material|Data|Use
All analyses , unless otherwise indicated , were undertaken using the R platform for statistical computing ( version 3 . 1 . 0 ) _CITE_ ( 67 , 69 ) ( and a range of library packages were implemented in Bioconductor ( 70 ) ( Supplementary Table S1 ).__label__Method|Code|Use
95 % credible intervals are shaded . The thin grey line shows the cumulative distribution of grid - cell level densities in the world . _CITE_ The most notable finding is that completeness has a U - shaped relationship with density . As shown in panel A of Fig 2 , OSM is most likely to be complete at low and high densities . Thus , interurban roads that traverse areas with minimal population are largely present in OSM , and high - density urban areas , with many potential local contributors and good Internet access , are also well mapped .__label__Supplement|Paper|Introduce
Assembly starts from aligned DNA - Seq reads to reconstruct the original DNA sequence computationally , which generates large , continuous regions of DNA sequence [ 3 ]. Many alignment software provide tools to perform the assembly after the read alignment ( e . g ., MAQ ), or standalone resources can be used ( SAMTOOLS [ 13 ], Emboss [ 14 ]) or commercial packages like Geneious ( _CITE_ ) and CLC - Bio ( http :// www . clcbio . com ). For organisms without a sequenced reference genome , it is not possible to perform any reference genome guided assembly of the reads , thus de novo assembly is always an essential step for data analysis . The majority of de novo assemblers that have been released follow two basic approaches : overlap graphs [ 15 ] and de Bruijn graphs [ 16 ].__label__Method|Tool|Use
From these , a final set of the most variable 48 primer combinations was chosen , with an average sequence variability of 2 . 7 % ( 0 . 8 %– 7 . 5 %) ( S1 Table ). For the nuclear set , we identified 51 PPR and 762 COSII loci that matched our criteria for further primer design ( i . e ., enough reads matching from low - coverage genomic data to attempt primer design ). The nuclear rDNA , PHOT1 , and PHOT2 alignments ( aligned lengths of 6 , 711bp , 578 bp , 1 , 272bp — Dryad Digital Repository : _CITE_ ) all contained multiple places to design primers based on our criteria . A total of 188 primer__label__Material|Data|Introduce
Programs were generated to analyze the depth , variation , and consensus quality of each SNP . Finally , a Perl script was written to select significant sites within the predicted SNP positions . The script can be downloaded at sourceforge ( _CITE_ ). Classification of intergenic , exonic and intronic SNPs To determine whether the SNP location within the transcript structure is intronic , exonic , or intergenic , we tracked information from the reference genome sequence and annotated the exon or intron at which the SNP was located if it was not intergenic . Gene Ontology ( GO ) was analyzed using a generic GO slim database composed of 366 , 327 proteins downloaded from the Gene Ontology website ( http :// archive . geneontology . org / lite / 2013 - 01 - 26 /), which lists high - level GO terms that provide a broad overview of the ontology content .__label__Method|Code|Produce
Triticeae are known to have low barriers against hybridization , which result in mixed or even recombinant © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Bernhardt et al . BMC Evolutionary Biology ( 2017 ) 17 : 141 Page 2 of 16 phylogenetic signals from nuclear data [ 10 , 20 ].__label__Supplement|License|Other
Since , for purposes of the contamination warning system , the syndromes monitored are the same for both acute and gradually developing events caused by biological , radiological , or chemical contaminants , the timeliness of anomaly detection coupled with subject matter expertise during subsequent alert investigations determines what type of contaminant ( s ) may be responsible . Haas et al . International Journal of Health Geographics 2011 , 10 : 22 Page 3 of 10 _CITE_ An example timeline for a contamination scenario is depicted in Figure 1 , demonstrating symptom onset , actions of an exposed individual , and the unique data outputs which can be analyzed by public health surveillance systems . The timeline illustrated in this figure is based on the expected symptom onset and potential health - seeking actions that would occur following exposure to a carbamate pesticide . Note that Poison Control Center ( PCC ) data entry is initiated by poison control specialists ( into the National Poison Data System [ 10 ]) immediately upon receipt of a call to the hotline .__label__Supplement|Paper|Introduce
( B ) Topology cartoon of AtPDF , free ( left ) or actinonin bound ( right ), in the same color code as ( A ). Actinonin ( represented by the yellow arrow ) binding to the ligand binding site allows the linkage of the two distinct b - sheets into one single b - sheet , by mimicking an additional b - strand . PDB sum ( _CITE_ ) was used . ( C ) 3 - D structure of AtPDF is represented showing the position of the residues discussed in the text , indicated in red . ( EPS ) Figure S2 Microcalorimetric titration of AtPDF with actinonin .__label__Method|Algorithm|Use
In the present study , we look at the sex discordance , and time - trends in sex discordance , of asthma by comparing data from two large , contemporary UK birth cohorts recruited a decade apart [ 11 ]. We investigated whether the increased male prevalence is indeed attenuating in recent years as some previous studies have indicated [ 8 , 9 ]. We analysed the sex discordance in prevalence of asthma and wheeze in The Avon Longitudinal Study of Parents and Children ( ALSPAC , recruited 1991 – 1992 ), a birth cohort where study children have reached puberty and the Millennium Cohort Study ( MCS , recruited 2000 – 2002 ) a more recent cohort where the study children have not yet transitioned through puberty but we may still expect to find a higher prevalence of males with asthma in early childhood . Longitudinal Studies in collaboration with the UK Data Service and is available online by application _CITE_ 2000031 # access . Funding : This research was specifically funded by Medical Research Council Integrative Epidemiology Unit ( MRC - IEU ) PhD Studentship RD1885 and further supported by the MRC Integrative Epidemiology Unit and the University of Bristol ( MC_UU_12013_2 , MC_UU_12013_5 and MC_UU_12013_8 ). The UK Medical Research Council , the Wellcome Trust ( grant ref : 102215 / 2 / 13 / 2 ) and the University of Bristol provide core support for ALSPAC .__label__Supplement|Website|Use
To evaluate the efficiency of correlation coefficient on real data , a publicly available rice RNA - Seq dataset was used in the analysis . The dataset included two replicates of root and shoot samples from plants before and after a one hour salinity stress treatment [ GenBank ID : DRX000191 , DRX000192 , DRX000193 , and DRX000194 ]. The software CLC Genomics Workbench ( _CITE_ com ; Aarhus , Denmark ) was used to map the reads on the rice Nipponbare reference genome to isolate different type of reads , i . e . total and uniexon and gene reads .__label__Method|Tool|Use
This data management system worked well until its replacement in 2013 . A first release of Fauna Europaea via the web - portal was presented on 27 September 2004 , whereas the most recent release ( version 2 . 6 . 2 ) was launched on 29 August 2013 . An overview of Fauna Europaea releases can be found at : _CITE_ about fauna versions . php . Quality control : Fauna Europaea data are unique in the sense that they are fully expertbased . Selecting leading experts for all groups provided a principal assurance of the systematic reliability and consistency of the Fauna Europaea data .__label__Supplement|Document|Produce
Large SNP datasets were available from cattle and sheep populations , where multiple animals per breed were drawn from a diversity of breeds and geographic regions . Clear evidence for population substructure was evident in cattle , which is composed of two sub - species ( Bos taurus and Bos indicus ) Hudson et al . BMC Bioinformatics 2014 , 15 : 66 Page 5 of 21 _CITE_ Figure 4 Compression efficiency against heterozygosity resolves non - human populations . Compression efficiency ( y - axis ) against heterozygosity ( x - axis ) for the four non - human data sets : ( A ) cattle ( n = 1 , 800 ), ( B ) sheep ( n = 1 , 222 ), ( C ) mouse ( n = 49 ) and ( D ) dog ( n = 83 ). The discrimination afforded by genome - wide compression efficiency and heterozygosity is effective in all species under consideration .__label__Supplement|Paper|Introduce
advantage of Cytoscape ’ s core application framework to provide a better interoperability within the whole tool . A large number of Apps have been developed and made available by the community ( _CITE_ ). Several of them are well known in the analysis of biological networks [ 17 ]. For example , BiNGO [ 76 ] is frequently used for GO annotation and related ontology enrichment analyses .__label__Supplement|Website|Introduce
Community - driven collections of images and ground truth , as well as “ model zoos ,” will be instrumental for this . We have also begun creating libraries ( Keras - ResNet [ https :// github . com / broadinstitute / keras - resnet ] and Keras - RCNN [ https :// github . com / broadinstitute / keras - rcnn ]) that will provide the foundation for interfaces that allow biologists to annotate , train , and use deep learning models . We expect that over time , PLOS Biology | _CITE_ July 3 , 2018 10 / 17 these models will reduce the amount of time biologists spend tuning classical image processing algorithms to identify biological entities of interest in images .__label__Supplement|Paper|Introduce
The publication costs for this article were funded by the corresponding author . This article has been published as part of BMC Medical Genomics Volume 6 Supplement 3 , 2013 : Selected articles from the IEEE International Conference on Bioinformatics and Biomedicine 2012 : Medical Genomics . The full contents of the supplement are available online at _CITE___label__Supplement|Document|Produce
Be aware that such an adjustment might dramatically increase the model estimation time and does not necessarily guarantee improved sampling performance . The failure of an adjusted model estimate might further suggest that such a model is not suitable for the current dataset , and that one may need to consider using alternative models to fit the data . If users encounter a problem and would like to seek help from the hBayesDM developers , they can ask questions to our mailing list ( _CITE_ ).__label__Supplement|Website|Produce
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ Metadata associated with this Data Descriptor is available at http :// www . nature . com / sdata / and is released under the CC0 waiver to maximize reuse . SCIENTIFIC DATA 1 2 : 150076 1 DOI : 10 . 1038 / sdata . 2015 . 76 6__label__Supplement|License|Other
We also suggest that assessment of the fitness of metadata for use be considered from the ‘ demand side ’ by asking how data have typically been used to best effect in the creation of biodiversity knowledge and policy . There are many technical publications - for example : Voss and Emmons ‘ Mammalian diversity in neotropical lowland rainforests : a preliminary assessment ’ [ 16 ], the US Fish and Wildlife Service ’ s ‘ Statistical guide to data analysis of avian monitoring programs ’ [ 17 ] or Agosti et al .’ s ‘ Ants : standard methods for measuring and monitoring biodiversity ’ [ 18 ] - that provide detailed descriptions of common data collection methods or of Moritz et al . BMC Bioinformatics 2011 , 12 ( Suppl 15 ): S1 Page 4 of 10 _CITE_ statistical processes applied to biodiversity data . Recently , the European Union Framework Projects 6 project EDIT ( European Distributed Institute for Taxonomy ) has developed a complete workflow , from data collection in the field to assembly of datasets and analyses [ 19 , 20 ]. These and many other works provide guidance in the development of standard ontologies for data description .__label__Supplement|Paper|Introduce
This study was carried out on the four large autosomes ( 2L , 2R , 3L , and 3R ) of D . melanogaster using release 5 of the Berkeley Drosophila Genome Project ( BDGP 5 , _CITE_ , last accessed May 2010 ) as the reference genome .__label__Method|Tool|Use
HITdb is available in GitHub at _CITE_ For direct download , use https :// github . com / microbiome / HITdb / archive / master . zip . The contained README file provides instructions and other information .__label__Method|Code|Introduce
These various web interfaces enable users to extract human genetic polymorphism annotations with userfriendly search systems . Availability VarySysDB can be downloaded and freely accessed , with no restriction to academic users only , from http :// h - invitational . jp / varygene /. A help document is also available from _CITE___label__Supplement|Document|Use
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE___label__Supplement|License|Other
Data and materials Availability : All data needed to evaluate the conclusions in the paper are present in the paper and / or the Supplementary Materials . Additional data related to this paper may be requested from the authors . Experimental data are available at _CITE_ The Python script used to reweight trajectories is available at https :// github . com / sbottaro / rr .__label__Material|Data|Produce
The use of MCODE was preferred for an easier comparison of ICan and the three single - factor networks , as the same modules were identified from the unweighted network . The edge - weighting procedure was performed separately for each network , and the M scores of each module were calculated according to a scoring formula ( see Additional file S4 Table for details ). A functional enrichment analysis was performed on the candidate cancer - related gene set and the genes inside the module using the DAVID tool [ 33 ] ( _CITE_ ).__label__Method|Tool|Use
Mindboggle ( _CITE_ ) is an open source brain morphometry platform that takes in preprocessed T1 - weighted MRI data and outputs volume , surface , and tabular data containing label , feature , and shape information for further analysis . In this article , we document the software and demonstrate its use in studies of shape variation in healthy and diseased humans . The number of different shape measures and the size of the populations make this the largest and most detailed shape analysis of human brains ever conducted .__label__Method|Tool|Introduce
After the variant calling file ( VCF ) has been obtained , a filtering procedure is often necessary for the downstream analysis . This filtering procedure aims to reduce the number of false positives corresponding either to low quality and common variants . In particular the common variants , which are assumed to have no implications in tumor development and progression , are filtered out by comparison with the germline polymorphisms collected in publicly available databases , such as dbSNP [ 29 ] or EVS ( Exome Variant Server , _CITE_ ). The most recent versions of dbSNP ( build 142 ) contains more than 110 million human SNPs , while the current EVS data release ( ESP6500SI - V2 ) include all the exome variant data from 6503 human samples . Some of the common tools used for filtering variants are SnpSift [ 30 ], GATK [ 20 ], VCFtools ( http :// vcftools . sourceforge . net ).__label__Material|Data|Introduce
Importantly , further information can be extracted from the Morlet ’ s wavelet complex decomposition ( Tallon - Baudry et al ., 1996 ) such as time - resolved and band - specific amplitude , power or phase . Critically , each one of these signal processing tools are reversible and can therefore be activated and deactivated without altering the original data and without any memory - intensive data copy . Finally , loaded data can be re - referenced directly from the interface by either re - referencing to a selected single channel or common - average ( frequently used for scalp EEG datasets ) or by using bipolarization , which consists of subtracting neural activity 10_CITE_ Frontiers in Neuroinformatics | www . frontiersin . org 3 September 2017 | Volume 11 | Article 60 Combrisson et al . Sleep Data Visualization FIGURE 1 | Illustration of the different sleep features observed in a polysomnographic recording of one individual . To see examples of automatic detection actually performed by our software , see Figure 4 .__label__Method|Code|Use
We used the genome data of 10 unrelated chimpanzees [ 28 ] and 108 unrelated rhesus macaques [ 29 ]. Out of 133 samples that were reported to be sequenced , only 108 were available for download . The whole genome sequencing data were obtained from the Sequence Read Archive ( SRA ; _CITE_ ). The NCBI BioProject database accession numbers were PRJEB1357 for the chimpanzee data and PRJNA251548 for the macaque data . The sample lists are provided in S3 and S4 Tables .__label__Material|Data|Use
The PDBe database can also be queried using database identifiers for various relevant resources such as PubMed , Pfam , SCOP , CATH , EC and InterPro . It is further possible to carry out FASTA ( Lipman & Pearson , 1985 ) searches against all protein sequences in the PDB from the home page . A tool to help new users of the website find the information that they are looking for is the PDBe Wizard ( _CITE_ wizard ). The Wizard first tries to establish what the user is looking for and what information they already have . Based on this , it either provides the user with a box in which to enter some input ( e . g .__label__Method|Tool|Introduce
All data and related metadata are deposited in an appropriate public repository : The study population ’ s data were from Taiwan NHIRD ( _CITE_ ) are maintained by Taiwan National Health Research Institutes ( http :// nhird . nhri . org . tw /) [ 27 ]. The National Health Research Institutes ( NHRI ) is a non - profit foundation established by the government . Air quality data were from Taiwan Air Quality Monitoring Network ( http :// taqm . epa . gov . tw / taqm / en / PsiMap . aspx ) in Taiwan Environmental Protection Administration ( http :// www . epa . gov . tw /) [ 28 ].__label__Material|Data|Use
Data are presented as mean with SEM of triplicate determinations ( n = 6 ). Statistical analyses were conducted using Student ’ s t - test (*** p & lt ; 0 . 001 ). _CITE_ pseudomallei 24 hours after infection ( Fig 2B ). Since hepatocytes have a large capacity for iron storage and are major producers of hepcidin , we also analysed the consequences of hepcidin administration in murine hepatoma Hepa1 - 6 cells . In line with macrophages , hepcidin led to higher bacterial load in hepatoma cells ( Fig 2B ).__label__Material|Data|Use
Fig 10 shows the two most dominant bias types for the four evaluated sites on sample A . These bias types and their associated transcript length histograms are similar for all sites . One of the clusters consists of distributions concentrated around the middle of comparatively short transcripts while the other cluster contains slightly 3 ’ biased distributions PLOS Computational Biology | _CITE_ May 15 , 2017 18 / 25 Mixture models yield accurate transcript concentration estimates from RNA - Seq data for comparatively long transcripts . Also the remaining clusters are similar for all sites as can be seen from Fig J to Fig P in S2 Appendix . Biases are also similar across samples as can be seen for Fig M to Fig P in S2 Appendix which show clusters for samples A to D of BGI .__label__Supplement|Paper|Introduce
© 2016 The Authors . Published by Elsevier Inc . on behalf of the Alzheimer ’ s Association . This is an open access article under the CC BY - NC - ND license ( _CITE_ ).__label__Supplement|License|Other
broadinstitute . org / pub / svtoolkit / misc / 1kg / NGPaper /. ( v ) The Coriell repository : http :// ccr . coriell . org . ( vi ) The R / ExomeDepth package : _CITE_ ExomeDepth - vignette . pdf . ( vii ) The Genome STRiP FAQ : http :// gatkforums . broadinstitute . org / discussion / 1490 / frequently - askedquestions .__label__Method|Code|Use
Heritability measured separately within each country was significant only in the Netherlands , at 26 % ( 12 %– 60 %) ( S2 Table , N = 434 , ΔAIC = – 12 . 2 for OU compared to the null model ). In Switzerland , the largest cohort in this study , GSVL was not significantly heritable ( S2 Table , N = 742 , ΔAIC = + 2 . 1 for OU compared to the null model ). This could reflect the limited genetic diversity of our Swiss samples ; in other countries , the lack of detected PLOS Biology | _CITE_ June 12 , 2017 9 / 26 Viral genetic variation explains variability in HIV - 1 pathogenesis heritability is most likely due to limited power to detect a phylogenetic signal . In males infected by subtype B viruses ( N = 1 , 446 ), GSVL heritability was 16 % under BM and 32 % under OU , but heritability was not significant in females ( N = 135 ). In MSM infected by subtype B viruses ( N = 1 , 196 ), GSVL heritability was 17 % under BM and 30 % under OU , but heritability was not significant in injecting drug users ( IDUs ) ( N = 110 ) and heterosexuals ( N = 211 ).__label__Supplement|Paper|Introduce
The KEGG pathways attributed to the essential , critical and ambiguous genes were compared between the three bacteria . Gene orthologues were also identified between Se4047 and S . pyogenes strain MGAS5005 ( reference strain used by Le Breton et al . for M1T1 5448 ), Se4047 and S . agalactiae strain A909 and between S . pyogenes strain MGAS5005 and S . agalactiae strain A909 using the online tool OrtholugeDB ( _CITE_ ) [ 34 ]. The essentiality calls of each orthologous gene pair were compared to determine concordance . All results generated from OrtholugeDB were included in the analysis , except for duplicated calls where multiple copies of a gene exist in either bacterium or when gene essentiality is not defined or non - conclusive .__label__Method|Tool|Use
NGS - QCbox can be used to process any paired end data from NGS experiments such as DNA - Seq , RAD - Seq , GBS , RNA - Seq . For processing single end read datasets one may need to use Raspberry independently . The NGS - QCbox pipeline and Raspberry are available alternatively from dockerhub ( _CITE_ ) as a docker image ( dadu / ngsqcbox : v0 . 2 . 1 for linux and dadu / ngsqcbox_win : v0 . 2 for Windows ). Docker ( docker . io ) is a popular and portable lightweight linux container based technology to host applications in a virtual environment . This technology eases the process of distributing the application and thereby helps solve application related__label__Supplement|Website|Use
© 2018 by the authors . Licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ( CC BY ) license ( _CITE_ ).__label__Supplement|License|Other
It is compatible with standard genotype file formats ( e . g . PLINK , vcf ). The scripts to run our method are publicly available at : _CITE_ Association of rare predicted gene KOs with anthropometric traits We analyzed BMI , adult height and BMI - adjusted WHR . We stratified all our analyses by ethnic group , and we only considered rare or low - frequency LoF variants with MAF & lt ; 5 %.__label__Method|Code|Produce
In building the LDP and the underlying infrastructure and processes , particular emphasis has been placed on data standards including the use of ontologies , extensive curated annotations of LINCS content , globally valid identifiers , and easily accessible APIs . These efforts have been guided by the FAIR principles to make LINCS data Findable , Accessible , Interoperable , and Reusable ( 10 ). For example , LINCS data are already indexed in bioCADDIE DataMed ( _CITE_ ) and OmicsDI ( http : // www . omicsdi . org ). Via its current functionality , the LDP enables a wide range of use cases . For example , if the user wishes to discover all LINCS registered compounds that are indicated to treat glioblastoma ( GBM ), they can query the Small Molecule Application for the disease by the Medical Subject Heading ( MeSH ) term or Experimental Factor Ontology ( EFO ) annotations ; this search returns 26 compounds .__label__Material|Data|Introduce
It targets the mRNA of green fluorescent protein ( GFP ). The sequences of the applied miRNAs from Arabidopsis thaliana were derived from the miRBase database ( http :// www . miRBase . org ). For the identification of the miRNAs 162 , 168 , and 403 from Nicotiana benthamiana , precursor sequences of the Nicotiana tabacum miR162 and miR168 and of the Solanum lycopersicum miR403 were blasted against the N . benthamiana draft genome sequences at _CITE_ . net . The N . benthamiana miRNAs ( Fig . 1B ) were deduced from the identified miRNA precursor sequences ( Fig .__label__Material|Data|Compare
For instance , a matrix of P - values resulting from a differential gene expression analysis between two conditions ( for example , cancer / normal ) can be visualized by map staining . In addition , users can perform a functional enrichment of ACSN modules statistical analysis from a gene list ( for example , a list of differentially expressed genes ) directly in the ACSN environment ( Supplementary Figure S9 ). The data visualization and analysis functionality is illustrated in several case studies using cancer data : in the live example of visualization of The Cancer Genome Atlas ( TCGA ) ovarian cancer data set , in the tutorial , and in the guide available from _CITE_ Visualization of siRNA drug - screening results . High - throughput data analysis generally results in a gene list : for example , a list of differentially expressed genes , the genes that contribute the most to a particular scoring , drug targets or hits of siRNA - based genome - wide screenings , and so on .__label__Material|Data|Produce
For instance , a matrix of P - values resulting from a differential gene expression analysis between two conditions ( for example , cancer / normal ) can be visualized by map staining . In addition , users can perform a functional enrichment of ACSN modules statistical analysis from a gene list ( for example , a list of differentially expressed genes ) directly in the ACSN environment ( Supplementary Figure S9 ). The data visualization and analysis functionality is illustrated in several case studies using cancer data : in the live example of visualization of The Cancer Genome Atlas ( TCGA ) ovarian cancer data set , in the tutorial , and in the guide available from _CITE_ Visualization of siRNA drug - screening results . High - throughput data analysis generally results in a gene list : for example , a list of differentially expressed genes , the genes that contribute the most to a particular scoring , drug targets or hits of siRNA - based genome - wide screenings , and so on .__label__Supplement|Document|Produce
In addition , we integrated Cytoscape [ 23 ] Java Web Start technology so that the association network generated by eLSA can be immediately visualized . Based on these efforts , we anticipate that our novel eLSA methodology , as implemented by the newly developed pipeline software , will significantly assist researchers requiring systematic discovery of time - dependent associations . More information about the software and web services is available from the eLSA homepage at _CITE___label__Method|Tool|Produce
In addition , we integrated Cytoscape [ 23 ] Java Web Start technology so that the association network generated by eLSA can be immediately visualized . Based on these efforts , we anticipate that our novel eLSA methodology , as implemented by the newly developed pipeline software , will significantly assist researchers requiring systematic discovery of time - dependent associations . More information about the software and web services is available from the eLSA homepage at _CITE___label__Supplement|Website|Produce
In addition , we integrated Cytoscape [ 23 ] Java Web Start technology so that the association network generated by eLSA can be immediately visualized . Based on these efforts , we anticipate that our novel eLSA methodology , as implemented by the newly developed pipeline software , will significantly assist researchers requiring systematic discovery of time - dependent associations . More information about the software and web services is available from the eLSA homepage at _CITE___label__Supplement|Document|Produce
Interestingly , the enhancers found in TAD1 form two closely located clusters ( E2 / E3 and E4 - E7 ), embedded into the genes of S100 family . These enhancer clusters showed extensive long - range intra - TAD chromatin contacts with multiple genes in the central part of the EDC ( TAD3 and TAD4 ) activated during terminal keratinocyte differentiation , suggesting that they might serve as the locus - control regions or super - enhancers for the EDC genes . In addition , PLOS Genetics | _CITE_ September 1 , 2017 18 / 32 Chromatin interactome of multi - TAD keratinocyte - specific gene locus we identified the gene enhancer ( E9 ) spatially interacting with Flg gene promoter ( Fig 4D ). These enhancers have been previously identified among the highly - conserved non - coding regions in several mammalian genomes and showed the activity in the reporter assay in cultured keratinocytes [ 33 ]. It will be important to determine if this conserved enhancer controls Flg gene expression in normal and diseased epidermis , as the defects in Flg gene and changes in its expression are associated with ichthyosis vulgaris , the most common disorder of epidermal differentiation , and also serve as strong risk factors for atopic eczema [ 62 ].__label__Supplement|Paper|Introduce
Additional file 1 : Table S3 shows the example of output of Defiant . Defiant is applicable to bisulfite - sequencing data , RRBS [ 19 ], HpaII tiny fragment enrichment by ligationmediated PCR - tag ( HELP - Tag ) data [ 62 ], and Tet - assisted bisulfite sequencing ( TAB - Seq ) [ 63 ]. The source code is available on _CITE___label__Method|Code|Produce
In this case , the BAC end sequence data from Twilight ’ s half brother named Bravo . To explore this possibility , we analyzed the “ allelic depth ( AD )” field from the VCF file . A detailed description of the VCF format can be found at _CITE_ Using the allelic depth field , we counted the number of homozygous variant positions ( the 83 , 535 and 720 , 843 variant positions from the Sanger and__label__Supplement|Document|Produce
( 7Z ) S11 Dataset . VTK file for the distribution of actomyosin prestress colour - coded in Fig 5a . The VTK file format is described on _CITE_ ( 7Z ) S12 Dataset . VTK file for velocity shown as arrows in Fig 5a .__label__Supplement|Document|Introduce
2010 ). Sinusoidal weight was set to 2 . Reconstruction of the spectra J Biomol NMR ( 2015 ) 62 : 31 – 41 33 by CS using iterative soft thresholding were performed by hmsIST ( _CITE_ index . html ) ( Hyberts et al . 2012 ). Number of iteration was set to 400 .__label__Method|Tool|Use
Following the incubation period , the vector becomes infectious , AI , and spends the rest of its life in this stage [ 38 – 40 ]. We assumed that an infected human passes to the exposed stage , E , and , after approximately w days , becomes infectious , I [ 36 , 38 , 40 – 42 ]. We followed previous modelling approaches [ 40 , 46 – 49 ] by assuming that an infection is captured by a surveillance system with PLOS ONE | _CITE_ March 31 , 2017 5 / 35 Albopictus - borne CHIKV transmission model a certain probability , aη , whereby a person becomes infectious and thus symptomatic . The infectious stage lasts approximately y days , and is either followed by recovery , R [ 38 , 40 , 41 , 43 ], or progression to the chronic stage , C [ 57 – 59 ]. We defined fixed daily probabilities , pR and pC , respectively , for each of these routes .__label__Supplement|Paper|Extent
The aim was to model the structure - activity relationship of a congeneric set of ligand molecules through molecular docking and scoring . To select the appropriate test sets , we focused on the target proteins already considered in the in situ scoring test . One data set for HIV - 1 protease , CA - 2 , BACE - I , and CHK - 1 , respectively , were selected among the “ validation sets ” from BindingDB ( _CITE_ ) [ 63 ]. Trypsin was excluded here because there was no validation set of trypsin inhibitors in the current release of BindingDB ( as by April , 2016 ). In order to select the data sets employed in our study , each data set must contain at least 10 ligand molecules with experimental binding data , and the binding affinity range must be larger than 10 folds .__label__Material|Data|Use
We downloaded the gene level processed expression data ( level 3 ) for TCGA ovarian cancer from the Firehose pipeline as of the March 2014 analysis freeze ( _CITE_ ) for all three platforms available for ovarian cancer ( Affymetrix U133A , Agilent g4502 , Human Exon array ). We first removed potential plate level batch effects with ComBat [ 25 ] for all expression datasets . As was done in the TCGA ovarian cancer study [ 23 ], we combined the three separate expression measurements for each of 11 , 864 genes to produce a single estimate of gene expression level by performing a factor analysis across the three studies .__label__Material|Data|Use
However , the association between subjective clinical measures and an individual ’ s © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Kobsar et al . Journal of NeuroEngineering and Rehabilitation ( 2017 ) 14 : 94 Page 2 of 10 response to treatment appears to be limited and inconsistent [ 7 ].__label__Supplement|License|Other
We also find these outlets valuable for occasional rapid technical exchanges with collaborating databases . Our blog ( http :// blog . guidetopharmacology . org /) includes detailed release descriptions , new features , and technical ‘ how to ’ items . One of us ( CS ) maintains an individual technical blog where GtoPdb topics are sometimes coupled by being briefly introduced in the GtoPdb blog but expanded on in the individual posts ( _CITE_ ). Our Slideshare account ( http :// www . slideshare . net / GuidetoPHARM ) is used for sharing slide sets and posters with the community and has proved popular . Users will find that presentations include descriptions of content , mining approaches and utilities that extend beyond what is documented on the site .__label__Supplement|Website|Produce
Raw signal intensities were log2 - transformed and normalized by RMA and quantile algorithms51 with Affymetrix ® Expression ConsoleTM 1 . 1 software . Gene set enrichment analysis ( www . broadinstitute . org / gsea ) was performed to determine whether the predefined gene sets were enriched in control or MTM - POT1a treated aged HSCs . The microarray data have been deposited in the Gene expression omnibus ( GEO , _CITE_ ) database , and have been assigned accession numbers GSE86386 and GSE85016 . Production of MTM proteins . The plasmid pET28a - MTM / N1 - POT1a or POT1 was constructed by cloning full - length Pot1a or POT1 cDNA into the pET28aMTM N1 vector ( Supplementary Fig .__label__Material|Data|Use
They found that when unpublished data were incorporated in these reviews , only 7 % of these meta - analyses predicted the drug in question to have © The Author ( s ) 2016 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Yerokhin et al . BMC Res Notes ( 2016 ) 9 : 475 Page 2 of 13 the same efficacy .__label__Supplement|License|Other
Patient characteristics such as age , gender , Ig types ( immunoglobuline ), hemoglobin level at presentation , bony lytic lesions , and renal impairment were noted . Details regarding types of chemotherapy regimens were also recorded . Patients were _CITE___label__Supplement|Document|Produce
© 2016 by the authors ; licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ( CC - BY ) license ( _CITE_ ).__label__Supplement|License|Other
Ensembl Genomes is updated 4 – 5 times a year in synchrony with updates to Ensembl , utilising the same software as the corresponding Ensembl release . The overall suite of Ensembl Genomes interfaces mirrors those provided for vertebrate genomes in Ensembl , allowing users to access genomic data from across the tree of life in a consistent manner . In addition , Ensembl Genomes contributes to collaborative database projects focused on various domains of life , including Gramene ( _CITE_ ) ( 8 ) for plants , PhytoPath ( http :// phytopathdb . org ) ( 9 ) for plant pathogens , VectorBase ( http :// www . vectorbase . org ) ( 10 ) for invertebrate vectors of human pathogens , and WormBase ( http :// www . wormbase . org ) ( 11 ) for helminths . In these projects , we work with our partners to develop common datasets , which are made available through both Ensembl Genomes and additional project - specific interfaces .__label__Supplement|Website|Introduce
It is well known that most of the problems arising from learning on class - imbalanced data arise in the region where the two class - specific densities overlap . When the difference between the class - specific densities is large enough , the class - imbalance does not cause biased classification for the classifiers that we considered , even in the highdimensional setting [ 7 ]. The other reason is that when Blagus and Lusa BMC Bioinformatics 2013 , 14 : 106 Page 13 of 16 _CITE_ a very large number of variables is measured for each subject , in most situations the vast majority of variables do not differentiate the classes and the signal - to - noise ratio can be extreme . For example , Sotiriou et al . [ 36 ] identified 606 out of the 7 , 650 measured genes as discriminating ER + from ER - samples in their gene expression study ; at the same time ER status was the known clinico - pathological breast cancer phenotype for which the largest number of variables was identified ( 137 out of the 7 , 650 genes discriminated grade , 11 out of the 7 , 650 node positivity , 3 out of the 7 , 650 tumor size and 13 out of the 7 , 650 menopausal status ).__label__Supplement|Paper|Introduce
Although many packages are highly relevant to medicine [ 21 , 22 ], there are only a limited number of packages for analyzing and modeling ranking data . There are some basic tools for ranking data , for example the Kendall package and the pspearman package for the computation of Kendall and Spearman rank correlation . Nonetheless , to the best of the authors ’ knowledge , the only statistical model currently available in R is the RMallow package ( _CITE_ ) for fitting a mixture of Mallows ’ models [ 23 ]. Here , we present pmr ( probability models for ranking data ), an R package for analyzing and modeling ranking data with a bundle of statistical tools . A review of statistical analysis for ranking data is given , prior to demonstrating the implementation of pmr .__label__Method|Code|Introduce
Genotype , clinical and background data were generated as part of an ongoing larger project ( _CITE_ net ; Drakeley et al ., in preparation ). In brief , genetic data refer to a total of 165 SNPs predominantly from malaria candidate genes ( e . g ., the sickle cell gene ; HbS ), ofwhich 110 passed our stringent quality control step ( minor allele frequency > 5 %, p - value for Hardy - Weinberg equilibrium in malaria - negative individuals > 0 . 001 , and missing data per SNP or individual > 10 %).__label__Method|Tool|Introduce
The molecular formulae of the pseudo - molecularions ([ M - H ]− or ([ M + H ]+) representing possible biomarkers were computed and selected based on the criterion that the mass difference between the measured and calculated mass was at / or below 5 mDa . In addition , a number of parameters , including isotopic fit ( iFit ) and double bond equivalent ( DBE ) values , were taken into account in order to increase the level of confidence in the molecular formulae obtained . The elemental composition was then searched against online libraries / databases : Dictionary of Natural Products ( DNP ) ( dnp . chemnetbase . com ), PubChem ( www . pubchem . ncbi . nlm . nih . gov ), Chemspider ( www . chemspider . com ), AraCyc ( www . arabidopsis . org / tools / aracyc ), PlantCyc PLOS ONE | DOI : 10 . 1371 / journal . pone . 0163572 September 22 , 2016 6 / 26 The Lipopolysaccharide - Induced Metabolome Signature in Arabidopsis ( www . plantcyc . org ), MetaCyc ( www . metaCyc . org ), KEGG ( www . genome . jp / kegg ), Metabolomics workbench ( _CITE_ ) and METLIN ( metlin . scripps . edu ). Moreover , annotation was based on interpretation of mass fragmentation patterns , MS / MS spectra , mass spectral library searches as well as published literature and datasets [ 47 ]. Another tool employed in the study for metabolite annotation / putative identification was the PUTMEDID_LC - MS workflow [ 7 , 48 ] that operates on the Taverna workbench ( http :// www . taverna . org . uk ).__label__Supplement|Website|Introduce
We demonstrate a positive correlation between the use of training data from species which are closely related to a species of interest and classification performance on a hold - out species , providing clear evidence that our speciesspecific methods successfully leverage phylogenetic information for classification . We further demonstrate improved performance of two SVM - based classifiers and one random forest - based classifier for miRNA studies on reptile , insect , plant and virus genomes . Trained species - specific miRNA prediction systems and all training and test data are freely available at _CITE_ As previously mentioned , this approach is particularly useful for niche species that are of important model organisms of study , but suffer from being phylogenetically distant from the model organisms that are typically used to create singlespecies or multi - species pooled training data .__label__Method|Tool|Use
We demonstrate a positive correlation between the use of training data from species which are closely related to a species of interest and classification performance on a hold - out species , providing clear evidence that our speciesspecific methods successfully leverage phylogenetic information for classification . We further demonstrate improved performance of two SVM - based classifiers and one random forest - based classifier for miRNA studies on reptile , insect , plant and virus genomes . Trained species - specific miRNA prediction systems and all training and test data are freely available at _CITE_ As previously mentioned , this approach is particularly useful for niche species that are of important model organisms of study , but suffer from being phylogenetically distant from the model organisms that are typically used to create singlespecies or multi - species pooled training data .__label__Material|Data|Use
A total of 99 . 9 % of Taiwan ’ s population is enrolled in the National Health Insurance ( NHI ) program , which was launched in 1995 . The electronic database of this program , the National Health Insurance Research Database ( NHIRD ), is maintained by the National Health Research Institutes and contains registration files as well as inpatient , outpatient , and pharmacy claims data ( _CITE_ ). All datasets of the NHIRD can be linked using scrambled personal identifications . The data used in this study were a subset of the NHIRD consisting of the registration files and longitudinal medical records of 1 million beneficiaries randomly selected from the NHI program .__label__Material|Data|Introduce
S5 shows that the mean IP3 activation latency has a minimum at intermediate  value . The parameters and transition rates for the CM are given in Tables S1 – S3 , whereas those for the DYKM are given in Tables S4 and S5 . The supplemental material is available at _CITE___label__Supplement|Document|Produce
iRNA - seq can then either analyze these summarized counts for differential expression by standard or blocked two - condition comparison using edgeR ( 24 ) or provide summarized non - normalized read counts for other purposes . iRNA - seq comes with gene , exon and intron lists for the human ( hg19 ), mouse ( mm9 ) and rat ( rn5 ) genomes , as well as a script to generate custom list for other genome versions or organisms . The pipeline and instructions on how to use it is available at : _CITE___label__Supplement|Document|Produce
All metabolic reconstructions generated by this study are publicly available at http :// hmpdacc . org / HMMRC . Taxonomic abundances derived from shotgun data are provided at http :// hmpdacc . org / HMSCP , and input Illumina reads at http :// hmpdacc . org / HMIWGS . The open source HUMAnN software can be obtained at _CITE___label__Method|Tool|Produce
Of these , between seven and nine features were selected as significantly different between the two groups ( Table 2 ). At peptide level , Tuli et al . Proteome Science 2012 , 10 : 13 _CITE_ Page 5 of 11 Figure 2 Base peak chromatograms of the MassPrep peptides . The chromatograms are zoomed into each of the 13 unique features whose m / z and retention time values match with those of the MassPrep peptides , in comparison of serum with spike - in peptides (+ MassPrep ) and serum alone (- MassPrep ) groups . Tuli et al .__label__Supplement|Paper|Introduce
Python ’ s multiprocessing module was used to implement batch processing based on the number of processors requested by the user . Bpipe ’ s inbuilt parallelization of task blocks feature was used towards computing genome coverage and variation information simultaneously . For benchmarking , a dataset of size 4 . 38 Gb , comprising of 100 bp paired - end reads were simulated from chickpea genome [ 5 ] using ART simulator [ 16 ] ( v2 . 1 . 8 ) ( _CITE_ ) at 10 fold genome coverage . The dataset is publicly available ( https :// github . com / CEG - ICRISAT / NGS - QCbox / blob / master / README . md # datasets - used - for - testing ) on iPlant resource [ 17 ] ( www . iplantcollaborative . org ).__label__Method|Tool|Use
While this is an exciting prospect , it must be appreciated that this study was limited in scope , and the experimental data was gathered over a period of several days in a relatively confined region . More extensive studies should be invoked to validate these conclusions , and ( if positive ) identify the boundaries of the correlations , as these will be dynamic in terms of spatial distribution and the relative impact of various influencing factors . Supplementary Materials : The supplementary materials are available online at _CITE_ Acknowledgments : The authors would like to acknowledge funding from SmartBay National Infrastructure Access Programme for funding for the project . The technical support provided by Shane Burke ( Smart Bay Ireland ) during the field campaign .__label__Supplement|Document|Produce
Specifically , we measured whether prevalent combinations of comorbidities ( as defined above ) produced statistically significant associations with VTE in multivariate models . Because we found Kapoor et al . BMC Geriatrics 2010 , 10 : 63 Page 3 of 7 _CITE_ these to be present in the TKR cohort , for the ease of interpretability , we report the associations between co - occurring comorbidities and outcomes using a categorical comorbidity variable for both the THR and TKR cohorts . We created a ten level categorical variable with a separate , mutually exclusive , level for each of nine comorbidities or co - occurring comorbidities ( with 1 additional for all other combinations of two or more comorbidities ). While this limited the population size for each comorbidity group , it allowed us to compare the risk of VTE for groups of older adults with single or co - occurring comorbidities against a common reference group of older adults without any of the nine comorbidities .__label__Supplement|Paper|Introduce
Immunohistochemical staining was analyzed in a Zeiss AxioExaminer D1 microscope ( 10x , 20x and 40x water immersion objectives ) or a Zeiss LSM700 confocal ( 40x and 63x oil - immersion objectives ), and captured images were processed by adjusting contrast in ImageJ ( 1 . 42q , National Institutes of Health , Bethesda , MD , USA ) to reduce background staining . Ca2 + imaging time traces were analyzed with a recently published method ( Malmersjo et al ., 2013 ; Smedler et al ., 2014 ). Regions of interest were marked for all cells based on the standard deviation of fluorescence intensity over time , by using a semiautomatic - adapted ImageJ script kindly provided by Dr . John Hayes ( The College of William and Mary , Williamsburg , VA , USA , _CITE_ ). The mean intensity value and coordinates were measured using ImageJ . Average intensities of regions of interest were quantified for each frame , and dynamic fluorescence signals were normalized to baseline values .__label__Method|Code|Use
It is important to note that the entire FIGURE 2 | Sagittal expression energy images of a pattern NE and OE gene . CamK2a displays pattern NE ( image series 79360274 ) and S100b shows pattern OE ( image series 924 ). Images were downloaded from the ABA web site ( _CITE_ ). While all expression information for the analysis is from coronal assays , we selected a sagittal view to better show interregional variability in a single section . FIGURE 3 | Principal components analysis .__label__Supplement|Website|Use
It is important to note that the entire FIGURE 2 | Sagittal expression energy images of a pattern NE and OE gene . CamK2a displays pattern NE ( image series 79360274 ) and S100b shows pattern OE ( image series 924 ). Images were downloaded from the ABA web site ( _CITE_ ). While all expression information for the analysis is from coronal assays , we selected a sagittal view to better show interregional variability in a single section . FIGURE 3 | Principal components analysis .__label__Supplement|Media|Use
Data from each of the aforementioned survey rounds were included in this study , with many of the selected variables representing the same questions asked at multiple time points ( WLS survey rounds ). In addition , DNA saliva samples were provided by a subset of the WLS ( 4562 graduates ) and 77 single - nucleotide polymorphisms ( SNPs ) were genotyped for each of these participants in 2009 , providing genetic data for a subset of the WLS as well . Additional information about the WLS survey data and participants can be found elsewhere74 75 ( _CITE_ ). The WLS has enjoyed high response rates across multiple survey waves . The data for the current analyses come primarily from the 1993 , 2004 , and 2011 survey waves when the cohort was 53 , 64 and 71 years old , respectively .__label__Supplement|Document|Use
However , the validity of THIN data on smoking prevalence at the regional level , which may be less precise as a result of smaller sample sizes and reduced representativeness , has yet to be demonstrated . © 2011 Langley et al ; licensee BioMed Central Ltd . This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited . Langley et al . BMC Public Health 2011 , 11 : 773 Page 2 of 6 http :// www . biomedcentral . com / 1471 - 2458 / 11 / 773 We therefore carried out a validation study comparing estimates of regional smoking prevalence from THIN with those from the GLF to assess whether THIN data can be used to monitor regional variation and trends in smoking prevalence .__label__Supplement|License|Other
Multiple residues can be grouped and analyzed simultaneously using built - in cluster analysis ( individual or global fit ). Maximum flexibility of data entry has been included . For example , peak lists containing peak intensities that are created by any other software can be imported ; protein sequences are read either from PDB files , retrieved from the internet using UniProt identifier _CITE_ expasy . ch or can be added manually ; CPMG frequencies are read directly from Bruker VD ( variable delay ) lists . Furthermore , NESSY is linked to the Bruker Protein Dynamic Center ( PDC , starting with version 1 . 1 , http :// www . bruker - biospin . com , in collaboration with Dr . Klaus - Peter Neidig ) so that projects can be exported in PDC and directly read into NESSY for extended data analysis .__label__Method|Tool|Use
Competing interests : The authors LB and JP are employed by The Biodiversity Consultancy – a commercial enterprise , and the authors KMB , CM , SB , MIJ , MJT , LVW and SEB are employed by WCMC which receives some funding from commercial companies via the Proteus partnership . This does not alter the authors ’ adherence to PLOS ONE policies on sharing data and materials . PLOS ONE | _CITE_ March 22 , 2018 2 / 16 Global terrestrial Critical Habitat screening and associated management plans . However , none of these tools are designed to help identify Critical Habitat as stipulated by IFC PS6 . In 2015 , Martin et al .__label__Supplement|Paper|Introduce
The partners felt that AGRIS backbone data should continue to be bibliographic metadata , but felt that linked data technologies should be fully exploited to allow the inclusion of other relevant content types . To further develop the coverage of AGRIS content and to prevent stagnation , the AGRIS team aims to work out a new adequate subject scope for the AGRIS knowledgebase and discover new sources of information and data in collaboration with community partners . There are possibilities of linking AGRIS with science blogs and automatically updated feeds , and of further strengthening the relationship between AGRIS and AgriFeeds ( _CITE_ ) ( for example , http :// esciencenews . com and other feeds from scientific presses and universities ). Using data mining , the AGRIS database could be a way to access already existing information . In the AGRIS e - consultation users expressed their demand for more additional data like statistics , multimedia , price data , daily crops prices etc .__label__Material|Data|Use
Racle et al . eLife 2017 ; 6 : e26476 . DOI : _CITE_ 14 of 25 Tools and resources Cancer Biology Computational and Systems Biology Cell proportions were based on viable cells only . In parallel total RNA was extracted using the RNAeasy Plus mini kit ( Qiagen ) following the manufactures ’ protocol . Starting material always contained minimum 0 . 2 x 106 cells .__label__Supplement|Paper|Introduce
These data support the existence of a continuum of quiescence depths . It is well documented that the levels of Cyclins D and E are dramatically reduced in cells forced into quiescence via serum starvation [ 42 , 67 ]. Here we compared the dynamics of PLOS Biology | _CITE_ September 11 , 2017 15 / 25 A single - cell chronology of the human cell cycle multiple key cell - cycle proteins , including Cyclin D1 and Cyclin E , in forced versus spontaneous quiescence . In contrast to the declining levels of Cyclin D1 and Cyclin E in serum - starved or contact - inhibited cells , the levels of Cyclin D1 and Cyclin E rise while cells are in the quiescent CDK2low state . We confirmed this result using endogenously tagged mCitrine - Cyclin D1 and further showed that the high levels of Cyclin D1 in CDK2low cells arise because of reduced cullin - RING ligase - mediated protein degradation in the CDK2low state .__label__Supplement|Paper|Compare
In this study , the analyses were performed on a data set consisting of hyper - variable V6 sequences of the 16S rRNA gene , which were obtained from the application of 454 MPTS on temperate subtidal sandy samples at three sediment depth layers ( 0 – 15 cm depth , with a 5 - cm interval ) taken over 2 years ( 2005 – 2006 ). Detailed sample processing and DNA extraction has been described earlier ( 10 ) and the 454 MPTS of the extracted DNA was processed as described previously ( 5 ). The output from 454 MPTS was retrieved from the publicly available Visualization and Analysis of Microbial Populations Structure ( VAMPS ) web site ( _CITE_ ). An automatic annotation pipeline [ Global Alignment for Sequence Taxonomy ( GAST ) ( 5 )] using several known databases ( Entrez Genome , RDP and SILVA ) allowed the taxonomic assignment of the sequences . Despite the limitations of current databases , only 6 % of sequences from this data set were not taxonomically identified at all .__label__Supplement|Website|Use
Phys . J . C ( 2014 ) 74 : 2981 Page 95 of 241 2981 the PNDME ( _CITE_ ) collaboration . The matrix elements relevant to the quark electric dipole moments are rather straightforward , involving isovector and isoscalar nucleon tensor matrix elements . The latter one requires disconnected diagrams with extra explicit quark loops .__label__Supplement|Paper|Introduce
Copyright : – 2016 Granato SA et al ; licensee International AIDS Society . This is an Open Access article distributed under the terms of the Creative Commons Attribution 3 . 0 Unported ( CC BY 3 . 0 ) License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .__label__Supplement|License|Other
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ The Creative Commons Public Domain Dedication waiver http :// creativecommons . org / publicdomain / zero / 1 . 0 / applies to the metadata files made available in this article . © The Author ( s ) 2018 SCIENTIFIC DATA 1 5 : 180082 1 DOI : 10 . 1038 / sdata . 2018 . 82 12__label__Supplement|License|Other
The main drawback of the tools described above is that they are not programmatically accessible and cannot be integrated into an analysis pipeline requiring batch processing . In addition , the required data upload step does not scale well for large and complex datasets . The statistical programming environment R _CITE_ along with the Bioconductor Project http :// www . bioconductor . org provide a plethora of methods and tools to analyze and visualize data . The software package described in this paper , GenomeGraphs , builds on this functionality by providing an integrated API for direct visualization of data from a variety of sources . GenomeGraphs allows complex customization to facilitate a more complete integration and representation of genomic datasets .__label__Method|Code|Use
obtained . ORF names of genes were checked by using the unified phosphoproteome data to determine whether the encoded protein was identified as a phosphoprotein . PPI network The S . cerevisiae PPI network was obtained as XML files ( Scere20081014 ) from DIP ( Database of Interacting Proteins ; _CITE_ ) [ 33 ]. We eliminated each interaction entry including three or more “ interactors ”( e . g ., in which multiple prey proteins were detected for one bait protein in one experimental assay ) and used only those including two “ interactors .” Every node in the PPI network was labeled by its corresponding UniProt ID provided in the same XML file . For the PPI assay , PPI data were further grouped into four categories : all kinds of experimental methods (“ ALL ”), yeast two - hybrid (“ Y2H ”), co - immunoprecipitation (“ IMM ”), and tandem affinity purification (“ TAP ”).__label__Material|Data|Use
In May 2011 , federal regulations prohibited vessels from approaching whales within 200 yards ( 183 m ) of whales , or positioning themselves within 400 yards ( 366 m ) of the path of a whale [ 34 ]. Research vessels operating under permit are exempt from federal regulations . An additional guideline recommends that vessels do not travel at speeds faster than 7 knots ( 13 kph ) within 400 yd ( 366 m ) of a whale ( _CITE_ ). These regulations apply only in U . S . waters . In Canada , whale watching is only subjected to the less stringent voluntary guideline of a 100 m minimum approach distance .__label__Supplement|Document|Introduce
Primary ( Level 1 ) WES data was obtained to create the TCGA dat sets . To access the Level 1 DNA sequencing data for breast and ovarian tumors from TCGA , a project request was submitted and approved by the National Center for Biotechnology Information Genotypes and Phenotypes Database ( NCBI dbGaP ) Data Access Request system , Protocol # 5309 “ BRCA1 and BRCA2 mutations in breast and ovarian cancer ”. Breast and ovarian samples potentially with controlledaccess Level 1 WES binary alignment (. bam ) files were identified from the Genomic Data Commons ( GDC ) ( _CITE_ ) ( n = 1098 breast tumors and 590 ovarian tumors ) ( Supplementary Fig . 1 ). After filtering for samples with WES data not available on the GDC commons ( n = 406 ) and samples not passing our sequencing pipeline quality control analysis ( n = 104 ), 1178 tumor / normal pairs were subjected to downstream analysis .__label__Supplement|Website|Use
Project name : fastBMA Project home page : _CITE_ Operating system ( s ): Linux ( MacOS and Windows support provided through the Docker container [ 25 ] and Bioconductor package [ 24 ]) Programming language : C ++ Other requirements : gcc version & gt ; 4 . 8 , OpenBLAS , mpich2 ( if MPI desired ) to compile code__label__Method|Code|Produce
Project name : fastBMA Project home page : _CITE_ Operating system ( s ): Linux ( MacOS and Windows support provided through the Docker container [ 25 ] and Bioconductor package [ 24 ]) Programming language : C ++ Other requirements : gcc version & gt ; 4 . 8 , OpenBLAS , mpich2 ( if MPI desired ) to compile code__label__Supplement|Website|Produce
First , all six are flanked by TTAA terminal direct repeats ( TDRs ), one copy of which is retained in the MAC ; this sequence feature is shared by only 2 % of all Tetrahymena IESs . Second , these six IESs have a distinctive terminal inverted repeat ( TIR ), internal to the TTAA direct repeat , with a consensus of 5 ’- CACTTT - 3 ’ ( Figure 8B , Supplementary file 3D ). This TIR resembles that of PiggyBac TEs of several species ( as found in RepBase : _CITE_ and ( Xu et al ., 2006 ), and also the two full - length piggyBac consensus sequences annotated in the Tetrahymena MIC genome ( Figure 6 — source datas 1 ; 5 ’- CCCT ( A / T ) T - 3 ’ for Contig [ 0117 ] and 5 ’- CCC ( A / T )( C / T ) T - 3 ’ for R = 3481 ). Third , the six coding region IESs are all exceptionally short ; in fact , they include the three shortest IESs we identified ( 136 , 188 and 194 bp , Supplementary file 3D ). Apart from their size and terminal sequences , no other conserved sequence features were detected , either within or flanking the IESs .__label__Material|Data|Introduce
On the other hand , none of the models mentioned in the background section seem to agree with these experimental observations . Formal statistical reasoning could unravel the underlying theoretical error model that leads to the power law relationship that was observed to be at the basis of the variance versus mean dependence in replicated microarray data . A PLGEM - based method successfully detects differential expression In spite of the lack of a theoretical statistical model , the empirical model presented here has proven its Page 9 of 12 ( page number not for citation purposes ) BMC Bioinformatics 2004 , 5 : 203 _CITE_ applicability in the identification of DEG , providing improved results under a wide range of different testing conditions . In comparison to other commonly used DEG identification methods , the proposed approach demonstrated improved specificity and sensitivity on the Latin Square data set and robustness to decreasing number of replicates on the 16iDC + LPS data set . The good performance of our proposed method is reasonably due to the fact that it relies on a global error model .__label__Supplement|Paper|Introduce
Despite its high efficacy , there is controversy about its psychological side effects . Objectives To summarize the efficacy and safety of mefloquine used as prophylaxis for malaria in travellers . Search methods We searched the Cochrane Infectious Diseases Group Specialized Register ; the Cochrane Central Register of Controlled Trials ( CENTRAL ), published on the Cochrane Library ; MEDLINE ; Embase ( OVID ); TOXLINE ( _CITE_ toxline . htm ); and LILACS . We also searched the World Health Organization ( WHO ) International Clinical Trials Registry Platform ( ICTRP ; http :// www . who . int / ictrp / en /) and ClinicalTrials . gov ( https :// clinicaltrials . gov / ct2 / home ) for trials in progress , using ’ mefloquine ’, ’ Lariam ’, and ’ malaria ’ as search terms . The search date was 22 June 2017 .__label__Supplement|Website|Introduce
We received 2634 responses . The responses from key stage 5 pupils were from a broad spectrum of pupils studying a wide variety of subjects . The questionnaires were scanned by the data collection company Kendata ( _CITE_ ), and an Excel database of responses was compiled and then imported into SPSS version 22 . For statistical analysis the year groups were collated according to key stage ( Table 2 ) and the Likert scale was coded in SPSS as strongly agree / agree ( 1 ); neither agree not disagree or unsure ( 2 ) and disagree / strongly disagree ( 3 ). The data was analysed using Pearson ’ s chi - square test .__label__Material|Data|Use
Thøgersen et al . BMC Bioinformatics 2013 , 14 : 279 Page 14 of 15 _CITE_ the transcriptomic profiles directly , whereas in this analysis the two proposed archetypal gene expression profiles for archetype 6 and 7 are compared , where archetype 6 represents hypermutators and archetype 7 represents non - mutators . We suggest that the characteristics of the archetype 6 are representative for general hypermutator characteristics , since archetype 6 accounts for hypermutators across different clonal types and across different experimental conditions ( study 3 and 4 ). The gene expression profile of archetype 6 therefore most likely can be linked to the hypermutator trait and its influence on adaptation in the CF lung .__label__Supplement|Paper|Compare
Promoters of PMDEGs were aligned with TF - binding motifs in the DMR - promoter - overlapping regions identified above using the MotifDb R package [ 32 ] ( criterion : matching value in pulsewidth modulation algorithm > 85 %). Then the corresponding TF - target pairs and Protein - Protein interactions ( PPIs ) of targets obtained from the STRING database ( _CITE_ ) [ 33 ] were used to construct a TF - target network , which was visualized using Cytoscape software [ 34 ]. Genes 2018 , 9 , 32 5 of 19 2 . 10 . Construction of micro RNA – Target Network Based on the information of miRNA – target gene pairs in the online database miRDB [ 35 ], the target genes of MDEmiRNAs were predicted ( criterion : target score > 50 ), and the MDEGs among these target genes were selected to construct a miRNA – target network , which was visualized by Cytoscape [ 34 ].__label__Material|Data|Use
( PDF 753 kb ) Additional file 14 : Ingenuity pathway analysis of DEGs after overexpressing either BRD1 - S or BRD1 - L . This Excel spreadsheet contains a list of enriched canonical pathways identified by Ingenuity pathway analysis using the identified DEGs after upregulating either BRD1 - S or BRD1 - L . ( XLSX 16 kb ) Additional file 15 : Spatiotemporal mRNA expression of BRD1 in human brain . A RNA - seq data ( obtained from Brainspan ; _CITE_ ) showing the temporal expression of BRD1 across 26 brain regions ; primary auditory cortex , core ( A1C ), amygdaloid complex ( AMY ), cerebellar cortex ( CBC ), cerebellum ( CB ), caudal ganglionic eminence ( CGE ), dorsolateral prefrontal cortex ( DFC ), dorsal thalamus ( DTH ), hippocampus ( hippocampal formation ) ( HIP ), posteroventral ( inferior ) parietal cortex ( IPC ), inferolateral temporal cortex ( area TEv ) ( ITC ), lateral ganglionic eminence ( LGE ), primary motor cortex ( area M1 ) ( M1C ), primary motor - sensory cortex ( samples ) ( M1C - S1C ), mediodorsal nucleus of thalamus ( MD ), anterior ( rostral ) cingulate ( medial prefrontal ) cortex ( MFC ), medial ganglionic eminence ( MGE ), occipital neocortex ( Ocx ), orbital frontal cortex ( OFC ), parietal neocortex ( PCx ), primary somatosensory cortex ( area S1 ) ( S1C ), posterior ( caudal ) superior temporal cortex ( area 22c ) ( STC ), striatum ( STR ), temporal neocortex ( TCx ), upper ( rostral ) rhombic lip ( URL ), primary visual cortex ( striate cortex ) ( V1C ), ventrolateral prefrontal cortex ( VFC ). Age on the x - axis is shown as log10 ( days ) ( log10 ( age )) and the gene expression is shown as the average reads per kilobase per million ( avg_rpkm ). B Expression microarray data ( obtained from the Human Brain Transcriptome , HBT ; http :// hbatlas . org ) showing the temporal expression of BRD1 across six brain regions : neocortex ( NCX ), hippocampus ( HIP ), amygdala ( AMY ), striatum ( STR ), mediodorsal nucleus of thalamus ( MD ), cerebellar cortex ( CBC ).__label__Material|Data|Use
PE libraries were sequenced on an Illumina Hiseq 2000 sequencing system to generate 2 × 125 PE reads at Beijing Genomics Institute at Shenzhen . RNA - seq mapping and differentially expressed genes After the low - quality reads had been trimmed and the adapters had been removed , the clean RNA - seq reads were mapped to the reference genome using Tophat2 ( version 2 . 0 . 13 ) with default parameters [ 60 ]. The genome sequence and gene structure annotation data of Wuzhishan pig and Sus scrofa ( strain : Duroc ) were downloaded from _CITE_ 100031 and Ensembl , respectively . The unique mapped reads were used to calculate the number of reads that mapped to every gene model using HTseq [ 61 ]. Multiple isoforms that belong to one gene were filtered , with the longest isoform being retained as representative of the group .__label__Material|Data|Use
Although previous systematic reviews have assessed individual outcomes [ 8 – 12 , 101 , 105 – 109 ], we have found no other published reviews synthesizing the evidence for all long - term risks and benefits of cesarean delivery relating to mother , baby , and subsequent pregnancies . There is a lack of documented evidence about medium - to long - term outcomes in women and their babies after a planned cesarean delivery or a planned vaginal birth [ 4 ]. Therefore , the findings of this review will form a valuable and necessary addition to discussions about mode PLOS Medicine | _CITE_ January23 , 2018 12 / 22 Meta - analysis of the long - term risks and benefits of cesarean delivery of delivery and consenting for planned cesarean delivery . Patients may attribute different weight to the outcomes ; for example , some might prioritize minimizing the risk of stillbirth in a future pregnancy , while others might prioritize minimizing the risk of respiratory morbidity for their baby . The information included in this review will allow women ( and their caregivers ) to make more personally relevant decisions .__label__Supplement|Paper|Extent
A Web Portal . The web portal was developed using ASP . NET in Microsoft Visual Studio 2010 and has been deployed in IIS server . It can be accessed via _CITE_ This web portal is useful for both clinical researchers and molecular biologists . For clinical researchers , they can learn the molecular mechanism of colorectal cancer , which may lead to better understanding about the diagnosis , therapy , and prognosis of colorectal cancer .__label__Supplement|Website|Produce
Reads can be filtered by the number of mismatches to the hairpin sequence , the read count and by experiment . Each experiment is annotated with species , tissue , stage and methodology information . These tags enable the user to search for experiments by tissue expression on the miRBase search page ( _CITE_ ). Read counts for mature microRNAs are commonly used as a proxy for relative expression levels . As the number of deepsequencing experiments increases , these data provide extensive information about the expression profiles of microRNAs across tissues , stages and organisms .__label__Supplement|Website|Use
Additional information Supplementary Information accompanies this paper at doi : 10 . 1038 / s41467 - 017 - 00680 - 8 . Competing interests : The authors declare no competing financial interests . Reprints and permission information is available online at _CITE_ Publisher & apos ; s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations . Open Access This article is licensed under a Creative Commons Attribution 4 . 0 International License , which permits use , sharing , adaptation , distribution and reproduction in any medium or format , as long as you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material .__label__Supplement|License|Other
Serologically determined O - groups were identified in the GenBank entries or associated literature for 40 of these genomes ( and H - types for 20 ) ( Table S2 ). The Achtman MLST scheme for E . coli ( Wirth et al ., 2006 ), now hosted at Warwick University ( http :// mlst . warwick . ac . uk / mlst / dbs / Ecoli ), was downloaded using the getmlst . py script included in the SRST2 package ( https :// github . com / katholt / srst2 ). An SRST2 - formatted version of the ARG - ANNOT antimicrobial resistance gene database ( Gupta et al ., 2013 ) was downloaded from _CITE_ Assembly and BLAST analysis . Illumina reads of 100 bp paired end were generated previously for 197 EPEC isolates ( Ingle et al ., 2016 ) and assembled using Velvet and Velvet Optimiser ( Zerbino & Birney , 2008 ). Reads and assemblies are available in the European Nucleotide Archive ( ENA ) under ERP001141 ( Ingle et al ., 2016 ).__label__Material|Data|Use
Expression patterns for putative biomarker sets can be constructed , and the performance of biomarker sets for detecting phenotypes can be tested . Moreover , the DrugMatrix Toolbox provides tools to mine the literature and to identify enriched literature annotations in groups of expression profiles . ToxFX ( _CITE_ ) is a rapid , automated toxicogenomics analysis tool including quality control , scoring of expression profiles with drug signatures , DEGs analysis and pathway analysis . ToxFX generates a detailed toxicogenomics report in five minutes .__label__Method|Tool|Introduce
In detail , 2 , 578 binding regions lie completely within repeat sequences marked by a merged set consisting of “ Repeat Masker ”, “ Segmental Dup ” or “ Simple Repeat ” data sets from the table browser of the UCSC Genome Browser , 35 binding regions lie within the Immunoglobulin heavy chain locus ( chr14 : 106053226 – 106330470 ) and 285 fall in the MHC region ( chr6 : 28477797 - 33448354 ). Motif word identification . We searched for the instances of CTCF motif in the discovered binding regions using the CTCF canonical 19 bp position weight matrix downloaded from the JASPAR database ( _CITE_ ). We extracted DNA sequences at the identified binding regions from human genome reference GRCh37 to construct a sequence database . The search was then performed using the software FIMO [ 54 ] in the MEME tool suite [ 55 ] using parameter – threshold 1E - 4 .__label__Material|Data|Use
PanDDA is implemented in Python and relies heavily on the CCTBX31 . It has been tested extensively for robustness and usability by users of Diamond ’ s XChem fragment screening facility . Source code is available on bitbucket ( _CITE_ ) or as part of CCP4 ( ref . 28 ). A manual and tutorial are available at https :// pandda . bitbucket . io .__label__Method|Code|Produce
family The full scientific name of the family in which the taxon is classified ( http :// rs . tdwg . org / dwc / terms / family ). familyNameId An identifier for the family name . genus The full scientific name of the genus in which the taxon is classified ( _CITE_ ). subgenus The full scientific name of the subgenus in which the taxon is classified . Values include the genus to avoid homonym confusion ( http :// rs . tdwg . org / dwc / terms / subgenus ).__label__Supplement|Document|Introduce
The theoretical input for each block would be 50 %. The phosphate conditions in the media are indicated on the right side ( Pi treatments ), as in panel B . Numerical values that underlie the data displayed in the panels are in _CITE_ MDS , multidimensional scaling ; Pi , phosphate ; 16S , small subunit ribosomal rRNA gene . ( TIF ) S7 Fig .__label__Material|Data|Use
; licensee BioMed Central Ltd . This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http :// creativecommons . org / licenses / by / 2 . 0 ), which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Tan et al . BMC Proceedings 2014 , 8 ( Suppl 1 ): S82 Page 2 of 6 http :// www . biomedcentral . com / 1753 - 6561 / 8 / S1 / S82 NIH , Genetic Analysis Workshop 18 ( GAW18 ) provides whole genome sequencing data and longitudinal blood pressure measurements on large pedigrees .__label__Supplement|License|Other
TE frequency estimates based on pooled - PCR approach were categorized in four classes : ‘ very rare ’ ( i . e ., TE frequency < 1 . 5 %), ‘ rare ’ ( i . e ., TE frequency ∼ 2 – 15 % but not absent ), ‘ common ’ ( i . e ., TE frequency ∼ 10 – 98 %) and ‘ fixed ’ ( i . e ., TE frequency > 98 %) ( 37 ). The D . melanogaster reference genome ( release 5 ) was downloaded from Flybase website ( http :// flybase . org ). The annotations of the TE insertions were extracted from the release 5 . 43 ( _CITE_ ). The resequencing data from 86 D . melanogaster DGRP single strains from the freeze__label__Method|Tool|Use
Based on a model we make guesses regarding the value of an instance . A score , skl , is given for each instance based on the guess , k E { 0 , 1 }, and the actual state , l E { 0 , 1 }. The four score values , { s00 , s11 , s10 , s01 }, will be chosen such that neither of the two states are preferred PLOS ONE | _CITE_ December 13 , 2017 10 / 16 Correlations between human mobility and social interaction reveal general activity patterns on average , while correct / incorrect guesses for all signal values yield a total score , S = _ ∑ i s ( i ), of ± 1 . These requirements are written down__label__Supplement|Paper|Introduce
* p < 0 . 05 , n . a : not analyzed . Source data are presented in Figure 2 — source data 1 . DOI : _CITE_ The following source data and figure supplements are available for figure 2 : Source data 1 . Source raw data for Figure 2B - E .__label__Supplement|Paper|Introduce
Modular Program Constructor ( MPC ): MPC focuses on using easily understood directives to extract generically coded JSim MML equations from files , changing the names of the generic variables to ontologically informative names and assembling the resulting code into new equations ( Raymond & Bassingthwaighte , 2011 ). For example , MPC can take MML code representing a single tissue exchange region ( 26 lines ), and generate a whole organ heterogeneous model for convection , diffusion , and reaction with 20 regions ( 1698 lines ). See _CITE_ MPC currently runs outside of JSim but is planned for incorporation into a future JSim release . Modular construction with SemSim : Precise semantic identification of variables and parameters is a prerequisite to merging of preconstructed submodels or modules into integrated systems or multiscale models .__label__Method|Tool|Use
These studies have drawn from laboratory findings about human behavior and cognition to test their explanatory power in real - world data . Skill acquisition in online games ( Stafford & Dewar , 2014 ) Stafford and Dewar used BONDS to explore skill acquisition in a naturalistic environment . The researchers investigated the relation of practice and performance in an online game ( Axon ; _CITE_ ). Focusing on over 45 , 000 individuals who had played the game at least nine times over a two - month period , Stafford and Dewar were able to quantify the effects of practice and exploration - versusexploitation strategies on player scores over time . The results confirmed previous experimental findings : Practice improved performance ; the best players started with the highest scores and improved more quickly ; and early exploration of game strategies correlated with better later performance .__label__Supplement|Media|Introduce
Users who have programming skills can readily add more modules into the workflow or modify the workflow according to their own requirements . Gan et al . BMC Bioinformatics 2014 , 15 : 69 Page 10 of 11 _CITE_ Currently , MAAMD works for Affymetrix microarray datasets . It can be expanded to support more microarray datasets such as Roche and Illumia as both AltAnalyze and arrayQualityMetrics are able to analyze such data platforms . Since AltAnalyze supports the meta - analyses of aligned junction and exon sequences , this makes it possible to extend the workflow for the meta - analyses of RNA sequencing data , integrating it with the proper preprocessing tools for sequencing data .__label__Supplement|Paper|Introduce
This represents values that would be expected by chance [ 2 ]. Values were computed for all grid points in the motor cluster and then spatially averaged . PLOS Biology | _CITE_ March 12 , 2018 14 / 19 Speech tracking in auditory and motor regions Actual MI values and surrogate data were compared using a dependent t test , and p - values for both tests were FDR corrected . Data were deposited in the Dryad repository ( https :// doi . org / 10 . 5061 / dryad . 1qq7050 ) [ 31 ].__label__Supplement|Paper|Introduce
There was no enrichment of PTV - associations with this community ( S13 Table ). In fact , the significant community within the generic - PLN ( Community 0 ; 1516 genes ) shows an extensive overlap involving 1004 genes with T2D - PLN Community 5 ( 2390 genes , S8 Table ) ( p & lt ; 10 − 16 , hypergeometric test ). Community 5 genes associated with T2D genetic risk are expressed in the pancreas and liver and are involved in lipid and glucose homeostasis We considered additional functional genomics resources , which had not been used to generate the T2D - PLN ( S2 Table ) to further elucidate the biological roles of the Community 5 PLOS Computational Biology | _CITE_ October23 , 2017 7 / 23 Type 2 diabetes phenotypic linkage network Fig 3 . Clustering analyses of different T2D - risk candidate gene sets within Community 5 . ( A ) Clustering analyses between different genes set impacted by PT - variants within Community 5 .__label__Supplement|Paper|Extent
doi : 10 . 1371 / journal . pone . 0060234 . g001 method is BLAST against NCBI ( National Center for Biotechnology Information ) database ( _CITE_ ). However , it is known that BLAST is a time - consuming process and the speed is a bottleneck , especially when analyzing immense amount of reads . An alternative is to reduce the size of the query data and perform BLAST to get the species information quickly .__label__Material|Data|Compare
This occurs due to Andreev et al . eLife 2018 ; 7 : e32563 . DOI : _CITE_ 5 of 20 Research advance Chromosomes and Gene Expression Computational and Systems Biology increasing incidence of collisions involving scanning and elongating ribosomes within the uORF and subsequent dissociation of scanning ribosomes . In other words , according to our ICIER model , long uORFs repress translation of downstream ORFs , which are de - repressed during stress . This is consistent with our earlier study , where we found that the best predictor of stress - resistant mRNAs is the presence of an efficiently translated uORF combined with very low translation of the downstream acORF ( Andreev et al ., 2015b ).__label__Supplement|Paper|Introduce
A literature search using Hubmed , Pubmed , and Google Scholar was performed from inception to August 5 , 2016 for specific markers ( e . g . metabolites , amino acids , neurotransmitters ) involved in glycolysis and the TCA cycle according to PRISMA ( Preferred Reporting Items for Systematic Review and Meta - Analyses ) guidelines [ 30 ]. Additionally , relevant publications were identified via citations within relevant publications and reviews , as well as metabolomic databases _CITE_ [ 31 ], allowing for an evaluation of the completeness of the search criteria for this meta - analysis .__label__Material|Data|Introduce
In order to personalize therapeutic strategies for cancer patients , medical researchers aim to identify and characterize the biomarkers of each type of cancer , so that they can provide the most accurate diagnosis to patients © 2016 Jurca et al . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Jurca et al . BMC Res Notes ( 2016 ) 9 : 236 Page 2 of 35 [ 2 ].__label__Supplement|License|Other
The small interdisciplinary teams that formed were encouraged to apply for NSF or NIH funding , e . g ., through the joint NSF - NIH Quantitative Biomedical Big Data program . Many of these teams successfully competed for funding , with success rates being much higher for lab participants than the general pool in the first year . PLOS Biology | _CITE_ July 17 , 2017 6 / 9 Impartial evaluation by a review panel coupled with positive feedback from participants testifies to the success of the innovation lab . Innovation labs are designed to bring together junior investigators from the biomedical and data sciences . To reach more senior data scientists , other tactics are needed .__label__Supplement|Paper|Compare
from data from ten CCD panels , background subtraction , and extraction of diffraction patterns suitable for the subsequent processing and analysis ( Fig . 3 ). In every raster scan , diffraction patterns from the data acquisition system of the MPCCD detectors are converted to a single file in HDF5 format ( _CITE_ ), which contains all diffraction patterns by X - ray pulses provided during the scan . First , TAMON reconstructs image data from the eight panels of the MPCCD - Octal detector and the two from the MPCCD - Dual into a single file using the geometrical parameters describing the relative positions and orientations of the detector panels ( Kameshima , 2012 ). The positional and angular accuracies of parameters are approximately 25 µm and 1 mrad , respectively .__label__Supplement|Document|Introduce
If all possible pairs of redundant loci are considered for pruning , © The Author ( s ) 2018 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Calus and Vandenplas Genet Sel Evol ( 2018 ) 50 : 34 Page 2 of 11 computation of pairwise LD between all available SNPs may not be computationally feasible . Several tools exist that compute pairwise LD between SNPs [ 5 , 6 ].__label__Supplement|License|Other
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE___label__Supplement|License|Other
The superior accuracy of the trained model came at a substantial cost in training time , despite the use of a dedicated GPU to accelerate training . Training of the entire model took over 32 h using an Two Intel Xeon E5 - 2620 processors with 24 cores running at 2 GHz and 48 GB of RAM and 1 Tesla K40 GPU with 2880 CUDA cores and 12 GB of RAM . The results show that the algorithm applied outperformed results from previous studies of identification of autism spectrum disorder 3 The source code for the training scripts is available from Github at _CITE_ and archived at Zenodo ( Heinsfeld et al ., 2017 ).__label__Method|Code|Produce
3 report the bar plots with the percentage of correct classifications averaged on the same β values and obtained on the test and training set of Rhino viruses , respectively . The comparison with the performance under random permutations of class memberships confirms that our method is able to identify meaningful signals in the data : correct classification rates on 100 randomly permuted instances are always below 20 %, and the related p - value below 0 . 001 . All the results described above , as well as the extracted subsequences for each virus data set are available at _CITE_ and provided as Additional file 2 , where the reader may also find all the specific nucleotides distinguishing the species for further investigation .__label__Material|Data|Produce
3 report the bar plots with the percentage of correct classifications averaged on the same β values and obtained on the test and training set of Rhino viruses , respectively . The comparison with the performance under random permutations of class memberships confirms that our method is able to identify meaningful signals in the data : correct classification rates on 100 randomly permuted instances are always below 20 %, and the related p - value below 0 . 001 . All the results described above , as well as the extracted subsequences for each virus data set are available at _CITE_ and provided as Additional file 2 , where the reader may also find all the specific nucleotides distinguishing the species for further investigation .__label__Supplement|Document|Produce
On September 28 at 14 : 59 hrs I started collecting ADAC traffic jam data , programmatically every five minutes , from the website ( _CITE_ ) of the German automobile club ADAC . Prior to resorting to data scraping I had contacted ADAC about access to their past data but unfortunately I could not elicit a response . The data collection for the purpose of this paper ends on November 14 2015 at 13 : 59 hrs .__label__Material|Data|Use
On September 28 at 14 : 59 hrs I started collecting ADAC traffic jam data , programmatically every five minutes , from the website ( _CITE_ ) of the German automobile club ADAC . Prior to resorting to data scraping I had contacted ADAC about access to their past data but unfortunately I could not elicit a response . The data collection for the purpose of this paper ends on November 14 2015 at 13 : 59 hrs .__label__Supplement|Website|Use
The Lumbrineris latreilli dataset was first processed with custom functions of CTAn ( thresholding , smoothing , noise removal ) to isolate the jaws from the surrounding tissue and saved as a separate dataset . This new dataset was subsequently loaded into Micro - computed tomography : Introducing new dimensions to taxonomy 9 the free image editor Fiji ( http :// fiji . sc ) and reduced in size to a stack of 320 images with dimensions of 205 × 173 pixels . These bitmaps were converted into TGA ( Truevision Graphics Adapter ) files with the free ImageMagick tool _CITE_ ) and rendered with the C ++ programme volren ( Ruthensteiner et al . 2010 ) which is based on the plotting library S2PLOT ( Barnes et al . 2006 ).__label__Method|Tool|Use
YT , ON , NB , ( Lohse et al . 1990 ; Majka and Klimaszewski 2008 ). Hydrosmecta newfoundlandica Klimaszewski & Langor , 2011 ** _CITE_ Map 27 ; illustrations in Klimaszewski et al . ( 2011 ). Material examined .__label__Supplement|Paper|Introduce
In humans , the onset of sexual maturation has continued to trend toward younger ages , likely as the result of yet unidentified environmental stimuli [ 41 , 42 ]. Elucidation of the signaling pathways mediating Pdda in C . elegans may provide insights in the control of developmental timing in higher animals , including humans . PLOS Genetics | _CITE_ April 10 , 2017 12 / 21 Larval crowding accelerates C . elegans development and reduces lifespan__label__Supplement|Paper|Introduce
These criteria yielded 552 , 351 SNPs . Next , using PLINK57 , we performed LD pruning using sliding windows of 50 SNPs , with steps of 5 and a pairwise r2 < 0 . 04 and found 28 , 663 SNPs . Ancestry was determined using clusterGem in GemTools ( arXiv : 1104 . 116260 , 61 , _CITE_ ). Gemtools found that 5 dimensions and 7 clusters were sufficient to describe the ancestry space . Because one sample was missing key phenotypic information , 667 subjects were assigned ancestry based on DNA genotypes .__label__Method|Tool|Use
If the confidence bands did not overlap , the model was considered invalid with respect to the assumption of additivity , which suggests chemical interactions . BMD comparisons As a quantitative measure of similarity or dissimilarity between dose – response curves , BMD and BMDL values calculated for the predicted dose – response curves generated by each of the models were compared with the observed BMD values . BMD values were calculated using the USEPA ’ s Benchmark Dose Software BMDS version 2 . 5 . 1 ( _CITE_ ) ( Davis et al . 2011 ). Data points ( doses and effect levels ) across the predicted dose – response curves generated using the CA , GCA , and IA models were selected from the mathematical modeling output for BMD modeling .__label__Method|Tool|Use
To maintain data utility , we must balance the number of iterations and the privacy budget spent on each iteration in the training process . To use the Newton - Raphson algorithm , we need to compute the gradient and the Hessian matrix from the Ji et al . BMC Medical Genomics 2014 , 7 ( Suppl 1 ): S14 Page 4 of 10 _CITE_ data sets . The simplest way to ensure differential privacy is to add Laplacian noise to the gradient and the Hessian matrix , and use the noisy version of these intermediary results to update parameters . Theoretically , the impacts of additive noise in Laplacian mechanism ( see Definition 4 ) tend to be much smaller when the number of samples approaches infinity , as the sensitivity of the gradient and the Hessian matrix is irrelevant to the size of a data set .__label__Material|Data|Extent
To maintain data utility , we must balance the number of iterations and the privacy budget spent on each iteration in the training process . To use the Newton - Raphson algorithm , we need to compute the gradient and the Hessian matrix from the Ji et al . BMC Medical Genomics 2014 , 7 ( Suppl 1 ): S14 Page 4 of 10 _CITE_ data sets . The simplest way to ensure differential privacy is to add Laplacian noise to the gradient and the Hessian matrix , and use the noisy version of these intermediary results to update parameters . Theoretically , the impacts of additive noise in Laplacian mechanism ( see Definition 4 ) tend to be much smaller when the number of samples approaches infinity , as the sensitivity of the gradient and the Hessian matrix is irrelevant to the size of a data set .__label__Supplement|Paper|Extent
Granaries were again at the top ( upper right ) of the log - log regression before and 5 months after spraying in Figueroa . Open sheds do not get much attention as potentially important habitats for bug control , perhaps because of the wide variability in bug counts between individual sheds . Granaries and open sheds had greater means and variances of bug PLOS Neglected Tropical Diseases | _CITE_ November 30 , 2017 28 / 34 Chagas disease vector control and Taylor ’ s law abundance than chicken coops , storerooms , and domiciles . The latter three habitats usually take all the attention of bug control personnel . Domiciles , storerooms and kitchens tend to appear on the upper end of the log variance - log mean regressions .__label__Supplement|Paper|Introduce
Unlike star trees and K & W ’ s profile model , which can only explain sequence similarity , the common ancestry models can also explain nested hierarchical sequence correlations . Hierarchical structure in a set of sequences is a direct prediction of genealogical models , in which branching of lineages occurs . For the real , universal protein data set , this hierarchical structure is the overriding factor for the superiority of the UCA Theobald Biology Direct 2011 , 6 : 60 Page 15 of 25 _CITE_ hypothesis over competing hypotheses , including various independent ancestry models . Moreover , the models used in the tests do not assume a priori that significant sequence similarity implies homology . Rather , sequence similarity is a consequence of the common ancestry models , and the common ancestry models simply explain the data best .__label__Supplement|Paper|Introduce
Note that the post - spike HAP is less conspicuous and of more varied appearance than in A . ( C ) shows the HH model with EPSCs at 600 Hz , and IPSCs at 300 Hz ( Repsc = 600 , Ripsc = 300 ), generating spikes at a mean rate of 1 . 6 spikes / s . Note that the HAP following spikes is even less conspicuous than in B . _CITE_ noisy , irregularly spiking , membrane potential that resembles an in vivo neuron [ 40 , 41 ] much more closely than the regularly - timed spikes produced by the applied current ( Fig 2A ). With EPSCs at 600 Hz , the HH model generates 5 . 6 spikes / s ( Fig 2B ), and adding IPSCs at 300 Hz reduces the firing rate to 1 . 6 spikes / s ( Fig 2C ). Fitting the IF model We fixed the IF model parameters Vrest = - 66 mV and Vthresh = - 48 mV to match the measured membrane properties of the HH model , and fixed the synaptic parameters ksyn = 8 ms , eh = 3 mV , and ih = - 3 mV to match the HH model ’ s PSP magnitudes and decay rates .__label__Supplement|Paper|Introduce
Since the appearance of [ 10 , 11 ], considerable interest in statistical applications of the distance correlation coefficient has arisen . Notably , distance correlation has been used for inferring gene regulatory networks [ 35 ], testing associations between the copy number variations of different genes [ 36 ] and assessing associations of familial relationships , lifestyle factors and mortatility [ 37 ]. PLOS ONE | _CITE_ November 28 , 2017 6 / 23 Clustering with mixed - type data The distance correlation coefficient RðX ; YÞ is a measure of dependence between apdimensional random vector X and a q - dimensional random vector Y , where p and q are arbitrary . The distance correlation is always positive with 0 < RðX ; YÞ < 1 and it equals 0 if and only if the vectors X and Y are independent . This property implies that the distance correlation can detect any dependence between X and Y .__label__Supplement|Paper|Introduce
* We separately analyzed and reported a group of studies ( Limited ) that initially met our broad inclusion criteria , but that were implemented at the individual context level and were limited in terms of frequency and / or duration of access to condoms ( e . g ., participants could take as many condoms as they wanted , but only at motivational sessions or when they made contact with a street outreach worker ). See S5 File for details . _CITE_ We observed a wide variation in measuring and reporting of sexual risk behavior outcomes in the literature . Given there is no universally - accepted standard metric for measuring condom use [ 17 ], we created two standardized outcomes : a ) condomless sex likelihood , at last episode ( most commonly reported , k = 5 ) or over a recall period ( k = 3 ), and b ) not always using condoms . For studies reporting condomless sex at episodes of sex other than last , we considered those outcomes similar enough and included as condomless sex likelihood .__label__Supplement|Paper|Extent
Fragments containing adapters on both ends were then enriched and amplified with PCR , quantified with qPCR , and run on the Agilent Bioanalyzer DNA - 1000 chip to estimate fragment size . Samples were then multiplexed and sequenced on the Illumina 2500 . The data was demultiplexed using CASAVA and run through FastQC ( _CITE_ ) to assess sequencing data quality . Paired - end 50 nucleotide read data from mRNA - Seq were mapped against the revised woodchuck transcriptome with Bowtie2 [ 56 ] and prioritized for concordant paired alignments with unique hits . The resulting SAM / BAM files were processed with SAMtools [ 57 ] to yield count data that was normalized and processed by DESeq [ 58 ] for differential expression analysis and subsequent pattern recognition and pathway analysis .__label__Method|Tool|Use
Previous studies have identified target site mutations in resistance phenotypes and a wide range of genes involved in metabolic detoxification , including carboxylesterase genes , cytochrome P450s , and glutathione S - transferase genes38 – 42 . To examine evidence of rapid evolutionary change underlying L . decemlineata ’ s extraordinary success utilizing novel host plants , climates , and detoxifying insecticides , we evaluated structural and functional genomic changes relative to other beetle species , using whole - genome sequencing , transcriptome sequencing , and a large community - driven biocuration effort to improve predicted gene annotations . We compared the size of gene families associated with particular traits against existing available genomes from related species , particularly those sequenced by the i5k project ( _CITE_ ), an initiative to sequence 5 , 000 species of Arthropods . While efforts have been made to understand the genetic basis of phenotypes in L . decemlineata ( for example , pesticide resistance ) 32 , 43 , 44 , previous work has been limited to candidate gene approaches rather than comparative genomics . Genomic data can not only illuminate the genetic architecture of a number of phenotypic traits that enable L . decemlineata to continue to be an agricultural pest , but can also be used to identify new gene targets for control measures .__label__Method|Tool|Compare
based transcriptomics data have been retrieved from five high - throughput microarrays experiments of different cell types that have been deposited in GEO as GSE24810 [ 18 ], GSE19018 ( _CITE_ ? acc = GSE19018 ), GSE15829 [ 19 ], GSE13496 [ 20 ], and GSE4352 [ 21 ]. Based on the cell types used , the five datasets ( series ) were grouped into four categories , namely , ( a ) Human Diploid Fibroblasts ( HDFs ) which include the HMF3A and HMF3S cells of GSE24810 , the IMR90 cells of GSE19018 , the HF cells of GSE15829 , and the WS1 , WI38 , and BJ cell lines of the GSE4352 series ; ( b ) Haematopoietic Progenitor / Stem Cells ( HSCs ) including the BMCD34 + and PBCD34 + 38 − cells of GSE13496 ; ( c ) T - cells of GSE13496 ; and ( d ) Human Mammary Epithelial Cells ( HMECs ) that include the 48R and 184 epithelial cells of the GSE4352 series .__label__Material|Data|Use
Removing loci based on high levels of pairwise LD is commonly known as LD pruning . If all possible pairs of redundant loci are considered for pruning , © The Author ( s ) 2018 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Calus and Vandenplas Genet Sel Evol ( 2018 ) 50 : 34 Page 2 of 11 computation of pairwise LD between all available SNPs may not be computationally feasible .__label__Supplement|License|Other
[ 68 ] have stressed that in northwestern Keos land accessibility was one of the factors influencing the decision to cultivate a particular land as of early 1900s , before the use of motorized transport became widespread . This situation now finds an interesting parallel in mid - 1800s Malta . PLOS ONE | _CITE_ February7 , 2018 20 / 28 GIS and 1860s cadastral data to model agricultural suitability in Malta before heavy mechanization The issue of land accessibility and its interpretation holds true for the distance to the nearest footpath , which has a similar negative effect : as seen , the increase in distance to footpaths is associated with a decrease in the odds for optimal land quality . It must be noted that the model ’ s results for the distance to the nearest minor road seem counterintuitive , and deserve comments . If seen from the standpoint of the secondary road network and of its ability to make the landscape accessible , one would expect minor roads to have an effect similar to that of secondary roads .__label__Supplement|Paper|Introduce
In this section we describe how to apply our algorithm to real data . All the code and data necessary to reproduce our analysis is available online on github ( _CITE_ copy archived at https :// github . com / elifesciences - publications / vdjRec /). We start with annotated TCR datasets ( CDR3 amino acid sequence , V - segment , J - segment ), one per donor . Such datasets are produced by MiXCR ( Bolotin et al ., 2015 ), immunoseq ( http :// www .__label__Method|Code|Produce
In this section we describe how to apply our algorithm to real data . All the code and data necessary to reproduce our analysis is available online on github ( _CITE_ copy archived at https :// github . com / elifesciences - publications / vdjRec /). We start with annotated TCR datasets ( CDR3 amino acid sequence , V - segment , J - segment ), one per donor . Such datasets are produced by MiXCR ( Bolotin et al ., 2015 ), immunoseq ( http :// www .__label__Material|Data|Produce
Data available from the Dryad Digital Repository : _CITE___label__Material|Data|Use
Two exceptions are consortia that have their own data repository or portal ( like CPTAC ) and organism - specific resources such as the ‘ Saccharomyces Genome database ’. This is because the first substantial obstacle for integrative data scientists is finding suitable data sets to link . This key issue is being addressed by the Omics Discovery Index ( OmicsDI ) ( _CITE_ ), a recently released portal for the discovery and access of data sets from various omics approaches and online resources [ 48 ]. Among other features , OmicsDI represents the concept of multiomics data sets by connecting different omics datasets cited in the same publication . For instance , in September 2016 OmicsDI knew about more than 30 multiomics data sets that contain both proteomics data and the corresponding gene expression data .__label__Supplement|Website|Introduce
Soil data were obtained from the SoilGrids1km database http :// www . isric . org / explore / soilgrids ) [ 16 ]. SoilGrid variables were created using spatial model predictions based on a global database of soil profiles and a combination of environmental covariates . PLOS Neglected Tropical Diseases | _CITE_ October 13 , 2017 3 / 17 Modeling and mapping anthrax in Ghana__label__Supplement|Paper|Extent
Spletter et al . eLife 2018 ; 7 : e34058 . DOI : _CITE_ 20 of 34 Tools and resources Cell Biology Developmental Biology and Stem Cells It is well established that general myogenic transcription factors , in particular Mef2 , are continuously required in muscles for their normal differentiation ( Sandmann et al ., 2006 ; Soler et al ., 2012 ). Mef2 regulates a suit of sarcomeric proteins in fly , fish and mouse muscle important for correct sarcomere assembly and maturation ( Hinits and Hughes , 2007 ; Kelly et al ., 2002 ; Potthoff et al ., 2007 ; Stronach et al ., 1999 ). In Drosophila , Mef2 cooperates with tissue - specific factors , such as CF2 , to induce and fine - tune expression of structural genes ( Gajewski and Schulz , 2010 ; Garcia - Zaragoza et al ., 2008 ; Tanaka et al ., 2008 ).__label__Supplement|Paper|Introduce
The sum of A , B , and Z corresponds to the extrapolated radioactivity in arterial plasma at steady state ( infinite infusion duration ). The fractional ð7Þ Alf et al . EJNMMI Research 2013 , 3 : 61 Page 5 of 14 _CITE_ areas under the curves fa , fb , and fz are defined by A , B , Z and α , β , ζ , as shown in Equations 8 , 9 , and 10 :__label__Supplement|Paper|Introduce
These functions include retrieving the druggable targets for a disease of interest , or obtaining the biological pathways for a list of disease genes . The disgenet2r package also expedites the integration of DisGeNET data with other R packages . The source code and documentation of disgenet2r package are available at _CITE_ group / disgenet2r .__label__Method|Tool|Produce
These functions include retrieving the druggable targets for a disease of interest , or obtaining the biological pathways for a list of disease genes . The disgenet2r package also expedites the integration of DisGeNET data with other R packages . The source code and documentation of disgenet2r package are available at _CITE_ group / disgenet2r .__label__Method|Code|Produce
These functions include retrieving the druggable targets for a disease of interest , or obtaining the biological pathways for a list of disease genes . The disgenet2r package also expedites the integration of DisGeNET data with other R packages . The source code and documentation of disgenet2r package are available at _CITE_ group / disgenet2r .__label__Supplement|Document|Produce
The sequences reported in this paper have been deposited in the European Nucleotide Archive Sequence Read Archive under study accession number ERP002428 and are available at _CITE___label__Material|Data|Use
Modules of highly correlated genes were detected through hierarchical clustering . Some genes were not correlated with other seasonal genes . The biological function of each module was examined through an over - representation pathway analysis carried out using the WEB - based GEne SeT AnaLysis Toolkit ( WebGestalt , _CITE_ ) 70 . The gene members of each module were uploaded to WebGestalt and tested for over - representation within KEGG pathways . Pathways with less than three genes within our gene lists were excluded .__label__Method|Tool|Use
acknowledges the financial support from the Magnus Ehrnrooth Foundation . Part of the computing has been done in T . Lappi ’ s project at CSC , the IT Center for Science in Espoo , Finland . Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . Funded by SCOAP3 .__label__Supplement|License|Other
**- Pipeline C generates a sum of all isoforms with the same seed sequence . Stokowy et al . BMC Research Notes 2014 , 7 : 144 Page 9 of 12 _CITE_ and 30e are more related since the attribution of an individual isoform to either of these miRNAs could be based on two single nucleotide mismatches ( C versus U ) in the miRBase entry sequences . The distance between 30a , 30d and 30e to 30c is a little higher ( 4 bases ). Very likely qPCR based on the target sequence for 30e will at least also partially amplify targets from 30a and 30d .__label__Supplement|Paper|Compare
In this work majority of the analysis have been done on MATLAB versions 2015b and 2016b , supported by FSL 5 . 0 . 9 for neuroimaging analysis . Inference on DVARS as well as DSE decomposition techniques proposed in this paper are available via MATLAB scripts , found at _CITE_ Also , a dedicated web page , http :// www . nisox . org / Software / DSE /, present the DSE decompositions of HCP and ABIDE cohort and is regularly updated with new publicly available resting - state data sets . Results and figure scripts presented in this work are publicly available for reproducibility purposes ( Diggle , 2015 ) on http :// www . github . com / asoroosh / DVARS_Paper17 .__label__Method|Code|Produce
Thus , our parser proved useful in managing large worldwide collections of crystallographic data . In addition , fast performance and the possibility to call the parser from different programs in the Perl , C and Python languages allows one to employ it in various crystallographic programs , and we hope that it will enable easier data exchange between researchers . The COD :: CIF :: Parser parser can be downloaded as part of the cod - tools software package ( _CITE_ ) under the GPL2 free software license . The package is also available as part of the supporting information for this article .__label__Method|Code|Use
Supplementary information S1 and figure S1 are available at Molecular Biology and Evolution online ( _CITE_ ).__label__Supplement|Website|Produce
Supplementary information S1 and figure S1 are available at Molecular Biology and Evolution online ( _CITE_ ).__label__Supplement|Document|Produce
The overall utility of combining host – parasite models with MTE in other systems will hinge on two factors . First , the population dynamics and interactions of hosts and parasites need to be described by an adequate model ( Eqs 1 – 3 in our system ), a problem of structural uncertainty that can be addressed by natural history knowledge and model selection statistics . PLOS Biology | _CITE_ February 7 , 2018 12 / 20 Metabolic theory and within - host parasite dynamics Second , the temperature dependence of model parameters needs to be described , requiring knowledge of both the functional relationship between temperature and a parameter ( e . g ., the Sharpe – Schoolfield model ) and estimates for each function ’ s hyperparameters ( e . g ., activation energies ). Appropriate a priori choices for these relationships and parameters can initially be informed using MTE [ 13 ] and subsequently refined in empirical studies as done here . The multiple interacting temperature dependencies of host – parasite systems complicate forecast attempts , but explicitly formulating these dependencies in models based on first principles is a key step toward a predictive framework that is both generalizable and customizable across different transmission modes , species , and regions .__label__Supplement|Paper|Introduce
Sujan Perera , Cartic Ramakrishnan , and Meena Nagarajan were employed by IBM at the time this research was completed . Ethics statement All human subject data used in this analysis were publicly available and used in a de - identified format whenever possible . Open Access This article is distributed under the terms of the Creative Commons Attribution - NonCommercial 4 . 0 International License ( _CITE_ ), which permits any noncommercial use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made .__label__Supplement|License|Other
_CITE_ pathology , whose data is independent of one another . Therefore , the outcome of the algorithm presented in the current study was more likely to reflect the ability of the algorithm rather than experimental group differences . Developing a machine learning algorithm using scalar quantities extracted from the waveforms of kinetic and kinematic variables [ 29 , 30 , 32 , 33 ] could result in the dismissal of important temporal data , thus power spectra of full waveforms have been employed [ 24 , 35 , 36 ] since each individual feature provides complementary information [ 42 ].__label__Supplement|Paper|Introduce
Supplementary information accompanies this paper at _CITE_ Competing financial interests : The authors declare no competing financial interests . How to cite this article : Slebos , R . J . C . et al . Proteomic analysis of colon and rectal carcinoma using standard and customized databases .__label__Supplement|Document|Produce
In the present study , we compared the gene expression changes in the MFC of elderly subjects with schizophrenia ( patients vs . controls ) with those in the developing MFC ( infants vs . relatively young adults [ 20 – 39 years ]; Figure 1C ), which revealed significant similarities between the two groups ( P = 2 . 6 × 10 − 8 ; Figure 2e ). This result might indicate Hagihara et al . Molecular Brain 2014 , 7 : 41 Page 10 of 18 _CITE_ that the relative change in gene expression patterns that define transcriptomic infancy can also be seen in a cohort of elderly subjects with schizophrenia . We also found that gene expression patterns in the schizophrenic MFC are similar to those in the normal infant MFC as compared to elderly adult MFC ( Additional file 3 : Figure S2 ), suggesting that both the DLFC and MFC of patients with schizophrenia represent juvenile - like gene expression profiles . In addition , we found a significant similarity in the gene expression patterns of the superior temporal cortex ( STC ) between normal infants and patients with schizophrenia ( Additional file 6 : Figure S5 ).__label__Supplement|Paper|Extent
JEnsembl Although the ActiveRecord design allows the API code to remain ‘ in sync ’ with the database schema automatically , no higher level data model is generated and scripts that run against a particular release of the Ensembl databases will not run against other releases if the names of tables or columns have been changed . Several Python - based APIs that have been made available have not evolved with schema changes and provide limited data models [ e . g . PyCogent ( _CITE_ ) ( Knight et al ., 2007 ), PyGr ( http :// code . google . com / p / pygr / wiki / PygrOnEnsembl ), cache - ensembl ( http :// pypi . python . org / pypi / cache_ensembl )]. A new , easily maintainable Java - based API to the Ensembl system would be a timely and highly effective addition to the bioinformatics toolbox . Such an API would allow integration between graphical user interfaces and Ensembl datasources and between other bioinformatic resources and libraries implemented in Java [ for example , the BioJava ( http :// www . biojava . org ) framework ; Holland et al ., 2008 ].__label__Method|Code|Compare
Supplementary information accompanies this paper at _CITE_ Competing Interests : The authors declare no competing interests . Publisher & apos ; s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations .__label__Supplement|Document|Produce
albopictus ( traps positive ), the total Ae . albopictus mosquitoes caught in gravid and light traps , and the abundance ( calculated as the number caught per trap location divided by the 23 weeks of surveillance ) for gravid , light , and both trap types together are shown in Table 1 and Fig 1 . PLOS Neglected Tropical Diseases | _CITE_ August 23 , 2017 6 / 19 Ae . albopictus and arbovirus transmission risk in New York City__label__Supplement|Paper|Introduce
We found that stat - GFP expression increased markedly in ISCs near ebd1 mutant clones ( Fig 9C – 9C ”; quantification : Fig 9D ), indicating that the Jak / Stat pathway was activated non - autonomously upon ebd1 inactivation . To determine whether the ectopic activation of Jak / Stat signaling mediates the overproliferation of ISCs resulting from loss of ebd1 , we concomitantly knocked down both upd and ebd1 in ECs using RNAi . Dual knockdown of ebd1 , and either upd2 or upd3 , reduced ISC proliferation in posterior midguts , as PLOS Genetics | _CITE_ July 14 , 2017 15 / 37 Transcription cofactors Earthbound and Erect wing mediate intestinal defects due to APC inactivation Fig 7 . Ebd inhibits the proliferation of intestinal stem cells . ( A - F ) Loss of ebd results in increased numbers of progenitor cells ( marked with esg > GFP , green ), including EBs ( marked with GBE - Su ( H )- lacZ , magenta ) ( compare D - F with A - C ).__label__Supplement|Paper|Introduce
higher accuracy than DOTUR , mothur , ESPRIT - Tree , UCLUST and Swarm . CLUSTOMCLOUD is written in JAVA and is freely available at _CITE___label__Method|Code|Produce
Since gene expression is not only dynamic in the treatment group but also in the control group , the inclusion of the time - course control data greatly improves the ability to detect truly differentially expressed genes , as the gene expression values are not referred to a single time point with static gene expression levels only . Comparing a treatment group to time point zero does not provide a proper control over the entire time - course , although it is widely practiced [ 26 – 28 ]. The proposed workflow is implemented in an open - source R - package splineTimeR and is available through Bioconductor ( _CITE_ ). Amongst a panel , the two lymphoblastoid cell lines that were different with regard to radiation sensitivity after irradiation with 10 Gy [ 20 ], also responded differently with regard to the__label__Method|Code|Use
Supplementary figures S1 – S4 and tables S1 and S2 are available at Genome Biology and Evolution online ( _CITE_ ).__label__Supplement|Website|Use
With respect to sample size , we required at least 384 participants for a confidence interval of 95 %, with a 50 % planned proportion estimate and absolute precision of 5 %. The study was conducted at all 312 HIV - OPCs in Vietnam , with the expectation of about two staff members at each HIV - OPC participating in the study . PLOS ONE | _CITE_ November 14 , 2017 3 / 16 Practices insecurity and confidentiality of HIV / AIDS patients ’ information__label__Supplement|Paper|Other
This systematic review was registered with the International Prospective Register of Systematic Reviews ( PROSPERO ; Registration no . CRD42016035270 ; available from _CITE_ ), and was conducted and reported following the Preferred Reporting Items for Systematic Reviews and Meta - Analyses ( PRISMA ) statement [ 22 ]. Eligibility criteria The Population , Interventions , Comparisons , Outcomes , and Study design ( PICOS ) framework [ 23 ] was used to identify key study concepts in the research question , and to facilitate the search process . The Author ( s ) BMC Public Health 2017 , 17 ( Suppl 5 ): 868 Page 67 of 215__label__Method|Tool|Use
This systematic review was registered with the International Prospective Register of Systematic Reviews ( PROSPERO ; Registration no . CRD42016035270 ; available from _CITE_ ), and was conducted and reported following the Preferred Reporting Items for Systematic Reviews and Meta - Analyses ( PRISMA ) statement [ 22 ]. Eligibility criteria The Population , Interventions , Comparisons , Outcomes , and Study design ( PICOS ) framework [ 23 ] was used to identify key study concepts in the research question , and to facilitate the search process . The Author ( s ) BMC Public Health 2017 , 17 ( Suppl 5 ): 868 Page 67 of 215__label__Supplement|Document|Use
Publish with BioMed Central and every scientist can read your work free of charge & quot ; BioMed Central will be the most significant development for disseminating the results of biomedical research in our lifetime .& quot ; Sir Paul Nurse , Cancer Research UK Your research papers will be : available free of charge to the entire biomedical community peer reviewed and published immediately upon acceptance cited in PubMed and archived on PubMed Central yours — you keep the copyright Submit your manuscript here : _CITE_ BioMedcentral Page 17 of 17 ( page number not for citation purposes )__label__Supplement|Website|Other
Table S4 Binding intensities ( FU ) for labeled BambL protein with glycan array chips v4 . 1 from the consortium for functional glycomics . Full data is available on the web site ( _CITE_ ). ( PDF )__label__Material|Data|Produce
Table S4 Binding intensities ( FU ) for labeled BambL protein with glycan array chips v4 . 1 from the consortium for functional glycomics . Full data is available on the web site ( _CITE_ ). ( PDF )__label__Supplement|Website|Produce
Finally , in rhesus macaques , circulating TSCM cells had a level of Ki - 67 expression that the author remarked was ‘ unexpectedly large ’ ( mean & gt ; 10 %) [ 36 ]; this is consistent with our finding that , on average , the TSCM population in peripheral blood turns over rapidly . Our proliferation rate estimates for naïve and TSCM cells can be compared with proliferation rate estimates of other T cell subsets obtained using stable isotope labelling . We found that the proliferation of naïve T cells is slowest , p = 0 . 0005d − 1 , and comparable with previous estimates [ 40 ], though PLOS Biology | _CITE_ June22 , 2018 12 / 22 Human TSCM cell dynamics in vivo in this latter study , the gating strategy would have inadvertently included TSCM cells in the naive cell gate . Next is the slow TSCM subpopulation with a proliferation rate an order of magnitude faster than naive cells ( p = 0 . 002d − 1 ), then the fast TSCM subpopulation with a median proliferation rate ofp = 0 . 015d − 1 comparable with that of memory cells ( 0 . 006d − 1 – 0 . 02d − 1 [ 29 , 41 ]). Giving the following rank order of proliferation rates : naive & lt ; slow TSCM & lt ; fast TSCM ≲ memory .__label__Supplement|Paper|Compare
To quantify the effect of long - term organic farming on total abundance and activity of soil microbial communities , results from 56 studies and 149 paired comparisons were analysed in a meta - analysis ( S1 Table ). In order to find pairwise comparisons of organic and conventional farming systems , a comprehensive literature search with the terms ‘ organic microb ’, ‘ organic farming microb ’, ‘ farming system microb ’, ‘ agriculure microb ’ and ‘ microbial community farming ’ was conducted in online search engines google scholar and web of science starting in June 2015 and ending in February 2016 . Furthermore , reference lists of identified studies were PLOS ONE | _CITE_ July 12 , 2017 3 / 25 Organic farming enhances soil microbial abundance and activity — A meta - analysis and meta - regression manually browsed to find additional articles not identified using search terms . Peer - reviewed scientific journals as well as grey literature were considered , as long as minimal experimental standards were met . Papers were considered eligible and included in the meta - analysis if the following criteria were met ( S1 Fig ):__label__Supplement|Paper|Introduce
The graph , as well as the individual intensity profiles , can be exported as a . png , . eps , . pdf and . tif file . For the conversion to the vector format ( eps and pdf ), the user has to manually install respective open - source programs ( Ghostscript at http :// www . ghostscript . com and pdftops at _CITE_ xpdf ). Statistical analysis within the manuscript The Kolmogorov - Smirnov test ( KS - test ) is used to compare two samples ( two - sample KS - test ), which tests whether the samples are drawn from the same distribution ( the null hypothesis ). The two - sample KS - test is one of the most useful and general non - parametric methods for comparing two samples , as it is sensitive to differences in both location and shape of the empirical cumulative distribution functions of the two samples .__label__Method|Tool|Use
To identify and estimate the number of potential orthologous gene families between E . breviscapus , Helianthus annuus , Cynara cardunculus , Solanum tuberosum , Solanum lycopersicum , V . vinifera , and O . sativa , we applied the OrthoMCL ( v . 2 . 0 . 9 ) pipeline [ 37 ] using standard settings ( BLASTP E - value < 1e − 5 ) to compute the all - against - all similarities . Gene sequences from S . tuberosum , S . lycopersicum , V . vinifera , and O . sativa were downloaded from Phytozome , v . 11 . 0 . Gene sequences from H . annuus and C . cardunculus were downloaded from the Sunflower Genome Database ( _CITE_ sunflowergenome . org ) and Globe artichoke GBrowse ( http :// gviewer . gc . ucdavis . edu / cgi - bin / gbrowse / Artichoke v1 1 ), respectively . Among the total 13 076 E . breviscapus gene families , 2336 ( 17 . 9 %) appear to be lineage specific .__label__Material|Data|Use
QTC was unable to evaluate the concatenated data set due to its reliance on the computationally intensive jackknife distance measure [ 9 ]. Our own implementation of Pearson correlation was used as a representation of hierarchical clustering , with the raw pairwise correlation value itself behaving as a parameter over which precision and recall were calculated . Page 4 of 13 ( page number not for citation purposes ) BMC Bioinformatics 2007 , 8 : 250 _CITE_ Implementations of GenClust and Aerie were provided by [ 20 ] and [ 17 ], respectively . GenClust was run for 1000 iterations with cluster counts k ranging from two to 30 by increments of two . GenClust failed to produce any output for the Hughes or concatenated data sets , apparently due to their high condition counts .__label__Supplement|Paper|Introduce
Collection of 50 , 048 unique metabolites and 101 , 500 metabolite – species pairs _CITE_ = drop_index__label__Material|Data|Produce
efforts to expand this import are in progress . Data for large collaborative projects , including Encyclopedia of DNA Elements ( ENCODE ) ( 5 ) and Roadmap Epigenomics ( 6 ), are deposited by Data Coordinating Centres and have dedicated data listings pages at _CITE_ and http :// www . ncbi . nlm . nih . gov / geo / roadmap / epigenomics /.__label__Material|Data|Produce
efforts to expand this import are in progress . Data for large collaborative projects , including Encyclopedia of DNA Elements ( ENCODE ) ( 5 ) and Roadmap Epigenomics ( 6 ), are deposited by Data Coordinating Centres and have dedicated data listings pages at _CITE_ and http :// www . ncbi . nlm . nih . gov / geo / roadmap / epigenomics /.__label__Supplement|Website|Produce
We obtained surgical data from 66 Member States ( Table 3 ; available at : _CITE_ ). Using multiple imputation , we extrapolated the volume of surgery for each country without reported surgical data ( Table 4 ; available at : http :// www . who . int / bulletin / volumes / 94 / 3 / 15 - 159293 ). For the year 2012 , we estimated the total global volume to be 312 . 9 million operations – an increase of 38 . 2 % from an estimated 226 . 4 million operations in 2004 .__label__Material|Data|Use
There is thus no clear advantage in using one or the other likelihood form for this scenario , but our use of monomorphic sites allows us to directly get absolute values of the parameters . We report in Figures 2 , S4 and S5 only the results obtained for data sets for which LaLi ’ s best log likelihood was less than 10 % lower than the largest log - likelihood obtained with the other data sets , and we considered LaLi not to have converged for the discarded data sets . Estimating demographic parameters from an ascertained SNP array Recently , Affymetrix developed a new SNP array including — 629 , 000 SNPs with known ascertainment scheme for population inference ( Axiom Genome - Wide Human Origins 1 Array , _CITE_ affx ? product = Axiom_GW_HuOrigin ) [ 43 ]. This array , abbreviated hereafter GWHO , is made up of SNPs defined in 13 discovery panels .__label__Supplement|Website|Introduce
Data exploration using the FANTOMS portal An online website is available ( _CITE_ ) where all data generated within the FANTOM5 project are collected , and visualization and browsing tools are publicly accessible . Users interested in downloading the whole genomic coordinates of CAGE TSSs , peaks , expression values and gene associations for macaque as well as other organisms , such as human , can access our ftp site fantom . gsc . riken . jp / 5 / datafiles / latest /.__label__Method|Tool|Produce
Data exploration using the FANTOMS portal An online website is available ( _CITE_ ) where all data generated within the FANTOM5 project are collected , and visualization and browsing tools are publicly accessible . Users interested in downloading the whole genomic coordinates of CAGE TSSs , peaks , expression values and gene associations for macaque as well as other organisms , such as human , can access our ftp site fantom . gsc . riken . jp / 5 / datafiles / latest /.__label__Material|Data|Produce
Data exploration using the FANTOMS portal An online website is available ( _CITE_ ) where all data generated within the FANTOM5 project are collected , and visualization and browsing tools are publicly accessible . Users interested in downloading the whole genomic coordinates of CAGE TSSs , peaks , expression values and gene associations for macaque as well as other organisms , such as human , can access our ftp site fantom . gsc . riken . jp / 5 / datafiles / latest /.__label__Supplement|Website|Produce
wrote the manuscript , with contributions from A . M . F . and S . J . F . Additional information Supplementary Information accompanies this paper at _CITE_ naturecommunications Competing interests : The authors declare no competing financial interests . Reprints and permission information is available online at http :// npg . nature . com / reprintsandpermissions / How to cite this article : Behjati , S . et al . Recurrent mutation of IGF signalling genes and distinct patterns of genomic rearrangement in osteosarcoma .__label__Supplement|Document|Produce
Financial support by the Austrian Science Fund ( FWF ): FI001380 granted to Bernhard Fügenschuh is gratefully acknowledged . The authors would like to thank Franz Neubauer and Dinu Pana ˘ for their comments and suggestions that have significantly improved the manuscript . Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made .__label__Supplement|License|Other
Multi - dimensional data ( level 3 ) for gastric cancer were derived from TCGA STAD cohort at Broad GDAC Firehose data run ( version : 2016_01_28 , _CITE_ ). A total of 272 patients for whom gene expression , miRNA expression , and copy number profiles were available were included in the analysis ( Table S1 ). Gene expression profiles of 29 paired tumor and normal tissue samples were measured as a reads per kilobase__label__Material|Data|Use
Overview of GlycoPep Evaluator . GlycoPep Evaluator ( GPE ) is a freely downloadable software tool that can be used to generate decoy glycopeptides for false discovery rate analysis . GPE is available for download at _CITE_ research . It has incorporated functionality to score all the targets and decoys against imported spectra using a previously published scoring algorithm . 24 GPE was written in Java and developed with Java Development Kit 7 ( JDK 7 ). The program has been tested to perform successfully under Windows and Linux systems , and Java Runtime Environment 7 ( JRE 7 ) is recommended to be installed prior to running GPE .__label__Method|Tool|Produce
ALICE , programmed in R and R - GUI , is the software with a user - friendly interface for an integrated genomic analysis of AF , LOH / LCSH , AI , and CNV / CNA . The software , reference databases , library files for APT , annotation files , test examples , and user manual can be downloaded from the ALICE homepage ( _CITE_ ). ALICE consists of three main components —“ Main Functions ” ( Additional file 11 ), “ Genome__label__Method|Tool|Produce
ALICE , programmed in R and R - GUI , is the software with a user - friendly interface for an integrated genomic analysis of AF , LOH / LCSH , AI , and CNV / CNA . The software , reference databases , library files for APT , annotation files , test examples , and user manual can be downloaded from the ALICE homepage ( _CITE_ ). ALICE consists of three main components —“ Main Functions ” ( Additional file 11 ), “ Genome__label__Method|Code|Produce
ALICE , programmed in R and R - GUI , is the software with a user - friendly interface for an integrated genomic analysis of AF , LOH / LCSH , AI , and CNV / CNA . The software , reference databases , library files for APT , annotation files , test examples , and user manual can be downloaded from the ALICE homepage ( _CITE_ ). ALICE consists of three main components —“ Main Functions ” ( Additional file 11 ), “ Genome__label__Material|Data|Produce
ALICE , programmed in R and R - GUI , is the software with a user - friendly interface for an integrated genomic analysis of AF , LOH / LCSH , AI , and CNV / CNA . The software , reference databases , library files for APT , annotation files , test examples , and user manual can be downloaded from the ALICE homepage ( _CITE_ ). ALICE consists of three main components —“ Main Functions ” ( Additional file 11 ), “ Genome__label__Supplement|Website|Produce
Animals were identified following International Union for the Conservation of Nature ( IUCN ) taxonomic authority sources for mammals [ 17 ] and following the mammal taxonomy of the IUCN Red List . Further details are available on the equipment used and camera trap settings for the TEAM Terrestrial Vertebrate Monitoring Protocol [ 11 ]. All data were uploaded to the TEAM portal and are publicly available at _CITE_ teamnetwork . org . A total of 22437 images were recorded between 2008 and 2012 , resulting from an effort of 8725 camera trap sampling days ( see Table S1 in File S1 for details ).__label__Material|Data|Produce
After days of feeding , the samples with different diets were evidently separated into different clusters . The classification result was better than that acquired by count - based approaches ( Additional file 1 : Figure S4 ), which confirmed the value of XIC intensity - based label - free quantitative strategy in metaproteomics . Generic type output format for post - analysis To further facilitate the analysis of metaproteomics dataset by biologists , a BIOM ( Biological Observation Matrix , _CITE_ ) [ 21 ] format result file was also exported . In this format , a table was used to store the amount of biological observations from all the samples . For example , in metagenomics , the observation is the taxa identity and the corresponding value is the count of reads .__label__Material|Data|Use
Supplemental Table S1 , full data set available at _CITE_ ). In the principal component subfeature space ( Pimentel et al ., 2014 ), interphase nuclei appeared as a single compact cluster , whereas the different outlier phenotype groups scattered broadly in different regions . All three novelty detection methods implemented in CellCognition Explorer accurately classified normal interphase nuclei as inlier objects and other morphologies as outliers , consistent with phenotype scoring with supervised analysis ( Figure 2b ).__label__Material|Data|Produce
Supplemental Table S1 , full data set available at _CITE_ ). In the principal component subfeature space ( Pimentel et al ., 2014 ), interphase nuclei appeared as a single compact cluster , whereas the different outlier phenotype groups scattered broadly in different regions . All three novelty detection methods implemented in CellCognition Explorer accurately classified normal interphase nuclei as inlier objects and other morphologies as outliers , consistent with phenotype scoring with supervised analysis ( Figure 2b ).__label__Supplement|Document|Produce
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ © The Author ( s ) 2018__label__Supplement|License|Other
Our blog ( http :// blog . guidetopharmacology . org /) includes detailed release descriptions , new features , and technical ‘ how to ’ items . One of us ( CS ) maintains an individual technical blog where GtoPdb topics are sometimes coupled by being briefly introduced in the GtoPdb blog but expanded on in the individual posts ( http :// cdsouthan . blogspot . com /). Our Slideshare account ( _CITE_ ) is used for sharing slide sets and posters with the community and has proved popular . Users will find that presentations include descriptions of content , mining approaches and utilities that extend beyond what is documented on the site . We have also added a set of generic slides which can be used by anyone presenting or teaching on GtoPdb .__label__Supplement|Website|Produce
Overall , SGN has contributed more than 20 , 000 manually curated gene and phenotype annotations for 14 Solanaceae species , and plans to develop PO annotations for expression data for each published Solanaceae transcriptome in the near future . The Maize Genetics and Genomics Database ( MaizeGDB ). The PO grew out of its third founding member MaizeGDB ’ s ( _CITE_ ) contribution to maize - specific controlled vocabulary ( Vincent et al . 2003 ). Currently , the maize data hosted in the PO database include genes , genetic stocks and gene models .__label__Material|Data|Introduce
MODIS Satellite data . The temporal variation of the C3 turbidity sensor , mounted in the Wave Glider float , was validated using imagery from the Moderate Resolution Imaging PLOS ONE | DOI : 10 . 1371 / journal . pone . 0128948 June 12 , 2015 6 / 19 Wave Glider Monitoring of Marine Sandbanks Spectroradiometer ( MODIS ) ( via MUMM / GRIMAS extraction tool ( _CITE_ ) [ 42 ]. The main motivation for this analysis was to have an independent dataset to verify and provide context for the variations in the field dataset . For each Wave Glider record , a nearest window of 25 pixels ( 1 km x 1 km ) was defined .__label__Method|Tool|Use
All the sequencing data have been deposited in the NCBI short read archive under the Bioproject PRJNA219226 ( _CITE_ ). Other supporting data are included as additional files .__label__Material|Data|Use
Archaeological evidence for infection with this parasite in indigenous , pre - Columbian populations is currently limited to a single report of P . vivax antigens being visualized by immunohistochemistry in the liver and spleen of South American mummies dating from 3 , 000 to 600 years ago [ 8 ]. Interestingly , specific antibodies failed to detect P . falciparum antigens in these same samples [ 8 ]. These findings are consistent with the hypothesis that P . vivax , but not P . falciparum , PLOS Neglected Tropical Diseases | _CITE_ July 31 , 2017 2 / 24 Plasmodium vivaxgenomes from the New World was brought to the New World by early human migrations from East Asia or the Western Pacific [ 5 ], but more specific molecular techniques are required to confirm them [ 9 ]. Nevertheless , present - day New World populations of P . vivax appear to be more closely related to extant African and South Asian parasites and now extinct European lineages than to East Asian and Melanesian strains [ 10 – 12 ], consistent with much more recent parasite migrations with European conquerors and African slaves during the colonial era [ 7 , 13 ]. Clinical isolates from Brazil are underrepresented in global genomic analyses of P . vivax [ 11 , 14 ].__label__Supplement|Paper|Introduce
RoM is calculated as the mean CSF concentration in the AD group divided by the mean CSF concentration in the Ctl group for each marker in each study . Standard error for the natural logarithm for the RoM in each study , SE ( ln [ RoM ]) was calculated according to Friedrich [ 32 , 33 ]: SE [ ln ( RoM )] = SQRT (( 1 / n AD ) ( Standard Deviation AD / mean AD ) 2 + ( 1 / n Ctl ) ( Standard Deviation Ctl / mean Ctl ) 2 ), where “ n ” is the number of patients in the AD and Ctl groups , respectively . These calculations were performed in an Excel spreadsheet prior to entering the resulting data into the meta - analysis software of the Cochrane Collaboration , Review Manager , Version 5 ( Revman , Cochrane Collaboration , Oxford , England , freely available at _CITE_ ) and analyzed using the random effects model . Revman software uses standard equations for inverse variance weighting . Random effects analysis was performed using the DerSimonian and Laird [ 34 ] method which incorporates heterogeneity leading to wider ( more conservative ) confidence intervals when heterogeneity is present .__label__Method|Tool|Use
A major challenge in examining human SNP data is assessing which variants are more likely to be involved in having damaging effects on the structure and function of a gene / protein . GeneSNP - VISTA is an interactive visual tool for highly efficient analysis of large amounts of SNP data to determine a subset that may be of relevance to Page 5 of 7 ( page number not for citation purposes ) BMC Bioinformatics 2005 , 6 : 292 _CITE_ explaining human disease . As shown in Figure 1 , all the information about a SNP ( type , location on genomic structure , frequency of occurrence , type of amino acid change and the positions conservation ) allows a scientist to determine whether a SNP is a possible causative mutation . By providing a visually integrated representation of SNPs data with genomic structure and protein conservation , GeneSNP - VISTA facilitates the screening of causative mutations from re - sequencing of a large set of appropriate candidate genes in individuals with a given disease .__label__Supplement|Paper|Introduce
The terrain and river network data used for this study are based on the Hydrosheds project [ Lehner et al ., 2008 ] and can be obtained from http :/ www . hydrosheds . org /. Corrections for vegetation biases employ the Simard et al . [ 2011 ] vegetation heights data set , available at _CITE_ Corrections for urban bias employ the impervious surface area data set derived from satellite luminosity data [ Elvidge et al ., 2007 ], available from the National Oceanic and Atmospheric Administration ( NOAA ) Earth Observation Group website https :// www . ngdc . noaa . gov / eog /. The flow data are derived from the method , results , and supplementary data presented by Smith et al .__label__Material|Data|Use
Genome - wide differentially marked regions Genome - wide regions that were differentially marked by histone modifications were identified and annotated to RefSeq genes by diffReps ( 35 ) with default settings . Prediction of enhancers and super - enhancers Enhancers were defined as H3K4me3low + H3K27achigh and predicted using CSI - ANN ( 36 ) with normalized H3K4me3 and H3K27ac signals as inputs . The predicted enhancers and H3K27ac signal were then used as inputs to predict super - enhancers using ROSE ( _CITE_ ). ROSE stitches enhancers within 15 kb and excludes enhancers within ± 2 kb from annotated TSSs . The coordinates of the super - enhancers were extracted as targets for GO term enrichment analysis using GREAT with default settings ( 37 ).__label__Method|Tool|Use
aMatlab Publishing Markup refers to specific keys such as %% or _ _ which allows not only inserting comments into your Matlab code , but also format it for then publish the code automatically into an executable and readable format , see _CITE_ bWhen uploading data to OpenfMRI you need to ensure the structural data are defaced appropriately – the website also offers to use their own defacing tool , see https :// github . com / poldrack / openfmri / tree / master / pipeline / facemask . cThanks to Dorothy Bishop for pointing to this .__label__Supplement|Document|Introduce
© The Author ( s ). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . He et al . BMC Plant Biology ( 2018 ) 18 : 44 Page 2 of 15 Liu et al .__label__Supplement|License|Other
Additionally , pregnancy is characterized by significant and complex changes in immune parameters which is likely to impact on sex specific differences in gene expression . Further investigation is warranted into the complexity of sex specific differences in gene expression throughout development . PLOS Genetics | _CITE_ September 15 , 2017 17 / 38 The sheep gene expression atlas Differential expression of genes between the Texel and TxBF The majority of commercially - produced livestock are a cross of two or more different production breeds with distinct desired traits [ 95 ]. For example , in the UK , the crossing of lighter upland sheep breeds with heavier lowland meat breeds optimises carcass quality , lambing rate , growth rate and survivability [ 95 ]. In developing countries , sustainable crossing of indigenous small ruminants with elite western breeds is one approach to improve productivity [ 96 , 97 ].__label__Supplement|Paper|Introduce
Mitogenome rearrangements are usually analyzed at the level of signed permutations that represent the © The Author ( s ) 2017 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated .__label__Supplement|License|Other
It should be noted that this validation is still far from perfect since bound enhancers can regulate transcription from a distance greater than the conservative limit considered here . We also perform Titsias et al . BMC Systems Biology 2012 , 6 : 53 Page 10 of 21 _CITE_ similar evaluation using TF - gene links in the Drosophila Interaction Database ( DroID ) [ 35 ]. This database in not specific to development and may thus include links that are not active in our data . We only include the 5521 test genes with some predicted TF regulators in the database .__label__Method|Tool|Use
It should be noted that this validation is still far from perfect since bound enhancers can regulate transcription from a distance greater than the conservative limit considered here . We also perform Titsias et al . BMC Systems Biology 2012 , 6 : 53 Page 10 of 21 _CITE_ similar evaluation using TF - gene links in the Drosophila Interaction Database ( DroID ) [ 35 ]. This database in not specific to development and may thus include links that are not active in our data . We only include the 5521 test genes with some predicted TF regulators in the database .__label__Supplement|Paper|Use
The mission of PDBe ( http :// pdbe . org /) is “ Bringing structure to biology ” and this underpins our ambition to become an integrated resource of structural and related biological information for use by the entire biomedical community ( Velankar and Kleywegt , 2011 ; Velankar et al ., 2010 , 2011 , 2012 ). As part of this effort , a number of web - based tools and services are being and have been developed that facilitate discovery and analysis of 3DEM data . Expert users benefit from a powerful search and browse service for EMDB data ( EMSearch ; _CITE_ EMBrowse ; http :// pdbe . org / embrowse ) as well as a data - mining tool ( EMStats ; http :// pdbe . org / emstats ). This paper describes a set of web - based tools developed at PDBe that facilitate visualisation , analysis and validation of archived 3DEM data by expert and non - expert users alike .__label__Supplement|Website|Extent
resistance and the mechanisms of resistance development and diffusion in both hospitals and the community . The manuscript management system is completely online and includes a very quick and fair peerreview system , which is all easy to use . Visit _CITE_ testimonials . php to read real quotes from published authors .__label__Method|Code|Introduce
patterns that can succeed with partial instantiation , yielding maximal “ best match ” result graph . Another challenge in implementation of the query engine was that unlike many graph database query languages such as Lorel or GraphLog , [ 39 ], SPARQL does not provide an explicit function for transitive closure to answer reachability queries ( _CITE_ ). Reachability queries involving computation of transitive closure is an important characteristic of provenance queries to retrieve the history of an entity beginning with its creation . In case of the provenance query engine , the query composer computes the transitive closure over the & lt ; process , preceded_by & gt ; combination to retrieve all individuals of the process class linked to the input value by the preceded_by property .__label__Material|Data|Use
All the statistical analyses were performed using R ( _CITE_ ). Sensitivity and specificity were estimated using individual PCR frequencies as we have the Tlex2 frequencies estimates using the same exact strains . Sensitivity and specificity were computed such as : ( i ) Sensitivity = number of correct presence calls / total number of presence calls in the validation data set and ( ii ) Specificity = number of correct absence calls / total number of absence calls in the validation data set .__label__Method|Code|Use
structural changes and its influence on gene expression status . vCGH prediction on an independent dataset of 176 DLBCLs We applied vCGH which is trained by the paired GEP and CGH data on the 190 DLBCLs , to an independent dataset of 176 DLBCLs with the GEP data [ 42 ]. The GEP data of the 176 DLBCLs were downloaded at _CITE_ Since the CGH data was not available for the 176 DLBCLs , we compared the vCGH - predicted CNAs for the 176 DLBCLs with the CGH - identified CNAs for the 190 DLBCLs because a specific tumor type would feature specific genetic abnormalities even in different patient cohorts . Figure 9 showed the prediction results on the 176 DLBCLs in comparison with the CGH data on the 190 DLBCLs .__label__Material|Data|Use
A second batch of 16 elite bulls were sequenced by AROS ( Denmark ) on a HiSeq 2000 platform according to the manufacturer ’ s protocols . Samples were prepared for paired - end sequencing ( 2 × 100 bp ) using the TruSeq kit and sequenced with the Illumina V3 kit ( Illumina , San Diego ) to generate an average coverage of 10 ×. Finally , the third batch that included 96 NR elite bulls and 31 NR cows were sequenced by the Norwegian Sequencing Centre ( _CITE_ no ) on a HiSeq 2500 platform according to the manufacturer ’ s protocols . Samples were prepared for pairedend sequencing ( 2 × 125 bp ) by using the TruSeq DNA PCR - free library preparation kits and sequenced with the Illumina V4 kit ( Illumina , San Diego ) to generate an average coverage of 9 ×.__label__Method|Tool|Use
expanded the sequence database to include an additional 1 , 342 sequences ( 6 , 066 total ). Data are stored through figshare ( figshare . com ) and can be retrieved via wget _CITE_ Phylogenetic analysis . Sequences retrieved from the HIV database were examined following one of two strategies .__label__Material|Data|Produce
The funders had no role in study design , data collection and interpretation , or the decision to submit the work for publication . Author contributions EG , RS , ES , Initiated , performed , and wrote the work Author ORCIDs Elad Ganmor , _CITE_ Ethics Animal experimentation : All experiments with salamanders were approved by Ben - Gurion University of the Negev IACUC , and were in accordance with government regulations of the State of Israel . Additional files Supplementary file · Source code 1 . MATLAB code used in the clustering and information analyses .__label__Supplement|Paper|Introduce
scientificName The full scientific name , with authorship and date information if known ( http :// rs . tdwg . org / dwc / terms / scientificName ). acceptedNameUsage The full name , with authorship and date information if known , of the currently valid ( zoological ) taxon ( http :// rs . tdwg . org / dwc / terms / acceptedNameUsage ). originalNameUsage The original combination ( genus and species group names ), as firstly established under the rules of the associated nomenclaturalCode ( _CITE_ ). family The full scientific name of the family in which the taxon is classified ( http :// rs . tdwg . org / dwc / terms / family ). familyNameId An identifier for the family name .__label__Method|Code|Use
Most dataset usage indicators are associated with scientometric and webometric analyses , except that so far linking behaviour has not developed with respect to biodiversity dataset use . On the other hand , similar to the Web , dynamics like versioning and additions to already stored datasets are feasible . Usage indicators Page 5 of 11 ( page number not for citation purposes ) BMC Bioinformatics 2009 , 10 ( Suppl 14 ): S2 _CITE_ commonly measure interests , recognition or impact of the objects analysed , via visits , viewing and downloading activities ( Nielsen Media , Ratings ). By visiting ( searching or retrieval ) and viewing dataset records one may assume interest in the dataset , whilst the volume of downloading volume may demonstrate usage . Logging and analysing these activities are common to Web search engine log analyses and the issues of isolation of search sessions done by the same ‘ user ’ or ‘ visitor ’ [ 72 ] (.__label__Supplement|Paper|Introduce
Sample processing . We generated WGS data for three species of oak gall wasp hosts , eight parasitoids , and one inquiline . We also reanalyzed WGS data generated for a pilot study for B . pallida , another gall wasp species ( 43 ) ( ENA Sequence Read Archive : _CITE_ ). For each species we sampled a minimum of two male individuals from each of three refugial populations spanning the Western Palearctic : Iberia ( west ), the Balkans ( center ), and Iran ( east ) ( SI Appendix , Table S2 ). Only a single eastern sample was available for two species , Ormyrus pomaceus and B . pallida , and a single western sample for S . umbraculus .__label__Material|Data|Use
© The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Abstract Background : Depression is a highly prevalent and costly disorder . Effective treatments are available but are not always delivered to the right person at the right time , with both under - and over - treatment a problem .__label__Supplement|License|Other
OBCS - specific terms were generated with IDs using the prefix “ OBCS_ ” followed by auto - generated seven - digit numbers . OBCS is released under Creative Commons Attribution ( CC BY ) 3 . 0 License , and the OBCS source code is freely available at : https :// github . com / obcs / obcs and also on the Ontobee [ 13 ] ( http :// www . ontobee . org / ontology / OBCS ) and NCBO BioPortal ( http :// purl . bioontology . org / ontology / OBCS ) websites . The summary information of ontology terms in OBCS based on term types and resources can be found here : _CITE_ The RDF triples for the OBCS ontology have been saved in the He group Triple store [ 13 ], which allows easy retrieval of related OBCS contents using Semantic Web SPARQL technology . OBCS can be queried from the Ontobee ’ s SPARQL query endpoint ( http :// www . ontobee . org / sparql ) [ 13 ].__label__Supplement|Document|Produce
For all demographic rates , the bias in the parameter of interest was & lt ; 5 % for the full range of biases in the survey data considered here ( i . e . male detectability being between - 70 % and 130 % that of females and calves ). However , if females and yearlings ( FY in PRE - surveys ) had a higher detectability than calves of the year , f was overestimated and f1 was underestimated , with an opposite pattern emerging if FY were more detectable than calves in the spring surveys ( Panel PLOS ONE | _CITE_ March 29 , 2018 11 / 19 Hierarchical integrated change - in - ratio model for wildlife monitoring Fig 4 . Sensitivity analysis . Sensitivity analysis examining the effects of unequal detection probability among age - and sex classes on estimated model parameters .__label__Supplement|Paper|Introduce
All authors contributed to discussion and revision of the final manuscript . Additional information Supplementary Information accompanies this paper at http :// www . nature . com / naturecommunications Competing financial interests : The authors declare no competing financial interests . Reprints and permission information is available online at _CITE_ How to cite this article : Herzschuh , U . et al . Glacial legacies on interglacial vegetation at the Pliocene - Pleistocene transition in NE asia . Nat .__label__Supplement|License|Other
The minipig whole genome shotgun project has been deposited at DDBJ / EMBL / GenBank under the accession LIDP00000000 . The microarray data from this study have been deposited at the NCBI Gene Expression Omnibus ( GEO ) ( _CITE_ ) under accession numbers GSE71438 and GSE71441 . PDB code for crystal structures ( http :// www . rcsb . org /): 3G8I , 1FLT . Additional files Additional file 1 : Table S1 .__label__Material|Data|Use
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ Metadata associated with this Data Descriptor is available at http :// www . nature . com / sdata / and is released under the CC0 waiver to maximize reuse . © The Author ( s ) 2016 SCIENTIFIC DATA 13 : 160073 1 DOI : 10 . 1038 / sdata . 2016 . 73 12__label__Supplement|License|Other
Supplementary Information accompanies this paper at https :// doi . org / 10 . 1038 / s41467017 - 01995 - 2 . Competing interests : The authors declare no competing financial interests . Reprints and permission information is available online at _CITE_ Publisher & apos ; s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations . Open Access This article is licensed under a Creative Commons Attribution 4 . 0 International License , which permits use , sharing , adaptation , distribution and reproduction in any medium or format , as long as you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material .__label__Supplement|License|Other
Data shown as mean ± s . e . m firing rate in Cartesian or polar coordinates . Scale bars indicate firing rates of 20 Hz . Data available at _CITE_ 4955408 . v1 . ( TIF ) S10 Fig .__label__Material|Data|Produce
We could investigate which of those numerous quality statistics are the most useful . We could also see which metrics are useless or redundant and therefore do not have to be generated at all . Under the most optimistic PLOS ONE | _CITE_ April 25 , 2018 12 / 14 Estimation of the proportion of true discoveries in single nucleotide variant detection for human data scenario , it might turn out that using just QUAL results in a level of accuracy that is so high that none of other predictors are required and the same regression coefficients work well for both WES and WGS . Researchers who develop variant calling applications could employ the methodology outlined in this paper for a similar purpose . That would also allow them to supply their software with an accurate PPV estimation tool that would be of great advantage to the end user .__label__Supplement|Paper|Introduce
Primary protein sequence database search for protein identification The acquired MS / MS data from the instrument were extracted and annotated with amino acid sequences from a custom built database using the ParagonTM Algorithm : 5 . 0 . 0 . 0 , 4767 [ 46 ] ( ProteinPilotTM Software 5 . 0 , Revision Number : 4769 , SCIEX , USA .). The custom composite database ( 62 , 025 sequences ; 29 , 099 , 284 residues ) used in ParagonTM , with added common contaminants was assembled in FASTA format downloaded on 29th July , 2015 from a repository of non - redundant and predicted protein sequences of Ovis aries , Bos taurus and Capra hircus sourced from UniProtKB ( Universal Protein Resource Knowledgebase - http :// www . uniprot . org /). Another sheep ( Ovis aries ) only custom database ( 27 , 393 sequences , 13 , 114 , 569 residues ) with added contaminants from The common Repository of Adventitious Proteins , cRAP ( _CITE_ ) was assembled in FASTA format ( 26 Jul , 2016 ) from UniProtKB was used for sheep protein validation . For ProteinPilotTM searches , the following settings were selected : Sample type : Identification ; Cys Alkylation : Iodoacetamide ; Digestion : Trypsin ; Instrument : TripleTOF 5600 +; Special Factors : Urea denaturation ; Species : None ; Search effort : Thorough ID ; ID Focus : Amino acid substitution ; Results Quality : Detected protein threshold [ Unused ProtScore ( Conf )] > 0 . 05 with false discovery rate ( FDR ) selected . Annotations were only retrieved from UniProt during composite searches .__label__Material|Data|Use
Here we used IBA as a proxy for migratory bird density , but including densities of migratory wild birds by migratory seasons could give insights of locations and time periods that poses higher risk for AI transmission . An alternative approach could be used in the future to model the fraction of positive cases instead of eliminating samples with identical locations . It could also be of PLOS ONE | _CITE_ January 31 , 2018 10 / 15 Detecting high risk areas for avian influenza outbreaks in California interest to analyze whether there are many false negatives due to small sample sizes at some locations using the probability of detection approaches as described in previous studies [ 53 ].__label__Supplement|Paper|Introduce
Availability of data and materials The code for time series generation and statistical analyses is available via the seqtime R package ( http :// hallucigenia - sparsa . github . io / seqtime /). Test time series are available in Github ( _CITE_ timeseries ).__label__Material|Data|Produce
Toward this end , valuable efforts have been made in the National Center for Biotechnology Information ( NCBI ) ( 4 ) and the European Bioinformatics Institute ( EBI ) ( 5 ). Specifically , dbSNP ( 6 ) and dbVar ( 7 ) are two major resources in NCBI for archiving worldwide genome variations , and the counterpart in EBI , European Variation Archive ( EVA ) ( 8 ), imports variation data primarily from these two resources . Unfortunately , it was just recently announced that dbSNP and dbVar will phase out support for non - human data and stop accepting non - human data submissions from 1 September 2017 ( _CITE_ ), consequently presenting formidable challenges in deposition and integration of publicly available variation data at a global scale , especially for non - human variant data . Besides , existing related databases do not well manage phenotype information , particularly for non - human species ; although they can be obtained from controlled - access repositories for human ( such as dbGaP ( 9 )), genotypeto - phenotype associations for non - human species are considerably absent in existing related databases . Here we present GVM ( Genome Variation Map ; http :// bigd . big . ac . cng / gvm /), a public data repository of genome variations , including single nucleotide polymorphisms__label__Material|Data|Introduce
The inset shows the thermal migration coefficient ( TMC ) that characterizes drift in the temperature gradient , calculated from three independent experiments ( such as that shown in the main panel ). Positive values of TMC correspond to thermophilic response , whereas negative values of TMC correspond to cryophilic response . DOI : _CITE_ The following source data and figure supplements are available for figure 1 :__label__Supplement|Paper|Produce
The BSF code , written in C ++, is an open source software project licensed under BSD . Source can be found at https :// github . com / PNNL - Comp - Mass - Spec / bsf - core . For ease of access , we have written a python wrapper to interface with the BSF C ++ library , _CITE_ Python extensions and numpy C - API are employed to implement the python wrapper . The BSF is accessed through an API which ensures that input data is appropriate and meaningful , and interprets the output tables which are returned .__label__Method|Code|Use
It is widely used across the world and is a core component of the emerging NIH - backed biomedical informatics backbone , including the Biomedical Informatics Research Network ( BIRN ) and National Alliance for Medical Image Computing ( NA - MIC ). XNAT includes a secure database backend and a rich web - based user interface . XNAT Central ( _CITE_ ) is a public access repository for neuroimaging and related data operated by the NRG . XNAT Central is built on the XNAT data management platform . XNAT Central includes a number of secure tools for storing and accessing images including a DICOM server , a web services API , and a user friendly website .__label__Material|Data|Introduce
The location of DHS2 is marked by the green shade at the center . ( D ) Significant positive selection on the 12 human regulatory sequences of ADRA2C ( DHS1 ~ DHS12 ). We applied INSIGHT [ 37 , 38 ] to infer selection on the collection of the human sequences from patterns of PLOS Genetics | _CITE_ April 19 , 2018 9 / 21 Selection on the regulation of sympathetic nervous activity in humans and chimpanzees polymorphism and divergence with chimpanzee as the outgroup . Dp indicates the number of divergences driven by positive selection and is used as a measure of positive selection . Pw indicates the number of polymorphisms under weak negative selection .__label__Supplement|Paper|Extent
( page number not for citation purposes ) BMC Bioinformatics 2009 , 10 ( Suppl 10 ): S2 _CITE_ dm : UMLClass and dm : UMLAttribute , respectively . UML class hierarchies are represented with the rdfs : subClassOf construct . UML class associations ( a has_a b ) are modeled as rdfs : subPropertyOf umlAssociation .__label__Supplement|Paper|Introduce
“ In terms of governance and medication safety ... I ' m a bit of a one - man band ... because ... There isn ' t any other people ( in my team )” P1 ( Pharmacist , LA ) This feeling of lack of support had detrimental effects on impetus for improving medication safety , some MedsST leads had overcome this lack of support by forming or joining external medication safety networks , where they could learn about how to use the MedsST and its data for improvement . The support networks were often developed by pro - active MedsST leads , and enabled wards , organisations and regions to share innovative methods of using data for improvement , in addition to learning how challenges with data collection could be overcome . In most organisations the MedsST lead was also an MSO , however , in one organisation the MSO was not involved with the MedsST and the role of MedsST lead had been offered to a PLOS ONE | _CITE_ February28 , 2018 8 / 19 An evaluation of the implementation of the Medication Safety Thermometer pharmacist with personal interest in quality improvement . This participant had unsuccessfully attempted to contact other organisations to learn how they had implemented the MedsST in primary care settings . Without internal or external support networks , the MedsST role had proven burdensome for the pharmacist who felt unsupported and that they were not being “ listened to ” ( P4 , Pharmacist , LA ) about how the MedsST data could be used for further improvement work .__label__Supplement|Paper|Introduce
To ensure individual social network metrics , i . e . eigenvector centrality , graph strength , and degree , were non - randomly distributed we conducted two types of network randomization procedures . First , we employed a randomization technique where social association matrices PLOS ONE | _CITE_ March1 , 2018 5 / 21 Social consistency and plasticity across changes in population density were permutated with 1 , 000 iterations and network metrics were re - calculated at each permutation [ 53 ]. Permutations were computed based on social association matrices as opposed to the data stream , i . e . frequency of associations generated from proximity collars , because all animals in our study were collared and proximity collars collect data continuously throughout the experiment .__label__Supplement|Paper|Extent
The 7633 genes were used to generate the topology overlap matrix . SGN codes are found at https :// solgenomics . net /. Supplementary Figure S3 | Gene expression profiles for AO , Solyc04g054690 ; GLD , Solyc10g079470 and MDHAR , Solyc09g009390 as obtained from the TomExpress tool ( _CITE_ ). Supplementary Figure S4 | Gene expression profiles for AO , Solyc04g054690 ; GLD , Solyc10g079470 and MDHAR , Solyc09g009390 as obtained from the Solgenomics expression atlas tool ( http :// tea . solgenomics . net ). Supplementary Table S1 | Ascorbate and dehydroascorbate levels in pericarp fruit 20 days after anthesis of transgenic lines and WT .__label__Method|Tool|Use
They have been used to explore the history of languages using synchronic data . For example , [ 40 ] compute a distance matrix between languages based on cognate words . The resulting Neighbor - Net reflected PLOS ONE | _CITE_ April5 , 2017 12 / 35 Why are some languages confused for others ? well - known historical splits in language families , but also clusters of areal phenomena resulting from languages of different genealogy influencing each other through contact ( cf . [ 41 ]).__label__Supplement|Paper|Introduce
Sample data used for demonstration of CARD functionality . We used three data sets to demonstrate different features of CARD and highlight the utility of the application . We also include template files in ‘ CARD format ’ in the help page ( at _CITE_ ) for new users to format their data for upload . The sample data sets used were as follows . HDAC - inhibitor screen data : a genome - scale siRNA screen ( using a Dharmacon pooled library — pool of 4 unique siRNAs per gene ) to identify genes that sensitize cells to HDAC - inhibitor ( vorinostat )- induced cell death17 .__label__Supplement|Document|Produce
The entire set of permissive enhancers found in the FANTOM5 data was downloaded in bed file format from http :// enhancer . binf . ku . dk / presets / perm issive_enhancers . bed on 26 August 2015 . The gene - report function was used in PLINK to search for any SNPs that were located within permissive enhancer regions . SNPs that were located within each enhancer region were then matched with the set of correlated expressed promoters for FANTOM5 enhancer transcription start sites downloaded from _CITE_ on 25 August 2015 .__label__Material|Data|Use
Three taxa of different cell or filament length were grown in near - saturating light while daily growth measurements were taken . When cultures reached exponential and then again during stationary phase , sinking assays were performed prior to diatom fixing for cell and filament sizing . PLOS ONE | _CITE_ October 3 , 2017 4 / 16 Phytoplankton sinking rate screening Fig 1 . Sinking assay schematic and data . One well is depicted at different time points , left to right , during the sinking assay .__label__Supplement|Paper|Introduce
The choice of the algorithm is critical , as different approaches might produce different results . Users may choose in fact the most suitable analytical model for their data or develop additional module descriptions interfacing other computational tools . Additional tools , workflows and updates are available on the LONI pipeline Navigator website : _CITE_ As a large number of different file formats are involved in these workflows , we provide a glossary and examples of all the formats encountered in Supplementary Table S1 . We have implemented a preprocessing module that allows extracting a subset of reads to perform a validation and initial testing of pipeline modules before running an entire dataset .__label__Supplement|Website|Use
The main clinical characteristics of mothers and offspring of the subpopulation for DNA microarray analysis are presented in Additional file 1 : Table S1 . Sedlmeier er al . BMC Genomics 2014 , 15 : 941 Page 4 of 19 _CITE_ Total RNAs with a mean RIN of 6 . 8 ± 0 . 3 ( SD ) were hybridized to Affymetrix Custom Array - NuGO_Hs1a520180 array , containing 17699 genes [ 28 ]. All steps and controls of DNA microarray processing were performed according to the manufacturer ’ s protocol ( Affymetrix Inc ., Santa Clara , CA , USA ). RNA from each individual placenta was applied to individual DNA microarrays which were then processed altogether .__label__Supplement|Paper|Introduce
More information and access to the web interface are provided online : http :// www . sanger . ac . uk / resources / databases / phenodigm . Related work A recent review undertaken by Bo ¨ rnigen et al . ( 18 ) ( summary of results available from _CITE_ ) showed that most of the investigated gene prioritization tools apply a ‘ guilt - by association ’ approach , e . g . Endeavour ( 19 ), GeneWanderer ( 20 ) or G2D ( 21 ). Using such an approach requires established gene – disease associations to use those to generate disease and gene profiles .__label__Supplement|Document|Produce
and statistical significance vs . SMA DMSO ( B .) or 3813 DMSO controls ( D ) was assessed using Student ’ s t - test : P & lt ; 0 . 05 (*), P & lt ; 0 . 01 (**) or otherwise non - significant where not indicated . _CITE_ pharmacological effect . These possibilities are consistent with the observation that members of this chemical series also caused activation of thymidine kinase promoter - driven β - lactamase activity [ 6 ]. It should also be noted that transcriptional activation of the SMN2 gene would be predicted to increase SMNΔ7 transcript levels ; however , as described above , the small PLOS ONE | https :// doi . org / 10 . 1371 / journal . pone . 0185079 September 25 , 2017 15 / 31 2 , 4 diaminoquinazoline inhibitors of DcpS and their effects on SMN transcripts and in SMA mice pathways were significantly affected ( P & lt ; 0 . 05 ) ( S1 Fig ).__label__Supplement|Paper|Introduce
We further rarefied the datasets to exclude the top 1 % of step lengths for each individual , which are thought to be associated with calf capture or predator avoidance behavior [ 8 ]. After rarefication , the mean per - collar fix rate ( number of successful fixes per number of attempts ; [ 22 ]) was 80 % ( range : 53 – 93 %). MR appeared to have a higher mean per - collar fix PLOS ONE | _CITE_ February 21 , 2018 5 / 16 Caribou movement rate and calf survival Fig 2 . Examples of a priori movement models used in an individual - based method to infer parturition and calf mortality events in female woodland caribou ( Rangifer tarandus caribou ) ( sensu 8 ) and actual movement characteristics of female caribou . Gray line indicates the movement pattern of female .__label__Supplement|Paper|Compare
In the last few decades the country has experienced a number of health and demographic shifts including the HIV pandemic , the rise in prevalence of noncommunicable disease [ 1 ], and the decline over time of fertility itself [ 2 ]. The calculation of fertility rates from various data sources across the country and sub - Saharan Africa as a whole has proven useful in looking at the impact of HIV / AIDS [ 3 , 4 ], increased education [ 5 ], delayed marriage [ 4 ], premarital reproduction [ 4 , 6 , 7 ], contraceptive use [ 4 ], and the development of refugee populations [ 8 ], as well as more © The Author ( s ) 2018 . This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Eyre et al .__label__Supplement|License|Other
Publish with BioMed Central and every scientist can read your work free of charge & quot ; BioMed Central will be the most significant development for disseminating the results of biomedical research in our lifetime .& quot ; Sir Paul Nurse , Cancer Research UK Your research papers will be : available free of charge to the entire biomedical community peer reviewed and published immediately upon acceptance cited in PubMed and archived on PubMed Central yours — you keep the copyright Submit your manuscript here : _CITE_ BioMedcentral 2 Page 9 of 9 ( page number not for citation purposes )__label__Supplement|Website|Produce
But viruses like HCMV , EBV , SV40 and low - risk HPVs were identified in some of the Chinese prostate tissues ( both cancer and adjacent samples ) in Data set 3 . The presence of both HPV and EBV gene sequences were present in an equal proportion of normal , benign , and PCa specimens from Australian males . However , using a range of analytical techniques including the in situ polymerase chain reaction ( IS - PCR ) and the standard liquid PCR before finally sequencing the product [ 21 ], no DNA virus transcripts were detected in the PCa samples using RNA - seq data from the TCGA Portal ( _CITE_ ) [ 61 , 62 ]. The different results from different studies are most likely attributable to sample differences : ethnic difference , pathological difference , etc . The Chinese sample 7N , 7T , 8N were infected by both P . acnes and other viruses .__label__Material|Data|Use
We found that the two most effective factors affecting sequencing data genotype quality were GQ and depth , and used both GQ & gt ; 20 and depth & gt ; 5 as genotype filters throughout the study unless otherwise specified . Guo et al . BMC Genomics 2012 , 13 : 194 Page 3 of 10 _CITE_ In addition , we selected 6 subjects from the Pilot 3 study of the 1000 Genomes Project , which designed a capture assay for the exons of 1000 genes ( 8 , 496 target regions , 1 . 4 million bases , average length 169 bp ) [ 8 ]. Currently , only SNP calls for positions inside the target regions were reported . We selected subjects that were also in the HapMap II so that we could evaluate SNP call quality of sequencing data by comparing them with HapMap genotypes .__label__Supplement|Paper|Introduce
Furthermore , we show that LOAD shares a similar localization of SNPs to monocyte - functional regions with Parkinson ’ s disease . Overall , we demonstrate that integrated genome annotations at the single tissue level provide a valuable tool for understanding the etiology of complex human diseases . Our PLOS Genetics | _CITE_ July24 , 2017 1 / 24 Tissue - specific functional annotation and Alzheimer ’ s disease genetics Council . PKC ’ s and SM ’ s efforts were supported by GenoSkyline - Plus annotations are freely available at http :// genocanyon . med . yale . edu / grant R01 AG042437 and U01 AG006781 . The GenoSkyline .__label__Supplement|Paper|Produce
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ © The Author ( s ) 2017__label__Supplement|License|Other
This second tier of regulation is more sensitive to H4K10 acetylation levels , resulting in downregulation of expression in HAT2 - depleted cells . This report presents the first data pointing to cell cycle - specific activation of promoters in trypanosomatids , thus uncovering new facets of gene regulation in this parasite family . PLOS Pathogens | _CITE_ September 22 , 2017 1 / 31 Cell cycle stage - specific transcriptional activation in Leishmania in study design , data collection and analysis , Author summary decision to publish , or preparation of the Leishmania donovani causes Visceral Leishmaniasis ( VL ), a disease that plagues the manuscript . world ’ s poor , particularly in Brazil , Sudan and the Indian sub - continent . If not treated in Competing interests : The authors have declared timely manner VL can be fatal , and due to emerging drug resistance the search for new that no competing interests exist .__label__Supplement|Paper|Introduce
This work is published under the standard license to publish agreement . After 12 months the work will become freely available and the license terms will switch to a Creative Commons AttributionNonCommercial - Share Alike 4 . 0 Unported License . Supplementary Information accompanies this paper on British Journal of Cancer website ( _CITE_ )__label__Supplement|Website|Produce
This work is published under the standard license to publish agreement . After 12 months the work will become freely available and the license terms will switch to a Creative Commons AttributionNonCommercial - Share Alike 4 . 0 Unported License . Supplementary Information accompanies this paper on British Journal of Cancer website ( _CITE_ )__label__Supplement|Document|Produce
Organizations could identify staff who are taking advantage of these resources , and target them for further professional development to boost their effectiveness ( although there could be privacy concerns with identifying staff in this way ). Furthermore , the importance of the CCNet coaches in internal knowledge sharing demonstrates their value to TNC in not only training staff on existing methods , but in adapting as the methods evolve as well . Finally , the finding that there is a substantial body of literature about CbD published without any TNC authors reveals that organizations may wish to consider collaborating with PLOS ONE | _CITE_ March 1 , 2018 19 / 24 Knowledge diffusion within a large conservation organization and beyond academics on these kinds of articles ( and sharing them ) as a potentially useful pathway of diffusion . Many scientists regularly monitor the peer - reviewed literature , and publishing is one way to broaden the audience reached on a given topic . How to improve future studies As noted above , the range of data used in this study to examine diffusion represents a significant advancement in the study of knowledge diffusion at large organizations , and we recommend that future research consider how to incorporate similar breadth and diversity of data .__label__Supplement|Paper|Introduce
The OIS score was used to define an initial data - driven list of diverse , representative genes that can serve as a proxy for the remainder of the genome . To ensure that the 1500 chosen genes adequately cover annotated biological pathways , we subsequently used the following procedure to iteratively refine this gene selection . Canonical pathways from the Molecular Signature Database ( MSigDB 4 . 0 ) database [ 14 ] were downloaded from the Broad Institute ( _CITE_ zip ). The descriptors for each of the 1 , 320 canonical pathways are included in S5 File . Then , the gene selection is iteratively refined to increase the pathway coverage until each pathway is represented by at least q genes .__label__Material|Data|Use
The OIS score was used to define an initial data - driven list of diverse , representative genes that can serve as a proxy for the remainder of the genome . To ensure that the 1500 chosen genes adequately cover annotated biological pathways , we subsequently used the following procedure to iteratively refine this gene selection . Canonical pathways from the Molecular Signature Database ( MSigDB 4 . 0 ) database [ 14 ] were downloaded from the Broad Institute ( _CITE_ zip ). The descriptors for each of the 1 , 320 canonical pathways are included in S5 File . Then , the gene selection is iteratively refined to increase the pathway coverage until each pathway is represented by at least q genes .__label__Supplement|Website|Use
Geospatial processing and analyses were conducted using ArcGIS v10 . 0 ( ESRI , CA , USA ), Geospatial Modelling Environment ( _CITE_ ), Matlab v R2013a__label__Method|Tool|Use
2009 ). As in Figure 1 , if the sequencing result matched Nipponbare genomic data , it was considered a Nipponbare - specific mutation and if it matched the japonica pools , it was considered a Nipponbare - specific sequencing error . Sequence acquisition , alignment , and comparison The reference genome of Nipponbare , version IRGSP 1 . 0 , used as the basis for this study , was downloaded from the International Rice Genome Sequencing Project 1 . 0 website ( _CITE_ ) on December 4 , 2014 ( Kawahara et al . 2013 ). O . sativa ssp .__label__Supplement|Website|Use
Specifically , our framework is used to perform comparative studies on two public benchmark datasets : OPPORTUNITY [ 15 ] and UniMiB - SHAR [ 16 ]. In addition , further experiments to assess the impact of hyper - parameters related to the data acquisition and segmentation steps of the ARC are carried out . To address the second issue regarding the lack of implementation details , we provide all the datasets , codes and details on how to use them needed to make the reproduction of our results as easy as possible ( all research materials used in our studies are provided at _CITE_ ). In addition , a generic evaluation script is made available . It can be re - used by other researchers to rigorously evaluate the performances of their own feature learning approaches on the datasets we used , in such a way that allows direct comparisons with our own results .__label__Supplement|Document|Produce
One of main challenges in designing such as a system is to come up with a scalable solution for storing and searching large amounts of data . We adopted a polyglot persistent model ( http :// martinfowler . com / bliki / PolyglotPersistence . html ), where the traditional relational database and the new NoSQL ( https :// en . wikipedia . org / wiki / NoSQL ) data stores are mixed to deliver performance . An Oracle database ( _CITE_ ) is used as the relational store for the biological and technical metadata , and multiple Solr stores ( http :// lucene . apache . org / solr /) are used for storing and enabling the search functionality for protein and peptide identifications , and spectra . The PRIDE Archive internal submission pipeline has three stages ( Figure 2 ): ( i ) file validation , where schema compliance is ensured for the XML - based file formats . In addition , a report is generated that is checked by PRIDE curators to ensure that the data are correct and can be submitted ( 24 ); ( ii ) data submission , where the data are actually submitted to PRIDE Archive .__label__Material|Data|Use
More details are provided in [ 1 ]. Transparency document . Supplementary material Transparency data associated with this article can be found in the online version at _CITE___label__Supplement|Document|Produce
Our results suggest that fishes may benefit from reduced access due to localized wave action at both sites , implying that wave power provides protection from fishing pressure [ 156 , 159 , 160 ]. These patterns have been observed elsewhere across the Hawaiian Archipelago , where coral reefs in wave exposed settings are often suppressed to a thin veneer and support high fish biomass , while coral reefs in sheltered areas have accreted slowly over time and support lower fish biomass [ 22 , 159 , 161 ]. PLOS ONE | _CITE_ March 14 , 2018 25 / 37 A linked land - sea modeling framework__label__Supplement|Paper|Introduce
RNA - seq reads for the wild - type AWC neurons are available in the NCBI Sequence Read Archive ( SRA ), under accession number SRP074082 ( _CITE_ SRP074082 ). RNA - seq reads for the two pools of whole C . elegans mixed - stage wild - type N2 larvae were previously published by Schwarz et al . ( 2012 ), and are available in the NCBI SRA under accession number SRP015688 ( http :// trace . ncbi . nlm . nih . gov / Traces / sra /? study = SRP015688 ).__label__Material|Data|Use
We also used R package glmnet ( https :// cran . r - project . org / web / packages / glmnet / index . html ) to compute lasso logistic regression with cross - validation . To assess the prediction accuracy of random forest and lasso regression , we computed the ROC curve and AUROC . To estimate the confidence interval for AUROC , we used the pROC R package ( _CITE_ ). We also computed the PR curve and AUPR to assess prediction accuracy when the classes were very imbalanced , especially for genome - wide analyses . For this , we used the PRROC R package ( https :// cran . r - project . org / web / packages / PRROC ).__label__Method|Code|Use
Exact implementations and specifications differ between pseudotime approaches particularly in the way “ similarity ” is defined . When applied to molecular data , pseudotime analysis typically captures some dominant mode of variation that corresponds to the continuous ( de ) activation of a set of biological pathways1 . Pseudotime analysis has gained particular popularity in the domain of single - cell gene expression analysis ( where each “ individual ” is now a single cell ) in which it has been applied to model the differentiation of single - cells2 – 9 ( a comprehensive catalogue of single - cell pseudotime algorithms can be obtained from _CITE_ ). Using advanced machine learning techniques , these methods can be applied to characterise complex , nonlinear behaviours , such as cell cycle , and modelling branching behaviours to allow , for example , the possibility of cell fate decision making and lineage reconstruction . However , these single - cell applications were predated b1y more general applications in modeling cancer progression 10 – 2 , as well as other progressive diseases1 - 16 Examples of such work provided early inspiration for single - cell pseudotime methods , e . g ., Monocle2 .__label__Method|Algorithm|Use
Data were saved in the 80 51 microcontroller memory , which was able to keep one all sensors recorded every hour along two weeks . Thus , it was necessary to download the twice a mo nth . Data were collected with this system during February and the last quarter e inconvenience of having to access every two weeks the vault cornice where the ller was located , in January 2008 it was decided to communicate the 8051 microcontroller der a LabVIEW ( _CITE_ ) environment in order to save the data in the e data acquisition frequency was increased to one recording per minute because the PC s able to store nearly an unlimited data amount . This system presented new problems : ( i ) the computer had to be connected permanently and ( ii ) the PC operative system crashed from time to time due to unknown causes . Because of this , data from several periods of 2008 were lost , and it was necessary to check periodically that the PC was working correctly .__label__Method|Tool|Use
points into neighborhoods , can avoid unnecessary cluster fragmentation ( common in hierarchical clustering ) by simultaneously clustering genes and cells [ 33 ]. We performed a comparative evaluation of SINCERA with three recently available singlecell RNA - seq analysis tools , SNN - Cliq [ 32 ], scLVM [ 27 ] and SINGuLAR Analysis Toolset ( _CITE_ ), using three single cell data sets produced by different techniques from a variety of contexts in human and mouse , including the E16 . 5 mouse lung single cells ( n = 148 ) used in the demonstration of the present work , human embryonic cells ( n = 90 ) from Yan et al . [ 28 ], and E18 . 5 mouse lung Epcam + epithelial cells ( n = 80 ) from Treutlein et al . [ 21 ].__label__Method|Tool|Use
We therefore use posterior model probabilities and patterns of LD to define sets of SNPs which have strong joint posterior support for the hypothesis that one member of the set is causal for the trait . These are analogous to the credible sets generated in the Bayesian fine mapping framework which assumes a single causal variant per region [ 4 ], but allow for multiple causal variants . Our adaptions around GUESS are available in an R package , GUESSFM ( _CITE_ ). We used GUESSFM to fine map the association of multiple sclerosis ( MS ) and type 1 diabetes ( T1D ) to an established susceptibility region for immunemediated diseases on chromosome 10p15 ( S1 Fig ), which contains the candidate causal gene IL2RA . IL2RA encodes a subunit CD25 ( IL - 2RA ) of the interleukin 2 ( IL - 2 ) receptor that is essential for the high affinity binding of IL - 2 [ 14 ].__label__Method|Code|Produce
Finally , the first and last 50 nts of each region were not included . As a result , 2 , 453 exons and 3 , 091 introns were identified on chromosome 1 , and 2 , 825 exons and 3 , 237 introns were identified on chromosome 13 , 14 , 15 , 21 , 22 . Obtaining rDNA array reads The mapped genome sequencing data ( BAM files ) of cancer patients generated by the TCGA project were downloaded from the Legacy Archive website of the GDC data portal ( _CITE_ ). We obtained 113 , 233 , 50 , 127 , 44 and 154 tumor samples ( 24 , 74 , 32 , 38 , 35 and 22 tumor - adjacent pairs ) for BLCA , LUAD , LUSC , STAD , KIRC and HNSC respectively ; 9 , 10 and 19 tumor - adjacent pairs for OV , BRCA and LIHC were also downloaded . We used two approaches to obtain reads mapped onto rDNA sequences .__label__Material|Data|Use
Finally , the first and last 50 nts of each region were not included . As a result , 2 , 453 exons and 3 , 091 introns were identified on chromosome 1 , and 2 , 825 exons and 3 , 237 introns were identified on chromosome 13 , 14 , 15 , 21 , 22 . Obtaining rDNA array reads The mapped genome sequencing data ( BAM files ) of cancer patients generated by the TCGA project were downloaded from the Legacy Archive website of the GDC data portal ( _CITE_ ). We obtained 113 , 233 , 50 , 127 , 44 and 154 tumor samples ( 24 , 74 , 32 , 38 , 35 and 22 tumor - adjacent pairs ) for BLCA , LUAD , LUSC , STAD , KIRC and HNSC respectively ; 9 , 10 and 19 tumor - adjacent pairs for OV , BRCA and LIHC were also downloaded . We used two approaches to obtain reads mapped onto rDNA sequences .__label__Supplement|Website|Use
In unusual situations , in particular if it is discovered that genomic data is incorrectly mapped to case or biospecimen data in a way that cannot be resolved by remapping , all affected data may be made indefinitely unavailable . The GDC will attempt to work with the submitter to resolve such issues without removing data if possible .’ Erroneous data detected in GEO in most cases would not be deleted . Instead , a comment indicating the problem would be added to the record , as stated on the GEO website _CITE_ ‘ Please keep in mind that updating records is preferable to deleting records . If the accessions in question have been published in a manuscript , we cannot delete the records . Rather , a comment will be added to the record indicating the reason the submitter requested withdrawal of the data , and the record content adjusted / deleted accordingly .’ The three examples above demonstrate the diversity of ways different databases handle duplication events among other types of data errors .__label__Supplement|Website|Use
_CITE_ experiments and samples in an intuitive way . Although properties for different sample and experiment types have to be defined in advance , all types are connected to an XML - based property in our data model implementation , which offers the possibility to extend the amount of metadata that should be stored with entities in a generic manner . These XML properties can be controlled by providing XML schemata that are used to present users with a robust option to record a diverse set of meta information as found in modern biomedical experiments .__label__Supplement|Document|Introduce
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ © The Author ( s ) 2018__label__Supplement|License|Other
Therefore , we sought to determine whether consensus Ewg DNA binding sites are present in the enhancers of genes deregulated by Apc1 loss . As the Wingless target gene reporters notum - lacZ , nkd ( UpE2 )- lacZ , and fz3 - RFP are each hyperactivated in an Ewg - dependent manner following Apc1 loss , and the enhancers within these reporters are well - characterized , we searched for potential Ewg and TCF binding sites in these enhancers . The transcriptional enhancers that drive expression of both the notum - lacZ and nkd PLOS Genetics | _CITE_ July 14 , 2017 9 / 37 Transcription cofactors Earthbound and Erect wing mediate intestinal defects due to APC inactivation ( UpE2 )- lacZ reporters , which are 2 . 2 kb [ 93 , 94 ] and 0 . 6 kb [ 97 ], respectively , are directly bound and regulated by TCF through distinct pairs of core consensus sites ( SSTTTGWWSWW ) and Helper sites ( GCCGCCR ) [ 5 , 96 – 98 , 100 ] ( S11A – S11C Fig ). We identified similar TCF core consensus binding sites and Helper sites in the 2 . 3 kb enhancer of the fz3 - RFP transgene [ 95 ] ( S11D Fig ). In addition , we found that the fz3 enhancer contains an Ewg consensus binding site ( GCGCABGY ) [ 54 – 57 ] ( S11A and S11D Fig ), and that this site is conserved among sequenced Drosophila species ( S12 Fig ) [ 101 ].__label__Supplement|Paper|Extent
We hypothesised that the expression levels of a set of tissuespecific markers could be used to determine the quality of EST expression data , to verify the identity of known libraries or to identify unknown libraries . In the current investigation we used human EST expression data to find a set of markers for determining tissue specificity of EST libraries . We chose to use libraries from the CGAP database ( _CITE_ ) because of the wide use of this repository for studying differential gene expression in cancer .__label__Material|Data|Use
1a , b , https :// github . com / theislab / scanpy_usage / tree / master / 170503_zheng17 ) [ 6 ]. Moreover , we demonstrate the feasibility of analyzing 1 . 3 million cells without subsampling in a few hours of computing time on eight cores of a small computing server ( Fig . 1c , _CITE_ one_million_cells ). Thus , SCANPY provides tools with speedups that enable an analysis of data sets with more than one million cells and an interactive analysis with run times of the order of seconds for about 100 , 000 cells . © The Author ( s ).__label__Supplement|Website|Use
The Facility Profile is completed by each facility that provides dialysis to ESKD patients on December 31st of each year . CORR publishes an Annual Data Report that provides the latest data on dialysis , organ transplants , waiting list and donors . The latest report issued in 2014 , reports on data from 2003 – 2012 and is available on CIHI ’ s website ( _CITE_ ). The Center - Specific Reports on Clinical Measures reports are new series of reports , released in 2014 , designed for dialysis centres and derived primarily from the clinical measures captured in the annual follow - up survey . This includes patient demographics , comorbidities , and laboratory and clinic indicator comparisons for the center , the province and Canada .__label__Supplement|Website|Introduce
The Facility Profile is completed by each facility that provides dialysis to ESKD patients on December 31st of each year . CORR publishes an Annual Data Report that provides the latest data on dialysis , organ transplants , waiting list and donors . The latest report issued in 2014 , reports on data from 2003 – 2012 and is available on CIHI ’ s website ( _CITE_ ). The Center - Specific Reports on Clinical Measures reports are new series of reports , released in 2014 , designed for dialysis centres and derived primarily from the clinical measures captured in the annual follow - up survey . This includes patient demographics , comorbidities , and laboratory and clinic indicator comparisons for the center , the province and Canada .__label__Supplement|Document|Introduce
All essential functionalities were integrated into the user - friendly interface of QCanvas . The simple and intuitive nature of this tool meets the practical needs of research scientists working on omics data who do not have expertise in bioinformatics approaches . The program is freely available with demo data and a step - by - step tutorial through the website ( _CITE_ ~ qcanvas ).__label__Method|Tool|Produce
All essential functionalities were integrated into the user - friendly interface of QCanvas . The simple and intuitive nature of this tool meets the practical needs of research scientists working on omics data who do not have expertise in bioinformatics approaches . The program is freely available with demo data and a step - by - step tutorial through the website ( _CITE_ ~ qcanvas ).__label__Material|Data|Produce
All essential functionalities were integrated into the user - friendly interface of QCanvas . The simple and intuitive nature of this tool meets the practical needs of research scientists working on omics data who do not have expertise in bioinformatics approaches . The program is freely available with demo data and a step - by - step tutorial through the website ( _CITE_ ~ qcanvas ).__label__Supplement|Website|Produce
All essential functionalities were integrated into the user - friendly interface of QCanvas . The simple and intuitive nature of this tool meets the practical needs of research scientists working on omics data who do not have expertise in bioinformatics approaches . The program is freely available with demo data and a step - by - step tutorial through the website ( _CITE_ ~ qcanvas ).__label__Supplement|Document|Produce
The data described in this Data note can be freely and openly accessed in the GEO repository via GEO - Accession No . GSE107419 _CITE_ GEO : GSE107419 . Please see Table 1 and reference list for details and links to the data .__label__Material|Data|Use
Provenance and peer review Not commissioned ; peer reviewed for ethical and funding approval prior to submission . Open Access This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ( CC BY - NC 4 . 0 ) license , which permits others to distribute , remix , adapt , build upon this work noncommercially , and license their derivative works on different terms , provided the original work is properly cited and the use is non - commercial . See : _CITE___label__Supplement|License|Other
The aim is not to use the data to infer or prove dispersal and vicariance events for the phylogeny of the Pomatiopsidae , instead the approach is to compare patterns in the phylogeny with the two main hypotheses for the historical biogeography and evolution of these snails and to assess their relative degrees of fit . Neither of these hypotheses was formulated in the present study , and the events or molecular clock rates used in this study are for the most Liu et al . BMC Evolutionary Biology 2014 , 14 : 29 Page 9 of 30 _CITE_ part independent of those used to derive these hypotheses . The earliest divergences in the hypotheses do assume that cladogenesis is due to plate movement and not post - breakup dispersal or divergence in allopatry prior to isolation by continental fragmentation . Such assumptions have been heavily criticised [ 71 , 75 ]; however , these criticisms often refer to speciation events .__label__Supplement|Paper|Introduce
The Supplementary Material for this article can be found online at : _CITE_ 2016 . 00018__label__Supplement|Document|Produce
Electronic supplementary material The online version of this article ( _CITE_ ) contains supplementary material , which is available to authorized users . ® João M . C . Teixeira joaomcteixeira @ gmail . com__label__Supplement|Document|Produce
Electronic supplementary material The online version of this article ( _CITE_ ) contains supplementary material , which is available to authorized users . ® João M . C . Teixeira joaomcteixeira @ gmail . com__label__Supplement|Paper|Produce
specificEpithet The name of the first or species epithet of the scientificName ( http :// rs . tdwg . org / dwc / terms / specificEpithet ). infraspecificEpithet The name of the lowest or terminal infraspecific epithet of the scientificName , excluding any rank designation ( http :// rs . tdwg . org / dwc / terms / infraspecificEpithet ). taxonRank The taxonomic rank of the most specific name in the scientificName ( _CITE_ ). scientificNameAuthorship The authorship information for the scientificName formatted according to the conventions of the applicable nomenclaturalCode ( http :// rs . tdwg . org / dwc / terms / scientificNameAuthorship ). authorName Author name information namePublishedInYear The four - digit year in which the scientificName was published ( http :// rs . tdwg . org / dwc / terms / namePublishedInYear ).__label__Supplement|Website|Introduce
Raw reads from whole genome shotgun sequencing of Buddleja globosa are available in the NCBI Sequence Read Archive , BioProject IDPRJNA419550 , SRA SRP125846 ( https :// www . ncbi . nlm . nih . gov / sra /? term = SRP125846 ). Assembled contigs from whole genome shotgun sequencing of Buddleja globosa are available on Dryad Digital Repository ( _CITE_ ). Target locus sequences in Buddleja globosa used for probe design for targeted sequence capture are available on Dryad Digital Repository ( https :// doi . org / 10 . 5061 / dryad . v6q0p ). Raw reads from sequencing after targeted sequence capture for 50 samples are available in the NCBI Sequence Read Archive , BioProject ID PRJNA419999 , SRA SRP125765 ( https :// www . ncbi . nlm . nih . gov / sra /? term = SRP125765 ).__label__Material|Data|Produce
Our work also shows that collaboration between groups developing ontologies and creating biological annotations and scientists and clinicians engaged in active research can lead to substantial improvements in the computational representation of biological knowledge . The GOC welcomes input from researchers about any aspect of our work , including changes to the ontology and suggestions of papers and gene products to annotate ( goannotation @ ucl . ac . uk , http :// geneontology . org / page / contributing - go ). A variety of GitHub repositories with issue trackers are in place for specific queries : for general inquiries about GO ( _CITE_ ), for specific questions about annotations or annotation - related topics ( https :// github . com / geneontology / go - annotation /), and for specific questions or suggestions about the content and structure of the ontology ( https :// github . com / geneontology / go - ontology /). Such collaborations improve the value of the GO resource for the benefit of the entire cardiovascular research community and facilitate the interpretation of high - throughput data sets , toward the identification of dysregulated cardiac pathways , as well as variants and risk alleles , associated with cardiovascular diseases . This has important implications for cardiovascular physicians needing to interpret potentially pathological variants in their patients as a resource for the most up - to - date bioinformatic data to inform diagnosis and cascade screening in families .__label__Method|Code|Introduce
NIH ( National Institutes of Health ). ( 2015 ). Principles and Guidelines for Reporting Preclinical Research _CITE_ Accessed June 18 , 2015 . NTP .__label__Supplement|Document|Produce
The published website also includes a link that allows users to download the . csv files that contain the information that is displayed , for additional computational exploration through other tools ( e . g ., by reading the data into scripts that implement machine learning algorithms ). The only requirement is that the user has a GitHub account and afqbrowser - publish will create the public repository , build the webpage , and launch the web server through GitHub . To create a centralized index of public AFQ - Browser instances , and to aggregate data across studies , we have implemented a centralized database at _CITE_ org , akin to the NeuroVault ( https :// neurovault . org ) database for functional MRI derivatives65 ( and capitalizing on the infrastructure from other open source database projects in the field5 , 66 ). In addition to launching an AFQ - Browser instance on GitHub , the afqbrowser - publish command also commits the data to the afqvault database .__label__Material|Data|Produce
The X - ray crystallography data set gives us an idea of how complexes might look in a complete and accurate interaction network , while the Y2H data set gives us an idea of how complexes look in our real error - prone data . For the complexes from MIPS , we were only able to look at the induced graphs from the Y2H data . The code used for calculating the statistics of protein complexes can be found at _CITE_ complex - stats .__label__Method|Code|Produce
We retrieved genes annotated to the Gene Ontology category “ metabolic process ” ( GO : 0008152 ) and its subcategories from the UniProtKB / Swiss - Prot database ( The UniProt Consortium 2015 ), using the following URL : _CITE_ 20musculus % 20 ( Mouse )% 2010090 ]% 22 + go : 8152 ( queried on August 2 , 2016 ). We performed a similar query to retrieve genes annotated to the category “ membrane ” ( GO : 0016020 ; supplementary text , Supplementary Material online ).__label__Material|Data|Use
DeDaL is a simplified Cytoscape 3 app implemented in Java language . For computing linear and non - linear principal manifolds , DeDaL uses VDAOEngine Java library ( _CITE_ ). For computing the eigenvectors of a symmetric Laplacian matrix , the parallelized Colt library has been used ( http :// acs . lbl . gov / ACSSoftware / colt /). Internal graph implementation is reused from BiNoM Cytoscape plugin [ 21 – 23 ].__label__Method|Code|Use
_CITE_ Medical and Health Sciences . The sample sizes for some of the disciplines were rather small ( n < 40 ).__label__Supplement|Document|Introduce
Finally , we also highlight the close connection between mutation signature models and the “ mixed - membership models ”, also known as “ admixture models ” [ 12 ] or “ Latent Dirichlet Allocation ” models [ 13 ] that are widely used in population genetics and document clustering applications . These connections should be helpful for future elaboration of computational and statistical methods for cancer mutation signature detection . Software implementing the proposed methods is available in an R package pmsignature ( probabilistic mutation signature ), at _CITE_ The core part of the estimation process is implemented in C ++ by way of the Rcpp package [ 14 ], which enables handling millions of somatic mutations from thousands of cancer genomes using a standard desktop computer . In addition , a web - based application of our method is available at https :// friend1ws . shinyapps . io / pmsignature_shiny /.__label__Method|Code|Use
This combined approach looks for a sparse model representation able to capture most of the information present in the observed spike trains using a series of parameters ( edge weights ) with well - defined values , with tight uncertainty intervals . In the following section we will briefly explain how we approach this minimization . PLOS ONE | _CITE_ May 2 , 2018 7 / 46 Active learning of cortical connectivity from two - photon imaging data__label__Supplement|Paper|Introduce
Phytozome v8 . 0 ( _CITE_ ) with A . thaliana 167 ( TAIR release 10 acquired from TAIR ), A . lyrata 107 ( JGI release v1 . 0 ), C . rubella 183 ( JGI annotation v1 . 0 on assembly v1 ), B . rapa 197 ( Annotation v1 . 2 on assembly v1 . 1 from brassicadb . org ) genome data . Identification of A . thaliana Lineage Specific New Genes that Originated through Gene Duplication To identify A . thaliana specific new genes , we selected new genes based on two criteria : first , the gene was not located in any of the syntenic regions between A . thaliana and the rest of three species A . lyrata , C . rubella , B . rapa ; second , the gene did not have any reciprocal ortholog in A . lyrata , C . rubella and B . rapa . Using the pipelines developed by UCSC genome browser [ 54 ], we constructed the reciprocal syntenic relationship between A . thaliana and A . lyrata / C .__label__Material|Data|Use
User Support can be contacted via email at mgihelp @ jax . org or by clicking the User Support link at the bottom of our web pages . The online documentation can be accessed by clicking on the question mark in the upper left corner of most pages . FAQs ( and other useful links ) can be found on the GXD home page ( _CITE_ ).__label__Supplement|Website|Produce
User Support can be contacted via email at mgihelp @ jax . org or by clicking the User Support link at the bottom of our web pages . The online documentation can be accessed by clicking on the question mark in the upper left corner of most pages . FAQs ( and other useful links ) can be found on the GXD home page ( _CITE_ ).__label__Supplement|Document|Produce
Thus , SigNetTrainer benefits from state - of - the - art - solvers for ILP problems which use a number of methodologies to deal with large - scale problems . For a more general introduction to ILP algorithms we refer to [ 30 ]. SigNetTrainer is easy to use ; the user has to provide three files to define network training problems : ( i ) the network topology in . sif format ( also used by Cytoscape _CITE_ ), ( ii ) an ASCII file describing the experimental scenarios ( i . e ., the imposed state changes ), and ( iii ) an ASCII file containing the experimentally measured state changes for each scenario . The user may then call different functions implementing the optimization routines as described herein . Source code and manual of both versions of SigNetTrainer are available on the following website : http :// www . mpi - magdeburg . mpg . de / projects / cna / etcdownloads . html .__label__Method|Tool|Introduce
Supplementary information accompanies this paper at _CITE_ Competing financial interests : The authors declare no competing financial interests . How to cite this article : Hughes , C . S . et al . Quantitative Profiling of Single Formalin Fixed Tumour Sections : proteomics for translational research .__label__Supplement|Document|Produce
RCPs were named after the four IPCC selected limits in radiative forcing increases attained by 2100 : + 2 . 6 Wm - 2 , + 4 . 5 Wm - 2 , + 6 Wm - 2 and + 8 . 5 Wm - 2 . We used climate models derived from RCP4 . 5 and RCP8 . 5 which resulted in intermediate - low and worst case scenarios , respectively [ 44 – 45 ]. Wheat harvested area regional data per EU country were accessed via the Eurostat data portal ( _CITE_ ). Non - EU country data originated mainly from the FAOstat data portal ( http :// www . faostat3 . fao . org /). For Ukraine and Russia , regional data ( oblast level ) were derived from maps published by the FAS - USDA ( http :// www . pecad . fas .__label__Material|Data|Use
Source code for genomic event analysis tools ( GEAT ) developed in our laboratory to perform the analysis is available at _CITE_ Nature . Author manuscript ; available in PMC 2017 August 15 .__label__Method|Code|Produce
We randomly selected 5 sets of samples ( 144 , 153 , 201 , 209 and 210 ) with two replicates each , one from the Yale laboratory source and one from the Argonne source . For these runs , the length of the reads was 36nt for Yale and 46nt for the Argonne - derived data . Since the length of the sequencing read influences the number of unique fragments and the mapping PLOS ONE | _CITE_ August8 , 2017 13 / 24 Alternative approaches for multi - level RNA - seq data to the reference transcriptome ( and , as a result , the gene expression ) we trimmed all reads to comparable lengths ( 35nt ) and mapped the reads to the reference human genome using full length , no mismatch or gap criteria and using PatMaN [ 50 ]. The subsampling , without replacement , was conducted on 7M reads ( the number of reads for the smallest sample was 7 . 1M , and for the largest sample was 8 . 7M ). This normalization was followed by a correction using a quantile normalization applied on the matrix of gene expressions .__label__Supplement|Paper|Introduce
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ © The Author ( s ) 2018__label__Supplement|License|Other
In this way , the differences between a target and a non - target stimulus are maximized . Electrodes having the highest maxRAUC in the ERP region of the signal are potentially likely to have better characteristics for detecting ERP effectively . Our method was tested with a linear classifier ( LDA ) based on the Krusienski method ( KM ) [ 1 ] and the dataset_IIb of the BCI competition ( _CITE_ ). This dataset contains the data of one user , divided into three sessions : two training sessions ( called 10 and 11 ) and one session to test the classifier . Users were stimulated through P300 Speller Paradigm described in the competition .__label__Material|Data|Use
htSeqTools [ 60 ] and SolexaQA [ 55 ] include quality assessment , processing and visualization functionality . In addition , software tools have been published that support only the Illumina platform ( i . e . FASTX - Toolkit ( _CITE_ ), PIQA [ 61 ] and TileQC [ 62 ]) or provide certain specialized functionality ( i . e . TagCleaner [ 63 ]).__label__Method|Tool|Introduce
Fortunately , NHANES used semiautomated initialization procedures so human errors were not possible . The file start dates were adjusted Sherar et al . BMC Public Health 2011 , 11 : 485 Page 9 of 13 _CITE_ from January 1st thru 7th in order to maintain the correct day of the week of the reconstituted . dat files . The year of the start date was listed as 2004 for all files in the 2003 - 04 NHANES dataset and was listed as 2006 for all files in the 2005 - 06 NHANES dataset .__label__Supplement|Paper|Introduce
DBChIP was not used with HOMER because HOMER does not provide summit locations . PePr . PePr v1 . 0 . 8 ( _CITE_ PePr ) was run in differential binding mode with the peak type set to ‘ broad ’ ( for histone mark simulations ) or ‘ sharp ’ ( for TF simulations ), and the shift size set to half the fragment size , i . e . 50 bp . Artifacts were not removed as duplicates were not simulated .__label__Method|Code|Use
Supplementary Information accompanies this paper at https :// doi . org / 10 . 1038 / s41467017 - 01577 - 2 . Competing interests : The authors declare no competing financial interests . Reprints and permission information is available online at _CITE_ Publisher & apos ; s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations . Open Access This article is licensed under a Creative Commons Attribution 4 . 0 International License , which permits use , sharing , adaptation , distribution and reproduction in any medium or format , as long as you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material .__label__Supplement|License|Other
Randomized © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Shokraneh and Adams Systematic Reviews ( 2017 ) 6 : 153 Page 2 of 3 To provide an example of how this may be shared , as a part of a funded project [ 5 ], we extracted the data from all randomized trials relevant to treatment of a disorder of movement and made them available [ 6 ]. This has the advantage of being the only PDF - independent method .__label__Supplement|License|Other
There are two ways to convert data from another format and create an SPM M / EEG dataset . The first is geared towards EEG and MEG datasets stored in their native formats or in formats of other analysis packages ( e . g ., EEGLAB ). This conversion facility is based on “ fileio ” module ( see _CITE_ ), which is shared between SPM8 , FieldTrip and EEGLAB toolboxes and jointly developed by the users of these toolboxes . At the moment , most common EEG and MEG data formats are supported . For some formats , it might be necessary to install additional MATLAB toolboxes ( there is an error message if these toolboxes are missing ).__label__Method|Tool|Produce
Offline phenotypic data are stored in the OrganMeasurement table , as well as file names of plant images taken by experimenters . A last table named Comment allows the storage of all events and remarks associated with an experiment . Additional supplementary material is available on the PHENOPSIS DB Web interface : _CITE___label__Supplement|Website|Produce
Offline phenotypic data are stored in the OrganMeasurement table , as well as file names of plant images taken by experimenters . A last table named Comment allows the storage of all events and remarks associated with an experiment . Additional supplementary material is available on the PHENOPSIS DB Web interface : _CITE___label__Supplement|Document|Produce
In our second ribosome profiling study , we found that the magnitude of the buildup of reads at the stall site was smaller for both PCSK9 ( Fig 4C ) and other targets , while the number of reads mapping 3 ´ to the stall site remained consistent . The variability in magnitude of the stall peak could be an artefact of the size - selection step during library preparation ; subsequent to our initial set of experiments , it was reported [ 25 , 26 ] that ribosome profiling of stalled ribosomes can yield a broader range of protected footprint sizes than the conventional 26 – 34 nt range [ 19 ]. To identify and quantify the sensitivity of individual proteins to PF - 06446846 , we adopted a computational approach to identify transcripts that could potentially have PF - 06446846 – PLOS Biology | _CITE_ March 21 , 2017 7 / 36 Selective inhibition of ribosome nascent chain complexes__label__Supplement|Paper|Extent
Radio frequency identification ( RFID ) can provide a simple , inexpensive solution for examining the behavior and movements of animals ( Bonter & Bridge , 2011 ). Bridge and Bonter ( 2011 ) are largely responsible for popularizing this technique by developing and sharing plans for a low - cost RFID logger that could be made simply and easily ( _CITE_ ). Fitting animals with inexpensive , lightweight PIT tags and placing RFID loggers on or near feeders or nests results in a simple technique for logging large numbers of visits to a particular location . Further , due to the low cost of PIT tags , they can be deployed on a large number of individuals , quickly resulting in large amounts of data that , while useful , may be overwhelming for many researchers .__label__Method|Tool|Introduce
One the one hand , it represents the proportion of comparisons for which intervention phase data improve baseline data . On the other hand , it can be conceptualized as the probability that a randomly selected post - intervention data point will improve ( here , be smaller than ) a randomly selected pre - intervention data point . The NAP can be computed via the online calculator _CITE_ by Vannest et al . ( 2011 ), where it is only necessary to enter the data from the different conditions in separate columns . It is also part of the output (“ A vs . B ” comparison ) of the R code for Tau - U https :// dl . dropboxusercontent . com / u / 2842869 / Tau_U . R ( Brossart et al ., 2014 ), which requires loading a data file with a single comma - separated column including “ Time ” ( 1 , 2 , ..., npre + npost ), “ Score ” ( denoting the measurements ) and “ Phase ” denoting the condition ( npre times the value of 0 followed by npost times the value of 1 ).__label__Method|Tool|Use
Supplementary data associated with this article can be found , in the online version , at _CITE_ 047 .__label__Supplement|Document|Produce
Competing interests AK has received consulting fees from Mars , Inc . JTD has received consulting fees or honoraria from Janssen Pharmaceutica , GSK , AstraZeneca and Hoffmann - La Roche and holds equity in NuMedii , Inc , Ayasdi , Inc and Ontomics , Inc . No writing assistance was used in the production of this manuscript . Provenance and peer review Not commissioned ; externally peer reviewed . Data sharing statement Source code and sample data are available on the companion website : _CITE_ Open Access This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ( CC BY - NC 4 . 0 ) license , which permits others to distribute , remix , adapt , build upon this work noncommercially , and license their derivative works on different terms , provided the original work is properly cited and the use is non - commercial . See : http :// creativecommons . org / licenses / by - nc / 4 . 0 /__label__Method|Code|Produce
Competing interests AK has received consulting fees from Mars , Inc . JTD has received consulting fees or honoraria from Janssen Pharmaceutica , GSK , AstraZeneca and Hoffmann - La Roche and holds equity in NuMedii , Inc , Ayasdi , Inc and Ontomics , Inc . No writing assistance was used in the production of this manuscript . Provenance and peer review Not commissioned ; externally peer reviewed . Data sharing statement Source code and sample data are available on the companion website : _CITE_ Open Access This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ( CC BY - NC 4 . 0 ) license , which permits others to distribute , remix , adapt , build upon this work noncommercially , and license their derivative works on different terms , provided the original work is properly cited and the use is non - commercial . See : http :// creativecommons . org / licenses / by - nc / 4 . 0 /__label__Material|Data|Produce
Competing interests AK has received consulting fees from Mars , Inc . JTD has received consulting fees or honoraria from Janssen Pharmaceutica , GSK , AstraZeneca and Hoffmann - La Roche and holds equity in NuMedii , Inc , Ayasdi , Inc and Ontomics , Inc . No writing assistance was used in the production of this manuscript . Provenance and peer review Not commissioned ; externally peer reviewed . Data sharing statement Source code and sample data are available on the companion website : _CITE_ Open Access This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ( CC BY - NC 4 . 0 ) license , which permits others to distribute , remix , adapt , build upon this work noncommercially , and license their derivative works on different terms , provided the original work is properly cited and the use is non - commercial . See : http :// creativecommons . org / licenses / by - nc / 4 . 0 /__label__Supplement|Website|Produce
2 . 4 . 1 . SNPs and Indels Calling and Annotation We explored and embedded in workflow at least three different frameworks to call SNPs and Indels from whole genome alignment data and produce a comprehensive mutation / functional analysis report ( Figure 7 ). For the Sequence Variant Analyzer ( SVA , _CITE_ [ 51 ]) v1 . 0 workflow , after SNPs and Indels have been called with SAMTOOLS [ 13 ] ( http :// samtools . sourceforge . net /) and CNVs have been called with & quot ; Estimation by Read Depth with Single Nucleotide Variants & quot ; ( ERDS ) software v1 . 02 ( http :// web . duke . edu /~ mz34 / erds . htm ), they undergo annotation and visualization through SVA ( Figure 7 ). SVA is a visualization platform for performing statistical analysis and filtering procedures as well . The workflow we have developed allows users to produce a . gsap file , which can be loaded into SVA to create a project with single or multiple annotated genomes .__label__Method|Tool|Introduce
In total this provides a feature vector of 27 values for every vertex point , which were used to differentiate distinct cortical areas . A principal component analysis suggested that the data actually contained around 9 or 10 significant degrees of freedom . However , to test the discriminative potential of the features , the full feature vectors were used as input to an off – the – shelf support – vector machine ( SVM ) classifier ( _CITE_ ). In Experiment 1 , data from the first acquisition were used to train the three – way SVM classifier on the full set of feature vectors from every vertex point within three regions . In Subject 1 ( male ) these three regions were MT + ( Extended middle temporal area based on retinotopy and quantitative T1 data [ 48 ]), Ang ( a nearby region of the angular gyrus within the so – called “ default mode network ” that is known to be lightly myelinated [ 49 ]) and STS ROI ( a visually responsive part of the superior temporal sulcus ), as displayed in Figure 1 .__label__Method|Tool|Use
Land cover data was derived from the Senegal Land Cover Database produced by the Food and Agriculture Organization of the United Nations ( FAO ) in the framework of Global Land Cover Network activities ( http :// www . fao . org / geonetwork / srv / en / main . home ? uuid = 545be438 - bc87 - 480b - 83ec - ba3f4e486daf ). Percentage of surface covered by water , particularly favorable for Culicoides [ 15 , 26 ] savannah and forest in the 1 km2 pixels comprising the trap were extracted . Ruminant ( cattle , sheep and goats ) livestock density data was obtained from the global distribution of livestock maps produced by the FAO ( _CITE_ ). All data layers were clipped on the Senegalese territory and projected in the same projection system with a spatial resolution of 1 km2 using R2 . 10 . 1 statistical language environment [ 38 ] using of the R - package : maps [ 39 ], mapdata [ 40 ], maptools [ 41 ], raster [ 42 ], rgdal [ 43 ] and sp [ 44 ]. Climatic data are provided in Additional file 2 : Figure S1 .__label__Material|Data|Use
Values were re - scaled from 0 to 1 in order to represent the relative level of new development with the final layer having 100 m cell sizes . Hawai ‘ i has the highest number of onsite waste disposal systems ( OSDS ) ( i . e . cesspools and septic tanks ) per capita in the U . S ., many of which are adjacent to the coastline ( EPA — _CITE_ ). These OSDS leach excess nutrients and pollutants into groundwater that flows to the ocean [ 58 ]. Excess nutrients can promote rapid algal growth , outcompeting corals and disrupting the ecosystem [ 59 – 60 ].__label__Supplement|Website|Introduce
Furthermore , the maximum - likelihood estimation is even more computationally demanding , as it is essentially a bound constrained optimization procedure . The R environment provides the ‘ optim ()’ function and the ‘ L - BFGS - B ’ algorithm [ 21 , 22 ] options for users to perform specific optimizations . To develop the PEPIS in C / C ++, we therefore adopted and incorporated the L - BFGS - B open - source codes in C ( _CITE_ ) for the optimization sub - routine , which is also utilized as an optimization tool in the R environment . Parallel Strategy for Distributed High - Performance Computing As our goal was to facilitate rapid epistatic QTL mapping , we first needed to analyze the computational complexity of the model and resolve the fundamental problems associated with the computationally demanding nature of these analyses . If we suppose that the number of individuals is represented by n and the number of markers / bins by m , then the number of total genetic effects is 2m + 4C ( m , 2 ) = 2m2 .__label__Method|Code|Extent
The final step is to populate Characteristics and Annotations . The project contains 198 samples , making the process of manual annotation very lengthy . To ease the process , we wrote an annotation file that can be imported directly within Djeen ( available from Djeen documentation page at _CITE_ ). However , it has to be formatted to be imported , especially to fit the sample database Ids . This is done by generating an empty annotation file by exporting current file annotation data using “ Files ”-> “ Save the file to annotation . txt ”, and editing “ annotation .__label__Supplement|Document|Use
The training dataset is made up of 108 microarray samples : 65 samples from Affymetrix , 43 from Illumina , and 24 RNA - Seq samples . On the other hand , the test set is made up of 120 samples of microarray ( 108 of Illumina and 12 of Affymetrix ) as well as 6 samples of RNA - Seq . These series are publicly available at _CITE_ acc = S . NAME where S . NAME is the name of each series at NCBI GEO .__label__Material|Data|Produce
Results for V1 at 50 ms and IT at 225 ms for artificial experimental condition 2 are shown ( averaged across participants ; error bars , s . d . ; gray dashed lines , significance level [ uncorrected PLOS ONE | _CITE_ June 18 , 2018 23 / 28 Information spreading by MEG source estimation P < 0 . 05 ]). 0 * indicates γ0 = 0 . ( PDF ) S4 Fig .__label__Supplement|Paper|Compare
Even though all cells carry the same genetic code , cellular differentiation is likely guided © The Author ( s ). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( http :// creativecommons . org / licenses / by / 4 . 0 /), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Huang et al . BMC Plant Biology ( 2018 ) 18 : 111 Page 2 of 14 by distinct GRNs .__label__Supplement|License|Other
⃝ c 2014 by the authors ; licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license ( _CITE_ ).__label__Supplement|License|Other
and sequencing of specimens ), Spanish Ministry of the two genera inside Allomalorhagida , and 3 ) species of Campyloderes appear in a basal Science and Technology ( _CITE_ Grant trichotomy within Kentrorhagata in the morphological tree , whereas analysis of the comno CGL 2009 - 08928 to FP — collecting of specimens ), bined datasets places species of Campyloderes as a sister clade to Echinoderidae and and Association of European Marine Biological Kentrorhagata . Laboratories ( http :// www . assemblemarine . org / access - rules / to FP , MH , and NS — collecting of specimens ). Competing Interests : The authors have declared that no competing interests exist .__label__Supplement|Website|Introduce
) for other user interface elements . For network and other analyses , the web server communicates with R using RServe ( _CITE_ ) to utilize existing R packages including topGO , vioplot and pheatmap to generate GO analyses , violin plots and heatmaps . Data are uploaded anonymously and privately , utilizing cookies to store the necessary information for linking the user ’ s browser with their uploaded data . Fig 1 summarizes the data flow and architecture of QuIN .__label__Method|Tool|Use
Therefore , although statistical approaches for addressing sparse networks have been proposed [ 52 ], there is no consensus as to the best approach and we did not apply them in this review . Finally , the inconsistency assessment should be based on joint assessment of statistical heterogeneity and network inconsistency [ 24 ]. However , our decision to downgrade the certainty PLOS ONE | _CITE_ February23 , 2018 22 / 29 Support surfaces for pressure ulcer prevention of evidence for inconsistency was largely based on the presence of moderate common heterogeneity ( I2 = 56 %) and not the network inconsistency . We did not find evidence of a global inconsistency by using both the design - by - treatment interaction model and the model of Lu and Ades [ 34 ]. There was one loop of linked data that was potentially inconsistent ( i . e .__label__Supplement|Document|Extent
2011 ) were loaded into CDinFusion . FASTA files containing over two million sequences with file sizes of two GigaBytes ( GB ) could be processed in less than three minutes in an AMDTM 64Bit , 2 GHz and 4 GB RAM environment . All described test cases were recorded with the Selenium IDE ( _CITE_ ) test case recorder . The test cases along with the test data , except for the metagenomic datasets , are deposited at http :// code . google . com / p / cdinfusion . Descriptions how to run the tests , can be found in the documentation section of the public CDinFusion installation at http :// www . megx . net / cdinfusion .__label__Method|Tool|Use
Within each patient sample , percent agreement and kappa values were calculated to assess agreement in the four behavioural variables determined by administrative data versus chart abstraction . All measures of accuracy , including sensitivity , specificity , positive predictive value ( PPV ) and negative predictive value ( NPV ), were calculated using chart notation as the “ gold standard .” In addition to accuracy measures for each sample , we also Kim et al . BMC Health Services Research 2012 , 12 : 18 Page 4 of 9 _CITE_ calculated unbiased estimates of various accuracy measures for the entire depression cohort during the study period from 4 / 1 / 1999 to 9 / 30 / 2004 . This was done using the combined mutually exclusive samples of suicide deaths and controls where the estimates were adjusted for sampling weights with each observation weighted inversely by the number of people each represents in the full depression cohort based on the sampling strata . Weighted accuracy estimates based on the antidepressant user sample were also calculated as unbiased accuracy estimates of a cohort of patients newly starting an antidepressant during the study period .__label__Supplement|Website|Introduce
Within each patient sample , percent agreement and kappa values were calculated to assess agreement in the four behavioural variables determined by administrative data versus chart abstraction . All measures of accuracy , including sensitivity , specificity , positive predictive value ( PPV ) and negative predictive value ( NPV ), were calculated using chart notation as the “ gold standard .” In addition to accuracy measures for each sample , we also Kim et al . BMC Health Services Research 2012 , 12 : 18 Page 4 of 9 _CITE_ calculated unbiased estimates of various accuracy measures for the entire depression cohort during the study period from 4 / 1 / 1999 to 9 / 30 / 2004 . This was done using the combined mutually exclusive samples of suicide deaths and controls where the estimates were adjusted for sampling weights with each observation weighted inversely by the number of people each represents in the full depression cohort based on the sampling strata . Weighted accuracy estimates based on the antidepressant user sample were also calculated as unbiased accuracy estimates of a cohort of patients newly starting an antidepressant during the study period .__label__Supplement|Paper|Introduce
Together , these data indicate that temperature may be a critical factor in the marked changes in brain size during evolution in different climate conditions . Specifically , Yu et al . BMC Evolutionary Biology 2014 , 14 : 178 Page 4 of 14 _CITE_ for a given body mass , warmer living conditions should result in larger brains . In addition to this temperature - driven brain enlargement , the difference in brain sizes of endotherms and ectotherms increases as body size increases ( Figure 1D ). For small body sizes ( below 1 gram ), the brains of endotherms are atleast 4 times larger than those of ectotherms ( black line in Figure 1D ).__label__Supplement|Paper|Introduce
Alex J . Eustace _CITE_ 92 - 1360__label__Supplement|Paper|Introduce
Finally , the design of a case study in strategy comparison is described . All analyses were performed using the R statistical programme , version 3 . 1 . 1 [ 19 ]. All computational tools for the comparison of modelling strategies can be found in the “ apricom ” package , available within the CRAN package repository ( _CITE_ ). A framework for strategy comparison It was proposed by Pestman et al . [ 17 ] that different strategies for linear regression model building could be compared prior to selecting a final strategy by means of a simple framework .__label__Method|Code|Use
was used in the BLAST searches to recover divergent sequences . All BLAST searches were performed using Geneious pro version 7 . 1 . 8 ( RRID : SCR_010519 , Kearse et al ., 2012 ) with the exception of the N . vectensis , A . pallida and S . pistillata genomes , for which searches were performed through the Joint Genome Institute online portal ( RRID : SCR_002383 ), NCBI ( RRID : SCR_004870 ) and the Reefgenomics online repository ( RRID : SCR_015009 , _CITE_ )( Liew et al ., 2016 ), respectively . A list of metazoan resources searched is provided in Table 1 . Sequences identified are tabulated in Supplementary file 1 .__label__Material|Data|Use
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : All results files for single - cell measurements , plasmid sequences , and numerical data for the graphs in the figures are available from the Dryad Digital Repository : _CITE_ Funding : This work was supported by Japan Society for the Promotion of Science ( https :// www .__label__Supplement|Website|Produce
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : All results files for single - cell measurements , plasmid sequences , and numerical data for the graphs in the figures are available from the Dryad Digital Repository : _CITE_ Funding : This work was supported by Japan Society for the Promotion of Science ( https :// www .__label__Supplement|Document|Produce
Genotyping was performed using the Affymetrix SNP 6 . 0 array . Imputation was performed using data from the 1000G ( CEU , March 2012 ) as the reference . Linear regression was performed to evaluate the association between genotypes and gene expression levels using the R ( _CITE_ ) package Matrix eQTL [ 32 ].__label__Method|Code|Use
Moreover , our previous study revealed that pterygium appears to be more prevalent among people with a low sociodemographic status [ 8 ]. In the present study , we confirmed that increasing age and living in a relatively rural area were associated with an increased risk of pterygium . Furthermore , income level was not associated with pterygium in the present study , and this result is also consistent with the findings of our previous cross - sectional study PLOS ONE | _CITE_ March 27 , 2017 7 / 10 Epidemiology of pterygium in South Korea [ 8 ]. However , we observed conflicting results regarding the association of pterygium and sex , as female sex was associated with this disease in the present study ( vs . male sex in our previous study ). This discrepancy might be caused by a higher detection rate among female South Koreans , as they are more likely to be concerned regarding cosmetic problems , compared to their male counterparts .__label__Supplement|Paper|Extent
H9N2 seroprevalence in humans with poultry exposure had a median of 9 % ( 1 – 43 %) by HI test and 2 . 7 % ( 0 . 6 – 9 %) by MN test according to another systematic review encompassing results from Asia , the Middle East , North America , Africa and Europe [ 57 ]. Pig - level seroprevalence for H9 ( 0 – 15 . 6 %) were usually higher compared to H5 and H7 . Regarding subtypes circulating commonly in pigs , i . e . H1N1 , H1N2 , and H3N2 , a high variability in the seroprevalence values was noted in the general population , but high values were observed in all continents with pig and herd - level seroprevalence means of 49 . 9 % and 72 . 8 % PLOS ONE | _CITE_ June 7 , 2017 18 / 25 Swine influenza epidemiology : A systematic review and meta - analysis respectively for the overall influenza values (‘ A , H1 + H3 ’). Lower pig and herd - level seroprevalence means were found for Africa ( 32 . 6 %; 29 . 3 %) and secondarily Asia ( 38 . 5 %; 52 . 8 %) compared to other continents ( Tables 1 and 3 ). Some study variables such as farm category , production type , biosecurity and pig subpopulation could not be included in meta - regression analyses as there were too many missing data .__label__Supplement|Paper|Introduce
In this paper , we performed a detailed assessment of the reliability of temperature signal in various indicators , based on natural phenomena as climate indicators . Specifically , we ( i ) assembled all readily available and relevant biological and physical time series predominantly in Finland that are commonly considered potential climate indicators , along with relevant temperature measurements . We then ( ii ) identified the season of the climate that each series PLOS ONE | _CITE_ June29 , 2017 3 / 20 Reliability of temperature signal in climate indicators characterizes best , and ( iii ) determined numerically the reliability of temperature signal in each single indicator series , and in a combination of different series . With these analyses , we aimed to highlight the important aspect to obtain comparable results of reliability of various indicators and their combinations . We assessed all the time series within the same statistical framework to obtain comparable results .__label__Supplement|Paper|Introduce
D1240 Nucleic Acids Research , 2014 , Vol . 42 , Database issue BLASTX algorithm ( 17 ) with the UniProtKB / Swiss - Prot ( 22 ) and TAIR Arabidopsis database ( 23 ). BLASTX matches with an expectation value & lt ; 1e - 20 were parsed , and EC codes were transferred to the Rosaceae gene models with Pathologic format ( used by PathwayTools ) using a Perl script , generate_pathologic_file . pl ( _CITE_ scripts / solcyc / generate_pathologic_file . pl ) The orthologous regions among the three sequenced genomes of Rosaceae ( 24 ), as detected by the Mercator program ( 25 ), are displayed using a synteny browser GBrowse_Syn ( 9 ). GBrowse_Syn is hyperlinked to GBrowse for access to genomic annotations including markers from the conserved syntenic regions shown in GBrowse_Syn . Comparative genomics data made available in the GDR thus allows site visitors starting with one rosaceous genome to explore genomic features , anchored trait loci and genetic markers within orthologous regions of another rosaceous genome .__label__Method|Code|Use
All BALF and plasma ( mouse and human ) samples were processed , tryptic digested , separated , and analyzed using liquid chromatography - mass spectrometry ( LC - MS ). Detailed information on animal and human sample collections and data collection is provided in Supplemental Information , which includes Supplemental Methods , brief descriptions and rationales of the framework , discussion on COPD data sets used in the current study , Supplemental Figures , Supplemental Tables , and References . Supplemental Information available online at _CITE_ 2 . 2 . Processing of Proteomic Data Sets .__label__Supplement|Document|Produce
cgi ? cmd = userReg . Temporary login is provided for evaluation purposes such as browsing the interfaces or viewing example analysis results . HIVE login URL : _CITE_ evaluation userid : xlhive @ yahoo . com ; password : pilotHive5 . Users can also install HIVE on their own hardware or use HIVE - in - abox , which is a low - cost alternative to analyze NGS data using predetermined workflows . For additional details , users are encouraged to contact the HIVE team ( http :// hive . biochemistry . gwu . edu / dna . cgi ? cmd = contact ).__label__Supplement|Website|Use
min to compute the predictions under minimal repair ( MCos ) for the unmeasured species of 105 experiment data sets each containing 1392 measurements . For further information visit _CITE___label__Supplement|Document|Produce
Data for sentenced persons released from Massachusetts state correctional facilities formatted for the National Corrections Reporting Program and HIV surveillance data from the Massachusetts HIV / AIDS Surveillance Program ( MHASP ) were obtained for the years 2012 and 2013 . The corrections release dataset included data for all releasees regardless of HIV status and included personal identifiers for linkage as well as the incarceration date and release date . HIV surveillance data included records for all individuals with known HIV diagnosed and PLOS ONE | _CITE_ February 12 , 2018 2 / 10 Use of viral load surveillance data to assess linkage to care for persons with HIV released from corrections reported to the MHASP . The records included identifiers for linkage together with the dates and results for all reported HIV viral load and CD4 + T - cell count assessments in 2012 and 2013 . Data linkage occurred by employing the algorithm used by the Health Resources and Services Administration ( HRSA ), HIV / AIDS Bureau ( HAB ) for its Ryan White Services Report ( RSR ) client - level data .__label__Supplement|Paper|Introduce
Page 4 of 35 F1000Research 2015 , 4 : 1075 Last updated : 19 JAN 2016 An initial lack of tools for the analysis of data obliged the MAP community to develop a series of bioinformatics solutions for exploring the native FAST5 data ( Table S2 ) produced by the MinION . Poretools ( Loman & Quinlan , 2014 , https :// github . com / arq5x / poretools ) and poRe ( Watson et al ., 2015 , http :// sourceforge . net / projects / rpore /) are packages for converting and visualising the raw data , whereas minoTour ( http :// minotour . github . io / minoTour ) provides real - time analysis and control of a sequencing run and post - run analytics . NanoOK ( Leggett et al ., 2015 , _CITE_ ) uses alignment - based methods to assess quality , yield , and accuracy of the data . New software packages such as marginAlign ( Jain et al ., 2015 , https :// github . com / benedictpaten / marginAlign ), NanoCorr ( Goodwin et al ., 2015 , http :// schatzlab . cshl . edu / data / nanocorr /), Nanopolish ( Loman et al ., 2015 , https :// github . com / jts / nanopolish /) and PoreSeq ( Szalay & Golovchenko , 2015 , https :// github . com / tszalay / poreseq ) were developed to address the relatively high error rate of the raw data and allow genome assembly and error - correction from MinION reads . Some of these tools were used for the MARC Phase 1 data analyses .__label__Method|Tool|Introduce
D . melanogaster ( S2 ) cells in culture were transfected with cDNA plasmids containing each of the Flock House virus genomic RNAs followed by a hepatitis D virus ( HDV ) ribozyme sequence . After induction , the HDV ribozyme regenerates the authentic 3 ’ end of the positive sense viral RNA , which is thus successfully recognized by the FHV RdRp allowing the PLOS Pathogens | _CITE_ May 5 , 2017 7 / 31 Pathways of FHV defective RNA evolution by ClickSeq and the MinION initiation of viral replication [ 54 ]. We choose to initiate replication with this method to ensure that the starting viral population would be homogeneous containing only the full - length RNAs derived from the plasmid cDNA . After transfection , the viral inoculum was allowed to amplify for 3 days ( Passage number = P0 ), after which most cells exhibited cytopathic effect .__label__Supplement|Paper|Introduce
CellCognition Deep Learning Module is a separate program for graphics processing unit ( GPU )- accelerated highperformance computing of deep learning features ( Supplemental Figure S2 ). The implementation as two separate programs provides optimal flexibility for installation of the interactive data exploration and workflow design tool , while enabling the efficient computation of deep learning features by dedicated GPU hardware . Both programs are controlled by graphical user interfaces and distributed as open source software embedded within the CellCognition platform ( Held et al ., 2010 ; _CITE_ ). Interoperability with widely used bioimage software packages , including ImageJ / Fiji ( Abramoff et al ., 2004 ; Schindelin et al ., 2012 ), CellProfiler ( Carpenter et al ., 2006 ), and Bioconductor ( Gentleman et al ., 2004 ), is enabled via the standardized file format cellH5 ( Sommer et al ., 2013 ).__label__Supplement|Website|Use
This work is accompanied by a web interface for browsing the discovered and literature motifs along with their enrichments ( Figure 1 ; _CITE_ encode - motifs ). In addition to the browsing interface , we provide several data files including all motif matrices and their matches to the genome , as well as software to compute enrichments and perform unified motif discovery with the five tools we use . Together , these permit both analyses of individual factors ( e . g .__label__Supplement|Website|Produce
The aph ( 3 ′)- IIa gene from the Escherichia coli transposon Tn5 ( Accession number V00618 , positions 151 – 945 ; 795 nts ) was used as query sequence . This reference sequence termed for clarification “ EcoAph3IIa ” was searched against the bacterial non - redundant nucleotide collection ( _CITE_ ) and the database of reference genomic sequences ( http :// www . ncbi . nlm . nih . gov / refseq /). The discontiguous megablast algorithm was used with default settings except for 250 hits to be displayed ( http :// blast . ncbi . nlm . nih . gov / Blast . cgi ? PROGRAM = blastn & PAGE_TYPE = BlastSearch & LINK_ LOC = blasthome ).__label__Material|Data|Compare
Additional information Supplementary Information accompanies this paper at https :// doi . org / 10 . 1038 / s41467017 - 02374 - 7 . Competing interests : The authors declare that they have no competing financial interests . Reprints and permission information is available online at _CITE_ Publisher ' s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations . Open Access This article is licensed under a Creative Commons Attribution 4 . 0 International License , which permits use , sharing , adaptation , distribution and reproduction in any medium or format , as long as you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material .__label__Supplement|License|Other
The gamma model of among - site rate variation was employed with empirical estimates of the extent of rate variation . In additional runs , we also used the recent version GARLI v2 . 0 [ 30 ]. To infer maximum parsimony trees we used both the PROTPARS program in the PHYLIP v3 . 67 package ( _CITE_ ) and PAUP * ( version 4 ) [ 31 ]. In data sets where parsimony method outputs multiple trees , only the best tree ( based on RF distance ) was used for average accuracy calculations . Finally , we tested the Bayesian method available in MrBayes 3 . 1 . 2 [ 32 ], incorporating its default settings with a mixed amino acid substitution model and a gamma model of among - site rate variation ( and in additional runs using a gamma model of rate variation with a proportion of invariable amino acid sites ).__label__Method|Code|Use
The development version of the data is available at GitHub ( _CITE_ ). Under the root repository , we named and organized folders and files as follows :__label__Material|Data|Produce
Compliance with ethical standards Approval for the use of the datasets was provided by the Central and South Bristol Research Ethics Committee ( ref 04 / Q2006 / 176 ). Disclosures None . Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made .__label__Supplement|License|Other
Our laptop ’ s configuration is described as follows : the Intel i5 - 5300 CPU with 2 . 30GHz , 4GB memory , Windows 10 OS . In the local area network , we deploy a local blockchain via go - ethereum that is a Go implementation of the Ethereum protocol ( https :// github . com / ethereum / go - ethereum ). In the blockchain network , we deploy four record - nodes ( miners ), and we use transaction simulator ( _CITE_ ) to simulate the hospital , servers and insurance company to generate and send transactions . Moreover , we record MIStore system ’ s data in the transaction ’ s payload . In the Ethereum blockchain , a transaction ’ s payload can record data of at most 1014 bytes .__label__Method|Tool|Use
Hybridization - based target enrichment protocols used for whole exome sequencing as well as experiments that start from low quantities of input material also require PCR amplification . If the number of unique DNA template molecules in the initial library is small or if there are steps in the library preparation that reduce the number of distinct DNA fragments , some fragments can end up being sequenced © The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . The Author ( s ) BMC Bioinformatics 2017 , 1B ( Suppl 3 ): 43 Page 114 of 175 multiple times .__label__Supplement|License|Other
Thus , indirect estimates of U5MR from SBHs should be used only for populations that have experienced either smooth mortality declines or only short periods of excess mortality in their recent past . Additional Information . Please access these websites via the online version of this summary at _CITE___label__Supplement|Document|Produce
The cloudiness and temperature in spring varies greatly between years , and this results in variation in the availability of energy for melting and hence to variation in the dates of ice melting . Compared to the melting of ice , the freezing of water bodies is a simpler phenomenon since the solar radiation plays a minor role in the energy budget of the surface water in late autumn . PLOS ONE | _CITE_ June29 , 2017 12 / 20 Reliability of temperature signal in climate indicators The cooling of surface water and finally the freezing is caused by the heat flux from the lake into cold air . In addition , evaporative cooling , and stream outflow are significant processes [ 67 ]. Freezing of lakes ( especially Ouluja ¨ rvi ) carried thus reliable information on autumn temperatures .__label__Supplement|Paper|Introduce
All genome - wide summary statistics generated in the discovery component of this study are available as Supplementary Tables . All discovery RNA sequencing data are available in the EGA repository , https :// www . ebi . ac . uk / ega / studies / EGAS00001001203 . All discovery methylation data are available in the EGA repository , _CITE_ The discovery proteomics data has been deposited in the PRIDE archive under study ID PXD002014 [ publication in process ].__label__Material|Data|Produce
See Table 1 and Methods for dataset details . GPCP , TRMM , and PERSIANN are gridded datasets comprised of RS and IS data sources ; CPC is a gridded dataset comprised of IS data ; UKP is a custom interpolation of IS and RS ( PERSIANN ) data sources ; UK , OK , IDW , and VP are custom interpolations of IS data . These maps were generated in R , Version 3 ( _CITE_ ) 76 . the rainfall datasets can be divided into two groups : the first ( I ) includes the gridded datasets GPCP ( RS ), CPC ( IS ), and TRMM ( RS ), and the nearest - neighbor interpolation VP ( IS ); and the second group ( II ) includes the remaining interpolations UKP ( IS , RS ), UK ( IS ), OK ( IS ), and IDW ( IS ), and the gridded PERSIANN ( RS ) dataset . Figure 3 shows that group II datasets report a greater number of wet days , but lower mean rainfall on those wet days , relative to group I . The lower mean wet - day rainfall of group II stems from the fact that group I data report more wet - day extremes ( see Supplementary Figures 3 and 6 ), which upwardly bias the mean wet - day rainfall of group I , despite those data showing fewer wet days .__label__Method|Code|Use
A sample size of 100 was chosen as it represented the average number of CRL observations for each GA in the INTERGROWTH - 21st data and is large enough to remove effects of sampling variation . The GA was between 5 and 17 weeks , the GA range of original data from which the equations were Ohuma et al . BMC Medical Research Methodology 2013 , 13 : 151 Page 4 of 14 _CITE_ obtained . We log transformed GA in all analyses to stabilise variance [ 2 , 15 , 20 , 26 ].__label__Supplement|Paper|Introduce
The average time cost for each 5 × WGS analysis was 2 . 39 min and the average expenditure was $ 3 . 62 . Table 10 describes time cost details in this case . GT - WGS is the champion solution of the WGS time optimization problem on the Wind and Cloud challenge held by the GCTA committee ( see _CITE_ for the news report ). This success can be attributed to : ( 1 ) GT - WGS always tries to get the lowest price via spot instances ;__label__Supplement|Document|Introduce
Only citizens of the Republic of China ( Taiwan ) who fulfill the requirements of conducting research projects are eligible to apply for the NHIRD . The use of NHIRD is limited to research purposes only . Applicants must follow the Computer - Processed Personal Data Protection Law ( _CITE_ ) and related regulations of National Health Insurance Administration and NHRI . The applicant and his / her supervisor must sign an agreement upon application submission . All applications are reviewed for approval of data release .__label__Supplement|License|Other
Water temperature is a major factor in determining the geographic distribution and preferred habitats of marine species , though the mechanism for these relationships is perhaps mediated by oxygen demand and availability [ 1 , 3 , 67 , 68 ]. We have shown that climate change in the 21st century will shift the location and available area of suitable thermal habitat for species inhabiting the North American shelf . These shifts in thermal habitat can interact in complex PLOS ONE | _CITE_ May 16 , 2018 15 / 28 Projecting thermal habitat shifts on the North American continental shelf Fig 6 . Projected change in thermal habitat availability . Mean percentage change in projected thermal habitat availability over the 21st century for low uncertainty species originating from seven regions of the North American shelf for ( A ) RCP 2 . 6 and ( B ) RCP 8 . 5 .__label__Supplement|Paper|Introduce
Non - synonymous SNPs ( nsSNPs ) from this validated list of rhesus macaque SNPs were converted to human genome ( hg18 ) co - ordinates using the UCSC liftOver tool from Genboree Galaxy and then analyzed for possible functional significance using PolyPhen - 2 ( _CITE_ ) [ 36 ]. All SNPs that did not convert cleanly to unique locations in hg18 , or where the human reference allele at that location matched the rhesus variant allele , were removed from further analysis . The resulting list of human homologues of our rhesus nsSNPs was submitted as a batch file for HumDiv model analysis in PolyPhen - 2 .__label__Method|Tool|Use
We also complement our findings on simulated data by evaluating the performance on a number of real life experimental datasets generated from a range of high - throughput platforms such as gene expression data from Khondoker et al . 1807 DNA microarrays , neuroimaging data from high - resolution magnetic resonance imaging ( MRI ) system , and event - related potential ( ERP ) data measuring brain activity derived from electroencephalogram ( EEG ) system . The simulation program ( simData ) and performance estimation program ( classificationError ) are provided as part of the R package optBiomarker available from the Comprehensive R Archive Network ( _CITE_ ).__label__Method|Tool|Use
We also complement our findings on simulated data by evaluating the performance on a number of real life experimental datasets generated from a range of high - throughput platforms such as gene expression data from Khondoker et al . 1807 DNA microarrays , neuroimaging data from high - resolution magnetic resonance imaging ( MRI ) system , and event - related potential ( ERP ) data measuring brain activity derived from electroencephalogram ( EEG ) system . The simulation program ( simData ) and performance estimation program ( classificationError ) are provided as part of the R package optBiomarker available from the Comprehensive R Archive Network ( _CITE_ ).__label__Method|Code|Use
DOI : https :// doi . org / 10 . 7554 / eLife . 30766 . 029 Figure supplement 1 — source data 1 . Figure 5 — figure supplement 1 — source data . DOI : _CITE_ 2014 ). In this study , we identified increased expression of an enzymatic effector of DNA methylation , Dnmt3a , as a key epigenetic driver of IR in vitro and in vivo . DNA methylation , like other forms of epigenomic modification , has been an attractive therapeutic target because of their plasticity and because they offer an opportunity to reprogram cells into a more healthy state .__label__Supplement|Paper|Introduce
We took a 900bp fragment , located between 3130 and 2230 bp upstream of the TSS and centered on this region , and tested its ability to drive expression of lacZ in vivo . This sequence , which we call the 3130 element , drives expression dorsally overlapping stripe 2 and stripes 5 through 7 ( Fig 7A and 7C ). This fragment drives stronger and more ventral expression within the PLOS ONE | _CITE_ July 17 , 2017 13 / 26 A sequence level model of an intact locus generated an equally good fit to data as those that did not incorporate chromatin status ( Fig 8A ). We no longer find expression driven by the 3130 element within the context of the intact locus ( Fig 8B ), but this fragment is still predicted to drive expression when removed from its native chromatin context ( S4 Fig ). The DNA regions that contribute to activation overlap with their corresponding enhancers ( Fig 8B ), and when we simulated the activity of known enhancers in silico each enhancer is still correctly predicted to drive expression of its corresponding stripes ( Fig 8C ).__label__Supplement|Paper|Compare
The parameters relevant to the ClueGO analysis are reported in Table S2 . The scatter and hierarchical clustering plots were drawn with Orange software ( v . 3 . 3 ; Demsar et al ., 2013 ), while functional networks were drawn with Cytoscape ( v . 3 . 3 ; Shannon et al ., 2003 ). The mitochondrial assignments were performed as described before ( Mäkelä et al ., 2016 ; Tikka et al ., 2016 ), by utilizing mitochondrial annotations from IMPI database ( _CITE_ ).__label__Material|Data|Use
lumbar lordosis ( 20 ° to 70 °) computed via X - ray [ 51 ]. As we measured the sagittal profiles of spinous processes labelled by skin markers , we decided to consider as reference values the mean profile values calculated from our sample population . Specifically , we defined kyphosis PLOS ONE | _CITE_ June22 , 2017 12 / 31 Normative 3D posture and spine data in healthy young adults and lordosis as backward - and forward - facing convex curves ( with related angle values ) without considering their specific location along the spine . When statistically significant changes approached this mean value , it was classified as an Improvement .__label__Supplement|Paper|Extent
Attribution License _CITE_ , which permits unrestricted use , provided the original author and source are credited .__label__Supplement|License|Other
LE and RLE at 5 years in the Matlab area were based on the demographic surveillance data from the year 2003 and 2008 , estimated to 69 . 3 and 69 . 7 for girls and 67 . 8 and 66 . 9 years for boys [ 27 , 28 ]. YLLs and YLDs were discounted with a discount rate of 3 %. The average PLOS ONE | _CITE_ February 15 , 2018 4 / 15 Cost - effectiveness of prenatal food and micronutrient interventions on under - five mortality and stunting proportion of recovery and incidence of stunting in each intervention group between 4 . 5 and 10 years were used to calculate YLDs from 5 to 10 years [ 29 ]. Stunted children were assumed to be equally affected by stunting throughout infancy , childhood and adulthood . It was assumed that children who were stunted at 4 . 5 years remained stunted at 5 years and that children who were stunted at 10 years remained so throughout life [ 30 ].__label__Supplement|Paper|Introduce
Previously published single cell ATAC - seq data are available from GSE74310 and GSE65360 . Bulk hematopeisis ATAC - seq data are available at GSE74912 . Macrophage bulk ATAC - seq data was obtained from GSE63341 , combinatorial scATAC - seq from GSM1647122 , and Roadmap Epigenomics data from the Roadmap Epigenomics Portal ( _CITE_ ).__label__Material|Data|Use
Previously published single cell ATAC - seq data are available from GSE74310 and GSE65360 . Bulk hematopeisis ATAC - seq data are available at GSE74912 . Macrophage bulk ATAC - seq data was obtained from GSE63341 , combinatorial scATAC - seq from GSM1647122 , and Roadmap Epigenomics data from the Roadmap Epigenomics Portal ( _CITE_ ).__label__Supplement|Website|Use
Proteins and phospho - peptides are grouped based on their biological function according to the lists in Supplementary Data 7 – 9 . L1000 assay . Extensive information about the L1000 method used in this paper can be found at _CITE_ Briefly , the L1000 assay is performed by amplifying mRNAs from cell lysates by ligation mediated amplification14 . Probes containing gene - specific sequences are annealed to reverse - transcribed cDNAs , ligated with Taq ligase , amplified by PCR and then hybridized to Luminex beads .__label__Supplement|Document|Produce
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ © The Author ( s ) 2017__label__Supplement|License|Other
We are grateful to Shirley Pledger and Richard Arnold for the discussions about the Pledger and Arnold ( 2014 ) paper and to Maurizio Vichi for sharing his double k - means Matlab code . Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any__label__Supplement|License|Other
the tetrode data , we selected four recordings with three single unit templates to generate four synthetic raw datasets . Each synthetic raw dataset was 30 min long with a sampling rate of 30 kHz . These simulated data for both electrode and tetrode are available at _CITE_ For both our simulated dataset and that from the Quiroga study , we defined an overlapping waveform as a waveform that was generated by the overlap of two single units with a phase shift smaller than 1 ms . In addition to simulated datasets , to assess the characteristics of our method when sorting real datasets , we sorted the real electrode and tetrode recordings described earlier , using our automated method .__label__Material|Data|Produce
As observed for the amino acid phylogenies , the Chlorellales remained sister to the Chlorophyceae + Ulvophyceae + core trebouxiophyceans when the three algae belonging to the Pedinophyceae were excluded from the sampled taxa ( data not shown ). Lemieux et al . BMC Evolutionary Biology 2014 , 14 : 211 Page 6 of 15 _CITE_ Figure 1 Phylogeny of 61 chlorophytes inferred using a data set of 15 , 549 positions assembled from 79 cpDNA - encoded proteins . The tree presented here is the best - scoring ML tree inferred under the GTR + f4 model . Support values are reported on the nodes : from top to bottom , or from left to right , are shown the posterior probability ( PP ) values for the PhyloBayes CATGTR + f4 analyses and the bootstrap support ( BS ) values for the RAxML GTR + f4 , LG4X and gcpREV + f4 analyses .__label__Supplement|Paper|Introduce
The metagenomic DNA concentration of each biological replicate was more than 10 ng / µL as measured by a Qubit Fluorometer ( Qubit 2 . 0 , Invitrogen , Carlsbad , USA ), thereby minimizing the variability in surveys of microbial communities [ 36 ]. DNA integrity was examined by using 1 % agarose gel electrophoresis before the DNA samples were stored at − 20 ° C in a freezer . PLOS ONE | _CITE_ February6 , 2018 3 / 25 Effects of an EPSPS - transgenic soybean ZUTS31 on root - associated bacteria 16S rDNA amplicon sequencing via Illumina MiSeq platform We used an improved dual - index high - throughput sequencing approach with paired - end 250 nt [ 37 ]. In brief , the fusion primers included the appropriate P5 or P7 Illumina adapter sequences , an 8 nt index sequence , and gene - specific primers for amplifying the V4 region of 16S rDNA , namely , 515F ( 5 & quot ;- GTGCCAGCMGCCGCGGTAA - 3 & quot ;) and 806R ( 5 & quot ;- GGACTAC HVGGGTWTCTAAT - 3 & quot ;)[ 14 , 38 ]. PCR amplification , PCR product purification , library quality determination , and library quantification were performed as previously described by Lu et al .__label__Supplement|Paper|Introduce
© 2017 by the authors ; licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ( CC BY ) license ( _CITE_ ).__label__Supplement|License|Other
The ND module was developed with plant breeding data ( Solanaceae , Rosaceae and legume species ) and large - scale natural diversity data ( mosquitoes ) in mind . These types of data are expected to grow rapidly due to the advance of high - throughput technology and the increasing effort toward marker - assisted breeding . The participating databases include GDR ( 5 ), SGN ( 4 ) and KnowPulse ( _CITE_ ). The collaboration was done mainly through conference calls and email correspondences . The development process was very efficient and productive ; presenting this type of collaboration as a good model for further development of open - source bioinformatics tools .__label__Material|Data|Use
We have used data collected from high - throughput phenotyping , which is based on a pipeline concept where a mouse is characterized by a series of standardized and validated tests underpinned by standard operating procedures ( SOPs ). The phenotyping tests chosen cover a variety of disease - related and biological systems , including the metabolic , cardiovascular , bone , neurological and behavioural , sensory and haematological systems and clinical chemistry . The IMPRESS database ( _CITE_ ), defines all screens , the purpose of the screen , the experimental design , detailed procedural information , the data that is to be collected , age of the mice , significant metadata parameters , and data quality control ( QC ). Experimental design . At each institute , phenotyping data from both sexes is collected at regular intervals on age - matched wildtype mice of equivalent genetic backgrounds .__label__Material|Data|Introduce
For the medium and high groups , exposure was started with a dose adaptation period of 2 hours / day on day 1 , and gradually increased to reach the target concentrations by day 17 . These exposure experiments were part of a larger study [ 33 ], which was approved in accordance with the Belgium Law on Animal Protection . Animal experiments were performed in an AAALAC - accredited facility [ 34 ], where care and use of the mice were in accordance with the American Association for Laboratory Animal Science ( AALAS ) Policy on the Humane Care and Use of Laboratory Animals ( _CITE_ ).__label__Supplement|Website|Introduce
SRM was performed on a linear gradient form 5 to 35 % ACN of 10 min on a Thermo Scientific TSQ Vantage using a 10 - cm column as described above , unscheduled with 40 ms dwell time and 0 . 8 Thompson isolation width . 0 . 5 µm sample was spiked with the stable isotope standards . The raw mass spectrometric data were stored at the public repository PeptideAtlas ( _CITE_ org , No . PASS00589 , the username is PASS00589 and the password is WF6554orn ).__label__Material|Data|Use
Three NDGs ( At1g30974 , At1g45190 , At2g13450 ) showed higher methylation levels in the promoters and two parental genes ( At4g04030 , At4g34080 ) showed higher methylation levels in the promoters compared to the common methylation level in the promoters of all the genes ( binomial test with correcting multiple testing with FDR < 0 . 05 ). The Cis - regulatory Motif Pattern of NDGs In addition to methylation pattern , we analyzed the cisregulatory elements annotated on the 100 gene pairs . The data was downloaded from AGRIS _CITE_ 32 of our NDGs and parental genes had annotated cis - regulatory elements . Only 2 NDG possessed the same cis regulatory element as the parental gene , the majority of NDGs and their parental genes had divergent cis - elements : ( 1 ) Seven parental genes had additional unique cis regulatory elements besides the ones shared with the NDGs .__label__Material|Data|Use
KO mice could not be generated for three genes , as heterozygous Pstk mice were infertile and , confirming published observations , KO of Cask49 and Dll450 resulted in heterozygous lethality . Lexicon ’ s KO strategies for Agpat2 , Clcn7 , Cldn18 , Fam20c , Gnptab , Lrp5 , Lrrk1 , Sgpl1 , Stk36 , Tph1 , Tph2 and Wnt16 are provided in the publications of these phenotypes . KO strategies used to generate 4077 of Lexicon ’ s KO mouse lines are provided at the Taconic Farms website ( _CITE_ ). Supplementary Table S1 summarizes KO strategies for all 93 genes discussed in this review . A total of 139 X - linked genes were KO ’ d , with bone data for 133 KOs reported .__label__Supplement|Website|Use
KO mice could not be generated for three genes , as heterozygous Pstk mice were infertile and , confirming published observations , KO of Cask49 and Dll450 resulted in heterozygous lethality . Lexicon ’ s KO strategies for Agpat2 , Clcn7 , Cldn18 , Fam20c , Gnptab , Lrp5 , Lrrk1 , Sgpl1 , Stk36 , Tph1 , Tph2 and Wnt16 are provided in the publications of these phenotypes . KO strategies used to generate 4077 of Lexicon ’ s KO mouse lines are provided at the Taconic Farms website ( _CITE_ ). Supplementary Table S1 summarizes KO strategies for all 93 genes discussed in this review . A total of 139 X - linked genes were KO ’ d , with bone data for 133 KOs reported .__label__Method|Algorithm|Use
In addition , graphical outputs that allow ready evaluation of processed multireplicate data sets and run to run feature comparisons are often absent . Perhaps most importantly , the lack of a true platform - independent quantitation program that could be used across multiple laboratories is clearly lacking , thus limiting the ability for large scale proteomic experiments or direct comparison of data sets from different laboratories and instruments . To address these limitations , we have adapted Skyline , a freely available open source software application ( _CITE_ ), to provide a label - free quantitation tool called Skyline MS1 filtering that efficiently extracts and processes ion intensity chromatograms from MS1 scans of peptide precursors across multiple experiments . We have extended the quantitative and graphical tools originally developed in Skyline for multiple reaction monitoring ( MRM )- MS experiments ( 18 ) to support interrogation of multiple sample acquisitions for MS1 filtering . These tools in Skyline provide graphical displays for peak selection ,__label__Method|Tool|Use
The HCLSIG profile extends the Open PHACTS approach to support a larger number of use cases . The Bio2RDF project provides scripts for converting biological datasets from their native format to an RDF representation together with SPARQL endpoints for interrogating and integrating the resulting data ( Callahan et al ., 2013 ). The conversion process from the original data source to the resulting RDF representation is captured by a provenance record that supplies core metadata about the source and resulting data ( _CITE_ , accessed June 2016 ) and is subsumed by the HCLSIG community profile . A key contribution to the HCLSIG community profile from the Bio2RDF project has been the need for providing rich statistics about the RDF data ( https :// github . com / bio2rdf / bio2rdf - scripts / wiki / Bio2RDFDataset - Summary - Statistics , accessed June 2016 ) the purpose of which are to support understanding of the dataset without needing to pose common queries — some of which can be expensive to compute . These recommendations have been included in the distribution level descriptions of the HCLSIG community profile .__label__Material|Data|Produce
To obtain a high temporal resolution of lake - extent changes , we examined and downloaded as many images as possible with a clear view of lake boundaries . In general , about 14 – 40 images were downloaded and processed for each lake ( Tables S3 – S7 ). Most Landsat images were already geometrically and radiometrically corrected by the USGS EROS Digital Image Processing Center ( _CITE_ ). For images that required additional geometric correction , we compared and geo - referenced them with other geometrically corrected images . Landsat MSS imagery has four spectral bands and the last two bands are sensitive to water bodies .__label__Supplement|Website|Use
Individual tag ID was used as a random effect in the GAMMs to account for autocorrelation within each track . Deviance explained was calculated as the ratio of the deviance of the full model compared to the deviance of the null model . Potential confounding between statistically significant covariates was investigated with variance inflation factors ( VIF ) [ 49 – 51 ] using the corvif function available at _CITE_ [ 50 , 51 ]. Model selection was performed manually , one variable at a time , and we retained candidate predictors that were statistically significant at the 0 . 05 level , maximized the amount of deviance explained and exhibited a VIF lower than 3 . Once the final model was chosen , the number of knots ( k ) was sometimes manually decreased based on the appearance of the smooth plots , to further reduce overfitting .__label__Method|Algorithm|Use
Dragon is a software package from Talete36 that calculates 4885 molecular descriptors . They cover 0D - 3D space and are subdivided into 29 different logical blocks . Detailed information on the descriptors can be found on the Talete Web site ( _CITE_ ). Chemaxon Descriptors ( 3D ). The Chemaxon Calculator Plugin produces a variety of properties .__label__Supplement|Website|Produce
Dragon is a software package from Talete36 that calculates 4885 molecular descriptors . They cover 0D - 3D space and are subdivided into 29 different logical blocks . Detailed information on the descriptors can be found on the Talete Web site ( _CITE_ ). Chemaxon Descriptors ( 3D ). The Chemaxon Calculator Plugin produces a variety of properties .__label__Supplement|Document|Produce
The implementation of both is detailed in Figure 4 . Taking into account the accessibility and interoperability requirements , the platform engine is implemented as a Java web application . For improved data handling , Hibernate ( _CITE_ ) was used as a data abstraction layer and object / relational mapper , thus reducing database coupling with the application . This shields the development from future changes in the domain model storage system and eases the use within the Java object - oriented environment . Additional components were also used , such as Spring Security ( http :// static . springsource . org / spring - security / site /) for improved security features , Apache POI ( http :// poi . apache . org /) for enhanced data import and export , Log4j for logging purposes and Apache Maven ( http :// maven . apache . org /) for project dependency management , building and deployment .__label__Method|Tool|Use
The triplestore contains administrative and accountancy data of more than 300K companies , amounting to almost 50M triples . The complexity of the schema is moderate / low , consisting of 11 classes , 16 object properties and 59 datatype properties ( see an excerpt in Fig 1 ). We have set up a PepeSearch instance to query this triplestore , available at _CITE_ In order to assess the effectiveness of PepeSearch , we arranged a search challenge with an award of a 1000 kroner book voucher for the best performer . This competition was advertised at the University of Oslo and at the Oslo Akershus University College .__label__Method|Algorithm|Produce
Pre - processing Images were checked for scanner artifacts and gross anatomical abnormalities ; reoriented along the anterior – posterior commissure ( AC – PC ) line with the AC set as the origin of the spatial coordinates ; segmented into gray matter ( GM ) and white matter ( WM ) using the segmentation procedure implemented in SPM8 ( _CITE_ ); and warped into a new study - specific reference space representing an average of all the subjects included in the analysis ( Ashburner and Friston , 2009 ; Yassa and Stark , 2009 ), using a fast diffeomorphic image registration algorithm ( DARTEL ; Ashburner , 2007 ). As an initial step , two different templates ( one for each data set ) and the corresponding deformation fields , required to warp the data from each subject to the new reference space , were created using the GM partition ( Ashburner and Friston , 2009 ). Each subject - specific deformation field was then used to warp the corresponding GM partition into the new reference space with the aim of maximizing accuracy and specificity ( Yassa and Stark , 2009 ).__label__Method|Tool|Use
By far the most interesting results were achieved analyzing the outcomes of Check 2b ), 6 , and 8 for the ANATOMY data set . These checks helped us detect a total of 30 erroneous correspondences that needed to be removed from the reference alignment ( this accounts for 2 % of the complete alignment and 5 % of its non - trivial subset ) and 14 new ones that we proposed to add to the alignment . The list of invalid and newly proposed correspondences was communicated to the anatomy alignment curators and Beisswanger and Hahn Journal of Biomedical Semantics 2012 , 3 ( Suppl 1 ): S4 Page 12 of 14 _CITE_ the organizers of the OAEI Anatomy track , and meanwhile has been incorporated in the OAEI 2011 campaign . In addition , the results of Checks 7 and 9 can be taken as a “ first aid ” for a possible effort to extend the alignment to subClassOf - based correspondences . Finally , Check 10 revealed that only one third of the correspondences in the ANATOMY alignment are non - trivial , i . e ., they cannot be detected by simple string matching tools .__label__Supplement|Paper|Introduce
In cases with a borderline ratio , additional FISH assays were performed in whole sections [ 42 ]. The data from the evaluation of TOP2A gene status were neither analyzed nor presented in the present manuscript . All primary image data of the TMA and whole tumor sections have been digitally scanned and made publicly available at : _CITE_ RNA isolation and quantitative reverse transcription - polymerase chain reaction ( qRT - PCR ) assessment Prior to RNA isolation , macrodissection of tumor areas was performed in most ( 69 %) of the FFPE sections ( all sections with & lt ; 50 % tumor cell content ). More than one FFPE section ( 2 – 8 sections , 10 µm thick ) was used for RNA extraction when the tumor surface of a given sample was less than 0 . 25 cm2 . From each FFPE section or macrodissected tissue fragments , RNA was extracted using a standardized fully automated isolation method for total RNA from FFPE tissue , based on germanium - coated magnetic beads ( XTRAKT kit , STRATIFYER Molecular Pathology GmbH , Cologne , Germany ) in combination with a liquid handling robot ( XTRAKT XL , STRATIFYER Molecular Pathology GmbH ), as previously described in detail [ 34 , 43 , 44 ].__label__Material|Data|Produce
We selected the longest sequence from each family as a representative . The set of representative sequences was then used to produce a multiple alignment with MAFFT v7 . 215 using the L - INS - i option and 1000 cycles of iterative refinement [ 90 ]. The alignment was manually trimmed to remove poorly aligned regions at the PLOS Pathogens | _CITE_ July24 , 2017 19 / 30 Capsule abundance and co - occurrence among prokaryotes extremities , using SEAVIEW [ 91 ]. The HMM profile was then built from the trimmed alignment using hmmbuild ( defaults parameters ) from the HMMER package v3 . 1b2 [ 86 ].__label__Supplement|Paper|Extent
To test if high - throughput data can be processed with CDinFusion , metagenomic FASTA files from the Global Ocean Survey ( GOS , _CITE_ , accessed : 16 . 03 . 2011 ), and metagenome data from the Microbial Interactions in Marine Systems project ( MIMAS , http :// www . mimas - projekt . de / mimas /, accessed : 16 . 03 . 2011 ) were loaded into CDinFusion . FASTA files containing over two million sequences with file sizes of two GigaBytes ( GB ) could be processed in less than three minutes in an AMDTM 64Bit , 2 GHz and 4 GB RAM environment .__label__Material|Data|Use
Our results suggested that the observed effect may reflect the intrinsic dynamic preferences of kinase clients , in which these N - lobe regions may be especially vulnerable to perturbations and form weak spots of kinase instability . These findings are consistent with similar effects seen in oncogenic kinases that are typically chaperone clients , where activation mutations are mainly assembled in a more flexible N - lobe that allows to readily modulating activation transitions [ 125 , 126 ]. Structural separation of stable PLOS ONE | _CITE_ December21 , 2017 13 / 34 A model of chaperoning kinase clients by exploiting weak spots of intrinsically dynamic kinase domains and flexible regions in the chaperone - kinase complex highlight the notion that protein polarity can be required to afford evolution of new functions without sacrificing fold stability [ 127 – 129 ]. Energetic analysis of protein - protein binding interfaces in the Hsp90 - Cdc37 - Cdk4 complex differentiates hotspot residues and quantifies functional role of the Cdc37 - M / C domain We also performed alanine scanning of the binding interfaces in the Hsp90 - Cdc37 - Cdk4 complex by computing changes in protein - protein binding free energies induced by mutations of the interfacial residues ( Figs 6 and 7 ). The central objective of this analysis was to determine energetic hotspots of protein binding and stability in the complex and quantify role of these functional residues in the allosteric mechanism of the chaperone - kinase cycle .__label__Supplement|Paper|Introduce
Additionally , interpretation of analysis results is a challenging issue here ; besides of getting results from the execution of a statistical algorithm , additional information is needed concerning data input format as well as the statistical model ’ s assumptions or parameters . Maintenance of this data provenance through appropriate metadata would enable researchers to repeat experiments with alternative assumptions or data sets [ 11 ]. This paper reports on the practical use of an innovative webbased collaboration support platform in a biomedical research context , which is in line with the above requirements and has been developed in the context of the Dicode EU FP7 research project ( _CITE_ ). The Dicode solution is generic , in that it is able to address collaboration and decision making needs of diverse contexts . Beyond the biomedical research context , its__label__Method|Tool|Introduce
Detergents can also complicate the accurate measurement of small - angle X - ray scattering ( SAXS ) data from solubilized proteins , where the background scattering from the buffer and free detergent micelles must be subtracted before data analysis can proceed . Whether this subtraction is even possible in a three - component system composed of protein – detergent complexes , free micelles and aqueous buffer depends strongly upon the type of detergent ( e . g . as discussed on the web site of the SIBYLS beamline at the Advanced Light Source , Berkeley , California , USA ; _CITE_ ). Furthermore , detergent micelles in a protein – detergent complex exhibit a different average electron density from the protein itself , with unknown structure , adding to the complexity of the data analysis . As a result of these fundamental limitations , there have been active efforts seeking other amphiphiles that are more suitable for structural studies .__label__Supplement|Website|Introduce
Survival curves were generated based on the Kaplan - Meier method using STATA 10 . 0 and 8 . 0 ( Stata Corporation ), or the online tool , OASIS ( _CITE_ ). Statistical significance was calculated using the non - parametric log - rank Mantel - Cox method . The students ’ ttest was used to calculate significance for the Q - PCR data and the Mann - Whitney test was used to calculate statistical power for the reproductive health assays .__label__Method|Tool|Use
Each of these interfaces simultaneously displays a specimen image , an OCR - rendered version of label data extracted from the image , and a collection of database fields into which data can be transferred . Apiary allows users to demarcate OCR regions of interest within the image and highlight OCR - generated text that can be transferred to associated data fields by mouse click . Symbiota provides for moving data to fields manually , but additionally includes functionality for searching the databases of the Consortium of North American Byrophyte Herbaria ( _CITE_ ) and Consortium of North American Lichen Herbaria ( http :// symbiota . org / nalichens /) for previously digitized duplicates from which data can be imported . Other institutions routinely process all images through OCR and store the OCRgenerated output in text files , or import it into a field within the database for subsequent editing , data cleaning , and searching . Popular OCR software packages included Tesseract ( http :// code . google . com / p / tesseract - ocr /), OCRopus ( http :// code . google . com / p / ocropus /), and JOCR ( GOCR ) ( http :// jocr . sourceforge . net /), all of which are open source , and the proprietary ABBYY Finereader corporate version ( http :// www . abbyy . com /) and Adobe Acrobat Professional version ( http :// www . adobe . com / products / acrobatpro . html ), both of which can batch process large numbers of images .__label__Material|Data|Use
The model was implemented in R version 3 . 1 [ 12 ]. The graphical output was generated with the R package ggplot2 [ 13 ]. The code is available on _CITE_ as an R package . It can be installed using the R library devtools . In R , use the command install . packages (& quot ; devtools & quot ;) to install devtools .__label__Method|Code|Produce
Availability of data and materials The datasets supporting the conclusions of this article are available in the NCBI Gene Expression Omnibus repository [ NCBI GEO : GSE68983 ]. In addition , all data analysis performed here , including raw data , processed data , software tools , and analysis scripts , has been reproduced in a publically accessible Linux virtual machine . See _CITE_ for details . The following publically available data sets were analyzed in the current study : Dl / Twi / Sna regions [ 19 ] obtained from http :// younglab . wi . mit . edu / dorsal / Dorsal_network_targets . txt , modENCODE cold / warm / hot transcription factor binding regions Dataset S8 [ 71 ] obtained from http :// data . modencode . org / publications / files / fly / DataS8 . gff , and Vienna Tiles and anatomical annotations from Additional file 2 : Table S1 [ 39 ].__label__Method|Tool|Produce
Availability of data and materials The datasets supporting the conclusions of this article are available in the NCBI Gene Expression Omnibus repository [ NCBI GEO : GSE68983 ]. In addition , all data analysis performed here , including raw data , processed data , software tools , and analysis scripts , has been reproduced in a publically accessible Linux virtual machine . See _CITE_ for details . The following publically available data sets were analyzed in the current study : Dl / Twi / Sna regions [ 19 ] obtained from http :// younglab . wi . mit . edu / dorsal / Dorsal_network_targets . txt , modENCODE cold / warm / hot transcription factor binding regions Dataset S8 [ 71 ] obtained from http :// data . modencode . org / publications / files / fly / DataS8 . gff , and Vienna Tiles and anatomical annotations from Additional file 2 : Table S1 [ 39 ].__label__Method|Code|Produce
Availability of data and materials The datasets supporting the conclusions of this article are available in the NCBI Gene Expression Omnibus repository [ NCBI GEO : GSE68983 ]. In addition , all data analysis performed here , including raw data , processed data , software tools , and analysis scripts , has been reproduced in a publically accessible Linux virtual machine . See _CITE_ for details . The following publically available data sets were analyzed in the current study : Dl / Twi / Sna regions [ 19 ] obtained from http :// younglab . wi . mit . edu / dorsal / Dorsal_network_targets . txt , modENCODE cold / warm / hot transcription factor binding regions Dataset S8 [ 71 ] obtained from http :// data . modencode . org / publications / files / fly / DataS8 . gff , and Vienna Tiles and anatomical annotations from Additional file 2 : Table S1 [ 39 ].__label__Material|Data|Produce
Availability of data and materials The datasets supporting the conclusions of this article are available in the NCBI Gene Expression Omnibus repository [ NCBI GEO : GSE68983 ]. In addition , all data analysis performed here , including raw data , processed data , software tools , and analysis scripts , has been reproduced in a publically accessible Linux virtual machine . See _CITE_ for details . The following publically available data sets were analyzed in the current study : Dl / Twi / Sna regions [ 19 ] obtained from http :// younglab . wi . mit . edu / dorsal / Dorsal_network_targets . txt , modENCODE cold / warm / hot transcription factor binding regions Dataset S8 [ 71 ] obtained from http :// data . modencode . org / publications / files / fly / DataS8 . gff , and Vienna Tiles and anatomical annotations from Additional file 2 : Table S1 [ 39 ].__label__Supplement|Website|Produce
FASTA files containing over two million sequences with file sizes of two GigaBytes ( GB ) could be processed in less than three minutes in an AMDTM 64Bit , 2 GHz and 4 GB RAM environment . All described test cases were recorded with the Selenium IDE ( http :// seleniumhq . org /) test case recorder . The test cases along with the test data , except for the metagenomic datasets , are deposited at _CITE_ Descriptions how to run the tests , can be found in the documentation section of the public CDinFusion installation at http :// www . megx . net / cdinfusion .__label__Material|Data|Produce
Because the requirement of equal means can produce the undesirable result of overfitting the data used to train the model , Maxent has a regularization multiplier ( p ) that can tune the model to avoid such overfitting . In this study , we used Maxent 3 . 3 . 3e ( MAXENT ; Phillips et al . 2006 , software available at _CITE_ ) with only the convergence threshold ( 0 . 00001 ) and the number of background points ( 10 , 000 ) set to their default values . To avoid overfitting , we increased the regularization multiplier ( 0 = 2 ). This choice produced less open - ended response curves .__label__Method|Tool|Use
One such receptor is the aryl hydrocarbon receptor ( AHR ). The AHR is the only ligand - activated member of the Per - ARNT - Sim ( bHLH / PAS ) family of transcription factors , all of which play important roles as environmental - and physiological stress - sensing proteins [ 8 ]. The AHR has been best studied for its ability to be activated by dioxins , polychlorinated biphenyls , and polycyclic aromatic hydrocarbons [ 9 ], all of which are high priority chemicals on the U . S . Agency for Toxic Substances and Disease Registry list of pollutants of greatest concern to human health ( _CITE_ ). Ligand - bound AHR induces P450 enzymes such as CYP1B1 and CYP1A1 , which are capable of generating mutagenic intermediates . However , more recent work suggests that the AHR , which is expressed at aberrantly high levels and is chronically active in several cancers , plays an ongoing role in tumor progression by enhancing tumor invasion and migration [ 10 – 15 ].__label__Supplement|Document|Introduce
The primer sequences were generated using the Primer - Blast tool ( http :// www . ncbi . nlm . nih . gov / tools / primer - blast /). Amplification of unintended targets was excluded by BLAST search ( http :// blast . ncbi . nlm . nih . gov / Blast . cgi ). Calculation of unfavorable secondary structures and primer - dimer amplification was done with the IDT oligo analyzer ( _CITE_ ). For absolute quantification of target genes , nested primers flanking the original primers were designed . Nested amplicons were purified from agarose gels , and serial dilutions were used to generate standard curves for each gene .__label__Method|Tool|Use
mouse following the gaze and using the pointer to keep track of the search path ) may be of particular interest for spatial and geographic tasks , such as route tracing on a map display . However , this connection is underexplored , perhaps for the lack of appropriate spatio - temporal analytical methodology to quantitatively describe the connection between the two respective movements . PLOS ONE | _CITE_ August4 , 2017 2 / 36 Quantifying gaze and mouse interactions on spatial visual interfaces In this paper , we propose a new spatio - temporal methodology for quantifying the connection between the gaze and mouse movements , which we base on contemporary developments from computational movement analysis and visualisation [ 21 ]. To evaluate and validate our new methodology , we devise an experiment which mimics a simple spatial task commonly performed by users of geographic displays – route tracing – and in which we synchronously collect gaze and mouse data . The experiment is designed to generate data with a known delay of the mouse behind the gaze and vice versa for control purposes , as well as data on how route tracing is done in a natural manner .__label__Supplement|Paper|Introduce
Bioinformatics and homology modelling . As the 3D structure of the DPP III from P . gingivalis has not yet been determined experimentally , we resorted to comparative modelling . The sequence was retrieved from the UniProt database ( _CITE_ ) and the domain structure and organization of PgDPP III was predicted using two different approaches , the web server Phyre2 , http :// www . sbg . bio . ic . ac . uk /~ phyre2 [ 40 ] and the stand - alone program Modeller9 [ 41 , 42 ]. The model was built using two templates . The model of the 3D structure of the DPP III domain ( amino acids 1 – 659 ) was determined using the experimentally determined structure of BtDPP III ( PDB_code : 5NA7 ) as a template using the Phyre2 server .__label__Material|Data|Use
v = elements # language ) that should name the language of the resource ; the described dataset in this case . Some providers , however , use the ‘ language ’ element to indicate the language of the metadata . Further , each element might contain a ‘ lang ’ attribute , indicating the language of the value of that particular field ( _CITE_ ). As gesisDataSearch should contain as much information as possible , we applied a simple procedure for handling language in our index :__label__Material|Data|Use
The corresponding range in swimming – speed based Reynolds number was 2 . 10 x 102 to 7 . 71 x 105 . For the swimming animals considered in this study from [ 4 ], the relationship between Re and Relat is shown in section 1 of S1 Appendix . We chose to analyze this PLOS ONE | _CITE_ June 27 , 2017 2 / 23 Optimal specific wavelength for maximum thrust production in undulatory propulsion data using Relat instead of Re to allow for a direct comparison to our translation – locked fin simulations , for which there is no measured swimming speed ( see Parametric Study ). Additionally for free – swimming simulations , Relat is a parameter that is known a priori and can be prescribed at the beginning of the simulation . On the other hand , Re is an output of the simulation , which is not preferable when conducting parametric studies .__label__Supplement|Paper|Introduce
This page contains mostly a list of links to the categories contained in the entry . By following these links , for example , http :// rdf . wwpdb . org / pdb / 1GOF / entityCategory , one finds a list of links to “ entity ” category elements . Each category element can be also accessed by a URL such as _CITE_ which contains the data describing the molecular entity whose the primary key is 1 ( in this particular example , the entity is galactose oxidase . The PDBx dictionary also defines relations between related categories , and this is reflected in PDB / RDF as URL links between elements of different categories . In each PDB entry , there are also references to other resources such as UniProt , Enzyme Commission numbers , PubMed , DOI ( document object identifier ), etc .__label__Material|Data|Use
Data Record 3 ). Users may also download imagery , on a site - by - site basis , for a specified date and time range . Colour channel information , for a user - defined ROI , can be extracted and processed to daily and three - day products , using the stand - alone precompiled PhenoCam GUI application ( _CITE_ ) or the PhenoPix R package ( http :// r - forge . r - project . org / projects / phenopix /) 62 . The software routines used to generate the data sets presented here are publicly available for reuse by the community . Code for image processing , including extraction of colour information , and generation of “ all - image ”, and “ summary ” time series data files , is available at https :// github . com / tmilliman / pythonvegindex /, while an R package57for time series processing , including interpolation and uncertainty characterization , as well as outlier detection and transition date extraction , is available at https :// khufkens . github . io / phenocamr /.__label__Method|Tool|Use
BMJ Open 2016 ; 6 : e013259 . doi : 10 . 1136 / bmjopen - 2016 - 013259 ► Prepublication history for this paper is available online . To view these files please visit the journal online ( _CITE_ bmjopen - 2016 - 013259 ).__label__Supplement|Document|Produce
In 2013 , the AEDC cohort was linked to several administrative data sets as detailed below . These included the children ’ s birth , mortality , health , school and child protection records , their mothers ’ perinatal records , and both parents ’ mortality , health and criminal records . The record linkage was conducted by an independent agency , the Centre for Health Record Linkage ( CHeReL : _CITE_ ) using ChoiceMaker software ( Choice Maker Technologies Inc .) to facilitate probabilistic record linkage methods that ensure strict privacy protocols are adhered to . Matching variables included name , date of birth , residential address and sex , and were obtained for each of the data sets . Definite and possible matches between these data sets were identified using ‘ blocking ’ and ‘ scoring ’, with 0 . 75 and 0 . 25 probability cut - off limits employed to ensure false positive links were minimised__label__Supplement|Website|Introduce
Supplemental material for this article may be found at _CITE___label__Supplement|Document|Produce
Identification of nonBRCA TCGA tumors . To create a nonBRCA tumor set from the remaining 1078 tumor / normal BAM pairs ( Supplementary Fig . 1 ), we analyzed primary WES data using Mutect62 , VarScan241 , and Sequenza38 ( _CITE_ ). Tumors were excluded if they were found to have : ( 1 ) a pathogenic somatic BRCA1 or BRCA2 mutation ( n = 23 breast and n = 18 ovarian ) and / or ( 2 ) homozygous copy number deletion of BRCA1 or BRCA2 ( n = 11 breast and n = 4 ovarian ). Finally , Level 3 RNAseq z - scores , microarray Z - scores and HK27 / HK450m methylation beta values were bulk downloaded from The Cancer Genomics Hub of the University of Santa Cruz ( https :// cghub . ucsc . edu /, project now completed ) for the tumor / normal pairs .__label__Method|Tool|Use
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ © The Author ( s ) 2018__label__Supplement|License|Other
The phylogenetic analysis from geminivirus . org enables a rapid visualization of phylogenetic relationships and groupings from the input sequence dataset ( _CITE_ ). Initially , the sequence of interest is submitted against the geminivirus sequences using the BLASTn algorithm . The selected sequences from the BLAST results are then automatically given as inputs to the MUSCLE algorithm to perform multiple sequence alignments .__label__Material|Data|Use
It is likely that adding labeled data from new sites will eventually ease this problem . We release all the tools open - source , along with the labels used in training and evaluation , the best performing classifier and all the derivatives of this work to allow researchers to improve its prediction and build alternative models upon this work . Software and data availability The pre - registered report is available online at _CITE___label__Supplement|Document|Produce
Libraries were constructed with 300 – 400 bp insert length , and 101 bp or 151 bp paired - end sequencing was performed on Illumina Hiseq instruments . The tumors were classified into four molecular subtypes as described previously by TCGA19 . We obtained WGS data of 40 GC tumors from TCGA ( _CITE_ ), 32 tumors from ICGC ( https :// ega - archive . org / datasets / EGAD00001003132 ), and 100 tumors from Wang et al . ( HK ) 20 . The molecular subtypes of tumors from the TCGA cohort were defined by TCGA .__label__Material|Data|Use
Libraries were constructed with 300 – 400 bp insert length , and 101 bp or 151 bp paired - end sequencing was performed on Illumina Hiseq instruments . The tumors were classified into four molecular subtypes as described previously by TCGA19 . We obtained WGS data of 40 GC tumors from TCGA ( _CITE_ ), 32 tumors from ICGC ( https :// ega - archive . org / datasets / EGAD00001003132 ), and 100 tumors from Wang et al . ( HK ) 20 . The molecular subtypes of tumors from the TCGA cohort were defined by TCGA .__label__Supplement|Website|Use
Lake - extent delineation based on Landsat images To systematically examine lake changes beyond the period of 2003 – 2009 , we selected 25 lakes from five regions across the Tibetan Plateau to determine detailed changes in lake extent since the 1970s using Landsat MSS and TM / ETM + images ( Table S2 ). Landsat images were downloaded from the U . S . Geological Survey ( USGS )’ s Global Visualization Viewer ( _CITE_ gov ). To obtain a high temporal resolution of lake - extent changes , we examined and downloaded as many images as possible with a clear view of lake boundaries .__label__Supplement|Website|Use
( PDF ) Figure S13 Effect of alignment to allele specific analysis . We performed local realignment using a variant aware aligner glia ( _CITE_ ) and compared the allelic bias in our significant allele specific sites between the two alignments . We saw that the effect of local realignment is minimum . ( PDF ) Figure S14 No QTLs with strong effect size in binding regions that do not show strong allele specificity .__label__Method|Tool|Use
For example , the Form Validator validates that for a user input value for a web form field is consistent with the ontology class definition or with the property range or domain constraints . The RDF Manager component of OntoANT defines a set of Application Programming Interfaces ( API ) that can be used by other OntoANT components to access , construct queries , and generate as well as validate RDF triples . OntoANT is currently being used in the T . cruzi SPSE project to deploy web forms ( OntoANT is accessible at : _CITE_ design . jsp ).__label__Method|Tool|Use
2 were downloaded from the DGRP website (^ 20x ; _CITE_ , Supplementary Table S1 ). We also called the same TE insertions in the DGRP population using a high - depth sequencing data from a pool of 92 D . melanogaster DGRP strains (^ 60x ) ( 39 ). Eighty - four strains are common in the two data sets ( Supplementary Table S1 ).__label__Supplement|Website|Use
In addition , six sensitivity analyses were undertaken . Sensitivity analyses included ( 1 ) using data reported for each cultivar or variety of crops separately and / or ( 2 ) treating data reported for different years in the same publication as separate events in the weighted or unweighted meta - analyses ( see online supplementary Table S5 ). The results of the sensitivity analyses are available on the Newcastle University website ( _CITE_ ). Effect sizes for all the weighted meta - analyses were based on standardised mean differences ( SMD ) as recommended for studies in which data obtained by measuring the same parameters on different scales are included in meta - analyses ( 25 , Both weighted and unweighted meta - analyses were carried out using the R statistical programming environment ( 30 ). Weighted meta - analyses , with the SMD as the basic response variable , were conducted using standard methods and the open - source ‘ metafor ’ statistical package ( 31 – 34 ).__label__Supplement|Website|Produce
With the establishment of the GBIF in 2001 , an attempt is being made to develop a global infrastructure to facilitate the discovery , inventory and access to the world ’ s primary biodiversity data . GBIF ’ s mission is to facilitate free and open access to the world ’ s biodiversity data to anyone , anytime , anywhere . Currently , GBIF facilitates access to over 190 million data records through its data portal , _CITE_ However , these primary biodiversity data records are just a minuscule component of the estimated voluminous amounts of data out there . For example , over 6500 natural history collections globally are believed to house in the range of 3 billion specimens [ 53 , 54 ], which many experts believe is an under - estimate .__label__Material|Data|Introduce
Funding Publication charges for this article have been funded by Case Western Reserve University institutional funding to T . L . The funding body played no role in study design or conclusions . Availability of data and materials The MitoDel tool can be downloaded at _CITE___label__Method|Tool|Produce
We thank Kyle Grimes for editing the manuscript . We also thank The Cancer Genome Atlas network for data access ( _CITE_ ). A . I . V . acknowledges financial support from National Institutes of Health grant 7 - R01DK - 062148 - 10 - S1 ; A . I . V .__label__Supplement|Website|Other
Ultimately , preservation of global health requires prioritization of and support for international collaboration . These and other principles were affirmed at the consultation ( Table 1 ) and codified into a consensus statement that was published on the WHO website immediately following the meeting ( http :// www . who . int / medicines / ebola - treatment / data - sharing_phe / en /). A more comprehensive set of principles and action items was made available in November 2015 , including the consensus statement made by the editorial staff of journals that attended the meeting ( _CITE_ ). The success of prior initiatives to accelerate timelines for reporting clinical trial results has helped build momentum for a broader data sharing agenda . As the quick and transparent dissemination of information is the bedrock of good science and public health practice , it is important that the current trends in data sharing carry over to all matters of acute public health need .__label__Supplement|Document|Use
https :// doi . org / 10 . 1371 / journal . pone . 0188955 . g005 blonde ray ’ s Bpa is already protected by the cuckoo ray MPA ?’. This results in one MPA map per species , and a single four - colour MPA map , with colours corresponding to the species responsible for that part of the MPA ( four - species cumulative closure maps for all four sorting scenarios shown in Fig 5 ). All maps generated by gbm . valuemap list the percentage of the PLOS ONE | _CITE_ December 7 , 2017 11 / 16 Gbm . auto : A software tool to simplify spatial modelling and Marine Protected Area planning avoid - variable ’ s total that is overlapped by the MPA , in the map legend . Finally a report is produced ( see S1 File ). The sorting strategies mentioned earlier are as follows .__label__Supplement|Paper|Extent
© 2018 by the authors . Licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ( CC BY ) license ( _CITE_ ).__label__Supplement|License|Other
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : The raw data of the BACI dataset about export flows cannot be made publicly available because this dataset has been purchased from CEPII _CITE_ However , the use of this dataset is not exclusive and anyone can purchase it . A free version for academic and research institutions is available on the United Nations COMTRADE website https :// comtrade . un . org / data /.__label__Material|Data|Introduce
This unspecified sequence of events is indicated with dotted arrows in all figures of this paper . Sin et al . BMC Systems Biology 2015 , 9 ( Suppl 3 ): S2 Page 3 of 9 _CITE_ Figure 2 The experiment performed by Braun et al . [ 15 ] consists of knocking down several permutations of the target ’ s degradation factors . The “ Control (-)” data result from an experimental setup in which the miR9 - b is not expressed ; the “ PANS & NOT1 KD ” data result from an experimental setup in which only the miR9 - b is expressed but both NOT1 and PANS are knocked down ; the “ NOT1 KD ” data result from a setup in which miR9 - b and PANS are expressed while NOT1 is knocked down ; the “ PANS KD ” data result from a setup in which miR9 - b and NOT1 are expressed while PANS is knocked - down ; the “ Control (+)” data result from a setup in which all three factors are expressed .__label__Method|Tool|Introduce
This unspecified sequence of events is indicated with dotted arrows in all figures of this paper . Sin et al . BMC Systems Biology 2015 , 9 ( Suppl 3 ): S2 Page 3 of 9 _CITE_ Figure 2 The experiment performed by Braun et al . [ 15 ] consists of knocking down several permutations of the target ’ s degradation factors . The “ Control (-)” data result from an experimental setup in which the miR9 - b is not expressed ; the “ PANS & NOT1 KD ” data result from an experimental setup in which only the miR9 - b is expressed but both NOT1 and PANS are knocked down ; the “ NOT1 KD ” data result from a setup in which miR9 - b and PANS are expressed while NOT1 is knocked down ; the “ PANS KD ” data result from a setup in which miR9 - b and NOT1 are expressed while PANS is knocked - down ; the “ Control (+)” data result from a setup in which all three factors are expressed .__label__Supplement|Paper|Introduce
This program is designed to replace the patient tags in all the DICOM files in a folder ( and sub - folders ) with anonymized strings assigned . This was done in accordance with the HIPAA , as designated by the DICOM standards committee Attribute Confidentiality Profile ( DICOM PS 3 . 15 : Appendix E ), which describes a standard procedure and documentation for removal of protected health information from DICOM images33 . A final DICOM de - identification quality assurance was applied using a software , ImageJ ( _CITE_ ), which collects attributes per patient in a report that was scanned to guarantee optimal anonymization accomplishment by the implemented DICOM anonymizer software . To authenticate that our anonymization process hasn ’ t affected the spatial information included in the DICOM header , we uploaded the anonymized DICOM files into VelocityAITM 3 . 0 . 1 software , where they were correctly viewed . Code availability ImageJ , a free software offered by the National Institutes of Health , USA , as a public domain Java processing program .__label__Method|Tool|Use
It is important to note why the HOLI pipeline was not tested in this study . First , the HOLI pipeline as referenced by Pedersen et al . ( 2016 ) was incomplete and lacked the last common ancestor designation step as well as any user guidance or documentation in the code repository ( _CITE_ 9 March 2016 version ). Furthermore , since that time , the HOLI pipeline has continued to be developed for a new study that is not yet published ( https :// github . com / ancient - eDNA / Holi ), and it may therefore be impossible to reproduce the results of Pedersen et al . ( 2016 ) using the current repository .__label__Method|Code|Compare
The samples were filtered through 0 . 7 um Whatman GF / F glass fibre filters ( 25 mm ) immediately after sampling , and then the filters were stored in aluminium foil and kept at - 20 ° C . Filters with chl - a were extracted with 90 % acetone and sonicated for 10 minutes , and then extracted at 40 ° C in the dark for 24 hours . The fluorescence method was used to measure the chl - a concentration with a Turner Designs model 10 - AU fluorometer within 15 days from the sampling date [ 17 , 18 , PLOS ONE | _CITE_ January 12 , 2018 3 / 17 Chl - α in the Beibu Gulf 19 ]. These in - situ data are used to validate the MODIS - derived chl - a concentrations . For the comparison between the satellite and in - situ data in the Beibu Gulf , we selected MODIS L2 data ( 1 km ) taken from the same locations as in - situ data ( 9 sampling stations in Fig 1B ) on the same day .__label__Supplement|Paper|Introduce
Additional information Supplementary Information accompanies this paper at _CITE_ Competing interests : The authors declare no competing interests . Reprints and permission information is available online at http :// npg . nature . com / reprintsandpermissions / Publisher ' s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations .__label__Supplement|Document|Produce
We eliminated three habitats in which the mean and variance were 0 before or after spraying ( chicken trees , chicken houses , and open sheds ), leaving eight habitats . We focus here on the regression through the origin , as above . PLOS Neglected Tropical Diseases | _CITE_ November 30 , 2017 20 / 34 Chagas disease vector control and Taylor ’ s law Fig 6 . In Figueroa [ 34 ], TL described the relationship between y = log10 vand x = log10 m of the relative abundance of T . infestans ( A ), T . guasayana ( B ), and T . garciabesi ( C ) in 10 ( peri ) domestic habitats with positive mean abundance surveyed just before community - wide spraying with insecticides in October 2003 and during follow - up monitoring surveys of house / habitat infestations in which reinfested houses were selectively re - sprayed with insecticides in March and October 2004 and March 2005 . Each point represents the sample mean and sample variance of bug abundance for one habitat on a specified date .__label__Supplement|Paper|Introduce
We chose ClinicalTrials . gov for our study because it is the largest database of its kind and because it covers the full range of clinical conditions , includes a broad group of trial sponsors [ 10 ], and has a regulatory mandate [ 1 ]. The date of download was chosen to coincide with the anniversary of the enactment of the FDA Amendments Act ( FDAAA ) 3 years earlier , which mandated the registration of certain trials of FDA - regulated drugs , biologics , and devices [ 1 ]. We downloaded the 2010 MeSH thesaurus ( _CITE_ ) and merged it with the AACT database , where it was used as a lookup table to locate corresponding tree numbers , referred to as MeSH IDs , for all MeSH terms associated with each clinical trial in ClinicalTrials . gov . Persons or organizations who submit studies to the registry are requested to provide the condition and keyword data elements as MeSH terms .__label__Material|Data|Use
The pathway gene set database ( _CITE_ , file Human_GOBP_AllPathways_no_GO_iea_April_01_2017_symbol . gmt ) 13 from the Bader lab dated April 1 , 2017 was used in all analyses . This database contains pathways from Reactome61 , NCI Pathway Interaction Database62 , GO ( Gene Ontology ) biological process63 , HumanCyc64 , MSigdb65 , NetPath66 and Panther67 . For GO , terms inferred from electronic annotation were excluded from our analyses .__label__Material|Data|Use
Adjusted models adjust for : cohort ( paliperidone palmitate or oral SGA ), patient demographics ( age , sex , race ), schizophrenia , baseline mental health and cardiometabolic co - morbidities , baseline psychotropic drug use , health care utilization ( case management , outpatient visits ), antipsychotic medication adherence , and frequency of mental - health hospitalizations or mental - health ED visits in the baseline period . Root et al . BMC Health Services Research 2014 , 14 : 355 Page 7 of 10 _CITE_ were the most important source of potential bias in this case . The difference between MHSA - and county - level random effects on model results is negligible , suggesting either could be used as a level - 2 unit in this analysis . Additional file 1 : Tables S2 and S3 present results for these models as well as additional models which included slopeonly and intercept and slope random effects .__label__Supplement|Website|Introduce
Adjusted models adjust for : cohort ( paliperidone palmitate or oral SGA ), patient demographics ( age , sex , race ), schizophrenia , baseline mental health and cardiometabolic co - morbidities , baseline psychotropic drug use , health care utilization ( case management , outpatient visits ), antipsychotic medication adherence , and frequency of mental - health hospitalizations or mental - health ED visits in the baseline period . Root et al . BMC Health Services Research 2014 , 14 : 355 Page 7 of 10 _CITE_ were the most important source of potential bias in this case . The difference between MHSA - and county - level random effects on model results is negligible , suggesting either could be used as a level - 2 unit in this analysis . Additional file 1 : Tables S2 and S3 present results for these models as well as additional models which included slopeonly and intercept and slope random effects .__label__Supplement|Paper|Introduce
had an average age of 70 years [ 36 ]. In a comparative analysis Hagihara et al . Molecular Brain 2014 , 7 : 41 Page 5 of 18 _CITE_ using data from subjects that were & gt ; 60 years old as an adult group in the developmental data set , we also found similarity between the two data sets ( P = 0 . 0053 , Additional file 3 :__label__Supplement|Paper|Introduce
Kim et al . J Cheminform ( 2016 ) 8 : 32 Page 2 of 15 standardization process . The BioAssay database ( _CITE_ ) contains descriptions and results of biological assay experiments . The record accessions used for the respective PubChem databases are the Substance ID ( SID ), Compound ID ( CID ) and Assay ID ( AID ). As of November 2015 , PubChem contains more than 150 million depositor - provided substance descriptions , 60 million unique chemical structures , and 225 million biological activity test results ( from over 1 million assay experiments performed on more than 2 million smallmolecules covering almost 10 , 000 unique protein target sequences that correspond to more than 5000 genes ).__label__Material|Data|Introduce
Hierarchical clustering using complete linkage algorithm and the Jaccard distance between identified peptides was used to determine the dissimilarity of the metaproteome [ 34 ]. Hierarchical clustering ( complete linkage ) was also applied to the oligonucleotide profiles using 1 - Pearson ’ s correlation coefficient ( r ) as the similarity measure . Split violin plots ( R package vioplot and _CITE_ ) were used to visualize the distribution of relative abundances of bacterial phyla both for phylogenetic microarray and proteomic data . To identify correlations between human and bacterial functional classes , Spearman correlation was applied , and the R package qvalue was used for calculating false discovery rate adjusted p - values ( q - values ). Results were filtered for a q - value < 0 . 05 .__label__Method|Tool|Use
Batch conversion of files is supported in both modes . Importantly , quantification results for the most popular techniques and 2D gel spot information can now be integrated in PRIDE XML files with PRIDE Converter 2 , by providing that information in a new Proteomics Standards Initiative ( PSI ) tab - delimited standard format called mzTab ( http :// code . google . com / p / mztab /). Detailed documentation for general users and developers is available at _CITE_ The PRIDE Converter 2 framework also includes the PRIDE mzTab Generator , PRIDE XML Merger and PRIDE XML Filter ( 23 ). A mechanism to make a combined submission to PRIDE and IntAct ( 34 ), the molecular interactions resource at the European Bioinformatics Institute , is also present in the PRIDE Converter 2 .__label__Supplement|Document|Produce
By using the SNPs held in common between those listed in Table S1 and the HapMap data , we calculated the frequencies in all eight of the HapMap populations of partial networks defined originally by the BlocBuster analysis of the 64 SNPs ( listed in Table 2 and Table S1 ) and the BlocBuster analysis of the 571 SNPs in VDR , MC1R , SLC24A5 , and SLC45A2 in the four HapMap populations CEU , CHB , JPT , and YRI . The networks in each case were partial because the additional four HapMap populations were not scored for as many SNPs , and we used only those SNPs that were scored in all the populations . Another possible data source is the 1000 genomes project ( _CITE_ ). However , imputation was used extensively in__label__Material|Data|Introduce
Raw sequence data are available under GEO series accession numbers GSE86337 ( pilot experiment ) and GSE64098 ( mixture experiment ). Analysis scripts and processed data are available from the Supplementary Information website at _CITE___label__Method|Code|Produce
Raw sequence data are available under GEO series accession numbers GSE86337 ( pilot experiment ) and GSE64098 ( mixture experiment ). Analysis scripts and processed data are available from the Supplementary Information website at _CITE___label__Material|Data|Produce
Raw sequence data are available under GEO series accession numbers GSE86337 ( pilot experiment ) and GSE64098 ( mixture experiment ). Analysis scripts and processed data are available from the Supplementary Information website at _CITE___label__Supplement|Website|Produce
Raw sequence data are available under GEO series accession numbers GSE86337 ( pilot experiment ) and GSE64098 ( mixture experiment ). Analysis scripts and processed data are available from the Supplementary Information website at _CITE___label__Supplement|Document|Produce
Zipped file containing data , accessory files and R code to reproduce all analyses and tables presented in this paper . In R , once the working directory is set to a folder containing these files , all code can be executed and run at once . The R code calls for the installation and use of all necessary packages although mac users may need to update their installation of XQuartz ( _CITE_ ), to render the three dimensional images presented in Fig 5 , and S3 Fig . For further details , please see the annotation in the preamble , and throughout the R code . ( ZIP ) S1 Table .__label__Method|Tool|Use
Our study design could not logistically include randomized replicates [ 30 , 51 , 56 ] because we focused on monitoring spraying in large neighborhoods of houses . A review of previous Ae . aegypti space spray studies [ 4 ] shows that each replicate included 50 or fewer houses so PLOS Neglected Tropical Diseases | _CITE_ April 6 , 2018 17 / 26 Impact of ULV spraying on Aedes aegypti that movement of adults from surrounding houses could have impacted results . In contrast , we monitored spraying in large numbers of houses : more than 1 , 100 houses ( up to 2 , 100 houses ) during the two experimental interventions , and a MoH citywide emergency spray program . Our experimental design reduced the potential impact of adults moving into the sprayed sector from unsprayed locations .__label__Supplement|Paper|Introduce
At rates above 0 . 5 , saturation diminishes the signal about the true state frequencies at the root . The most accurate inference of both root and stationary state frequencies occurs at intermediate rates ( 0 . 05 to 0 . 2 ), but even in those cases , the difference between root and stationary frequencies is often underestimated . When the trees are not fixed to the true topology and very high rates are assumed ( two or more expected substitutions between root and tips ), we observe false positives in the recovery of the directional model , and the same is the case with entirely random data ( Supplementary File S4 , available on Dryad at _CITE_ ). Further analyses show that under such circumstances , that is without reliable information about topology and relative branch lengths , and in combination with an inadequate branch length prior ( we used an exponential distribution with rate 10 throughout ), the analyses recover a balanced tree with extreme directionality because this parameter combination increases the probability of observing a large number of substitutions on short branches .__label__Material|Data|Produce
At rates above 0 . 5 , saturation diminishes the signal about the true state frequencies at the root . The most accurate inference of both root and stationary state frequencies occurs at intermediate rates ( 0 . 05 to 0 . 2 ), but even in those cases , the difference between root and stationary frequencies is often underestimated . When the trees are not fixed to the true topology and very high rates are assumed ( two or more expected substitutions between root and tips ), we observe false positives in the recovery of the directional model , and the same is the case with entirely random data ( Supplementary File S4 , available on Dryad at _CITE_ ). Further analyses show that under such circumstances , that is without reliable information about topology and relative branch lengths , and in combination with an inadequate branch length prior ( we used an exponential distribution with rate 10 throughout ), the analyses recover a balanced tree with extreme directionality because this parameter combination increases the probability of observing a large number of substitutions on short branches .__label__Supplement|Document|Produce
The Supplementary Material for this article can be found online at : _CITE___label__Supplement|Document|Produce
Strikingly , at 72 hr APF salmIR fibers fail to stop contracting and moreover show frequent and uncoordinated spontaneous contractions in which different myofibril bundles of the same fiber twitch at different times ( Figure 7M – O , Figure 7 — Spletter et al . eLife 2018 ; 7 : e34058 . DOI : _CITE_ 13 of 34 Tools and resources Cell Biology Developmental Biology and Stem Cells__label__Supplement|Paper|Introduce
Validity and reliability evidence for the child questionnaire was established during the pilot study , results of which are presented later . McMinn et al . BMC Public Health 2011 , 11 : 958 Page 6 of 12 _CITE_ Parent questionnaire The parent questionnaire elicited similar information to the child questionnaire , but answered from the parent ’ s perspective . In addition , the parent questionnaire gathered data on : ( a ) the child ’ s health status and ethnicity ; ( b ) parent ’ s age and various socioeconomic indicators such as car ownership and employment status ; and ( c ) street connectivity in their area . Validity and reliability evidence for the parent questionnaire was not investigated , however the questionnaire was compiled using items from the following existing questionnaires : ( a ) Pupil questionnaire in the Travelling Green resource ; ( b ) the Traffic and Health in Glasgow Questionnaire [ 31 ]; and ( c ) the ‘ Active Where ’ Parent - child Survey 1 [ 34 , 35 ].__label__Supplement|Paper|Introduce
The quality filtered FASTQ files ( Paired end ) for each sample were then mapped against the Human Reference Genome build hg19 ( http :// hgdownload . soe . ucsc . edu / goldenPath / hg19 / bigZips / chromFa . tar . gz ) usingthe Burrows Wheeler alignment ( BWA ) tool version 0 . 7 . 10 ( http :// bio - bwa . sourceforge . net /). The whole genome alignment was performed using ‘ BWA - MEM ’ algorithm with default parameters [ 28 ]. The aligned reads in the Sequence Alignment / Map ( SAM ) format were then sorted using ‘ SortSam ’ algorithm of Picard tool v . 1 . 118 ( _CITE_ ). The Sorted SAM file was converted to binary version of a SAM file ( BAM file ) using the SAMtools ( http :// samtools . sourceforge . net /). The resulting BAM file was then sorted and indexed using SAMtools ( http :// samtools . sourceforge . net /) for variant calling .__label__Method|Algorithm|Use
maxdLoad2 development was funded by the NERC Environmental Bioinformatics Centre ( NEBC ), and maxdBrowse by the Biotechnology and Biological Sciences Research Council Investigating Gene Function Initiative . The authors would like to acknowledge : Helen Parkinson and Tim Rayner , from the European Bioinformatics Institute ( EBI ) for debugging the MAGEML output , and advising on best practice for import into the ArrayExpress repository ; Lucy Bridges ( NERC ) for testing the maxdLoad2 and maxdBrowse software , contributing tools and documentation and providing the information in table 1 ; Georgina Moulton from the Northwest Institute for Bio - Health Informatics for providing training , publicity and feedback on maxdLoad and maxdBrowse ; Afsaneh Maleki , Irena Spasic , Andy Tseng and Page 13 of 14 ( page number not for citation purposes ) BMC Bioinformatics 2005 , 6 : 264 _CITE___label__Supplement|Paper|Introduce
program “ R Studio ” from _CITE_ , but this is not necessary . Basic procedures for issuing commands in R are available in many locations , or may be obtained from a colleague already familiar with the program . In the instructions below , it should be noted that the result of a command can be assigned a name using the “& lt ;-” operator , and that R will recognize a forward - slash (“/”) or double back - slash (“\\”), but not a single back - slash (“\”).__label__Method|Tool|Introduce
© 2015 by the authors ; licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license ( _CITE_ ).__label__Supplement|License|Other
No other selection criteria other than availability were applied for this study . Specimens for proteomic study were sectioned sequentially from the tumor blocks used for genomic studies , hence the ‘ bottom ’ slides from the genomic studies are representative of the material used for proteomics . The slides are available from the TCGA data portal ( _CITE_ ). All samples were frozen within 1 h of collection and histopathological examination confirmed that they contained at least 60 % tumor nuclei as described earlier3 . Prior CPTAC studies have shown that global protein levels , but not phosphorylation states , are stable within this 1 h time period9 .__label__Material|Data|Use
Regarding this , we would like to point out that researchers differ in their views regarding under which circumstances research practitioners in educational and psychological sciences should consider sensitivity analysis . Given that the MAR and MNAR assumptions cannot be tested [ 11 , 12 ] in everyday research practice , one cannot rule out MNAR missingness for a given dataset . Some methodologists indicate that this absence of information is in itself enough to underpin the need for sensitivity analysis [ 2 , 12 , PLOS ONE | _CITE_ September 13 , 2017 15 / 21 A tutorial example of sensitivity analysis : Gauging the influence of the missing - data technique 13 , 15 ]. In other words , when confronted with missing data , sensitivity analysis should be conducted . Others have argued that the chances of data violating the MAR assumptions to such a degree as to impact the results obtained are slim [ 35 , 38 ].__label__Supplement|Paper|Introduce
The single - point mutation ( E64R ) is less deleterious than mutating all 4 acidic residues ( APM ), and SNF2h remodels E64R nucleosomes - 40 fold more slowly than WT nucleosomes as opposed to 200 - fold more slowly with APM nucleosomes ( Figure 1 — figure supplement 3 ). However , this remodeling rate is still very slow relative to the timescales typically measured by smFRET , which posed two additional challenges : an increase in the number of noise events ( dye blinking , intensity fluctuations , etc ) per remodeling trajectory , and an increase in the amount of data to be analyzed . These challenges were addressed through the use of custom in - house smFRET analysis software (‘ Traces ’, _CITE_ Traces ) ( Zhou et al ., 2018 ; Johnson et al ., 2018 ; copy archived at https :// github . com / elifesciencespublications / Traces ) to streamline the analysis of large data sets , and the adaptation of a computationally fast , versatile , open - source hidden Markov model ( HMM ) library called pyhsmm to quantify the durations of the pauses ( see Materials and methods ). As shown in the example trajectories in Figure 3C , remodeling of E64R nucleosomes by SNF2h proceeds through the same alternating pause and translocation phases as does remodeling of WT nucleosomes . However , the pauses are noticeably longer with E64R nucleosomes , by at least a factor of 2 ( Figure 3D ), indicating that the acidic patch epitope , like other substrate cues , is indeed sensed during the regulatory pause phase .__label__Method|Tool|Use
Source data are presented in Figure 4 — source data 1 . DOI : https :// doi . org / 10 . 7554 / eLife . 32222 . 017 The following source data and figure supplement are available for figure 4 : Source data 1 . Source raw data for Figure 4A - D . DOI : _CITE_ Source data 2 . Source raw data for Figure 4 — figure supplement 1B and C . DOI : https :// doi . org / 10 . 7554 / eLife . 32222 . 020 Figure supplement 1 . Requirement of INO80 and ARP8 for RAD51 binding to the BCR of the MLL gene .__label__Material|Data|Use
Source data are presented in Figure 4 — source data 1 . DOI : https :// doi . org / 10 . 7554 / eLife . 32222 . 017 The following source data and figure supplement are available for figure 4 : Source data 1 . Source raw data for Figure 4A - D . DOI : _CITE_ Source data 2 . Source raw data for Figure 4 — figure supplement 1B and C . DOI : https :// doi . org / 10 . 7554 / eLife . 32222 . 020 Figure supplement 1 . Requirement of INO80 and ARP8 for RAD51 binding to the BCR of the MLL gene .__label__Supplement|Paper|Use
It can be launched in a web browser locally or located on a webserver . The NaviCell factory is currently embedded in the BiNoM Cytoscape plugin but will be soon available as a stand - alone command line package . The detailed guide to use the NaviCell factory is provided at _CITE_ Common exchange formats The generated maps in CellDesigner and exposed in the NaviCell format , can also be provided in common exchange formats , as BioPAX and PNG formats . In addition , the modular composition of the maps can be provided in the form of GMT files .__label__Supplement|Document|Produce
Post - processing of the records was performed using Python scripts developed in - house ; these scripts remove any duplicate records ( an artifact of duplicate gene annotations within the GenBank file ) as well as create FASTA - formatted sequence files . The PyMongo library ( https :// pypi . python . org / pypi / pymongo /) was used to insert the data into the MongoDB . All scripts can be found online at _CITE_ HIV data collection . HIV - 1 genomes were downloaded as GenBank files from the NCBI nucleotide database specifying the following : “ Human immunodeficiency virus 1 ” ( porgn : _txid11676 ) AND ( 8000 : 11000 [ Sequence Length ]).__label__Method|Code|Produce
Metatranscriptome reads of each HMY and LMY sample were mapped to the two reference genomes as well as metagenome and metatranscriptome reads to all reassembled ldh genes and all genes in the custom lcdA genes database ( for information on database construction , see Additional file 5 : text S1 ) using BBmap ( http :// sourceforge . net / projects / bbmap /) with an ID cut - off of 98 % sequence similarity for ldh genes and genome sequences , and 60 % sequence similarity for lcdA genes and counting ambiguous reads for all matching genes . Read counts were normalised to RPM , and statistical analysis of normalised read counts was conducted in R via the WRS test and Benjamini - Hochberg correction ( for all genes in isolate genomes and ldh genes ) to select genes or transcripts with significantly different abundances between the HMY and LMY animals . Functional comparison to the Hungate 1000 genomes Functional identifiers of KEGG orthology genes from the metagenome dataset that showed significant correlation to methane yield in both the WRS test and sPLS analyses were uploaded into IMG / MER and used as screening IDs for all the bacterial genomes available from the Hungate 1000 project ( _CITE_ ) and all additionally available bacterial genomes derived from rumen habitats in June 2015 using the “ functions versus genomes ” tool in IMG / MER . Additional files Additional file 1 : Table S1 . Overview of samples analysed in this study and methods of analysis conducted .__label__Method|Tool|Use
Metatranscriptome reads of each HMY and LMY sample were mapped to the two reference genomes as well as metagenome and metatranscriptome reads to all reassembled ldh genes and all genes in the custom lcdA genes database ( for information on database construction , see Additional file 5 : text S1 ) using BBmap ( http :// sourceforge . net / projects / bbmap /) with an ID cut - off of 98 % sequence similarity for ldh genes and genome sequences , and 60 % sequence similarity for lcdA genes and counting ambiguous reads for all matching genes . Read counts were normalised to RPM , and statistical analysis of normalised read counts was conducted in R via the WRS test and Benjamini - Hochberg correction ( for all genes in isolate genomes and ldh genes ) to select genes or transcripts with significantly different abundances between the HMY and LMY animals . Functional comparison to the Hungate 1000 genomes Functional identifiers of KEGG orthology genes from the metagenome dataset that showed significant correlation to methane yield in both the WRS test and sPLS analyses were uploaded into IMG / MER and used as screening IDs for all the bacterial genomes available from the Hungate 1000 project ( _CITE_ ) and all additionally available bacterial genomes derived from rumen habitats in June 2015 using the “ functions versus genomes ” tool in IMG / MER . Additional files Additional file 1 : Table S1 . Overview of samples analysed in this study and methods of analysis conducted .__label__Material|Data|Use
Fastq reads were pseudo - aligned to the mm10 genome assembly using Kallisto66 and transcript read counts were aggregated to Ensembl Gene IDs for further analysis . RNAseq data has been uploaded to Gene Expression Omnibus ( GEO ) with the ID GSE94315 . The reviewer can access the data under the private link : _CITE_ Differential gene expression analysis was performed via the R library sleuth67 using a linear model that accounted for mouse age , heteroplasmy and batch effects . Significance and effect sizes of differential gene regulation were calculated from the likelihood ratio and the Wald test , respectively , as implemented in the sleuth package .__label__Material|Data|Produce
Images were converted from DICOM into nifti format . Origins for all images were manually set to the anterior commissure . Functional data were spike - corrected to reduce the impact of artifacts using AFNI ’ s 3dDespike routine ( _CITE_ ). Functional images were corrected for differences in slice timing using sinc - interpolation and head movement using a leastsquares approach and a 6 parameter rigid body spatial transformation . Images were corrected for distortion and movement - by - susceptibility artifacts using the FieldMap toolbox ( Andersson et al ., 2001 ).__label__Method|Tool|Use
accessRights Information about who can access the resource or an indication of its security status ( http :// purl . org / dc / terms / accessRights ). taxonID An identifier for the set of taxon information ( http :// rs . tdwg . org / dwc / terms / taxonID ) parentNameUsageID An identifier for the name usage of the direct parent taxon ( in a classification ) of the most specific element of the scientificName ( http :// rs . tdwg . org / dwc / terms / parentNameUsageID ). scientificName The full scientific name , with authorship and date information if known ( _CITE_ ). acceptedNameUsage The full name , with authorship and date information if known , of the currently valid ( zoological ) taxon ( http :// rs . tdwg . org / dwc / terms / acceptedNameUsage ). originalNameUsage The original combination ( genus and species group names ), as firstly established under the rules of the associated nomenclaturalCode ( http :// rs . tdwg . org / dwc / terms / originalNameUsage ).__label__Supplement|Document|Produce
Over 60 , 000 SmartMesh networks have been deployed so far . One vendor alone , Emerson , claims over 31 , 900 networks , with cumulated node operating hours above 9 billion ( http :// www . emerson . com / en - us / expertise / automation / industrial - internet - things / pervasive - sensing - solutions / wireless - technology ). While SmartMesh IP was designed for industrial applications , it has been used in numerous other spaces , including smart buildings ( _CITE_ ), smart cities ( http :// www . linear . com / docs / 41387 ) and smart agriculture [ 53 ]. SmartMesh IP is a proven technology ; we chose to use it because our system operates well within the limits of a SmartMesh IP network . For example , SmartMesh IP network as a whole cannot generate more than 36 packets per second , with each packet carrying at most 90 bytes of application payload .__label__Supplement|Document|Introduce
Finally , we excluded proteins with interquartile range < 70 % quantile . The resulting data matrix consisted of 1759 proteins . Level - three RNAseq data of breast tumor samples were obtained from the TCGA Web site ( _CITE_ ). First , we log - transformed the data and then replaced all missing values with 0 . Then , we standardized each sample to have median 0 and MAD 1 .__label__Material|Data|Use
Finally , we excluded proteins with interquartile range < 70 % quantile . The resulting data matrix consisted of 1759 proteins . Level - three RNAseq data of breast tumor samples were obtained from the TCGA Web site ( _CITE_ ). First , we log - transformed the data and then replaced all missing values with 0 . Then , we standardized each sample to have median 0 and MAD 1 .__label__Supplement|Website|Use
Between 2008 – 2009 the prevalence of GAM ( weight - forheight z - score & lt ;- 2 ) persisted at levels indicating serious / critical public health significance ( 10 to > 15 %) in all camps , further highlighting the poor nutritional status of vulnerable groups [ 26 ]. The aim of this study was to analyse the effectiveness of Nutributter ® distribution in reducing anaemia and stunting in refugee children between 2008 and 2011 . PLOS ONE | _CITE_ June 7 , 2017 2 / 18 SQ LNS effectiveness in refugees in the Horn of Africa__label__Supplement|Paper|Produce
The total health expenditure per capita was the most highly correlated variable with surgical rate ( Spearman correlation , r = 0 . 87297 ; P < 0 . 0001 ; Table 2 ; available at : _CITE_ ). The sensitivity analyses showed that after adjusting for total health expenditure per capita , none of the other variables remained significant . WHO regions were also not significantly associated with surgical rate ( P = 0 . 09 ).__label__Supplement|Document|Produce
article distributed under the terms of the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : The data underlying this study belong to the National Health Insurance Research Database ( NHIRD ) of Taiwan and are accessible using the following link : _CITE_ The authors did not have any special access privileges . Funding : This study was supported by grants from__label__Material|Data|Use
Different vendors may implement different communication protocols into their medical sensors , such as ISO , IEEE , and HL7 , each one with its own communication mechanism , data format , and so forth . To solve this problem , the IEEE approved the X73 standard [ 16 ], which provides an efficient real - time exchange method for plug - and - play operation . Due to the complexity of the X73 protocols , in the industry , currently only Continua Health Alliance ( _CITE_ ) produces a hardware which is fully compatible with this standard . In the academia , Korean scholars have reported that a sensor could be adapted to the X73 protocols by means of an extra hardware [ 17 ], but this will increase the cost of the sensor . Developing of software middleware as an adapter is proposed in this paper .__label__Supplement|Website|Introduce
Additionally , putative PPI data can be filtered using functional information or correlation to large scale protein interaction networks . However , usually data analysis programs aim to filter the experimental data either using control experiments or by integration of functional information . The software PIPINO ( Protein - Protein Interaction Optimizer , _CITE_ ) is a novel attempt to integrate and combine the strengths of both approaches . PIPINO allows standardizing the data analysis process and offers a semiautomatic analysis pipeline . Beside various statistical methods for evaluating the data the software is capable of functionally annotating and enriching / filtering data entries with additional information .__label__Method|Tool|Produce
The electrode array was self - made : glass - coated platinum wires ( Thomas Recording , Germany ) were sharpened ( outer diameter : 80 µm , platinum diameter : 25 µm , impedance was about 500 kO ), and 5 of them were arranged parallel to each other . The distance between the tips was 250 µm . To verify that action potentials propagate through the middle part of the corpus callosum used for subsequent counting of cells , we exposed the surface of the cortex contralateral to the implanted PLOS Biology | _CITE_ August22 , 2017 23 / 32 Neuronal activity and oligodendrocyte precursor cells electrode array and attached the recoding ball electrode to it . We then applied low frequency stimulation to the array and tested whether we were able to record field potentials at the contralateral side . This was possible in each mouse .__label__Supplement|Paper|Compare
This suggests that it is possible to consistently infer transcriptional regulatory elements , irrespective of the data sets used . This also suggests that cells use a limited number of transcription regulatory elements to adjust themselves to diverse environmental conditions . The combinatorial nature of transcription factors Page 9 of 12 ( page number not for citation purposes ) BMC Bioinformatics 2006 , 7 : 330 _CITE_ is one way to ensure an effective adaptation to diverse conditions , and is utilized in many genes . Many researchers have applied the combinatorial nature of transcription factors to the computational prediction of transcriptional networks with great success [ 12 , 27 ]. We plan to adopt the combinatorial analysis to our method and expect to further improve this method .__label__Supplement|Paper|Introduce
All analyses were conducted using R ( R Core Team , 2017 ). Primary R packages used in the analysis included dplyr ( Wickham et al ., 2017 ), tidyr ( Wickham , 2017 ), gimms ( Detsch , 2016 ), sp ( Pebesma & Bivand , 2005 ; Bivand , Pebesma & Gomez - Rubio , 2013 ), raster ( Hijmans , 2016 ), prism ( PRISM Climate Group , 2004 ), rdataretriever ( McGlinn et al ., 2017 ), forecast ( Hyndman & Khandakar , 2008 ; Hyndman , 2017 ), git2r ( Widgren and others , 2016 ), ggplot ( Wickham , 2009 ), mistnet ( Harris , 2015 ), viridis ( Garnier , 2017 ), rstan ( Stan Development Team , 2016 ), yaml ( Stephens , 2016 ), purrr ( Henry & Wickham , 2017 ), gbm ( Ridgeway et al ., 2017 ), randomForest ( Liaw & Wiener , 2002 ). Code to fully reproduce this analysis is available on GitHub ( _CITE_ ) and archived on Zenodo ( Harris , White & Taylor , 2017a ). Harris et al . ( 2018 ), PeerJ , DOI 10 . 7717 / peerj . 4278 9 / 27__label__Method|Code|Produce
Higher correlation within clusters implies that the cluster is homogenous , with highly similar expression profiles over the time points . Inter - cluster correlation was calculated as the average correlation between the cluster centroids and should be as low as possible . This is because high correlation between clusters implies that the clustering PLOS ONE | _CITE_ June27 , 2017 6 / 23 A data analysis framework for biomedical big data partition is fragmented , with the same profile shape in several clusters . The analysis revealed a principle increase in both intra - and inter - cluster correlation with larger values of k , which meant it was necessary to identify an acceptable trade - off . Based on this procedure , k - means clustering was applied using Pearson correlation coefficient as distance measure and the number of clusters was set to 10 ( k = 10 ).__label__Supplement|Paper|Introduce
S2 Fig provides approximations for the areas covered by wMel in different seasons after the releases . For EHW , the area covered in which wMel has at least frequency 0 . 5 is about 1 . 3 km2 in D1 , and this rises to about 2 . 2 km2 in W2 . We can calculate wave speed per generation PLOS Biology | _CITE_ May 30 , 2017 9 / 28 Spread of dengue - suppressing Wolbachia in Aedes aegypti ( assuming 10 generations a year ) or per day using alternative geometric approximations described in the Methods section : approximation 4 assumes a circular release area , approximation Eq ( 5 ) assumes a rectangle ( for which we approximate parameter y = 2 [ i . e ., a release area twice as long as wide ]), or approximation 6 assumes a rectangle in which spread does not occur ( or is not monitored ) in one direction . ( Note that very little spread occurred to the south at EHW .) The resulting estimates of wave speed per day are , respectively , cd = 0 . 35 m per day , 0 . 31 m per day , and 0 . 45 m per day .__label__Supplement|Paper|Introduce
( FSC = forward scatter , SSC = side scatter , norm . = normalized ). DOI : _CITE_ The following figure supplements are available for figure 3 :__label__Supplement|Paper|Introduce
PacBio sequencing . The PacBio sequences for H1 - ESC were obtained from the data used in the original IDP paper12 . It can be found at Gene Expression Omnibus ( GEO ) database , _CITE_ ( accession no . GSE51861 ). For the PacBio raw sequences , it will be provided upon contacting Kin Fai Au ( kinfai - au @ uiowa . edu ).__label__Material|Data|Use
The reaction was stopped by addition of 4 x 95 ° C Laemmli buffer , samples were boiled and separated using 40 cm long 8 % SDS - PAGE . Proteins were transferred to a 0 . 2 mm Hybond ECL nitrocellulose membrane by tank blotting and detected using an a - Tar antibody as primary antibody and a IRDyes 800 - conjugated secondary antibody ( Rockland , Limerick , PA ) using an Odyssey Imager ( LI - COR , Bad Homburg , Germany ). Protein bands were quantified using the line - scan tool in ImageJ ( _CITE_ ).__label__Method|Tool|Use
Regional filtering of FANTOM5 CAGE data33 , 139 was performed using the ZENBU browser140 , the CAGE peak data table downloaded , and summaries produced around appropriate sample types . Browser link for : AKAP1 analysis : http :// fantom . gsc . riken . jp / zenbu / gLyphs /# config = ONHzqgf2E5X tmnpsh2gURB ; loc = hg19 :: chr17 : 55162544 . 55162663 . Browser link for MSI2 analysis : _CITE_ Analysis of Gene ATLAS / UK Biobank GWAS data . We examined SNPs in certain association signals for whether they were also associated with UK Biobank traits that had been analyzed by the Gene ATLAS ( http :// geneatlas . roslin . ed . ac . uk ) 63 – 65 .__label__Supplement|Website|Produce
Publish with BioMed Central and every scientist can read your work free of charge & quot ; BioMed Central will be the most significant development for disseminating the results of biomedical research in our lifetime .& quot ; Sir Paul Nurse , Cancer Research UK Your research papers will be : available free of charge to the entire biomedical community peer reviewed and published immediately upon acceptance cited in PubMed and archived on PubMed Central yours — you keep the copyright Submit your manuscript here : _CITE_ BioMedcentral Page 15 of 15 ( page number not for citation purposes )__label__Supplement|Website|Produce
However , RSSI is only available in the iOS CoreBluetooth API used for Bluetooth Low Energy ( BLE ) and not in BluetoothManager API used in the current study . It is currently not feasible to use BLE to map social networks , due to the inability of iOS devices to detect another iOS device when both are in a locked state [ 20 ]. PLOS ONE | _CITE_ December20 , 2017 9 / 13 Validation of app to map social networks Differences in network structure may also be partly due to participant behaviour , for example when someone carries their phone with them but leaves the badge behind in their office , or vice versa . The battery of the sociometric badges need to be regularly charged and badges need to be turned on when entering the office . Participants may forget to do this as they are less used to wearing and using the sociometric badges , which would result in missing data .__label__Supplement|Paper|Introduce
That is , we assigned 50 % of the patients with lower risk scores into the low risk group and vice versa . After that , we used log rank test to test if the risks of the two groups were significantly different ( p - value : 50 . 05 ). Kaplan Meier curves and the log rank test were performed by a tool ( _CITE_ ). Evaluation of discrimination performance across several data sets . In order to evaluate to what extent the signature can discriminate patients with different risks in various data sets , we defined the Discrimination score ( Dscore ) as follows :__label__Method|Tool|Use
© The Author ( s ). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . Abstract Background : Increasing age is the biggest risk factor for dementia , of which Alzheimer ’ s disease is the commonest cause .__label__Supplement|License|Other
We have implemented our algorithm using C ++. The POSMO program can be freely downloaded from _CITE___label__Method|Code|Produce
The package also provides detailed help pages , which makes it user friendly . Furthermore , we standardize the distribution , installation and maintenance of this package . It is available on the Comprehensive R Archive Network ( CRAN ) at _CITE___label__Method|Tool|Use
Therefore , we present them only for chosen companies from WIG 30 in this paper . The chosen stocks are good representative examples of different economic sectors in Poland like : banking ( PKO BP ), telecommunication ( Orange ), fuel and energy sector ( PKN Orlen ) or insurance sector ( PZU ). Our findings have been compared with the corresponding statistical properties of returns PLOS ONE | _CITE_ November30 , 2017 4 / 24 Asymmetry of price returns — A non - extensive statistical physics point of view Fig 1 . Probability density function in log - linear scale of normalized and centered returns for Orange stock from WIG 30 stock index , calculated for various time - lags Δt in the period : March 2013 — March 2015 . The plots for different Δt were drawn as dots and are relatively vertically shifted for better display .__label__Supplement|Paper|Other
The main steps in Illumina ' s image analysis are i ) All pixel intensities are altered using a sharpening transformation . The intensity of a particular pixel is made higher / lower if its intensity is higher / lower in comparison to the intensities of the pixels surrounding it . Page 12 of 15 ( page number not for citation purposes ) BMC Bioinformatics 2008 , 9 : 85 _CITE_ ii ) Foreground intensities are calculated as a weighted average of signals obtained using the four pixels nearest to each bead centre as a " virtual bead centre ". Sharpened pixel intensities are used in the calculation . The value returned is unlogged .__label__Supplement|Paper|Compare
The following information was supplied regarding data availability : All data and code are available on GitHub : https :// github . com / jbloomlab / phydms . Detailed documentation is at http :// jbloomlab . github . io / phydms . Hilton , Doud , and Bloom , bioRxiv , 2017 . _CITE___label__Supplement|Paper|Introduce
Eureka ! is available as open source under the Apache 2 license or the GNU General Public License ( GPL ) version 3 . The GPL - licensed version differs from the Apache - licensed version in supporting an additional action for writing data and phenotypes to the Neo4j graph database ( _CITE_ ). In addition to loading data into i2b2 , both versions of the software also support loading data into a delimited file . Figure 6 illustrates Eureka !__label__Material|Data|Use
The second part is a hands - on tutorial using part of our own work as a running example . We applied TC to automatically extract nursing job tasks from nursing vacancies to augment nursing job analysis ( Kobayashi , Mol , Kismiho ´ k , & Hesterberg , 2016 ). The findings from this study were used in the EU - funded Pro - Nursing ( _CITE_ ) project which aimed to understand , among others , how nursing tasks are embedded in the nursing process . We also address validity assessment because the ability to demonstrate the validity of TC outcomes will likely be critical to its uptake by organizational researchers . Thus , we discuss and illustrate how to establish__label__Method|Tool|Use
Estimation of kinship coefficients using genome - wide SNP data Before embarking on a detailed comparison of different methods , we explored the use of different SNP sets ( containing different numbers of SNPs ) for estimating pairwise kinship measures , in order to identify a robust set of SNPs that could be used for subsequent comparisons . We considered using either the full genome - wide set of SNPs ( 545 , 433 SNPs ), a ‘ pruned ’ set of 50 , 129 SNPs selected to have minor allele frequencies > 0 . 4 and chosen to be in approximate linkage equilibrium via the -- indep 50 5 2 command in PLINK [ 27 ]), or a ‘ thinned ’ set of 1900 evenly - spaced SNPs that were selected from the ‘ pruned ’ SNPs based purely on physical position using the software package MapThin ( _CITE_ ). In addition to exploring the kinship estimates provided by various LMM software packages , we also investigated those provided by the software packages PLINK [ 27 ] and KING [ 28 ]. KING implements two different kinship estimation methods : KINGhomo ( KING_H ), which assumes population homogeneity , and KING - robust ( KING_R ), which provides robust relationship inference in the presence of population substructure .__label__Method|Tool|Use
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : All relevant data are within the paper and its Supporting Information files . Funding : Ministry of Education , Culture , Sports , Science and Technology , Japan _CITE_ Innovative Cell Biology by Innovative Technology ( Cell Innovation Program ) to Yoshihide Hayashizaki . The funder had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript .__label__Supplement|Website|Introduce
HOTAIR gene expression , copy number , DNA methylation and clinical data were downloaded from TCGA ; available from : _CITE_ , accessed 2017 ) [ 57 ]. TCGA Agilent ’ s G4502A 244K gene expression profiles from 572 GBMs , 27 grades II and III gliomas ( Supplementary Table 1 ), and 10 unmatched normal samples were analyzed , and “ level 3 ” values of HOTAIR ( probe A_32_P168442 ) and HOXA9 ( probe A_23_ P500998 ) were used . HOTAIR - high was considered when “ level 3 ” value & gt ; 0 , and HOXA9 - high when & gt ; 2 .__label__Material|Data|Use
HOTAIR gene expression , copy number , DNA methylation and clinical data were downloaded from TCGA ; available from : _CITE_ , accessed 2017 ) [ 57 ]. TCGA Agilent ’ s G4502A 244K gene expression profiles from 572 GBMs , 27 grades II and III gliomas ( Supplementary Table 1 ), and 10 unmatched normal samples were analyzed , and “ level 3 ” values of HOTAIR ( probe A_32_P168442 ) and HOXA9 ( probe A_23_ P500998 ) were used . HOTAIR - high was considered when “ level 3 ” value & gt ; 0 , and HOXA9 - high when & gt ; 2 .__label__Supplement|Website|Use
HOTAIR gene expression , copy number , DNA methylation and clinical data were downloaded from TCGA ; available from : _CITE_ , accessed 2017 ) [ 57 ]. TCGA Agilent ’ s G4502A 244K gene expression profiles from 572 GBMs , 27 grades II and III gliomas ( Supplementary Table 1 ), and 10 unmatched normal samples were analyzed , and “ level 3 ” values of HOTAIR ( probe A_32_P168442 ) and HOXA9 ( probe A_23_ P500998 ) were used . HOTAIR - high was considered when “ level 3 ” value & gt ; 0 , and HOXA9 - high when & gt ; 2 .__label__Supplement|Document|Use
This article has been published as part of BMCMedical Genomics Volume 11 Supplement 2 , 2018 : Proceedings of the 28th International Conference on Genome Informatics : medical genomics . The full contents of the supplement are available online at _CITE___label__Supplement|Document|Produce
1 . Transcriptomic and Genomic data . The microarray dataset ( study number 1 , Table 1 ) was downloaded from the Myers laboratory website ( _CITE_ , GEO reference : GSE15222 ). The complete dataset consists of transcriptome - wide gene expression data from post - mortem cerebral cortical area ( temporal , parietal and frontal cortices ), all cortical areas involved in the disease , of Alzheimer ’ s__label__Material|Data|Use
1 . Transcriptomic and Genomic data . The microarray dataset ( study number 1 , Table 1 ) was downloaded from the Myers laboratory website ( _CITE_ , GEO reference : GSE15222 ). The complete dataset consists of transcriptome - wide gene expression data from post - mortem cerebral cortical area ( temporal , parietal and frontal cortices ), all cortical areas involved in the disease , of Alzheimer ’ s__label__Supplement|Website|Use
These data will be added to MPD in the future . MPD accepts data submissions including protocols and individual animal experimental data from studies of inbred strains , hybrids , mutants , and mapping populations . Information about how to contribute data is located on the MPD website ( _CITE_ ). By placing primary data in a single repository , users may conveniently access and compare results across studies . Large data submissions from major aging studies including the NIA Intervention Testing Program ( 40 ) and a collaborative project on the QTL analysis of age - related phenotypes centered at Penn State ( PI : G . McClearn ) along with CC and DO data from ongoing JAX NSC studies will enable investigators to place their data in the context of known variation and to test hypotheses about relationships among phenotypic variants across life span ( 41 ).__label__Material|Data|Produce
These data will be added to MPD in the future . MPD accepts data submissions including protocols and individual animal experimental data from studies of inbred strains , hybrids , mutants , and mapping populations . Information about how to contribute data is located on the MPD website ( _CITE_ ). By placing primary data in a single repository , users may conveniently access and compare results across studies . Large data submissions from major aging studies including the NIA Intervention Testing Program ( 40 ) and a collaborative project on the QTL analysis of age - related phenotypes centered at Penn State ( PI : G . McClearn ) along with CC and DO data from ongoing JAX NSC studies will enable investigators to place their data in the context of known variation and to test hypotheses about relationships among phenotypic variants across life span ( 41 ).__label__Supplement|Website|Produce
These data will be added to MPD in the future . MPD accepts data submissions including protocols and individual animal experimental data from studies of inbred strains , hybrids , mutants , and mapping populations . Information about how to contribute data is located on the MPD website ( _CITE_ ). By placing primary data in a single repository , users may conveniently access and compare results across studies . Large data submissions from major aging studies including the NIA Intervention Testing Program ( 40 ) and a collaborative project on the QTL analysis of age - related phenotypes centered at Penn State ( PI : G . McClearn ) along with CC and DO data from ongoing JAX NSC studies will enable investigators to place their data in the context of known variation and to test hypotheses about relationships among phenotypic variants across life span ( 41 ).__label__Supplement|Document|Produce
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : All relevant data are within the paper and its Supporting Information files . Matlab scripts are available via Zenodo ( _CITE_ ). Funding : The study was funded by the following__label__Method|Code|Produce
We downloaded the Panicum virgatum AP13 genome reference from Phytozome ( _CITE_ ). For all analyses , we utilized the genomic assembly hardmasked for repetitive sequence , to ensure spurious alignments to repetitive regions did not affect results . We also downloaded from the NCBI Sequence Read Archive whole genome shotgun reads from AP13 , from accession numbers SRX109496 , SRX109498 , SRX109499 , SRX109501 , SRX109503 , SRX109505 , SRX110233 and SRX110234 .__label__Material|Data|Use
For example , age groups can be defined on - the - fly versus being hard coded and user - defined syndromes can be accommodated with a minimum of effort . A scheduled periodical scheduled job improves query performance for common elements by generating aggregated tables that contain a smaller number of stratifying elements ( jurisdiction ID , ED visit date ) and synchronizes the master database with the two slave databases to improve performance and support scaling . The system is implemented as a series of integrated virtual machines ( VM ) on a KVM framework [ _CITE_ The complete national implementation is distributed among 3 VMs hosted on a single commercial - grade server ( 16 cores , 32 GB , 400 GB RAID 10 ). Additional server hardware in alternate locations is available to support high reliability .__label__Method|Tool|Use
We found that , although eat - 2 mutant worms start laying eggs later than wild type , the time difference between the start of egg laying of eat - 2 ISO and HD worms , relative to total time to maturity , is similar to that of wildtype ( Fig 1E , S4 Table ). Pdda was also unchanged in egl - 4 mutant worms , which are egg laying - defective due to egg retention [ 14 ] ( Fig 1E , S4 Table ), supporting that Pdda is unrelated to egg laying defects or inappropriate egg retention . In addition , Pdda was not affected by mutations of the NADH dependent histone deacetylase SIR - 2 . 1 , which may play a role for some aspects of dietary restriction - mediated lifespan changes [ 3 ] and is required for ascaroside - mediated increased lifespan and stress resistance [ 17 ], the nuclear receptor NHR - 49 , which is required for entering the starvation - induced adult reproductive diapause ( ARD , [ 18 ]), and the transcription factor heat shock factor - 1 ( HSF - 1 ), a general PLOS Genetics | _CITE_ April 10 , 2017 4 / 21 Larval crowding accelerates C . elegans development and reduces lifespan Fig 2 . Pdda is counteracted by ascaroside signaling . A ) Pdda is increased in peroxisomal β - oxidation mutants compared to wildtype .__label__Supplement|Paper|Introduce
Although the topic of when and how to version data is of great interest , use cases vary and consensus is elusive . Other groups have discussed change management consideration and “ content drift ” in more depth [ 2 , 30 , 31 ]. PLOS Biology | _CITE_ June 29 , 2017 12 / 18 Lesson 7 . Do not reassign or delete identifiers Identifiers that you have exposed publicly , whether as http URIs or via APIs , may be deprecated but must never be deleted or reassigned to another record . If you issue identifiers , consider their full life cycle : there is a fundamental difference between identifiers which point to experimental datasets ( GenBank / ENA / DDBJ , PRIDE , etc .)__label__Supplement|Paper|Compare
The cells were sonicated , and an equal volume of absolute ethanol was added to fix the cells , which were stored at 4 ° C overnight ( or longer ). Fixed cells were washed twice in sterile nuclease - free water before processing for FACS or budding index . Samples for budding index analysis were resuspended in 1 mL of sterile nuclease - free water and stained with 2 µg / mL final concentration of Hoechst 33342 ( Invitrogen , _CITE_ ) for 2 h . Cells were then briefly sonicated , washed once in sterile nuclease - free water , and wet - mounted on slides for observation on a fluorescence microscope as described below except that single focus images were acquired . After acquisition , image files in TIFF format were automatically converted to Omero files and uploaded to an image storage server using Omero Insight ’ s importer . 60 We used composite images of the phase - contrast and Hoechst channels to score unbudded G1 cells , small - budded S - phase cells in which the bud is < 1 / 3 the size of the mother , large - budded G2 / M cells with a single aggregate of DNA ( nucleus ), and largebudded anaphase / telophase cells with an elongated nucleus in the mother - bud neck or with mother and bud both containing a single nucleus , respectively . Samples for FACS analysis of DNA content were sonicated , pelleted , and resuspended in 1 mL of RNase A ( Sigma - Aldrich ) solution ( 50 mM Tris - CL , pH 8 . 0 , 15 mM NaCl , 2 mg / mL RNase A ), and incubated at 37 ° C for 4 h . 61 After incubation , proteinase K ( New England Biolabs ) was added directly to the solution to a final concentration of 200 µg / mL , and the tube was incubated at 37 ° C for a further 1 h . Cells were then centrifuged , resuspended in 50 mM Tris - Cl , pH 8 . 0 , and stained for immediate FACS analysis or stored at 4 ° C .__label__Supplement|Website|Use
Each line corresponds to minimum 50 % mutual overlap with line thickness corresponding to degree of overlap . Cellular processes for gene set clusters were manually curated . Candidate point mutations in RNA - Seq data sets were called using a pipeline based on the GATK Toolkit ( _CITE_ ). Transcriptomic reads were mapped ( to mm9 , hg19 ) using the Tophat ( v2 . 0 . 4 ) spliced aligner and subjected to local realignment and score recalibration using the GATK Toolkit . Mutations were called in KO samples ( individual and pooled ) against WT samples ( individual and pooled ) with a minimum base quality threshold of 30 .__label__Method|Tool|Use
From our experience , we advise using any nodes from the r3 family when running several jobs concurrently ( cohort ) as NFS issues may arise when using general - purpose nodes . If a few samples are to be run , generalpurpose nodes are recommended . Separately , we produced a Dockerfile that will install our tools and additional dependencies using the galaxy - stable Docker image ( _CITE_ ). This is available in our GitHub repository , which also hosts the individual tools ( https :// github . com / morinlab / tools - morinlab ). This Docker image was successfully built with automatic installation of tools and dependencies on our local Linux server and on the Google Cloud .__label__Method|Tool|Use
To support such analysis and capture these patterns comprehensively , data with high temporal and spatial resolution and the low signal - to - noise ratio is needed . Recent advances in invasive monitoring technologies such as electrocorticography ( ECoG ) have risen to this challenge by recording high - resolution electrical signals © The Regents of the University of California . 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . The Regents of the University of California BMCBioinformatics 2017 , 18 ( Suppl 6 ): 236 Page 2 of 45 captured by electrodes placed directly on the cortical surface of the brain .__label__Supplement|License|Other
Andreas Drakos , Dear George , thank you very much for your comments . The main concept of the agINFRA project was to re - use main infrastructure elements , connect with existing generic solutions and create something specified for the agricultural research domain . While in this article we tried to summarize the work of agINFRA , the specific aspects of each element has been presented in the numerous publications made from the partners ( they can all be found at _CITE_ ). Page 10 of 11 F1000Research 2015 , 4 : 127 Last updated : 21 AUG 2015 The agINFRA vision was to improve research collaboration and data exchange , but during the lifetime of the project , this had to be limited in creating the necessary building blocks ( e - infrastructure ) for services to built upon ( as for example the new AGRIS ). Hopefully this effort will continue and in the future we will be able to present a number of different end - user services specialized for the agricultural research community .__label__Supplement|Document|Introduce
Among many nonparametric approaches that can be used to estimate the impulse response function ( IRF ) in each region i , IRFi ( t ) = KiRi ( t ), from known CP ( t ) and CTi ( t ), we proposed using singular value decomposition ( SVD ) with data - driven selection of the threshold that we described elsewhere [ 26 ]. Hybrid deconvolution approach In the context of PET reversible radiotracers , Ri ( t ) can be interpreted as the fraction of tracer molecules remaining in the tissue over time , and these molecules can be specifically bound to the target , free in water or bound to other molecules . HYDECA decomposes each region Ri ( t ) into the sum of a parametric non - displaceable component , which is approximated as a monoexponential function depending on VND , assumed common across regions ( see details below and comments on the validity of this approximation in the Discussion ), and a nonparametric PLOS ONE | _CITE_ May 1 , 2017 3 / 29 Estimating PET binding without a reference region specific component . For any choice of VND defining the non - displaceable component , the nonparametric specific component can be estimated by subtraction . Performing such a decomposition for observed PET data can be challenging , but the goal of HYDECA is to objectively ascertain a “ reasonable ” VND value by examining data across regions .__label__Supplement|Paper|Introduce
from the DrugBank website ( _CITE_ ) [ 21 ]. After identifying homologous genes to the target sequences using BLASTP [ 22 ], we used a custom python script to parse the results for input into Matlab (“ listPossibleTargets . py ”, available in repository ). Linear gap filling We implemented a linear ( as opposed to binary ) gap filling algorithm in Matlab , based on the algorithms FASTGAPFILL and FastGapFilling [ 17 , 18 ].__label__Supplement|Website|Use
Data are however available from the authors upon reasonable request and with permission of the data providers . The deprivation datasets for Ghana are in the 2010 Population and housing census analytical reports available online for download , whilst names of healthcare facilities were provided with permission by Ghana Health Services . The District League Table data for Ghana are available for download from UNICEF Ghana and the Ghana Centre for Democratic Development : _CITE_ 2006 census data for Nigeria were acquired under licence from the National Population Commission , whilst school names are available at https :// 9jaskulnews . blogspot . co . uk .__label__Material|Data|Use
Data are however available from the authors upon reasonable request and with permission of the data providers . The deprivation datasets for Ghana are in the 2010 Population and housing census analytical reports available online for download , whilst names of healthcare facilities were provided with permission by Ghana Health Services . The District League Table data for Ghana are available for download from UNICEF Ghana and the Ghana Centre for Democratic Development : _CITE_ 2006 census data for Nigeria were acquired under licence from the National Population Commission , whilst school names are available at https :// 9jaskulnews . blogspot . co . uk .__label__Supplement|Website|Use
1We developed the modules using Articulate Storyline eLearning software and delivered them over the same company ’ s learning management system ( Articulate Global Inc ., New York , NY ; _CITE_ ). 2Much of the work on digital badges has been advanced by the Macarthur Foundation , and more information on digital badges can be found on their website ( https :// www . macfound . org / programs / digital - badges /). Additional files Additional file 1 : Alpa test user log .__label__Method|Tool|Use
The same has not been accomplished for habitats ; however , the components required to accomplish this are falling into place . For example , the LifeMapper ( Prajapati , 2009 ) and Map of Life ( Jetz , McPherson & Guralnick , 2012 ) projects use ecological niche modeling to map species distributions based on environmental conditions . Additionally , the Encyclopedia of Life TraitBank ( _CITE_ ) links taxa to their habitat type and phenotypic traits , but not to geographic coordinates ( Parr et al ., 2015 ). Once greater ontological representation of the link between species and their environments is accomplished , robustly linking species ’ phenotypes to their environments and locations become readily achievable . In addition to spatial variation , environments show considerable variation over time and often change over daily and seasonal cycles .__label__Supplement|Website|Introduce
Finally , the outcome of capsule transfer is likely to depend on the environmental challenges faced by the bacteria and will be affected by the abovementioned epistatic interactions . The capsule increases environmental breadth A substantial part of the previous literature on capsule systems has focused on bacterial pathogens and on the role of capsules as virulence factors . For instance , it has been shown that PLOS Pathogens | _CITE_ July24 , 2017 14 / 30 Capsule abundance and co - occurrence among prokaryotes acquisition of certain capsule types by horizontal gene transfer in Neisseria meningitidis allowed the bacteria to increase in pathogenicity and going from non - pathogenic carriage to infectious state [ 52 , 53 ]. It was thus surprising that non - pathogens are more likely to encode capsules , and that , among pathogens , the ones establishing obligatory antagonistic interactions with their hosts typically lacked a capsule . The abundance of capsules across most phyla and environmental classes , and their rarity among obligatory pathogens , suggest they play important roles beyond pathogenesis .__label__Supplement|Paper|Introduce
The individual sinusoidal components of the periodic signal were chosen to be characteristic of the time series as a whole , with periods determined using a Fourier transform of the median - filled time series . A linear model of the form N Aisin / 27r t � þ B ; cos C27r t � þ ε Ey 1 / 4 I \ Pi Pi i1 / 41 was fit to the data surrounding each gap , where y is hourly fish count , Pi is the ith period of the above set , and Ai and Bi are amplitudes obtained by minimizing error ε . Each model was fit using a subset of the time series that was within 1 gap - width or 48 hr to either side of a gap , PLOS ONE | _CITE_ May 11 , 2017 7 / 20 Patterns in fish presence in high - velocity tidal channel whichever was longer . Short gaps did not benefit from the use of comparably long periods , so the longest period included in each gap model was limited to twice the length of subset used for fitting . Once the models were fit to data surrounding each gap , they were used to predict missing values .__label__Supplement|Paper|Introduce
Bis - seq amplicons for validation and mapping of the DMR are indicated by the numbered rectangles . The bis - seq data ( bottom panel ) is visualized by the circles representing consecutive CpGs with black circles indicating methylated CpGs and white circles unmethylated CpGs , with each line being a unique DNA clone . doi : 10 . 1371 / journal . pone . 0170859 . g001 broadinstitute . org / gsea ) [ 28 ], [ 29 ] and with gene ontology annotation using DAVID ( _CITE_ ) [ 35 , 36 ]. By these unbiased approaches ( Methods ), the GOEA was more informative than GSEA , revealing sets of DM regulatory regions , which are localized in multiple genes encoding TFs and signaling components , and showing significant enrichments in genes that control cell proliferation , nervous system development , and immunity ( S4 Table ).__label__Method|Tool|Use
An example of this would be House 10 , which moved IAM 2 from a Freezer to a Toaster on 25 / 06 / 2014 . Energy aggregator The EnviR aggregator with an IHD came bundled with the CurrentCost transmitter used for measuring the household aggregate . The EnviR ( _CITE_ html ) ties all of the CurrentCost devices together acting an an energy aggregator . Its display provides information about all of the CurrentCost devices which were installed with a simple interface using buttons as navigation .__label__Method|Tool|Introduce
A comparison of smartphones to paperbased questionnaires for routine influenza sentinel surveillance , Kenya , 2011 - 2012 . BMC Med Inform Decis Mak BioMed Central2014 ; [ cited 2016 Aug 9 ] 14 : 107 . Available from : _CITE_ 22 Jamison RN , Raymond SA , Slawsby EA , McHugo GJ , Baird JC . Pain assessment in patients with low back pain : comparison of weekly recall and momentary electronic data . J Pain 2006 ; 7 : 192 – 9 .__label__Supplement|Paper|Compare
c The multichannel topological convolutional neural network architecture with topological features from “ S223 ” data set . dThe multi - task multichannel topological convolutional neural network ( MM - TCNN ) architecture trained with an auxiliary task of globular protein prediction using the “ S2648 ” data set . _CITE_ performance in RP ( RMSE ) significantly decreases from 0 . 81 ( 0 . 94 ) to 0 . 77 ( 1 . 02 ) for the task of “ S350 ” set prediction in the mutation impact example . This shows that the construction of lower level features in the lower sparse layers benefits from sharing filters along the distance scale and indicates the existence of some common rules for feature extractions at different distance scales . Intuitively , the dimension 0 inputs describe pairwise atomic interactions , which clearly contribute to the prediction of the target properties .__label__Material|Data|Use
Hybridization was performed as described . 42 Briefly , cells were first fixed by incubation in 4 % ( v / v ) paraformaldehyde ( Electron Microscopy Sciences , http :// www . emsdiasum . com / microscopy / products / chemicals / formaldehyde . aspx ) for 45 min . After washing 3 times in Buffer B ( 1 . 2 M Sorbitol , 100 mM KHPO4 , pH 7 . 5 ), cells were resuspended in spheroplast buffer ( 1 . 2 M Sorbitol , 100 mM KHPO4 pH 7 . 5 ), 20 mM Ribonucleoside - Vanadyl complex ( New England Biolabs , _CITE_ ), 20 mM β - mercaptoethanol , and 25 U lyticase ( Sigma - Aldrich , http :// www . sigmaaldrich . com / catalog / product / sigma / l2524 ), and incubated at 30 ° C for 7 min to partially digest the cell walls . Cells were placed on poly - L - lysine ( Sigma - Aldrich ) coated coverslips and allowed to settle for 30 min at 4 ° C . The coverslips were washed once with Buffer B to remove unattached cells and stored overnight in 70 % ethanol .__label__Supplement|Website|Use
The following six benchmark microarray data sets have been extensively studied and used in our experiments to compare the performances of our methods with others . Data sources that are not specified are available at : _CITE_ 1 ) The LEUKEMIA data set consists of two types of acute leukemia : 48 acute lymphoblastic leukemia ( ALL ) samples and 25 acute myeloblastic leukemia ( AML ) samples with over 7129 probes from 6817 human genes . It was studied by Golub et al .__label__Material|Data|Produce
Annotations of lncRNAs and protein - coding genes were obtained from GENCODE v24 ( _CITE_ ), and only the longest transcript was selected for each gene with several transcripts . As the TE annotations , we used the mapping result of Repeat Library 20140131 to hg38 , which is published by RepeatMasker [ 20 ], after excluding simple repeats , low - complexity , non - coding RNA , and satellites . Overlaps between lncRNAs and TEs were detected using our own Perl script .__label__Material|Data|Use
Population structure analysis . To explore their phylogenetic relationship , the whole - genome autosomal SNPs were extracted to construct the phylogenetic tree , and genotypes of sheep sequence were used to provide out - group information at corresponding positions . The neighbor - joining tree was constructed using the PHYLIP 3 . 696 software ( _CITE_ ) based on distance matrix methods55 . iTOL ( http :// itol . embl . de ) was used to illuminate and visualize the phylogenetic tree56 . For both of principal component ( PCA ) and population structure analysis , a thined SNPs dataset with a window of size 50 SNPs advanced by 5 SNPs at a time and an linkage - disequilibrium r2 threshold of 0 . 5 were filtered__label__Method|Tool|Use
Third , the expressions in the ETL rules must be in the language of the local DBMS . For rules to be used and reused across different DBMSs , a rule conversion tool that automatically translates operators and functions from one SQL dialect into another is needed . Open source tools , such as SQL Renderer from the OHDSI community , _CITE_ could be a potential solution to this problem . Finally , even though rules are composed in plain text format , a graphical presentation of the structure of the rule will improve ETL rule maintenance and help ETL team members understand complex rules created by others . Data harmonization is an important step towards data interoperability which supports the progression of comparative effectiveness research .__label__Method|Tool|Use
Mapping Brain Networks Using EEG toolbox . Accordingly , all EEG datasets later used for source localization had the same number of signals . Subsequently , we band - pass filtered the EEG data in the frequency range 1 – 80 Hz and we decomposed them into ICs by using the fast fixed - point ICA ( FastICA ) algorithm _CITE_ , to identify and remove artifacts of biological origin ( Mantini et al ., 2008 ). Artifactual ICs were automatically identified by using information from the signal kurtosis , the power spectrum and the correlation with horizontal and vertical electrooculogram ( hEOG and vEOG ) and electromyogram ( EMG ). Finally , we re - referenced the cleaned EEG signals using the average reference approach ( Liu et al ., 2015 ).__label__Method|Algorithm|Use
ISA - Tab format is currently in use and supported by public data repositories such EMBL - EBI Metabolights ( accounting for about 200 datasets , 90 of which are currently publicly available ), but also several major european toxicogenomics projects ( Carcinogenomics , DiXa and InnoMed PredTox and ToxBank ). These projects fully exploit the capability of the ISA - Tab format to support an array of assay type allowing to recording multi - omics assays . Furthermore , ISA developers have a range of tools for converting from various sources ( ArrayExpress , SRA _CITE_ ) into ISA - Tab format . Ultimately , we hope that COSMOS will help experts in NMR spectroscopy and MS - based metabolomics to communicate their results in a more objective comprehensive , persistent and efficient way , and spanning and integrating multiple domains such as medical , environmental , plant and food sciences . Although funding by the European Community is limited to a number of European expert scientists , COSMOS links to major initiatives world - wide .__label__Material|Data|Extent
To evaluate the influence of Td and to find the optimal threshold for each degree metric , repeated measures ANOVA followed by the Bonferroni multiple comparison test were carried out among each degree metric under different Td . To evaluate the influence of region - size impact on test – retest reliability , paired two - sample t - tests were carried out between the ICC of the traditional and modified degree metrics . To evaluate the influence of region - size impact on degree metrics , paired twosample t - tests were carried out between each type of traditional and modified degree metric , and the results were visualized with the BrainNet Viewer _CITE_ . To investigate the performance of traditional degree metrics and modified degree metrics in statistical group comparisons , twosample t - tests were carried out between the degree metrics of the In this study , we proposed a data - driven method to reduce the influence of functional region sizes on degree metrics . A simple example can explain how our method works ( Figure 1 ).__label__Method|Tool|Use
The final integrated network has 16 , 435 nodes , 175 , 841 edges and 17 connected components , with a high average diameter ( 9 ) and low clustering coefficient ( 0 . 289 ). The average degree is 21 . 398 and the degree distribution follows a power law ( Figure 2 ). Gene expression data for various tissues and tumors were downloaded from ArrayExpress , GEO , ProteinAtlas _CITE_ , and TCGA ( see text footnote 1 ). Table 2 lists the gene expression datasets integrated within SPECTRA , the platform used to detect the expressions and the number of covered tissues and tumors . Figure 3 depicts a Venn diagram of common tissues and tumors across expression datasets .__label__Material|Data|Use
The operator was visually cued to start and stop the movements in synchrony with the acquisitions but did not receive any other prompts such as prompts for the frequency of flexing . The data sets were individually processed for each volunteer . The analysis pipeline was automated using a custom - written toolbox MRIST ( MR Imaging and Spectroscopy Toolbox ), which combines tools mainly from SPM12b , FSL 5 . 0 . 1 _CITE_ , MATLAB ( R2012a , 1984 – 2012 The Math Works , Inc .) and BASH into a Debian Linux “ squeeze ” version 6 . 0 . 7 command line based data analysis pipeline . Pre - processing was performed with SPM12b and MATLAB after DICOM to NIfTI image file format conversion . Functional EP images were corrected for susceptibility - related distortion based on a voxel displacement map ( in the phase encoding direction ) determined from the B0 fieldmap .__label__Method|Tool|Use
This approach is commonly applied in geographical studies of epidemiology for the relative risk estimation using penalized log - likelihood maximization . The normalized difference vegetation index ( NDVI ) and the normalized difference water index ( NDWI ) were calculated from Terra - MODIS 8 - day composite data from 2010 to 2013 and averaged to the mean values for each year . A digital elevation model at 30 - m resolution was extracted from the ASTER GDEM database _CITE_ ( 46 ) and used to calculate the topographic wetness index ( TWI ). Based on findings from our previous study ( 36 ), we extracted data from surrounding circular buffers within a 5 - km radius diameter for the NDVI and the TWI , and within a 1 - km radius diameter from villages for the NDWI , where the strongest correlation between each variable and the EBSMR was observed within the studied range of distance . Extracted data were aggregated and averaged by the number of villages at the district level to reflect the overall condition of target districts .__label__Material|Data|Use
Journal of Biomedical Semantics ( 2017 ) 8 : 42 Page 6 of 20 advanced queries and view the results of their execution , while the query engine executes queries across remote SPARQL endpoints . PIBAS FedSPARQL was implemented in PHP and Python . The JQuery library _CITE_ was used to develop an interactive and user - friendly interface , while sparqllib was used to run Federated SPARQL queries . The list of available datasets used for creating predefined Federated SPARQL queries is placed in the local DataSources ontology [ 49 ] developed using Protégé 4 . 0 . 2 [ 50 ]. The user query interface was implemented in HTML , JQuery and JavaScript .__label__Method|Code|Use
It uses a rules - based scheme with sharp cut - offs to classify miRNAs based on five criteria : the lack of base pairing in the mature miRNA , the difference in length between the two candidate miRNA strands , the fraction of base - paired nucleotides in the hairpin , and two measures of energetic stability . As a second filtering step , it considers only hairpins where the sequenced RNAs map in consistence with Drosha / Dicer processing . MIReNA can consider several potential miRNA duplexes within one precursor structure , e . g ., within multiple stem precursors , giving it the potential to predict non - canonical miRNAs _CITE_ . miREvo build on the miRDeep2 predictor ( above ) but extends it for evolutionary analyses ( Wen et al ., 2012 ). Specifically , it uses whole - genome alignments to identify miRNA homologs in related species .__label__Material|Data|Use
In November 2012 Dr . Guido Rasi , the current executive director of the European Medicines Agency ( EMA ), announced that “... we are not here to decide if we publish clinical - trial data , but how !” at an EMA workshop on clinical trial data and transparency . Furthermore , he stated that “ we need to do this in order to rebuild trust and confidence in the whole system ”. This meeting can be considered as a milestone in EMA ’ s activities _CITE_ on transparency and release of clinical trial data . In principle any EU citizen can request any document s / he is interested in from any EU institution ( see Article 255 of the treaty establishing the European Community ), including from the EMA . However , in the past the EMA was heavily criticized on several occasions for not releasing ( sufficient ) information after external requests .__label__Supplement|Website|Introduce
GMD uses both alkanes and fatty acid methyl esters ( FAMEs ) for RI calculation whereas FiehnLib ( Kind et al ., 2009 ), a commercial MSRI library , uses FAMEs rather than alkanes . The Spectral Database for Organic Compounds ( SDBS ) includes a wide range of mass spectra for organic compounds , such as polysaccharides . MassBase _CITE_ is a mass spectral archive for LC -, GC -, and Capillary electrophoresis – MS ( CE - MS ). SetupX and BinBase are a Laboratory Information Management System ( LIMS )/ database system for automated metabolite annotation and mass spectra , respectively . The Adams library ( Adams , 2007 ), Terpenoids Library , and VocBinBase ( Skogerson et al ., 2011 ) are GC - specific MSRI libraries for volatile compounds .__label__Material|Data|Introduce
Indels were not included in this model . While human data was used to train the error model , patterns of sequencing errors are largely based on sequencing platform and are likely to be similar between experimental systems . We converted BAM alignment files generated by SimSeq into SAM format using SAMtools ( Li et al ., 2009a ), and ultimately into FastQ short - read libraries using the BAMtoFastQ program in the Picard Tools ( v1 . 48 ) package _CITE_ . Paired - end short - read libraries were aligned to the reference sequence generated above using BWA ( Li and Durbin , 2009 ) allowing for an edit distance of 4 between each read and the reference , except for samples from the population structure models which we mapped with an edit distance of 5 to accommodate the higher number of SNPs in these samples . Reads with duplicate mapping positions were removed with the rmdup function in SAMtools ( Li et al ., 2009a ).__label__Method|Code|Use
The data consisted of 10 classifications ( the environments ), 27 response variables ( the functional metabolic groups ), and 212 observations ( the metagenomes ). As the number of publicly available metagenomes increases the number of metabolic groups could be increased . We compared the outcome of the seven statistical analysis with the detailed methods are discussed below , and further discussion and source code for all of these operations are provided in the online accompanying material _CITE_ . A brief summary of each method is given in the results . K - means clustering is an unsupervised method which aims to classify observations into K groups , for a choice of K . This approach partitions observations into clusters in order to minimize the sum of squared distances from each observation to the mean of its assigned group .__label__Supplement|Document|Produce
However , Google provides researchers with a platform to execute sample API calls on the publicly available 1000 Genomes dataset through the Google Developer Console . Complete details on the API and its implementation can be obtained through the Google Genomics website . _CITE_ An example request response workflow using Google Genomics is shown below in Fig . 2 . The Google Genomics API has methods that can work with both read and variant data as input .__label__Supplement|Website|Use
Briefly , total RNA was extracted from the 18 - day - old aerial part of each mutant and WT sample using the RNeasy plant mini kit ( Qiagen ) according to the manufacturer ’ s instructions . Three independent hybridizations were performed using the Affymetrix ATH1 GeneChip microarray , according to the manufacturer ’ s instructions ( Affymetrix ). _CITE_ A single biological replicate was used for each hybridization . Preprocessing and normalization / summarization of all CEL files were performed using R , the Bioconductor ( Gentleman et al ., 2004 ), and a robust multi - chip average ( RMA ) ( Bolstad et al ., 2003 ; Irizarry et al ., 2003 ). The quality of the GeneChip data was assessed using the AFFYPLM package ( Bolstad et al ., 2005 ).__label__Method|Tool|Use
The Web provides access to large - scale sets of interlinked data from heterogeneous scientific domains , and – in particular for the life science researchers – develops into a source of reference data from scientific experiments [ 1 ]. The comprehensive set of Linked Open Data ( LOD ) covers over 60 billion triples provided by more than 1 ’ 000 different data sets . The small but important portion of the Linked Open Data cloud is composed of the Life Science Linked Open Data ( LS - LOD ), which results to 8 % ( 83 data sets ) of the overall LOD cloud _CITE_ . The life science data contributes significantly to the ongoing research in semantic Web technologies , since the life science research community gathers and exposes their expertise in form of high quality ontologies , which support innovative retrieval methods across distributed SPARQL endpoint engines as presented in this publication . Significant contributions in terms of data integration and data provision have been made available from the Bio2RDF project , the Linked Life Data initiative , the Neurocommons group , through the Healthcare and Life Sciences knowledge base ( HCLS Kb ), from the Linked Cancer Genome Atlas ( Linked TCGA ) [ 2 , 3 ], and the W3C HCLSIG Linking Open Drug Data ( LODD ) initiative .__label__Supplement|Website|Introduce
Our work with digital interactive anatomy atlases dates more than 20 years ( Shenton et al ., 1995 ). We have produced several anatomy atlases consisting of source radiology images , segmented voxel maps labeling different anatomical structures , polygonal models of those structures , all arranged in a hierarchical organization . Examples of these atlases include an MRI - derived neuroanatomic atlas as well as knee , abdominal and head and neck _CITE_ atlases based on CT imaging . Until recently , however , these atlases have been developed in a closed process by a small number of people , with only the resulting data made available . Separate from the atlases themselves , we have also developed atlas viewing tools .__label__Supplement|Document|Produce
