Khaltar and Fujii ’ s method was also evaluated for comparison . We used Moses ( Koehn et al ., 2007 ) with the standard configuration and GIZA ++ ( Och et al ., 2003 ) with the grow - diag - final - and heuristic for word - alignment . Our parallel data set was collected from web sites ( _CITE_ and http :// mongolia . usembassy . gov /), and consists of law and news domains . Example En - Mn sentence pairs in our data are shown below . En1 : Occupational safety and health measures shall not involve any expenditure for the workers .__label__Material|Data|Use
Meanwhile , our method waives nearly all computational burdens of SS at test - time — the effective running time for proposals is just 10 milliseconds . Using the expensive very deep models of [ 19 ], our detection method still has a frame rate of 5fps ( including all steps ) on a GPU , and thus is a practical object detection system in terms of both speed and accuracy ( 73 . 2 % mAP on PASCAL VOC 2007 and 70 . 4 % mAP on 2012 ). Code is available at _CITE___label__Method|Code|Produce
We use a CRF ++ based POS tagger for Hi , which is freely available from _CITE_ For En , we use the Twitter POS tagger ( Owoputi et al ., 2013 ). It also has an inbuilt tokenizer and can work directly on unnormalized text .__label__Method|Tool|Use
Many of the commonly used tasks that otherwise require writing programs , can be performed with one or more queries . Overcoming the language barrier in the Indian sub - continent is a very challenging task . Sampark _CITE_ is an effort in this direction . Sampark has been developed as part of the consortium project called Indian Language to India Language Machine translation ( ILMT ) funded by TDIL program of Department of Information Technology , Government of India . Work on this project is contributed to by 11 major research centres across India working on Natural Language Processing .__label__Method|Tool|Introduce
Besides the common features used in traditional Named Entity Recognition ( NER ) systems , we also utilize extensive external resources to build various name lists and word clusters . Following the traditional BIO scheme used in sequential labeling , we assign a label for each word in the sentence , where “ B - TERM ” indicates the start of an aspect term , “ I - TERM ” indicates the continuation of an aspect term , and “ O ” indicates not an aspect term . All sentences are tokenized and parsed using the Stanford Parser _CITE_ . The parsing information is used to extract various syntactic features ( e . g . POS , head word , dependency relation ) described in the next section .__label__Method|Tool|Use
As additional evaluations , we also tested our model on the SUPPORT2 and RHC datasets ( available at _CITE_ ), which record the survival time for patients hospitalized with severe illnesses . SUPPORT2 contains over 9000 patients ( 32 % censored ) while RHC contains over 5000 patients ( 35 % censored ). Table 4 ( top ) shows the MSE on survival probability prediction over the SUPPORT2 dataset and RHC dataset ( we omit classification accuracy due to lack of space ).__label__Material|Data|Use
Sujan Perera , Cartic Ramakrishnan , and Meena Nagarajan were employed by IBM at the time this research was completed . Ethics statement All human subject data used in this analysis were publicly available and used in a de - identified format whenever possible . Open Access This article is distributed under the terms of the Creative Commons Attribution - NonCommercial 4 . 0 International License ( _CITE_ ), which permits any noncommercial use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made .__label__Supplement|License|Other
Naturally , this is only an overall view of the results obtained . The new extended WordNet . PT version is also a crucial resource allowing for contrastive studies on lexicalization patterns depending on semantic domains or on frequency of use , for instance , for all or for specific Portuguese varieties . In order to make these data publicly available , a new WordNet . PT version , the WordNet . PTglobal has been released on the WWW _CITE_ . Releasing the WordNet . PT fragment extended to Portuguese varieties online involved developing an updated version of the web interface for wordnet online navigation . In Section 3 we present the main features of this web interface and how users can navigate and straightforwardly access the data on Portuguese varieties .__label__Method|Tool|Produce
2 , where the solid line represents training process and the dotted line represents testing process . The Korean analyzer at the center of the figure takes Korean texts as an input and generates several raw features as an output , such as results of morphological analysis , Part - Of - Speech ( POS ) tags , Named - Entity ( NE ) tags , and results of dependency parsing ( Lim et al ., 2006 ). The number of possible POS tags is 45 , which follows the definition of Sejong Treebank _CITE_ . The number of possible NE tags is 178 , where each of them belongs to one of 15 super NE tags . The generated raw features are used to define a set of features for machine - learning models and a set of hand - crafted rules .__label__Supplement|Document|Use
The objective of TrgCmb is to maximize Ot defined in equation ( 6 ). And the constraints of TrgCmb are defined by equations ( 7 - 9 ). In this paper , we employ lpsolve _CITE_ to solve all ILP models . In our experiments , we use the Xinhua News portion of Chinese and English data in LDC OntoNotes Release 3 . 0 . This data is a Chinese - English parallel proposition bank described in ( Palmer et al ., 2005 ).__label__Method|Tool|Use
The small interdisciplinary teams that formed were encouraged to apply for NSF or NIH funding , e . g ., through the joint NSF - NIH Quantitative Biomedical Big Data program . Many of these teams successfully competed for funding , with success rates being much higher for lab participants than the general pool in the first year . PLOS Biology | _CITE_ July 17 , 2017 6 / 9 Impartial evaluation by a review panel coupled with positive feedback from participants testifies to the success of the innovation lab . Innovation labs are designed to bring together junior investigators from the biomedical and data sciences . To reach more senior data scientists , other tactics are needed .__label__Supplement|Paper|Compare
I don ' t imagine the use cases of these type of scientists ' behaviour in other countries are that different , but I would expect some pointers to this wider context . The referenced literature is good and covers many of the key sources I would refer to . However my own organisation in the UK has been advising on data documentation , including use of Excel and conversion issues , for some years , so it would be good to cite some examples of other efforts to address these issues on the non - ecology field and offer examples of non US resources that provide extensive data management advice ( _CITE_ ). On page 6 the checklist of issues is very clear and useful and great to alert researchers to these issues upfront . In terms of platforms for the tool , I think a Mac version will be important .__label__Supplement|Document|Produce
The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in a credit line to the material . If material is not included in the article ’ s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use , you will need to obtain permission directly from the copyright holder . To view a copy of this license , visit _CITE_ © The Author ( s ) 2018__label__Supplement|License|Other
Thus , this feature serves to constrain the Arg1 search space for intra - sentential argument span extraction . The value of the feature is either ARG2 suffixed for whether a token is Inside ( I ), Begin ( B ), or End ( E ) of the span , or ‘ O ’ if it does not belong to the Arg2 span . These features are expanded during training with n - grams ( feature of CRF ++ _CITE_ ): tokens with 2 - grams in the window of + 1 tokens , and the rest of the features with 2 & 3 - grams in the window of The in - domain performance of argument span extraction models is provided in the following section , after the description of the evaluation methodology . In this Section we first describe the evaluation methodology and then the experiments on crossdomain evaluation of argument position classification and argument span extraction models . The experimental settings for PDTB are the following : Sections 02 - 22 are used for training and Sections 23 - 24 for testing .__label__Method|Tool|Use
Assembly starts from aligned DNA - Seq reads to reconstruct the original DNA sequence computationally , which generates large , continuous regions of DNA sequence [ 3 ]. Many alignment software provide tools to perform the assembly after the read alignment ( e . g ., MAQ ), or standalone resources can be used ( SAMTOOLS [ 13 ], Emboss [ 14 ]) or commercial packages like Geneious ( _CITE_ ) and CLC - Bio ( http :// www . clcbio . com ). For organisms without a sequenced reference genome , it is not possible to perform any reference genome guided assembly of the reads , thus de novo assembly is always an essential step for data analysis . The majority of de novo assemblers that have been released follow two basic approaches : overlap graphs [ 15 ] and de Bruijn graphs [ 16 ].__label__Method|Tool|Use
As shown in Figure ( 1 . a ), we select the initial ML ( Mainland China ) vocabulary ( about 50 , 000 words ) and HK ( Hong Kong ) ML or TW ( Taiwan )- ML parallel news website as our data source . The preprocessing phase illustrated in Figure 1 includes sentence boundary detection , word segmentation , part - of - speech and name entity recognition ( the name of people , or the name of locations , or the name of organizations ). In specific , we firstly adopt the jsoup _CITE_ utility to iteratively crawl the parallel texts written in simplified script for Chinese Mainland and traditional script for Hong Kong and Taiwan from the Wikipedia . Secondly , we take punctuations of & quot ;.& quot ; or & quot ;!& quot ; or & quot ;?& quot ; or & quot ;;& quot ; as the sentence boundary , and employ ICTCLAS and Ikanalyzer to generate word segmentatio and part - of - speech and name entity identification for the sentence . Then , we generate parallel sentence pairs written in simplified script for Chinese Mainland and sentences written in traditional script for Hong Kong and Taiwan , respectively .__label__Method|Tool|Use
Open Access This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution ( CC BY 4 . 0 ) license , which permits others to distribute , remix , adapt and build upon this work , for commercial use , provided the original work is properly cited . See : _CITE___label__Supplement|License|Other
Figure 1 ( c ) plots the convergence of objective value against the runtime when n = 4000 . BADMM converges faster than ADMM even when the initial point is further from the optimum . BADMM vs Gurobi : Gurobi ( _CITE_ ) is a highly optimized commercial software where linear programming solvers have been efficiently implemented . We run Gurobi on two settings : a Mac laptop with 8G memory and a server with 86G memory , respectively . For comparison , BADMM is run in parallel on a Tesla M2070 GPU with 5G memory and 448 cores1 .__label__Method|Tool|Compare
We use 5 - fold cross validation to determine the optimal A1 and A2 whose candidate values are chosen from n × { 0 . 01 , 0 . 1 , 0 . 5 , 1 , 5 , 10 , 100 } and the optimal number of nearest neighbors from { 5 , 10 , 15 , 20 }. The classification error is used as the performance measure . We compare our method , which is denoted as MT - KNN , with the KNN classifier which is a single - task learning method , the multi - task large margin nearest neighbor ( mtLMNN ) method [ 14 ] _CITE_ which is a multi - task local learning method based on the homogeneous neighborhood , and the multi - task feature learning ( MTFL ) method [ 2 ] which is a global method for multi - task learning . By utilizing hinge and square losses , we also consider two variants of our MT - KNN method . To mimic the real - world situation where the training data are usually limited , we randomly select 20 % of the whole data as training data and the rest to form the test set .__label__Method|Algorithm|Compare
In this paper we studied the application of compressionbased distance measures for the problem of sequence comparison , with a special focus on NGS short read data . Their key advantages are assembly - free , alignment - free , and parameter - free . We conducted extensive validation on various types of sequence data : NGS short reads , 16S rRNA sequences , mtDNA sequences , and whole genome Tran and Chen BMC Research Notes 2014 , 7 : 320 Page 12 of 13 _CITE_ sequences . The sequence data was obtained from several mammalian and bacteria genomes at different taxonomy levels , as well as from microbial metagenomic samples . The results show that the compression - based distance measures produced comparably accurate results as the kmer based methods , and both were in good agreement with the alignment - based approach and with existing benchmarks in the literature .__label__Material|Data|Use
We assume that we have already completed 50 iterations of an optimization of the same model on the related USPS digits task . The USPS data is only 1 / 6 the size of MNIST and each image contains 16 x 16 pixels , so it is considerably cheaper to evaluate . Convolutional neural networks on pixels We applied convolutional neural networks _CITE_ ( CNNs ) to the Street View House Numbers ( SVHN ) [ 21 ] dataset and bootstrapped from a previous run of Bayesian optimization using the same model trained on CIFAR - 10 [ 22 , 6 ]. At the time , this model represented the state - of - the - art . The SVHN dataset has the same input dimension as CIFAR - 10 , but is 10 times larger .__label__Method|Algorithm|Use
A total of 786 glycan array files for plant lectins were downloaded using a custom made script from Consortium for Functional Glycomics ( CFG ) as of Dec 2013 . CFG provides extensive glycomics resources so that one can explore functions of glycans and glycan - binding proteins that play important roles in human health and disease [ _CITE_ All of these 786 files were further processed into a single input file , which consists of rows of protein - carbohydrate pairs . Three datasets were generated by filtering the protein - carbohydrate pairs using the cutoff values of relative fluorescence units ( RFU ) 5000 , 10000 and 20000 .__label__Material|Data|Use
Combining all edges ( SR + LM + SB ) does not influence the results any more , but in any case the hybrid configuration achieves the best overall recall ( 0 . 87 ). In conclusion , our experiments on all four datasets consistently demonstrate that combining Dijkstra - WSA with a similarity - based approach as a back - off yields the strongest performance . The results of these best alignments will be made freely available to the research community on our website ( _CITE_ ).__label__Supplement|Document|Produce
The amount of � 1 regularization ( λ2 ) is selected to give an approximate / 10 nonzero coefficients . Implementation details are available in Appendix E . We chose the 3 datasets described in Table 1 Results . We compare three parallel asynchronous methods on the aforementioned datasets : PROXASAGA ( this work ), _CITE_ ASYSPCD , the asynchronous proximal coordinate descent method of Liu & Wright ( 2015 ) and the ( synchronous ) FISTA algorithm ( Beck & Teboulle , 2009 ), in which the gradient computation is parallelized by splitting the dataset into equal batches . We aim to benchmark these methods in the most realistic scenario possible ; to this end we use the following step size : 1 / 2L for PROXASAGA , 1 / L , for ASYSPCD , where L , is the coordinate - wise Lipschitz constant of the gradient , while FISTA uses backtracking line - search . The results can be seen in Figure 1 ( top ) with both one ( thus sequential ) and ten processors .__label__Material|Data|Use
The clustering procedure described in [ 2 ] is applied to microarray data in order to identify 18 co - occurrences arising from different environmental stresses or growth factors ( path source ) and terminating in the production of SAPK / JNK or NFκB proteins . The reconstructed network ( combined SAPK / JNK and NFκB signal transduction pathways ) is depicted in Figure 2 . This structure agrees with the signalling pathways identified using traditional experimental techniques which test individually for each possible edge ( e . g ., “ MAPK ” and “ NF - κB Signaling ” on _CITE_ ).__label__Method|Tool|Use
RH data recorded in 2007 were arranged in a matrix comprised by 432 observations ( time instants , in rows ) by 25 variables ( RH sensors , in columns ). This matrix was row - centered as described in [ 21 ]. Next , a principal components analysis ( PCA ) was carried out using the software SIMCA - P 10 . 0 ( _CITE_ ). The same analysis was repeated with sensor data recorded in 2008 with 409 , 312 observations and 2010 with 429 , 012 observations . Results from these three models were compared in order to check if the relationships among sensors were maintained year after year .__label__Method|Algorithm|Use
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : All relevant data are within the paper and its Supporting Information files . Funding : Ministry of Education , Culture , Sports , Science and Technology , Japan _CITE_ Innovative Cell Biology by Innovative Technology ( Cell Innovation Program ) to Yoshihide Hayashizaki . The funder had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript .__label__Supplement|Website|Introduce
This systematic review was registered with the International Prospective Register of Systematic Reviews ( PROSPERO ; Registration no . CRD42016035270 ; available from _CITE_ ), and was conducted and reported following the Preferred Reporting Items for Systematic Reviews and Meta - Analyses ( PRISMA ) statement [ 22 ]. Eligibility criteria The Population , Interventions , Comparisons , Outcomes , and Study design ( PICOS ) framework [ 23 ] was used to identify key study concepts in the research question , and to facilitate the search process . The Author ( s ) BMC Public Health 2017 , 17 ( Suppl 5 ): 868 Page 67 of 215__label__Method|Tool|Use
Ideally , publication of data sets will encourage the communities to adopt standardized formats and ensure complete population of experimental metadata with adequate accuracy to support reprocessing . While the SBDG immediately serves the well - defined area of X - ray crystallography , our pilot project has demonstrated that our infrastructure can preserve additional data types , such as decoy data sets for NMR computations or MicroED data sets . SBDG will duplicate XFEL data sets that are currently accessible through the Coherent X - ray Imaging Data Bank ( _CITE_ ) and support their distribution by DAA . In addition , SBDG will collaborate with MicroED and XFEL collection curators who will moderate development of community driven efforts to automate data analysis pipelines to parallel automatic processing of X - ray diffraction data sets with packages like DIALS or xia2 . We envision that the tools and technologies that arise from this project will ultimately lead to the development of a fully featured , primary data publication system .__label__Material|Data|Use
Sample collection and genotyping . Genographic sample collection was conducted according to the ethical protocol of The Genographic Project ( _CITE_ Geno2 . 0_Ethical - Framework . pdf ), with oversight provided by the University of Pennsylvania and regional IRBs ( specified in the original reports from which data analyzed in this study were taken ). IRBs were obtained for new collections in Italy , UK , Denmark , Greece , Germany and Romania ( sample and data collection were undertaken with approval from the IRB , Comite ` E ` tic d ’ Investigacio ´ ClinicaInstitut Municipal d ’ Assiste ` ncia Sanitiria ( CEIC - IMAS ) in Barcelona ( 2006 / 2600 / I )); Peru ( sample and data collection were undertaken with approval from the local IRB at Universidad San Martin de Porres , Lima , Peru ; Federal Wide Assurance ( FWA ) for International Protection of Human Subject 0001532 ; US Health and Human Services ( HHS ) International Review Board IRB0000325 ); Puerto Rico ( sample and data collection were undertaken with approval from the University of Pennsylvania IRB # 8 and the support of Liga Guakia Taina - Ke ); Mexico ( sample and data collection were undertaken with approval from the University of Pennsylvania IRB # 8 , the Centro de Investigacio ´ n y de Estudios Avanzados del Instituto Polite ´ cnico Nacional ( CINVESTAV - IPN ), and the Comision Nacional para el Desarrollo de los Pueblas Indigenas ( CDI )); Egypt , Iran , Kuwait , Lebanon and Tunisia ( the sample and data collection protocol were originally approved by the IRB committee of the Lebanese American University ); North Eurasia ( North Eurasia sample and data collection were undertaken with approval from the Ethical Committee of the Research Centre for Medical Genetics RAMS and the Academic Council of the same Research Centre ); India ( the sample and data collection protocol were originally approved by IRB of Madurai Kamaraj University , Madurai , India ). Approval for further sampling , studies and collaborations have been obtained from IRB of Chettinad Academy of Research & Education , Kelampakka , India .__label__Method|Tool|Extent
In the Filter stage , the language and a hint of the polarity of a microblog post are detected based on the This work is licenced under a Creative Commons Attribution 4 . 0 International License . Page numbers and proceedings footer are added by the organizers . License details : _CITE___label__Supplement|License|Other
For each word in the tweet , its corresponding binary prefix string representation is used as the feature value . K - means clusters are generated using two different methods . The first method uses the word2vec tool ( Mikolov et al ., 2013 ) _CITE_ . By varying the minimum occurrences ( 15 , 10 , 201 ), word vector size ( 150 , 100 , 200 , 500 , 10001 ), cluster size ( 150 , 100 , 200 , 500 , 10001 ) and sub - sampling threshold ( 10 . 00001 , 0 . 001 }), different cluster files are generated and tested . Similar to the Brown cluster feature , the name of the cluster that each word belongs to is used as the feature value .__label__Method|Tool|Use
Table S4 Binding intensities ( FU ) for labeled BambL protein with glycan array chips v4 . 1 from the consortium for functional glycomics . Full data is available on the web site ( _CITE_ ). ( PDF )__label__Supplement|Website|Produce
While no distinct policy was created to establish medicine prices in the RPI , the management applied minimal markups sufficient to cover their estimated operating costs . Retail mark - ups initially averaged approximately 30 - 50 % for most medicines . Surprisingly , as the rural pharmacy initiative emerged , the private pharmacies in the district center appeared to be changing their prices on key mediPage 2 of 15 ( page number not for citation purposes ) International Journal for Equity in Health 2009 , 8 : 43 _CITE___label__Supplement|Paper|Introduce
activation function . For the kernel MVA algorithms we used a Gaussian kernel k ( xi , xj ) = exp (− IIxi − xjII 2 / 2u ) using 10 - fold cross - validation ( 10 - CV ) on the training set to estimate u . To obtain some reference accuracy rates , we also trained a v - SVM with Gaussian kernel , using the LIBSVM implementation _CITE_ and 10 - CV was carried out for both the kernel width and v . Accuracy error rates for rKOPLS and different values of R are displayed in the first rows and first columns of Table 3 . Comparing these results with SVM ( under the rbf - SVM column ), we can dard deviation of the estimation are given for 10 different runs of rKOPLS and KPLS2 , both when using the pseudoinverse of the projected data together with the “ winner - takes - all ” activation function ( first rows ), and when using a v - SVM linear classifier ( last rows ). The results achieved by an SVM with linear classifier are also provided in the bottom right corner .__label__Method|Tool|Use
We use 4 different document sets in our experiments , as summarized in Table 1 . The NIPS and PATENTS document sets are used for perplexity experiments and the AP and FR data sets for retrieval experiments . The NIPS data set is available online _CITE_ and PATENTS , AP , and FR consist of documents from the U . S . Patents collection ( TREC Vol - 3 ), Associated Press news articles from 1998 ( TREC Vol - 2 ), and articles from the Federal Register ( TREC Vol - 1 , 2 ) respectively . To create the sampled AP and FR data sets , all documents relevant to queries were included first and the rest of the documents were chosen randomly .__label__Material|Data|Use
Most existing HLT pipelines assume the input is pure text or , at most , HTML and either ignore ( logical ) document structure or remove it . We argue that identifying the structure of documents is essential in digital library and other types of applications , and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved . Many off - the - shelf Human Language Technology ( HLT ) pipelines are now freely available ( examples include LingPipe , OpenNLP , GATE ( Cunningham et al ., 2002 ), TextPro _CITE_ ( Pianta et al ., 2008 )), and although they support a variety of document formats as input , actual processing ( mostly ) takes no advantage of structural information , i . e . structural information is not used , or stripped off during preprocessing . Such processing can be considered safe , e . g .__label__Method|Tool|Introduce
In each dataset , we randomly select 300 images and each image is resized to the scale of 12 × 12 . Eight settings of CS ratios are adopted with resents an entire image which generally has unique statistics , it is infeasible to find suitable training data in practice . Therefore , GMM - TP and KSVD - OMP are not compared to in this experiment _CITE_ . For PLE , Sparse - GMM and Sparse - GMM ( G ), the minimum - norm estimates from the measurements , � xi = arg min . { kxk 2 : Φix = yi } = Φ i ( ΦiΦ i )− yi , i = 1 , ... , N , are used to initialize the GMM .__label__Material|Data|Use
DOI : https :// doi . org / 10 . 7554 / eLife . 30766 . 002 The following source data and figure supplements are available for figure 1 : Source data 1 . Figure 1 — source data . DOI : _CITE_ Figure supplement 1 . Expression of Dnmts in various metabolic tissues . DOI : https :// doi . org / 10 . 7554 / eLife . 30766 . 003 Figure supplement 1 — source data 1 .__label__Supplement|Paper|Introduce
This approach is denoted as PRvis + Per ( PMI ). This section discusses the experimental design for evaluating the proposed approaches to labelling topics with images . To our knowledge no data set for evaluating these approaches is currently available and consequently we developed one for this study _CITE_ . Human judgements about the suitability of images are obtained through crowdsourcing . We created a data set of topics from two collections which cover a broad thematic range : police , officer , crime , street , man , city , gang , suspect , arrested , violence game , season , team , patriot , bowl , nfl , quarterback , week , play , jet military , afghanistan , force , official , afghan , defense , pentagon , american , war , gates categories ( e . g .__label__Material|Data|Produce
© 2015 by the authors ; licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license ( _CITE_ ).__label__Supplement|License|Other
The output layer is a SoftMax classifier that predicts , after the “ GO ” symbol is read , one of the following three labels : 1 , if a word is to be retained in the compression , 0 if a word is to be deleted , or EOS , which is the output label used for the “ GO ” input and the end - of - sentence final period . Input representation : In the simplest implementation , that we call LSTM , the input layer has 259 dimensions . The first 256 contain the embedding - vector representation of the current in put word , pre - trained using the Skipgram model _CITE_ ( Mikolov et al ., 2013 ). The final three dimensions contain a one - hot - spot representation of the goldstandard label of the previous word ( during training ), or the generated label of the previous word ( during decoding ). For the LSTM + PAR architecture we first parse the input sentence , and then we provide as input , for each input word , the embedding - vector representation of that word and its parent word in the dependency tree .__label__Method|Algorithm|Use
With these two resources combined , there are four stages of word level matching in our system : exact match , stem match , WordNet match and unigram paraphrase match . The stemming module uses Porter ’ s stemmer implementation and the WordNet module uses the JAWS WordNet interface . Our metric only considers unigram paraphrases , which are extracted from the paraphrase database in TERP using the script in the METEOR _CITE_ metric . The metric described in ( Owczarzak et al ., 2007 ) does not explicitly consider word order and fluency . METEOR , on the other hand , utilizes this information through a chunk penalty .__label__Method|Code|Use
For annotation , task - specific tools are being used , e . g . EXMARaLDA , annotate , RSTTool , and MMAX . _CITE_ Data is then converted into a standoff data interchange format , which is fed into the linguistic database ANNIS . ANNIS aims at providing functionalities for exploring and querying the data , offering suitable means for both visualization and export . Central requirements evolving out of the scenario sketched above and , as we believe , for multilevel annotation in general are Data heterogeneity , Data reuse , and Accessibility ( cf .__label__Method|Tool|Use
[ 25 ] in preparing the data . We use their splits to divide the classes into 100 training , 50 validation , and 50 test . For images we use 1 , 024dimensional features extracted by applying GoogLeNet [ 31 ] to middle , upper left , upper right , lower left , and lower right crops of the original and horizontally - flipped image _CITE_ . At test time we use only the middle crop of the original image . For class meta - data we use the 312 - dimensional continuous attribute vectors provided with the CUB dataset .__label__Material|Data|Use
In some cases , the test taker ’ s spoken response was nearly identical to an identified source ; in other cases , several sentences or phrases were clearly drawn from a particular source , although some modifications were apparent . Table 1 presents a sample source that was identified for several of the 239 responses in the data set . _CITE_ Many of the plagiarized responses contained extended sequences of words that directly match idiosyncratic features of this source , such as the phrases “ how romantic it can ever be ” and “ just relax yourself on the beach .” In total , 49 different source materials were identified for all of the potentially plagiarized responses in the corpus . In addition to the source materials and the plagiarized responses , a set of non - plagiarized control responses was also obtained in order to conduct classification experiments between plagiarized and non - plagiarized responses . Since the plagiarized responses were collected over the course of more than one year , they were drawn from many different TOEFL iBT test forms ; in total , the 239 plagiarized responses comprise 103 distinct Independent test questions .__label__Material|Data|Use
The Supplementary Material for this article can be found online at : _CITE___label__Supplement|Document|Produce
When the network is updated according to ( 1 ), then under certain conditions the network state becomes asymptotically independent of initial conditions . More precisely , if the network is started from two arbitrary states x ( 0 ), 31 ( 0 ) and is run with the same input sequence in both cases , the resulting state sequences x ( n ), x ( n ) converge to each other . If this condition holds , the reservoir network state will asymptotically depend only on the input history , and the network tained in a tutorial Mathematica notebook which can be fetched from _CITE_ is said to be an echo state network ( ESN ). A sufficient condition for the echo state property is contractivity of W . In practice it was found that a weaker condition suffices , namely , to ensure that the spectral radius Amax of W is less than unity .__label__Supplement|Document|Produce
In high - dimensional complex domain , the ADMM algorithm demonstrates superior performance in our simulated examples and real images . Finally , the paper also provides practical guidelines to practitioners at large working on other similar nonsmooth SDP applications . To aid peer evaluation , the source code of all the algorithms have been made available at : _CITE___label__Method|Code|Produce
Kim et al . J Cheminform ( 2016 ) 8 : 32 Page 2 of 15 standardization process . The BioAssay database ( _CITE_ ) contains descriptions and results of biological assay experiments . The record accessions used for the respective PubChem databases are the Substance ID ( SID ), Compound ID ( CID ) and Assay ID ( AID ). As of November 2015 , PubChem contains more than 150 million depositor - provided substance descriptions , 60 million unique chemical structures , and 225 million biological activity test results ( from over 1 million assay experiments performed on more than 2 million smallmolecules covering almost 10 , 000 unique protein target sequences that correspond to more than 5000 genes ).__label__Material|Data|Introduce
Raw sequence data are available under GEO series accession numbers GSE86337 ( pilot experiment ) and GSE64098 ( mixture experiment ). Analysis scripts and processed data are available from the Supplementary Information website at _CITE___label__Material|Data|Produce
Here we approximated each MAP with its LP relaxation ( as in STRIPES ), so that both STRIPES and Nilsson come with certificates of optimality when their LP solutions are integral . BMMF relies on loopy BP to approximate the M best solutions . _CITE_ We used M = 50 in all experiments . To compare the algorithms , we pooled all their solutions , noting the 50 top probabilities , and then counted the fraction of these that any particular algorithm found ( its solution rank ). For run - time comparisons , we normalized the times by the longest - running algorithm for each example .__label__Method|Algorithm|Extent
Evaluation Metrics and Baseline Algorithms : We select the receiver operating characteristic ( ROC ) curve as our performance metric because the discrimination thresholds of diseases vary . We first compare the accuracy and efficiency of VISKM with Gibbs sampling ( Gibbs ) and particle filtering ( PF ) on the Social Evolution data set [ 7 , 8 ]. _CITE_ Both Gibbs sampling and particle filtering iteratively sample the infectious and susceptible latent state sequences and the infection and recovery events conditioned on these state sequences . Gibbs - Prediction - 10000 indicates 10 , 000 iterations of Gibbs sampling with 1000 burn - in iterations for the prediction task . PF - Smoothing - 1000 similarly refers to 1000 iterations of particle filtering for the smoothing task .__label__Material|Data|Use
were not filtered in any way by the presence or absence of an actual structure − activity relationship , and so it can be expected that some proportion of the data sets are simply not suited to modeling . We consider it reasonable to operate under the assumption that the data sets extracted from ChEMBL are representative of the kinds of real world drug discovery scenarios for which this method will be used . The collection of structures and activities that were used for this validation exercise can be downloaded from _CITE_ composite - bayes . 32 For each data set , 10 % of the entries were set aside to use as the test set . These entries were selected using the greedy clustering algorithm described earlier , which means that in general the choice of test set is nondiabolical and falls within the same domain as the training set ( although it should be noted that when we repeated the experiment with a random selection of testing sets there was no bias in favor of preclustering ). Data sets for which the partition detection method was not able to detect at least three bins were left out of the results .__label__Supplement|Document|Produce
Here , we consider an important dimension of style , namely , simplicity . Systems that can rewrite text into simpler versions promise to make information available to a broader audience , such as non - native speakers , children , laypeople , and so on . One major effort to produce such text is the Simple English Wikipedia ( henceforth SimpleEW ) _CITE_ , a sort of spin - off of the well - known English Wikipedia ( henceforth ComplexEW ) where human editors enforce simplicity of language through rewriting . The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories , thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW . Importantly , not all the changes on SimpleEW are simplifications ; we thus also make use of ComplexEW edits to filter out non - simplifications .__label__Material|Data|Produce
Enclitics in Manipuri fall into six categories : determiners , case markers , the copula , mood markers , inclusive / exclusive and pragmatic peak markers and attitude markers . The role of the enclitics used and its meaning differs based on the context . Using factored approach , a tighter integration of linguistic information into the translation model is done for two reasons _CITE_ : ■ Translation models that operate on more general representations , such as lemma instead of surface forms of words , can draw on richer statistics and overcome the data sparseness problem caused by limited training data . ■ Many aspects of translation can be best explained at a morphological , syntactic or semantic level . Having such information available to the translation model allows the direct modeling of these aspects .__label__Method|Algorithm|Introduce
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ Metadata associated with this Data Descriptor is available at http :// www . nature . com / sdata / and is released under the CC0 waiver to maximize reuse . © The Author ( s ) 2016 SCIENTIFIC DATA 13 : 160094 1 DOI : 10 . 1038 / sdata . 2016 . 94 11__label__Supplement|License|Other
After purification of the PCR products and gel electrophoresis on a 2 % ( w / v ) low melting agarose gel , libraries with insert sizes of 270 – 370 bp were selected and quantified using the Quant - iTTM PicoGreen ® double stranded DNA ( dsDNA ) reagent and kits ( Invitrogen , Carlsbad , CA , USA ). Afterwards , the DNA was sequenced for 200 cycles on a next - generation sequencing instrument ( Illumina Hiseq 2000 with a TruSeq SBS Kit v3 - HS ) [ 18 ]. The raw MB - seq data were deposited in the Sequence Read Archive ( SRA ) database ( _CITE_ ) of the National Center for Biotechnology Information ( NCBI ) with accession number of SRP067471 .__label__Material|Data|Use
( And what do they even mean when indels are the main error ?) Characterize context - dependence of errors , e . g . homopolymers , CCXGG context ( _CITE_ ). Can rearrangement errors be characterized ? Long reads are promising for finding rearrangements ( e . g .__label__Material|Data|Introduce
1 . Transcriptomic and Genomic data . The microarray dataset ( study number 1 , Table 1 ) was downloaded from the Myers laboratory website ( _CITE_ , GEO reference : GSE15222 ). The complete dataset consists of transcriptome - wide gene expression data from post - mortem cerebral cortical area ( temporal , parietal and frontal cortices ), all cortical areas involved in the disease , of Alzheimer ’ s__label__Supplement|Website|Use
Nouns were extracted from WordNet ’ s noun list . Words starting with lower case and upper case letters were determined as NN and NNP , respectively . Nouns in NNS and NNPS categories were collected from the results of POS tagging articles from Plos Biology Journal _CITE_ with TreeTagger . Verbs were extracted from WordNet ’ s verb list . We manually curated VBD , VBN , VBG and VBZ verbs with irregular inflections based on WordNet .__label__Material|Data|Use
More importantly , the latent variables sj in our model are automatically estimated from data , deciding how important each eigenvector is for the classification task in a principled Bayesian framework . We evaluated the new sparse Bayesian model , the EigenNet , on both synthetic and real data and compared it with three representative variable selection methods , the lasso , the elastic net , and an ARD approach ( Qi et al ., 2004 ). For the lasso and the elastic net , we used the Glmnet software package that uses cyclical coordinate descent in a pathwise fashion _CITE_ . Like the EigenNet , the ARD approach also uses EP to approximate the model marginal likelihood . For the lasso and the elastic net , we used cross - validation to tune the hyperparameters ; for the EigenNet , we estimated av from data and tuned as by cross - validation .__label__Method|Tool|Use
Ancestor F1 : Measures the precision , recall , and F1 = 2PR /( P + R ) of correctly predicted ances12This is somewhat different from our general setup where we work with any given set of terms ; they start with a large set of leaves which have substantial Web - based relational information based on their selected , hand - picked patterns . Their data is available at _CITE_ downloads . html .__label__Material|Data|Use
We use the Web 1T 5 - gram corpus ( Brants and Franz , 2006 ) to compute the language model score for a sentence . Each of the three classifiers ( article , preposition , and noun number ) is trained with the multi - class confidence weighted algorithm ( Crammer et al ., 2009 ). The training data consists of all non - OCR papers in the ACL Anthology _CITE_ , minus the documents that overlap with the HOO 2011 data set . The features used for the classifiers follow those in ( Dahlmeier and Ng , 2012a ), which include lexical and part - of - speech n - grams , lexical head words , web - scale n - gram counts , dependency heads and children , etc . Over 5 million training examples are extracted from the ACL Anthology for use as training data for the article and noun number classifiers , and over 1 million training examples for the preposition classifier .__label__Material|Data|Use
Finally , section 6 discusses future work and states our conclusions . Tools for statically analyzing binary executables differ in the details of their workings but they all share the same high level logic , which is called recursive disassembly . _CITE_ The tool starts by obtaining the address of the first instruction from a specific location inside the executable . It then places this address on a stack and executes the following steps while the stack is non - empty . It takes the next address from the stack and disassembles ( i . e .__label__Method|Tool|Introduce
Programs were generated to analyze the depth , variation , and consensus quality of each SNP . Finally , a Perl script was written to select significant sites within the predicted SNP positions . The script can be downloaded at sourceforge ( _CITE_ ). Classification of intergenic , exonic and intronic SNPs To determine whether the SNP location within the transcript structure is intronic , exonic , or intergenic , we tracked information from the reference genome sequence and annotated the exon or intron at which the SNP was located if it was not intergenic . Gene Ontology ( GO ) was analyzed using a generic GO slim database composed of 366 , 327 proteins downloaded from the Gene Ontology website ( http :// archive . geneontology . org / lite / 2013 - 01 - 26 /), which lists high - level GO terms that provide a broad overview of the ontology content .__label__Method|Code|Produce
The parameters relevant to the ClueGO analysis are reported in Table S2 . The scatter and hierarchical clustering plots were drawn with Orange software ( v . 3 . 3 ; Demsar et al ., 2013 ), while functional networks were drawn with Cytoscape ( v . 3 . 3 ; Shannon et al ., 2003 ). The mitochondrial assignments were performed as described before ( Mäkelä et al ., 2016 ; Tikka et al ., 2016 ), by utilizing mitochondrial annotations from IMPI database ( _CITE_ ).__label__Material|Data|Use
Getting all the resources into one single compilation is a challenge . These resources were brought together and suitably compiled into a format that can be easily processed by Semantex ( Srihari , 2008 ), a text extraction platform provided by Janya Inc . Lists of places , organizations and names of famous personalities in Pakistan were also compiled using the Urdu - Wikipedia and NationalMaster _CITE_ . A list of most common names in Pakistan was composed by retrieving data from the various name databases available on the internet . The word segmentation model uses the Urdu corpus released by CRULP as the training data .__label__Supplement|Website|Use
One limitation of our method is that it cannot achieve high yield for PHvst whenever only a small number of paraphrase patterns can be extracted from the bilingual corpus ( see also Figure 5 ). Both the ratio of PHvst to PSeed and the relative yield could probably be increased by scaling up the monolingual corpus . For instance , in the patent domain , monolingual documents 10 times larger than the one used in the above experiments are available at the NTCIR project _CITE_ . It would be interesting to compare the relative gains brought by in - domain versus general - purpose corpora . ( left : probability - based ( 0 . 01 < the < 0 . 9 , th , g = e ), right : similarity - based ( e < th , g < 0 . 9 , the = 0 . 01 )) Finally , we investigated how the number of paraphrase pairs varies depending on the values for the two thresholds , i . e ., thp on the conditional probability and ths on the contextual similarity , respectively .__label__Method|Tool|Introduce
We have tested the performance of the proposed method on mixtures of different voice and music signals . The sample rate of the mixtures is 22 . 05kHz . Audio files for all the experiments are accessible at the website _CITE_ . Figure 2 shows experimental results . In experiments 1 and 2 , the mixed signals consist of one voice signal and one music signal .__label__Supplement|Media|Produce
© 2018 by the authors . Licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ( CC BY ) license ( _CITE_ ).__label__Supplement|License|Other
In this section , we present experimental results to compare CLIME - ADMM with existing algorithms and show its scalability . In all experiments , we use the low rank property of the sample covariance matrix and do not assume any other special structures . Our algorithm is implemented in a shared - memory architecture using OpenMP ( _CITE_ ) and a distributed - memory architecture using OpenMPI ( http :// www . open - mpi . org ) and ScaLAPACK [ 15 ] ( http :// www . netlib . org / scalapack /).__label__Method|Tool|Use
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ © The Author ( s ) 2017__label__Supplement|License|Other
where each example may have more than one labels . Here , we maintained only a single label for each data point in order to apply standard multiclass classification . The maintained label was the first label appearing in each data entry in the repository files _CITE_ from which we obtained the data . Figure 3 displays convergence of the lower bounds ( and for the exact softmax cost ) for all methods . Recall , that the methods SOFT , OVE and BOUCHARD are non - stochastic and therefore their optimization can be carried out by standard gradient descent .__label__Supplement|Document|Use
The set of possible hyperparameter values was determined on early validation tests using subsets of the citation and Reddit data that we then discarded from our analyses . The appendix contains further implementation details . _CITE_ on the full test set ( 79 , 534 nodes ). B : Model performance with respect to the size of the sampled neighborhood , where the “ neighborhood sample size ” refers to the number of neighbors sampled at each depth for K = 2 with S1 = S2 ( on the citation data using GraphSAGE - mean ). Our first two experiments are on classifying nodes in evolving information graphs , a task that is especially relevant to high - throughput production systems , which constantly encounter unseen data .__label__Supplement|Document|Produce
Given our 72 animals and 54 features , we ask a human annotator to mark each animal - feature pair with a ‘ probability ’, expressed as a quantifier . Possible values are no , few , some , most , all . The guidelines for the annotation task can be seen at _CITE_ iwcs13 - annot . pdf .__label__Supplement|Document|Produce
The same has not been accomplished for habitats ; however , the components required to accomplish this are falling into place . For example , the LifeMapper ( Prajapati , 2009 ) and Map of Life ( Jetz , McPherson & Guralnick , 2012 ) projects use ecological niche modeling to map species distributions based on environmental conditions . Additionally , the Encyclopedia of Life TraitBank ( _CITE_ ) links taxa to their habitat type and phenotypic traits , but not to geographic coordinates ( Parr et al ., 2015 ). Once greater ontological representation of the link between species and their environments is accomplished , robustly linking species ’ phenotypes to their environments and locations become readily achievable . In addition to spatial variation , environments show considerable variation over time and often change over daily and seasonal cycles .__label__Supplement|Website|Introduce
Our experiments are conducted on 4 public datasets : POS , ChineseOCR , RCV1 - regions , and EURLex ( directory codes ). For sequence labeling we experiment on POS and ChineseOCR . The POS dataset is a subset of Penn treebank _CITE_ that contains 3 , 808 sentences , 196 , 223 words , and 45 POS labels . The HIT - MW ChineseOCR dataset is a hand - written Chinese character dataset from [ 17 ]. The dataset has 12 , 064 hand - written sentences , and a total of 174 , 074 characters .__label__Material|Data|Use
For comparison , consider the results of [ 26 ] obtained for the compression of the fully - connected layers of the Krizhevsky - type network [ 13 ] with the Fastfood method . The model achieves compression factors of 2 - 3 without decreasing the network error . In all experiments we use our MATLAB extension of the MatConvNet framework _CITE_ [ 24 ]. For the operations related to the TT - format we use the TT - Toolbox implemented in MATLAB as well . The experiments were performed on a computer with a quad - core Intel Core i5 - 4460 CPU , 16 GB RAM and a single NVidia Geforce GTX 980 GPU .__label__Method|Code|Extent
In addition , they present a very userfriendly graphical interface that makes easy the development of very complex dialogues , besides the incorporation of predefined libraries for typical dialogues states such as requesting card or social security numbers , etc ., and additional assistants for debugging , logging and simulate the service . In contrast to commercial platforms , research or academic platforms ( e . g . CSLU - RAD , DialogDesigner _CITE_ , Olympus , Trindi - kit , etc .) do not necessary incorporate all the above - mentioned features ; especially because they are limited to the number of standards that they are able to handle and to the integration level with other platforms , as well as the number of capabilities that they can offer to the users and programmers . However , they allow more complex dialogue interactions , most of them are freely available as open source , and using third party modules it is possible to extend their functionalities .__label__Supplement|Website|Compare
The HOG system ( Callmeier et al ., 2004 ) for the integration of shallow and deep linguistic processors ( using a pipeline making use of XML plus XSLT transformations to pass data between processors ) was developed during the Deep Thought project , as was a standard for the integration of semantic analyses produced by diverse components : RMRS ( Copestake , 2003 ) allows underspecification of semantic analyses in such a way that the analysis produced by a shallow component may be considered an underspecification of a fuller semantic analysis produced by a deeper component . Other work ( Waldron et al ., 2006 ) has provided a representation of partial analyses at the level of tokenization / morphology – using a modification of MAF ( Clement and de la Clergerie , 2005 ). Current work within the SciBorg project _CITE_ is investigating more fine - grained integration of shallow and deep processors . Our standoff annotation framework borrows heavily from the MAF proposal . The key components of our framework are ( i ) grounding in primary linguistic data via flexible standoff pointers , ( ii ) dec oration of individual annotations with structured content , ( iii ) representation of structural ambiguity via a lattice of annotations and ( iv ) a structure of intra - annotation dependencies .__label__Method|Tool|Introduce
However , because text - processing is more advanced in this regard than image - processing , we shall concentrate on NER , which is performed with a system called Oscar . A preliminary overview of the system was presented by Corbett and Murray - Rust ( 2006 ). Oscar is open source and can be downloaded from _CITE_ As a first step in representing biomedical content , we identify Gene Ontology ( GO ) terms in full text . 1 ( The Gene Ontology Consortium , 2000 ) We have chosen a relatively simple starting point in order to gain experience in implementing useful semantic markup in a publishing workflow without a substantial word - sense disambiguation effort . GO terms are largely compositional ( Mungall , 2004 ), hence incomplete matches will still be useful , and that there is generally a low level of semantic ambiguity . For example , there are only 133 single - word GO terms , which significantly reduces the chance of polysemy for the 20000 or so others .__label__Method|Code|Use
The second part is a hands - on tutorial using part of our own work as a running example . We applied TC to automatically extract nursing job tasks from nursing vacancies to augment nursing job analysis ( Kobayashi , Mol , Kismiho ´ k , & Hesterberg , 2016 ). The findings from this study were used in the EU - funded Pro - Nursing ( _CITE_ ) project which aimed to understand , among others , how nursing tasks are embedded in the nursing process . We also address validity assessment because the ability to demonstrate the validity of TC outcomes will likely be critical to its uptake by organizational researchers . Thus , we discuss and illustrate how to establish__label__Method|Tool|Use
In this paper , we use 3 . 6M parallel patent sentences with the highest scores of sentence alignment . As a toolkit of a phrase - based SMT model , we use Moses ( Koehn et al ., 2007 ) and apply it to the whole 3 . 6M parallel patent sentences . Before applying Moses , Japanese sentences are segmented into a sequence of morphemes by the Japanese morphological analyzer MeCab _CITE_ with the morpheme lexicon IPAdic . For Chinese sentences , we examine two types of segmentation , i . e ., segmentation by characters and segmentation by morphemes . As the result of applying Moses , we have a phrase table in the direction of Japanese to Chinese translation , and another one in the opposite direction of Chinese to Japanese translation .__label__Method|Tool|Use
These data sets was chosen for a variety of reasons , including 1 ) the relevance of the experimental perturbation to modulating the types of cell proliferation that can occur in cells of the normal lung , 2 ) the availability of raw gene expression data , 3 ) the statistical soundness of the underlying experimental design , and 4 ) the availability of appropriate cell proliferation endpoint data associated with each transcriptomic data set . In addition , the perturbations used to modulate cell proliferation in these experiments covered mechanistically distinct areas of the Cell Proliferation Network , ensuring that robust coverage of distinct mechanistic pathways controlling lung cell proliferation were reflected in the network . Data for GSE11011 and GSE5913 were downloaded from Gene Expression Omnibus ( GEO ) _CITE_ , while data for E - MEXP - 861 was downloaded from ArrayExpress http :// www . ebi . ac . uk / microarray - as / ae /. The data from PMID15186480 was obtained from a link within the online version of the paper http :// jbiol . com / content / 3 / 3 / 11 . Raw RNA expression data for each data set were analyzed using the “ affy ” and “ limma ” packages of the Bioconductor suite of microarray analysis tools available for the R statistical__label__Material|Data|Use
For lexicographers , the computational environment fills the need for a corpus workbench which supports WSD . Results under simulated lexicographic use on the English lexical - sample task show precision comparable with supervised systems ', without using the laboriously - prepared training data . WASP - Bench _CITE_ is a web - based tool supporting both corpus - based lexicography and Word Sense Disambiguation . The central premise behind the initiative is that deciding what the senses for a word are , and developing a WSD program for it , should be tightly coupled . In the course of the corpus analysis , the lexicographer explores the textual clues that indicate a word is being used in one sense or another ; given an appropriate computational environment , these clues can be gathered and used to seed a bootstrapping WSD program .__label__Method|Tool|Introduce
We experimented on six languages from diverse language families . The segmentation data for English , Finnish and Turkish was taken from MorphoChallenge 2010 ( Kurimo et al ., 2010 ). _CITE_ Despite typically being used for UMS tasks , the MorphoChallenge datasets do contain morpheme level labels . The German data was extracted from the CELEX2 collection ( Baayen et al ., 1993 ). The Zulu data was taken from the Ukwabelana corpus ( Spiegler et al ., 2010 ).__label__Material|Data|Use
Comments are denoted by a #. Abbreviations for sequencing center and tissue source site are as follows : BCGSC — British Columbia Genome Sequencing Center ; Broad — The Broad Institute of MIT and Harvard ; Fox Chase — Fox Chase Cancer Center ; MSKCC — Memorial Sloan Kettering Cancer Center ; UCSF — University of California San Francisco ; UNC — University of North Carolina ; Wash Univ St . Louis — Washington University in St . Louis . This file is available at Dryad with the following link : _CITE_ ( GZ 259 kb ) Additional file 3 : Figure S2 . These histograms illustrate the variation in counts of bacterial taxa per sequencing run across cancer types .__label__Supplement|Website|Use
These sentences constitute the impact - based summary of the paper . Despite the use of citation contexts and anchor text in many information retrieval and natural language processing tasks , to our knowledge , we are the first to propose the incorporation of citation context information available in citation networks in a co - training framework for topic classification of research papers . The dataset used in our experiments is a subset sampled from the CiteSeer " digital library _CITE_ and labeled by Dr . Lise Getoor ’ s research group at the University of Maryland . This subset was previously used in several studies including ( Lu and Getoor , 2003 ) and ( Kataria et al ., 2010 ). The dataset consists of 3186 labeled papers , with each paper being categorized into one of six classes : Agents , Artificial Intelligence ( AI ), Information Retrieval ( IR ), Machine Learning ( ML ), HumanComputer Interaction ( HCI ) and Databases ( DB ).__label__Material|Data|Use
Raw reads from whole genome shotgun sequencing of Buddleja globosa are available in the NCBI Sequence Read Archive , BioProject IDPRJNA419550 , SRA SRP125846 ( https :// www . ncbi . nlm . nih . gov / sra /? term = SRP125846 ). Assembled contigs from whole genome shotgun sequencing of Buddleja globosa are available on Dryad Digital Repository ( _CITE_ ). Target locus sequences in Buddleja globosa used for probe design for targeted sequence capture are available on Dryad Digital Repository ( https :// doi . org / 10 . 5061 / dryad . v6q0p ). Raw reads from sequencing after targeted sequence capture for 50 samples are available in the NCBI Sequence Read Archive , BioProject ID PRJNA419999 , SRA SRP125765 ( https :// www . ncbi . nlm . nih . gov / sra /? term = SRP125765 ).__label__Material|Data|Produce
Data Availability : The authors confirm that all data underlying the findings are fully available without restriction . All relevant data are within the paper and its Supporting Information files . Funding : This work was supported by Institut National de la Sante ´ et de la Recherche Me ´ dicale ( INSERM ), Centre National de la Recherche Scientifique ( CNRS ), Strasbourg University , Agence Nationale de la Recherche ( ANR - 11 - BSV1 - 026 ), Association Francaise contre les Myopathies ( 15352 ), Muscular Dystrophy Association ( 186985 ), and Myotubular Trust ( _CITE_ ). OAN was supported by a fellowship from CAPES Foundation , Ministry of Education of Brazil , process number 1286 / 51 - 2 ( http :// www . capes . gov . br /). The funders had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript .__label__Supplement|Website|Introduce
Finally , we would like to mention that Equation 2 represents our ranking function for our full model which accounts for predicate similarity between our target argument ( data or backing ) and original claim . Our baseline model does not include predicate similarity between the targeted argument and original claim . Given the five topic motion phrases animal testing , death penalty , cosmetic surgery , smoking in public places , and junk food from schools that were randomly selected from the iDebate , a popular , wellstructured online debate platform , Top 100 Debate list _CITE_ , we construct 5 Toulmin instantations for the topic motion ban ( House , Y )), where Y is a topic motion phrase . Similarly , we construct 5 Toulmin instantations for the topic motion not ban ( House , Y )), which serves as a counterclaim . For each topic motion , we use WordNet [ 12 ] to collect the full hyponyms and lemmas of the topic motion keyword .__label__Material|Data|Use
The second dataset is Boston Housing with n = 506 , d = 10 and p = 1 . We add 10 irrelevant variables randomly drawn from Uniform ( 0 , 1 ) to evaluate the variable selection performance . The third one , Space ga _CITE_ , is an election data with spatial coordinates on 3107 US counties . Our task is to predict the x , y coordinates of each county given 5 variables regarding voting information . For Space ga , we normalize the responses to [ 0 , 1 ].__label__Material|Data|Use
In this setting , we assume that we do not have access to any French tagger that we can exploit to improve projection . Hence , all we can do is to employ the three steps involved in the projection approach as described at the beginning of this section to create coreference - annotated data for French . Specifically , we translate a French text to an English text using GoogleTranslate , and create coreference chains for the translated English text using Reconcile _CITE_ ( Stoyanov et al ., 2010 ). To project mentions from English to French , we first align the English and French words in each pair of parallel sentences , and then project the English mentions onto the French text using the alignment . However , since the alignment is noisy , the French words to which the words in the English mention are aligned may not form a contiguous text span .__label__Method|Tool|Use
For the NOC - SVM method , we used the implementation provided by the authors 6 . The LibSVM package [ 1 ] was used to implement the HMVE and I - OCSVM methods . An implementation of our q - OCSVM estimator is available from : _CITE_ All experiments were carried out with a Gaussian kernel ( γ = 1 2 . 5__label__Method|Code|Produce
Discourse Topic is an under - theorized notion in linguistic theory : not all linguists agree that the notion of Discourse Topic is required in discourse analysis at all ( cf . Asher , 2004 ). For our purposes , however , we formulated a set of patterns for identifying Discourse Topics on the basis of the output of the CMU Link Parser _CITE_ the system uses . Paradigmatically , we counted ordinary subjects of the first sentence of a passage as expressive of the Discourse Topic . So , if we found an expression of the topic there , either in full or reduced form , we took that as an instance of the topic appearing as Discourse Topic in that passage and ranked that passage highly .__label__Method|Tool|Use
We tested the algorithm using fMRI data collected from 10 subjects viewing a movie split into 2 sessions separated by a short break . The data was preprocessed following [ 5 ]. For each subject , a structural scan was acquired before each session , from which the cortical surface model was derived (§ 2 ) and then anatomically aligned to a template using FreeSurfer ( Fischl , _CITE_ ). Similar to [ 5 ], we find that anatomical alignment based on cortical curvature serves as a superior starting point for functional alignment over Talairach alignment . First , functional connectivity was found for each subject and session : Ck , i , k = 1 , ... , Ns , i = 1 , 2 .__label__Method|Tool|Use
Interestingly , the enhancers found in TAD1 form two closely located clusters ( E2 / E3 and E4 - E7 ), embedded into the genes of S100 family . These enhancer clusters showed extensive long - range intra - TAD chromatin contacts with multiple genes in the central part of the EDC ( TAD3 and TAD4 ) activated during terminal keratinocyte differentiation , suggesting that they might serve as the locus - control regions or super - enhancers for the EDC genes . In addition , PLOS Genetics | _CITE_ September 1 , 2017 18 / 32 Chromatin interactome of multi - TAD keratinocyte - specific gene locus we identified the gene enhancer ( E9 ) spatially interacting with Flg gene promoter ( Fig 4D ). These enhancers have been previously identified among the highly - conserved non - coding regions in several mammalian genomes and showed the activity in the reporter assay in cultured keratinocytes [ 33 ]. It will be important to determine if this conserved enhancer controls Flg gene expression in normal and diseased epidermis , as the defects in Flg gene and changes in its expression are associated with ichthyosis vulgaris , the most common disorder of epidermal differentiation , and also serve as strong risk factors for atopic eczema [ 62 ].__label__Supplement|Paper|Introduce
We call clusters that satisfy this property singleton clusters . To obtain Brown clusters for the source and target languages , we used code from Liang ( 2005 ). _CITE_ We used the data from the news commentary corpus along with the first 500K sentences of the additional monolingual newswire data also provided for the WMT shared tasks . We used 300 clusters , ignoring words that appeared only once in this corpus . We did not use the hierarchical information from the clusters but merely converted each cluster name into a unique integer , using one additional integer for unknown words .__label__Method|Code|Use
We now show the benefits of our approach on large - scale datasets , since we exploit the efficiency of random features with the performance of kernel - learning techniques . We perform experiments on three distinct types of datasets , tracking training / test error rates as well as total ( training + test ) time . For the adult _CITE_ dataset we employ the Gaussian kernel with a logistic regression model , and for the reuters dataset we employ a linear kernel with a ridge regression model . For the buzz dataset we employ ride regression with an arc - cosine kernel of order 2 , i . e . P0 = N ( 0 , I ) and φ ( x , w ) = H ( wTx ) ( w x ) , where H (·) is the Heavyside step function [ 7 ].__label__Material|Data|Use
After the SemEval task , we crawled the full articles from _CITE_ , cleaned the corpus and annotated it with the exact publication date of the article , its title and the URL from which it was retrieved . The Daikon Corpus is made up of articles from the British Spectator news magazine from year 828 to 2008 . The Daikon corpus can be used for future diachronic studies and epoch identification tasks ; it provides a complementary dataset to the gold standard provided by task .__label__Material|Data|Use
Such discrepancy emphasizes the need for caution when working with model results , but also suggests that incorporating data of the benthic communities can substantially enhance the quality of ecosystem models . All these aspects add to the complexity of the system and the result ´ s interpretation . The outcome of our analysis reflects some general principles of how abiotic variables drive the high PLOS ONE | _CITE_ April19 , 2017 15 / 22 Is interpretation of long - term data in transitional waters more than speculation ? Fig 11 . Median anomaly of abundance , biomass and species number at station 109 .__label__Supplement|Paper|Extent
Financial support by the Austrian Science Fund ( FWF ): FI001380 granted to Bernhard Fügenschuh is gratefully acknowledged . The authors would like to thank Franz Neubauer and Dinu Pana ˘ for their comments and suggestions that have significantly improved the manuscript . Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made .__label__Supplement|License|Other
Kleene has been approved by SAP AG for release as free , open - source code under the Apache License , Version 2 . 0 , and will be available by August 2012 for downloading from http :// www . kleene - lang . org . The design , implementation , development status and future plans for the language are discussed . Kleene is a finite - state programming language in the tradition of the AT & T Lextools ( Roark and Sproat , 2007 ), _CITE_ the SFST - PL language ( Schmid , 2005 ), the Xerox / PARC finite - state toolkit ( Beesley and Karttunen , 2003 ) and FOMA ( Huld ´ en , 2009b ), all of which provide higher - level programming formalisms built on top of low - level finite - state libraries . Kleene itself is built on the OpenFst library ( Allauzen et al ., 2007 ), developed by Google Labs and NYU ’ s Courant Institute . The design and implementation of the language were motivated by three main principles , summarized as Syntax Matters , Licensing Matters and Open Source Matters .__label__Method|Tool|Introduce
In this paper we studied the application of compressionbased distance measures for the problem of sequence comparison , with a special focus on NGS short read data . Their key advantages are assembly - free , alignment - free , and parameter - free . We conducted extensive validation on various types of sequence data : NGS short reads , 16S rRNA sequences , mtDNA sequences , and whole genome Tran and Chen BMC Research Notes 2014 , 7 : 320 Page 12 of 13 _CITE_ sequences . The sequence data was obtained from several mammalian and bacteria genomes at different taxonomy levels , as well as from microbial metagenomic samples . The results show that the compression - based distance measures produced comparably accurate results as the kmer based methods , and both were in good agreement with the alignment - based approach and with existing benchmarks in the literature .__label__Supplement|Paper|Use
For the univariate densities , we use a standard Gaussian kernel density estimator ( see , for example , [ Bowman and Azzalini , 1997 ]). Using an identical procedure , we learn a linear Gaussian BN baseline where Xi ∼ N ( apai , Qi ) so that each variable Xi is normally distributed around a linear combination of its parents Pai ( see [ Koller and Friedman , 2009 ] for details on this standard approach to structure learning ). For the GCBN model , we also compare to Nonparametric BP ( NBP ) [ Sudderth et al ., 2010a ] using D . Bickson ’ s code [ Bickson , 2008 ] and A . Ihlers KDE Matlab package ( _CITE_ ihler / code / kde . html ), which relies on a mixture of Gaussians for message representation . In this case , since our univariate densities are constructed using Gaussian kernels , there is no approximation in the NBP representation and all approximations are due to message computations . To carry out message products , we tried all 7 sampling - based methods available in the KDE package .__label__Method|Code|Use
When a numerical expression is accompanied by a modifier such as over , about , or more than , we updated the value and modifier fields appropriately . Internally , all values are represented by ranges ( e . g ., 75 is represented by the range [ 75 , 75 ]). We developed an extractor and a normalizer for Japanese numerical expressions _CITE_ . We will outline the algorithm used in the normalizer with an example sentence : “ Roughly three thousand kilograms of meats have been provided every day .” We used a dictionary to perform procedures 2 and 3 ( Table 3 ). If the words that precede or follow an extracted number match an entry in the dictionary , we change the semantic representation as described in the operation .__label__Method|Tool|Produce
For this purpose , we created the Paraphrase for Plagiarism corpus ( P4P ) annotating a portion of the PAN - PC - 10 corpus for plagiarism detection ( Potthast et al . 2010 ) on the basis of a paraphrase typology , and we mapped the annotation results with those of the Second International Competition on Plagiarism Detection ( Pan - 10 competition , hereafter ). _CITE_ The results obtained provide critical insights for the improvement of automatic plagiarism detection systems . The rest of the article is structured as follows . Section 2 sets out the paraphrase typology used in this research work .__label__Material|Data|Use
− Inkurdish : a new and high - quality translation between Sorani Kurdish and English . − English Kurdish Translation : especially can translate words in Kurmanji and English together . − Freelang _CITE_ : supports 4000 words in kurmanji . It currently has more than 12 , 000 Sorani and 20 , 000 Kurmanji10 articles . One useful application of these entries is to build a parallel collection of named entities across both dialects .__label__Method|Tool|Introduce
Passatutto has been implemented as a Java v1 . 6 program . It reads and writes spectra in MassBank file format and fragmentation trees in the SIRIUS DOT file format . Source code is available from _CITE_ , Java executables ( JAR files ) are available from https :// bio . informatik . uni - jena . de / passatutto /. Passatutto contains modules for ( a ) generating a decoy database , ( b ) database searching in locally stored data sets , and ( c ) estimating q - values either by means of the target - decoy approach or by empirical Bayes estimation . For generating a decoy database using the fragmentation tree - based method , SIRIUS can be used for the computation of fragmentation trees , which is available from https :// bio . informatik . uni - jena . de / sirius /.__label__Method|Code|Use
It can be seen that even for condition numbers as high as 200 , n — 1500 measurements suffices for IHT to exactly recovery w *, whereas GOMP with the same setting is not able to recover w * even once . Tumor Classification , Breast Cancer Dataset We next compare the aforementioned methods on a gene selection problem for breast cancer tumor classification . We use the data used in [ 8 ] _CITE_ . We ran a 5 - fold cross validation scheme to choose parameters , where we varied 77 E { 2 - , 2 - , ... , 2 } k E { 2 , 5 , 10 , 15 , 20 , 50 , 100 } T E { 2 , 2 , ... , 213 }. Figure 2 ( Right ) shows that the vanilla hard thresholding method is competitive despite performing approximate projections , and the method with full corrections obtains the best performance among the methods considered .__label__Material|Data|Use
In order to investigate the empirical behavior of FRFs as models of large - scale networks , we design two different groups of experiments ( in link prediction and graph generation respectively ), using collaboration networks drawn from the arXiv e - print repository ( _CITE_ ), where nodes represent scientists and edges represent paper coauthorships . Some basic network statistics are reported in Table 1 . Link prediction .__label__Material|Data|Use
Videos of these behaviors can be found at _CITE_ These behaviors were trained using feedback from the authors .__label__Supplement|Media|Produce
Section 5 presents experiments on KTH and KITTI datasets with comparison to related attention - based trackers . Section 6 discusses the results and intriguing properties of our framework and Section 7 concludes the work . Code and results are available online _CITE_ . A number of recent studies have demonstrated that visual content can be captured through a sequence of spatial glimpses or foveation [ 22 , 12 ]. Such a paradigm has the intriguing property that the computational complexity is proportional to the number of steps as opposed to the image size .__label__Method|Code|Produce
The evolutionary processes underlying the origin and maintenance of species diversity in tropical forests — some of the oldest and most diverse ecosystems on Earth — have long been a source of fascination and debate among biologists [ 1 , 2 ]. Although tropical forests are known to be species rich , precise estimates of their diversity , and variation across space , are difficult to obtain due to limitations of sampling and our ability to accurately circumscribe species . PLOS ONE | _CITE_ June 15 , 2018 1 / 20 Reconciling species diversity in Canarium funders had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript . Competing interests : The authors have declared that no competing interests exist . Refining these estimates is important , however , as it directly impacts a variety of fields that rely on species as units of analysis , including conservation , macroecology , and macroevolution [ 3 , 4 ].__label__Supplement|Paper|Introduce
Relevant domain data . Furthermore , we were interested if simply adding more unlabelled data , not necessarily from the relevant domain , produced the same increase in accuracy . We obtained the plaintext claims and description parts of 13 , 600 patents freely available in the Global IP Database which is based on the Espacenet _CITE_ , creating a corpus with 42 . 5 mln tokens , i . e . which was similar in size to the reviews and weblogs sections of the SANCL unlabelled dataset . Table 8 compares results achieved when building clusters from the patents corpus and when using the reviews and weblogs texts from the SANCL unlabelled dataset .__label__Material|Data|Extent
We use two machine learning methods in this section . They are maximum entropy method ( ME ) ( Beger et al . 96 ) and support vector machine ( SVM ) ( cristianini00 ) _CITE_ , both of which have been shown to be quite effective in natural language processing . The task of a machine learning method is to make a classifier that can decide whether a response is paraphrasable by te - hoshii or not . A response X is tagged possible if it is paraphrasable Given training data , a machine learning method produces a classifier that outputs possible or impossible according to a given feature vector .__label__Method|Algorithm|Use
EM - LDS was able to correct some of the deformation errors of EM - Gaussian . The average Z error for EM - LDS on the shark sequence after 100 EM iterations is 1 . 24 %. Videos of the shark reconstructions and the Matlab software used for these experiments are available from _CITE_ In highly - constrained cases — low - rank motion , no image noise , and no missing data ILSQ achieved reasonably good results . However , EM - Gaussian gave better results in nearly every case , and dramatically better results in underconstrained cases .__label__Supplement|Document|Use
The overall home ranges estimated with a - LoCoHs and T - LoCoHs were very similar ( Fig 2 ), which was expected as T - LoCoH was developed as an extension of the location - based a - LoCoH [ 30 ]. The area estimates were also smaller than the GCM and BRB , which is supported by simulated LoCoH studies showing the hulls created essentially ‘ hug ’ the data [ 25 , 30 ]. However , this also means that the LoCoH PLOS ONE | _CITE_ March31 , 2017 14 / 23 Evaluating home range estimators using GPS collars methods are not as strong at modelling spatial uncertainty associated with GPS fixes [ 30 ]. They both perform most effectively with large data sets [ 76 ]; a - LoCoH has been shown to converge on the true range as sample size increases [ 25 ]. BRB appeared to show the best overall performance , producing high and robust AUC values , while not showing as much sensitivity to sample size or fix frequency as GCM , which had similarly high AUC values .__label__Supplement|Paper|Introduce
It therefore makes sense to think of this as a separable part of the theory , the “ upper ontology ”. At the top level , datastructures ( instances of Object ) belong to one of the concepts Ordered , Set and Primitive . Ordered structures are divided ' These are both available in full from _CITE_ up in terms of the number of components ( concepts Arity - 1 , Arity - 2 etc ) and whether they are Tuples or Sequences . For convenience , union types such as Arity - atleast - 2 are also defined . The RAGS NLG ontology ( see Figure 3 for an overview ) contains the main substance of the RAGS type definitions .__label__Method|Algorithm|Use
We compared performance of RETAIN to RNNs and traditional machine learning methods . Given space constraints , we only report the results on the learning to diagnose ( L2D ) task and summarize the disease progression modeling ( DPM ) in Appendix C . The RETAIN source code is publicly available at_CITE___label__Method|Code|Produce
1 . We show how to incorporate this information in Policy Gradient RL [ 30 ], and show that we can improve over RL that has access to the same amount of ground - truth captions . Our code and data will be released ( _CITE_ ) to facilitate more human - like training of captioning models .__label__Method|Code|Produce
Thus , a total of 20 , 000 AMT HITs were collected , and a total of 100 , 000 images were shown to the participants . On average , each participant took approximately one minute to finish each HIT , spending about 3 seconds per query image . The dataset is publicly available at _CITE___label__Material|Data|Use
More importantly , well - clustered datasets are abundant in big - data scenarios . For instance , although there are more than 1 billion of users on Facebook , the intrinsic “ feature vectors ” of these users can be naturally categorized by the users ’ occupations , nationalities , etc . As another example , although there ∗ The full version of this paper can be found on _CITE_ † These two authors equally contribute to this paper . 30th Conference on Neural Information Processing Systems ( NIPS 2016 ), Barcelona , Spain .__label__Supplement|Document|Produce
Sample data used for demonstration of CARD functionality . We used three data sets to demonstrate different features of CARD and highlight the utility of the application . We also include template files in ‘ CARD format ’ in the help page ( at _CITE_ ) for new users to format their data for upload . The sample data sets used were as follows . HDAC - inhibitor screen data : a genome - scale siRNA screen ( using a Dharmacon pooled library — pool of 4 unique siRNAs per gene ) to identify genes that sensitize cells to HDAC - inhibitor ( vorinostat )- induced cell death17 .__label__Supplement|Document|Produce
To better address the agglutinative nature of Basque , the word alignments were obtained over the lemmas , and were then projected to the original word forms to complete the training process . The system was trained on translation memory ( TM ) data containing academic books , software manuals and user interface strings (- 12 million words ), and web - crawled data (- 1 . 5 million words ) made available by Elhuyar . _CITE_ For the language model , the Basque text of the parallel data and the Basque text of Spanish - Basque TMs of administrative text made available by Elhuyar (- 7 . 4 million sentences ) was used . Again , a set of 1 , 000 indomain interactions were used for tuning after manually translating the original text into Basque . The system was evaluated on a second test - set of 1 , 000 in - domain interactions , obtaining a BLEU score of 20 . 24 .__label__Material|Data|Use
MEDUSA was implemented in python programming language and requires the numpy package ( http :// www . numpy . org /). MEDUSA makes use of standalone tools such as FASTX , bowtie2 [ 26 ] and GEM [ 27 ] that need to be callable from the Unix command line . The MEDUSA pipeline together with databases and results are available at _CITE_ medusa .__label__Material|Data|Produce
prediction appearing in computational biology , where our method obtains less than a half of the error rate of the best competing HMM - based method . Our predictions are available at Wormbase : _CITE_ Additional data and results are available at the project ’ s website http :// www . fml . mpg . de / raetsch / projects / msplicer . Acknowledgments We thank K .- R . M ¨ uller , B . Sch ¨ olkopf , E . Georgii , A . Zien , G . Schweikert and G . Zeller for inspiring discussions .__label__Supplement|Website|Produce
However , COCO - QA questions are actually easier to answer than DAQUAR from a human point of view . This encourages the model to exploit salient object relations instead of exhaustively searching all possible relations . COCO - QA dataset can be downloaded at _CITE___label__Material|Data|Produce
In this prior - free framework , we obtain competitive shape and illumination estimation results under a variety of models and lighting conditions , requiring fewer assumptions than competing methods . The generic viewpoint assumption ( GVA ) [ 5 , 9 , 21 , 22 ] postulates that what we see in the world is not seen from a special viewpoint , or lighting condition . Figure 1 demonstrates this idea with the famous Necker cube example _CITE_ . A three dimensional cube may be observed with two vertices or edges perfectly aligned , giving rise to a two dimensional interpretation . Another possibility is a view that exposes only one of the faces of the cube , giving rise to a square .__label__Method|Algorithm|Use
If � kXL + U is equal to kXL , then there is no new sense in XU . Otherwise (� kXL + U > kXL ) new senses of w may be represented by the groups in which there is no instance from XL . We evaluated the ELP based model order identification algorithm on the data in English lexical sample task of SENSEVAL - 3 ( including all the 57 English words ) _CITE_ , and further empirically compared it with other state of the art classification methods , including SVM 10 ( the state of the art method for supervised word sense disambiguation ( Mihalcea et al ., 2004 )), a one - class partially supervised classification algorithm ( Liu et al ., 2003 ) 11 , and a semi - supervised k - means clustering based model order identification algorithm . The data for English lexical samples task in SENSEVAL - 3 consists of 7860 examples as official training data , and 3944 examples as official test data for 57 English words . The number of senses of each English word varies from 3 to 11 .__label__Material|Data|Use
More information and access to the web interface are provided online : http :// www . sanger . ac . uk / resources / databases / phenodigm . Related work A recent review undertaken by Bo ¨ rnigen et al . ( 18 ) ( summary of results available from _CITE_ ) showed that most of the investigated gene prioritization tools apply a ‘ guilt - by association ’ approach , e . g . Endeavour ( 19 ), GeneWanderer ( 20 ) or G2D ( 21 ). Using such an approach requires established gene – disease associations to use those to generate disease and gene profiles .__label__Supplement|Document|Produce
We then present two applications on real - world data , where we assess differences in the persistent homology of functions on 3D surfaces of lateral ventricles and corpora callosa with respect to different group assignments ( i . e ., age , demented / non - demented ). In all experiments , filtrations and the persistence diagrams are obtained using DrPaA2 , which can directly handle our types of input data . Source code to reproduce the experiments is available at _CITE___label__Method|Code|Produce
We used the data sets evaluated in [ 27 ] ( plant , nonpl , psortPos , and psortNeg ), which consist of either 3 or 4 classes and use 69 biologically motivated sequence kernels . Furthermore , we also considered the proteinFold data set of [ 28 ], which consists of 27 classes and uses 12 biologically motivated base kernels . _CITE_ The results are summarized in Table 1 : they represent mean accuracy values with one standard deviation as computed over 10 random splits of the data into training and test folds . The fraction of the data used for training , as well as the total number of examples , is also shown . The optimal value for the parameter 0 E 12i , i = 0 , 1 , ... , 81 was determined by cross - validation .__label__Material|Data|Use
For all the experiments , the 1 - Nearest - Neighbor ( 1NN ) algorithm is applied for classification and ten - fold cross validation is used for computing the classification accuracy . Datasets : We use three face datasets in our study : PIX , ORL , and PIE , which are publicly available . PIX ( available at _CITE_ ), contains 300 face images of 30 persons . The image size is 512 x 512 . We subsample the images down to a size of 100 x 100 = 10000 .__label__Material|Data|Use
All the sentences are collected from students ’ written essays . All the data are in traditional Chinese . The dictionary used in SSSP algorithm is SogouW _CITE_ dictionary from Sogou inc ., which is in simplified Chinese . The OpenCC converter is used to convert it into traditional Chinese . For similar character map the data set provided by ( Liu et al ., 2011 ) is used .__label__Material|Data|Use
This is also the method applied during the recent WMTs , where humans are asked to rank machine translation output by using APPRAISE ( Federmann , 2012 ), a software tool that integrates facilities for such a ranking task . In WMT , human MT evaluation is carried out by the MT development teams , usually computer scientists or computational linguists , sometimes involving crowd - sourcing based on Amazon ’ s Mechanical Turk . Being aware of the two communities , machine translation and translation studies , we took the available online data from the WMT2013 _CITE_ and tried to reproduce the ranking task with translation studies students for the English to German translations . The three questions we want to answer are : We concentrate on English - German data since the majority of our evaluators were native speakers of German and since , from a translation studies point of view , professional translation should be performed only into the mother tongue . 2 The WMT2013 English - German Data Before presenting the experimental setting and outcomes , we present the WMT data .__label__Material|Data|Use
genotyping missingness rate , ( b ) p values of Hardy – Weinberg test , and ( c ) allele frequencies . Protein - truncating variant annotation . We annotated 784 , 257 autosomal variants extracted from the mapping bim files provided by the UK Biobank using VEP version 87 and the LOFTEE plugin ( _CITE_ ) and identified 27 , 057 putative PTVs77 . We first removed 8118 PTVs specific to the UK BiLEVE Axiom Array or with missingness > 1 % among the subjects genotyped on the UK Biobank Axiom Array . Despite a missingness rate of 28 % on the Axium Biobank Array , we kept rs141992399 ( CARDS ) in the analysis .__label__Method|Tool|Use
While POS taggers such as TreeTagger are common , and there some supertaggers are available , notably that of Clark and Curran ( 2007 ) for CCG , no standard supertagger exists for HPSG . Consequently , we developed a Maximum Entropy model for supertagging using the OpenNLP implementation . _CITE_ Similarly to Zhang and Kordoni ( 2006 ), we took training data from the gold – standard lexical types in the treebank associated with ERG ( in our case , the July - 07 version ). For each token , we extracted features in two ways . One used features only from the input string itself : four characters from the beginning and end of the target word token , and two words of context ( where available ) either side of the target .__label__Method|Code|Use
We collected observed meteorological data ( including daily mean temperature , precipitation , solar radiation , wind speed , and vapor pressure ) for 1970 – 2010 from 117 stations within and around the Tibetan Plateau ( Table S8 ) from the China Meteorological Data Sharing Service System ( _CITE_ ). We calculated annual average temperature and annual precipitation for each station using the observed daily data . Since most stations have no directly observed evaporation data , we estimated the potential evapotranspiration of each station based on the Penman - Monteith model recommended by the Food and Agriculture Organization of the United Nations ( FAO ) [ 50 ].__label__Method|Tool|Use
We collected pairs of articles spanning from 1 / 1 / 2001 through 10 / 05 / 2005 . The corpus consists of 2 , 327 documents , with 0 - 8 documents per day . The corpus is available on our web page at _CITE_ cogcomp /. The English side was tagged with a publicly available NER system based on the SNoW learning architecture ( Roth , 1998 ), that is available on the same site . This set of English NEs was hand - pruned to remove incorrectly classified words to obtain 978 single word NEs .__label__Material|Data|Produce
To recognize the fact that H is “ entailed ” by T , we often need to use some background commonsense knowledge . For instance , in the example above it is essential that financing is an activity . The approach to recognizing textual entailment employed in Bos and Markert ( 2005 ) and implemented in the system Nutcracker _CITE_ can be summarized as follows : Related work is described in Akhmatova ( 2005 ); Fowler et al . ( 2005 ). The approach to the problem proposed in Baral et al .__label__Method|Tool|Introduce
For each pair , corresponding sequences were chosen from the respective pools at random . The volunteers were only told that the sequences were either real or artificial , and were asked to either select the real video or to indicate that they could not decide . The test is kept available on - line for validation at _CITE_ The results are shown in Table 1 . The first row , e . g ., shows that when comparing Brand ’ s model with the DPDS , people thought that the sequence generated with the former model was real in 5 cases , could not make up their mind in 7 cases , and thought the sequence generated with DPDS was real in 54 instances .__label__Method|Algorithm|Produce
We train MT systems using a significant portion of the training data and use these models as well as TM outputs to obtain a recommendation development data set . MT systems can be either in - house , e . g . a Moses - based system , or externally available systems , such as Microsoft Bing _CITE_ or Google Translate . For each sentence in the development data set , we have access to the reference as well as to the outputs for each of the MT and TM systems . We then select the best MT ( or TM ) output as the translation with the lowest TER score with respect to the reference and label the data accordingly .__label__Supplement|Website|Introduce
We could investigate which of those numerous quality statistics are the most useful . We could also see which metrics are useless or redundant and therefore do not have to be generated at all . Under the most optimistic PLOS ONE | _CITE_ April 25 , 2018 12 / 14 Estimation of the proportion of true discoveries in single nucleotide variant detection for human data scenario , it might turn out that using just QUAL results in a level of accuracy that is so high that none of other predictors are required and the same regression coefficients work well for both WES and WGS . Researchers who develop variant calling applications could employ the methodology outlined in this paper for a similar purpose . That would also allow them to supply their software with an accurate PPV estimation tool that would be of great advantage to the end user .__label__Supplement|Paper|Introduce
All the results shown in Table I are averaged over 10 random permutations of the training sequence . The columns marked 6Notice that B and C in part I do not satisfy this relationship . & apos ; Available on Y . LeCun & apos ; s home page : _CITE_ & quot ; Corn & apos ; s & quot ; give the total number of corrections made in the training phase for the 10 labels . The first three rows of Table 1 are taken from [ 4 , 12 , 13 ].__label__Supplement|Website|Produce
We evaluate the residual transfer network against state of the art transfer learning and deep learning methods . Codes and datasets will be available at _CITE___label__Material|Data|Produce
We thank the anonymous reviewers for their detailed comments . Our research was funded by the LEAD Graduate School ( GSC 1028 , _CITE_ ), a project of the Excellence Initiative of the German federal and state governments , and the European Commission ’ s 7th Framework Program under grant agreement number 238405 ( CLARA ).__label__Supplement|Website|Other
Interestingly , we show Wasserstein GAN [ 8 ] is the special case of the proposed MMD GAN under certain conditions . The unified view shows more connections between moment matching and GAN , which can potentially inspire new algorithms based on well - developed tools in statistics [ 15 ]. Our experiment code is available at _CITE___label__Method|Code|Produce
After getting all significant probes from SFC , we converted the probe level significance to gene level using an annotation file . Venn diagrams showed the significant genes with differential expression . Pathway and gene ontology ( GO ) enrichment analyses were performed by using the Database for Annotation , Visualization and Integrated Discovery ( DAVID ; _CITE_ ) with the Bonferroni correction - adjusted P - values < 0 . 05 [ 9 ]. Mouse transplantation data have been deposited in NCBI ’ s Gene Expression Omnibus [ 10 ] and are accessible through GEO Series accession no . GSE89340 .__label__Material|Data|Use
LanguageTool was developed by Naber ( 2003 ). It can run as a stand - alone program and as an extension for OpenOffice . Org1 and LibreOffice2 . LanguageTool is distributed through LanguageTool ’ s website : _CITE___label__Supplement|Website|Produce
Reddit comprises ‘ sub - reddits ’, which focus on specific topics . For example , http :// reddit . com / r / politics features articles ( and hence comments ) centered around political news . The current version of the corpus is available at : _CITE_ ACL - 2014 - irony . Data collection and annotation is ongoing , so we will continue to release new ( larger ) versions of the corpus in the future . The present version comprises 3 , 020 annotated comments scraped from the six subreddits enumerated in Table 1 .__label__Material|Data|Produce
In this section , we compare our proposed Bayesian DA algorithm with the commonly used DA technique [ 19 ] ( denoted as PMDA ) on several image classification tasks ( code available at : _CITE_ ). This comparison is based on experiments using the following three datasets : MNIST [ 20 ] ( containing 60 , 000 training and 10 , 000 testing images of 10 handwritten digits ), CIFAR - 10 [ 18 ] ( consisting of 50 , 000 training and 10 , 000 testing images of 10 visual classes like car , dog , cat , etc . ), and CIFAR - 100 [ 18 ] ( containing the same amount of training and testing samples as CIFAR - 10 , but with 100 visual classes ).__label__Method|Code|Compare
Many of these cell lines will grow as subcutaneous xenografts , thus cell lines sensitive to an agent in vitro were often subjected to further analyses in xenografts derived from those cell lines . The NCI - 60 panel has been extensively molecularly characterized , with data available for gene expression , DNA variation ( mutation and SNPs ), protein expression , DNA methylation , microRNA expression and metabolomics ( http :// dtp . cancer . gov / mtargets / mt_index . html ) [ 3 - 9 ]. The COMPARE algorithm ( _CITE_ compare . html ) allows investigators to correlate NCI - 60 drug activity profiles with all other open agents in the database and with molecular characteristics of the cells [ 10 ]. While the in vitro grown cells have been characterized , the corresponding subcutaneous xenografts had not . However , other studies have succeeded in molecular profiling of other xenografts [ 2 , 11 , 12 ].__label__Method|Algorithm|Introduce
This work is licensed under a Creative Commons Attribution 4 . 0 International License . The images or other third party material in this article are included in the article ’ s Creative Commons license , unless indicated otherwise in the credit line ; if the material is not included under the Creative Commons license , users will need to obtain permission from the license holder to reproduce the material . To view a copy of this license , visit _CITE_ Metadata associated with this Data Descriptor is available at http :// www . nature . com / sdata / and is released under the CC0 waiver to maximize reuse . SCIENTIFIC DATA 1 2 : 150040 1 DOI : 10 . 1038 / sdata . 2015 . 40 6__label__Supplement|License|Other
In ( Widdows et al ., 2002 ) tools are presented that visualize meanings of nouns as vector space representation , using LSA ( Deerwester et al ., 1990 ) and graph models using co - occurrences . There is also a range of text - based tools , without any quantitative statistics , e . g . Textpresso ( M ¨ uller et al ., 2004 ), PhraseNet and Walden _CITE_ . For searching words in context , Luhn ( 1960 ) introduced KWIC ( Key Word in Context ) which allows us to search for concordances and is also used in several corpus linguistic tools e . g . ( Culy and Lyding , 2011 ), BNCWeb , Sketch Engine ( Kilgarriff et al ., 2004 ), Corpus Workbench and MonoConc ( Barlow , 1999 ).__label__Method|Tool|Introduce
We are grateful to Shirley Pledger and Richard Arnold for the discussions about the Pledger and Arnold ( 2014 ) paper and to Maurizio Vichi for sharing his double k - means Matlab code . Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any__label__Supplement|License|Other
( iv ) Furthermore , with Precision - Recall - Gain curves it is possible to calibrate a model for Fp in the sense that the predicted score for any instance determines the value of p for which the instance is on the Fp decision boundary . ( v ) We give experimental evidence in Section 4 that this matters by demonstrating that the area under traditional Precision - Recall curves can easily favour models with lower expected F1 score than others . Proofs of the formal results are found in the Supplementary Material ; see also _CITE___label__Supplement|Document|Produce
We obtained surgical data from 66 Member States ( Table 3 ; available at : _CITE_ ). Using multiple imputation , we extrapolated the volume of surgery for each country without reported surgical data ( Table 4 ; available at : http :// www . who . int / bulletin / volumes / 94 / 3 / 15 - 159293 ). For the year 2012 , we estimated the total global volume to be 312 . 9 million operations – an increase of 38 . 2 % from an estimated 226 . 4 million operations in 2004 .__label__Material|Data|Use
RNA - seq reads for the wild - type AWC neurons are available in the NCBI Sequence Read Archive ( SRA ), under accession number SRP074082 ( _CITE_ SRP074082 ). RNA - seq reads for the two pools of whole C . elegans mixed - stage wild - type N2 larvae were previously published by Schwarz et al . ( 2012 ), and are available in the NCBI SRA under accession number SRP015688 ( http :// trace . ncbi . nlm . nih . gov / Traces / sra /? study = SRP015688 ).__label__Material|Data|Use
This naive approach to rule extraction from refined advice is shown here only to illustrate that it is possible to produce very useful domain - expert - interpretable rules from refinement . More efficient and accurate rule extraction techniques inspired by SVM - based rule extraction ( for example , [ 7 ]) are currently under investigation . Wargus _CITE_ is a real - time strategy game in which two or more players gather resources , build bases and control units in order to conquer opposing players . It has been widely used to study and evaluate various machine learning and planning algorithms . We evaluate our algorithms on a classification task in the Wargus domain developed by Walker et al .__label__Supplement|Media|Introduce
If each video is reviewed completely by two observers , nR nO will be 2 . 0 . Thus , the product nR nO reflects that observers may make repeated Trask et al . BMC Medical Research Methodology 2013 , 13 : 124 Page 4 of 14 _CITE_ observation of the same video frames , which is one way of improving precision of exposure estimations obtained by observation [ 7 ]. Substituting Eq . s ( 2 ) and ( 4 ) into Eq . ( 1 ) results in models specific to the observation method :__label__Supplement|Paper|Introduce
The X - ray crystallography data set gives us an idea of how complexes might look in a complete and accurate interaction network , while the Y2H data set gives us an idea of how complexes look in our real error - prone data . For the complexes from MIPS , we were only able to look at the induced graphs from the Y2H data . The code used for calculating the statistics of protein complexes can be found at _CITE_ complex - stats .__label__Method|Code|Produce
cgi ? cmd = userReg . Temporary login is provided for evaluation purposes such as browsing the interfaces or viewing example analysis results . HIVE login URL : _CITE_ evaluation userid : xlhive @ yahoo . com ; password : pilotHive5 . Users can also install HIVE on their own hardware or use HIVE - in - abox , which is a low - cost alternative to analyze NGS data using predetermined workflows . For additional details , users are encouraged to contact the HIVE team ( http :// hive . biochemistry . gwu . edu / dna . cgi ? cmd = contact ).__label__Supplement|Website|Use
Also , the number of words in a document D will usually be much smaller than the vocabulary size V . The final model , which we refer to as Document NADE ( DocNADE ), is illustrated in Figure 1 . A pseudocode for computing p ( v ) and the parameter learning gradients for a given document is provided in the supplementary material and our code is available here : _CITE___label__Method|Code|Produce
Additionally , convergence of CONCORD - ISTA and CONCORD - FISTA will be illustrated . Section 4 . 2 has timing results from analyzing a real breast cancer dataset with outliers . Comparisons are made to the coordinate - wise CONCORD implementation in gconcord package for R available at _CITE_ For implementing the proposed algorithms , we can take advantage of existing linear algebra libraries . Most of the numerical computations in Algorithms 1 and 2 are linear algebra operations , and , unlike the sequential coordinate - wise CONCORD algorithm , CONCORD - ISTA and CONCORD - FISTA implementations can solve increasingly larger problems as more and more scalable and efficient linear algebra libraries are made available .__label__Method|Code|Compare
We run regression experiments on 2008 US flight data with 2 million records and perform classification tests on MNIST using the latent variable model . We show that GPs perform better than many common models which are often used for big data . The proposed inference was implemented in Python using the Map - Reduce framework [ Dean and Ghemawat , 2008 ] to work on multi - core architectures , and is available as an open - source package _CITE_ . The full derivation of the inference is given in the supplementary material as well as additional experimental results ( such as robustness tests to node failure by dropping out nodes at random ). The open source software package contains an extensively documented implementation of the derivations , with references to the equations presented in the supplementary material for explanation .__label__Method|Code|Produce
Fig . 2 visualizes the structure learning process . _CITE_ This example is similar to that above but includes some uncorrelated random variables to show how they are treated by CorEx . We set b = 5 clusters of variables but we used m = 10 hidden variables . At each iteration , t , we show which hidden variables , Yj , are connected to input variables , Xi , through the connectivity matrix , a ( shown on top ).__label__Method|Algorithm|Introduce
Also , the viewpoint presented here is that of interlingua users who experience R & D for a given NL , and not of its authors . By ' high quality ' we mean ' at least allowing for readability and understandability by any user '. The UNL Project _CITE_ has been launched by the United Nations University to foster and ease international web communication by means of NLP systems . Its main strength lies on the development of the UNL , as a unique semantic ( or meaning ) representation that can be interchanged with the various languages to be integrated in the ICBMT system . In the UNL Project , plug - in software to encode NL texts onto UNL ones ( NL - UNL encoders ) and to decode UNL into NL texts ( UNL - NL decoders ) have been developed by R & D groups in their own native languages .__label__Method|Tool|Introduce
The best global factor is determined by crossvalidation . These results suggest that VRM with adaptive kernel widths can outperform state of the art classifiers on small training sets . b ) MNIST " 1 " versus other digits — A second test was performed using the MN1ST handwritten digits _CITE_ . We considered the sub - problem of recognizing the ones versus all other digits . The testing set contains 10000 digits ( 5000 ones and 5000 non - ones ).__label__Material|Data|Use
We believe that performance of this algorithm on our data reflects the state - of - the - art in content error correction . Next , we show how learner data and distribution of confusion pairs can be used to improve the performance of this algorithm . In our experiments , we use three publicly - available datasets of learner errors in AN combinations : the AN dataset extracted from the Cambridge Learner Corpus ( CLC ) _CITE_ and annotated with respect to the learner errors in the choice of adjectives and nouns ; the AN dataset extracted from the CLCFCE dataset ; and the set of errors in ANs that we have extracted for the purposes of this work from the training and development sets used in the CoNLL2014 Shared Task on Grammatical Error Correction . We discuss these datasets below . We use the dataset of AN combinations released by Kochmar and Briscoe ( 2014 ).__label__Material|Data|Use
Our implementation is in C ++, using the Bundle Methods for Risk Minimization ( BMRM ) of [ 18 ] as a base . Source code is available under the Mozilla Public License . _CITE_ In our first set of experiments we compared our model to published results on the Macro - F1 score . We strived to make our comparison as broad as possible , but we limited ourselves to methods with published results on public datasets , where the experimental setting was described in enough detail to allow us to make a fair comparison . We therefore compared our model to Canonical Correlation Analysis [ 3 ] ( CCA ), Binary Method [ 9 ] ( BM ), Classifier Chains [ 4 ] ( CC ), Subset Mapping [ 19 ] ( SM ), Meta Stacking [ 12 ] ( MS ), Ensembles of Binary Method [ 4 ] ( EBM ) , Ensembles of Classifier Chains [ 4 ] ( ECC ), Ensembles of Pruned Sets [ 11 ] ( EPS ) and Random K Label Subsets [ 10 ] ( RAKEL ).__label__Method|Code|Produce
Accordingly , there is a wide range of prices for EEG devices , from brain — computer interface systems designed for a specific task to medical - grade devices with hundreds of high quality electrodes . These measurement devices are all based on the same principle , neurons communicate through chemical neurotransmitters and electrical impulses , giving rise to electromagnetic waves . Electrodes are then used in EEG to measure oscillatory signals related to action PLOS ONE | _CITE_ May 24 , 2018 1 / 21 Collective signal improvement in an EEG headset Competing interests : The authors have declared that no competing interests exist . potential across different regions of the brain . In EEG devices , it is generally believed that most of the measured signal is provided by pyramidal neurons of the cortex [ 6 , 7 ].__label__Supplement|Paper|Use
Semantic These features explore the polysemy of target and source words , i . e . the number of senses existing as entries in a WordNet for a given target word ti or a source word si . We employ the Universal WordNet , _CITE_ which provides access to WordNets of various languages . Pseudo - reference This binary feature explores the similarity between the target sentence and a translation for the source sentence produced by another MT system . The feature is 1 if the given word ti in position i of a target sentence 5 is also present in a pseudo - reference translation R . In our experiments , the pseudo - reference is produced by Moses systems trained over parallel corpora .__label__Method|Tool|Use
Compared with its performance on CTB5 ( in Table 4 ), our Nonlocal & Cluster system also got 0 . 8 % improvement . All these results show that our approach can become more powerful when given more labeled training data . To better understand the linguistic behavior of our systems , we employed the berkeley - parseranalyser tool _CITE_ ( Kummerfeld et al ., 2013 ) to categorize the errors . Table 6 presents the average number of errors for each error type by our parsing systems . We can see that almost all the Worst numbers are produced by the Pipeline system .__label__Method|Tool|Use
The weights associated to feature functions are optimally combined using a discriminative training framework ( Och , 2003 ) ( Minimum Error Rate Training ( ME RT ), see details in Section 5 . 4 ), using the provided newstest2009 data as development set . om agiven word - aligned pair of sentences ( top ). 310 a word - aligned corpus ( using MGIZA ++ _CITE_ with default settings ) in such a way that a unique segmentation of the bilingual corpus is achieved , allowing to estimate the n - gram model . Figure 1 presents a simple example illustrating the unique tuple segmentation for The resulting sequence of tuples is further refined to avoid NULL words in the source side of the tuples ( 2 ). Once the whole bilingual training data is segmented into tuples , n - gram language model probabilities can be estimated .__label__Method|Tool|Use
While this is an exciting prospect , it must be appreciated that this study was limited in scope , and the experimental data was gathered over a period of several days in a relatively confined region . More extensive studies should be invoked to validate these conclusions , and ( if positive ) identify the boundaries of the correlations , as these will be dynamic in terms of spatial distribution and the relative impact of various influencing factors . Supplementary Materials : The supplementary materials are available online at _CITE_ Acknowledgments : The authors would like to acknowledge funding from SmartBay National Infrastructure Access Programme for funding for the project . The technical support provided by Shane Burke ( Smart Bay Ireland ) during the field campaign .__label__Supplement|Document|Produce
MLLE can recover the generating parameter perfectly up to an affine transformation . Next , we consider a data set containing N = 4400 handwritten digits (’ 2 ’-’ 5 ’) with 1100 examples of each class . The gray scale images of handwritten numerals are at 16 × 16 resolution and converted m = 256 dimensional vectors _CITE_ . The data points are mapped into a 2 - dimensional space using LLE and MLLE respectively . These experiments are shown in Figure 5 .__label__Material|Data|Use
We eliminated three habitats in which the mean and variance were 0 before or after spraying ( chicken trees , chicken houses , and open sheds ), leaving eight habitats . We focus here on the regression through the origin , as above . PLOS Neglected Tropical Diseases | _CITE_ November 30 , 2017 20 / 34 Chagas disease vector control and Taylor ’ s law Fig 6 . In Figueroa [ 34 ], TL described the relationship between y = log10 vand x = log10 m of the relative abundance of T . infestans ( A ), T . guasayana ( B ), and T . garciabesi ( C ) in 10 ( peri ) domestic habitats with positive mean abundance surveyed just before community - wide spraying with insecticides in October 2003 and during follow - up monitoring surveys of house / habitat infestations in which reinfested houses were selectively re - sprayed with insecticides in March and October 2004 and March 2005 . Each point represents the sample mean and sample variance of bug abundance for one habitat on a specified date .__label__Supplement|Paper|Introduce
We show a set of toy experiments to illustrate the general behavior of v - Arc . As base hypothesis class G we use the RBF networks of [ 11 ], and as data a two - class problem generated from several 2D Gauss blobs ( cf . Banana shape dataset from _CITE_ ). We obtain the following results :__label__Material|Data|Use
We take meta search as the target application , and use the LETOR [ 13 ] benchmark datasets in the experiments . LETOR is a public collection created for ranking research . _CITE_ There are two meta search datasets in LETOR , MQ2007 - agg and MQ2008 - agg . In addition to using them , we also composed a smaller dataset from MQ2008 - agg , referred to as MQ2008 - small , by selecting queries with no more than 8 documents from the MQ2008 - agg dataset . This small dataset is used to perform detailed investigations on the CPS model and other baseline models .__label__Material|Data|Use
This machine is then used to evaluate the systems , which makes the experiments directly reproducible in the future . System submissions are currently becoming increasingly popular in shared tasks . For example , the CoNLL 2015 shared task on shallow discourse parsing _CITE_ applies this technology . We plan to use the same system as the CoNLL task , TIRA ( Gollub et al ., 2012 ), which is already successfully applied in the PAN workshops on plagiarism detection . We propose a shared task for mining the argumentative structure in newspaper editorials .__label__Method|Algorithm|Introduce
We evaluate our mixture model on four different test sets . On the three most AAC - like test sets , we found substantial reductions in not only perplexity but also in potential keystroke savings when used in a predictive keyboard interface . Finally , to aid other AAC researchers , we have publicly released our crowdsourced AAC collection , word lists and best - performing language models _CITE_ . As we mentioned in the introduction , there are unfortunately no publicly available sources of genuine conversational AAC messages . We conjectured we could create surrogate data by asking workers on Amazon Mechanical Turk to imagine they were a user of an AAC device and having them invent things they might want to say .__label__Method|Algorithm|Produce
We conducted new analyses to inform future estimates . We found that acute watery diarrhoea was associated with 87 % ( 95 % CI 83 – 90 %) of U5 diarrhoea hospitalisations based on data from 84 hospital sites in 9 countries , and 65 % ( 95 % CI 57 – 74 %) of U5 diarrhoea deaths based on verbal autopsy reports from 9 country sites . We reanalysed data from the Global Enteric Multicenter Study ( GEMS ) and found 44 % ( 55 % in Asia , and 32 % in Africa ) rotavirus - positivity among U5 acute watery diarrhoea hospitalisations , and 28 % rotavirus - positivity among U5 acute PLOS ONE | _CITE_ September 11 , 2017 1 / 18 Estimating rotavirus deaths in children aged < 5 years watery diarrhoea deaths . 97 % ( 95 % CI 95 – 98 %) of the U5 diarrhoea hospitalisations that tested positive for rotavirus were entirely attributable to rotavirus . For all clinical syndromes combined the rotavirus attributable fraction was 34 % ( 95 % CI 31 – 36 %).__label__Supplement|Paper|Introduce
Population structure analysis . To explore their phylogenetic relationship , the whole - genome autosomal SNPs were extracted to construct the phylogenetic tree , and genotypes of sheep sequence were used to provide out - group information at corresponding positions . The neighbor - joining tree was constructed using the PHYLIP 3 . 696 software ( _CITE_ ) based on distance matrix methods55 . iTOL ( http :// itol . embl . de ) was used to illuminate and visualize the phylogenetic tree56 . For both of principal component ( PCA ) and population structure analysis , a thined SNPs dataset with a window of size 50 SNPs advanced by 5 SNPs at a time and an linkage - disequilibrium r2 threshold of 0 . 5 were filtered__label__Method|Tool|Use
Thus label propagation is especially suited when annotation data is extremely sparse . One reason for mincuts performing badly with few seeds is because they generate degenrate cuts . In order to demonstrate the ease of adaptability of our method for other languages , we used the Hindi WordNet _CITE_ to derive the adjective synonym graph . We selected 489 adjectives at random from a list of 10656 adjectives and this list was annotated by two native speakers of the language . The annotated list was then split 50 - 50 into seed and test sets .__label__Method|Tool|Use
Table 3 shows the performance of our Ngrambased system using the SMR technique . First row is the WMT07 baseline system which can be reproduced following the instructions in _CITE_ This baseline system uses a non - monotonic search . Second row shows the results of the Ngram - based system presented in section 2 using the weighted reordering graph trained with the best configuration found in the above section ( 200 statistical classes and an Ngram of length 5 ).__label__Supplement|Document|Produce
4 . The Conf . abstracts set data contains abstracts ( including papers and posters ) from six international conferences : CIKM , ICML , KDD , NIPS , SIGIR and WWW ( _CITE_ ). It has 3733 unique terms , around 173K observed words and an average of 46 unique terms per document . The data are from 2005 - 2008 .__label__Material|Data|Introduce
All data used here have been released to the public and are available through the Planetary Data System ( _CITE_ ) and other sources . 2 . 2 . 1 . CheMin Diffraction patterns for the Windjana powder , as two - dimensional images from the CheMin CCD [ Blake et al ., 2012 ], were acquired for a total of 23 h over sols 623 – 632 .__label__Material|Data|Use
They include samples from individuals X310763260 , X311245214 , X316192082 , X316701492 , and X317690558 [ 28 ], designated within this article as HF1 – 5 , respectively . Only samples labeled as “ Whole ” ( samples preserved by flash - freezing ) were selected for analysis [ 28 ]. The published wastewater sludge microbial community datasets ( MG and MT ) were obtained from NCBI Bioproject with the accession code PRJNA230567 ( _CITE_ ). These include samples A02 , D32 , D36 , and D49 , designated within this article as WW1 – 4 , respectively [ 43 ]. The published biogas reactor microbial community data set ( MG and MT ) was obtained from the European Nucleotide Archive ( ENA ) project PRJEB8813 ( http :// www . ebi . ac . uk / ena / data / view / PRJEB8813 ) and is designated within this article as BG [ 29 ].__label__Material|Data|Use
A word graph is here described as a directed acyclic graph G = ( V , E ) with one root node n0 E V . Edges are labeled with tokens ( words or translation units ) and optionally with accumulated scores . We will use ( ns ( ne ′′ t ′′ s )), to denote an edge starting at node ns and ending at node ne , with token t and score s . The file format of word graphs coincides with the graph file format recognized by the CARMEL _CITE_ finite state automata toolkit . We can mainly find two applications for which word graphs are used as input of an SMT system : the recognition output of an automatic speech recognition ( ASR ) system ; and a reordering graph , consisting of a subset of the whole word permutations of a given input sentence . In our case we are using the input graph as a reordering graph .__label__Method|Tool|Use
onsDimensional ityreduct ionmeth odst hatt ake lab el s orparamet ersi nto acco unth averecen tlyfo u ndaresurge nc einintere st . Our st udy wasmotiva te dby thespec ificprobl emsrela te dtoelectrophysiol og - i cald atase ts . Them ain ai mof ourmethod — demix ingparame ter dependenc ie sofhigh - dimensio nald ata sets — ma y beuse fu linot hercont ex t aswe ll . V erysimi larprobl ems ar is einf MRIda ta , for in - stan ce , andd PCAco uldprov i de ause fulalternat iv etoot herdimensional ityreduct ionmeth odss uc has C CA , P LS , orSupervi sed PCA [ 1 , 12 , 5 ]. Furthermo re , thegene ral ai mofdemix ingdependenc iesco uldlik el ybeexten de dtoot hermeth ods ( s uc hasI CA ) aswe ll . Ultimate ly , we seed PC A asap ar - ticu lard atavisualizat iontechni quet hatw illpr oveuse fu l ifademix in gofparame terdependenc iesa id sinunderstand ingda ta . The source code both for Python and Matlab can be found at _CITE_ a__label__Method|Code|Produce
The code used in this work is provided at _CITE_ There are two main scripts in this folder - 1 ) joptb88vdw . py and 2 ) master . py . The joptb88vdw . py script heavily utilizes the Pymatgen8 and ASE42 codes for file and data management .__label__Method|Code|Produce
The top graphs in Figure 7 ( a ) display the original timeseries ( true change points were manually annotated ) and change scores obtained by KLIEP and LSDD . The graphs show that the LSDD - based change score indicates the existence of change points more clearly than the KLIEP - based change score . Next , we use a dataset taken from the Human Activity Sensing Consortium ( HASC ) challenge 2011 ( _CITE_ ), which provides human activity information collected by portable three - axis accelerometers . Because the orientation of the accelerometers is not necessarily fixed , we take the 22 - norm of the 3 - dimensional data . The HASC dataset is relatively simple , so we artificially added zero - mean Gaussian noise with standard deviation 5 at each time point with probability 0 . 005 .__label__Material|Data|Use
However these are difficult to develop and domain sensitive . To surmount these obstacles , application of machine learning approaches to NER became a research subject . Various state - of - the - art machine learning algorithms such as Maximum Entropy ( Borthwick , 1999 ), AdaBoost ( Carreras et al ., 2002 ), Hidden Markov Models ( Bikel et al ., ), Memory - based Based learning ( Tjong Kim Sang , 2002b ), have been used _CITE_ . ( Klein et al ., 2003 ), ( Mayfield et al ., 2003 ), ( Wu et al ., 2003 ), ( Kozareva et al ., 2005c ) among others , combined several classifiers to obtain better named entity coverage rate . Nevertheless all these machine learning algorithms rely on previously hand - labeled training data .__label__Method|Algorithm|Introduce
These approaches are representative of different solutions that have been proposed in the literature This work is licensed under a Creative Commons Attribution 4 . 0 International Licence . Page numbers and proceedings footer are added by the organizers . License details : _CITE_ ( Pradhan et al ., 2013 ), which can be broadly classified in the following types :__label__Supplement|License|Other
Some data types cannot be fully shared ( e . g ., EHR data — see rule 5 ), but most algorithms and summary results / statistics are shareable . Each of these types of open data necessitates a different platform for data sharing . Figshare ( _CITE_ ) allows users to share data involving published figures . Github ( https :// github . com /) allows users to share code that is in development or published . For code that is well developed , open - source packages can be created , for example , an R library , which can be deposited in CRAN or bioconductor .__label__Method|Tool|Introduce
When working with fully - connected models we use stochastic GRU - style state updates rather than the stochastic residual updates in Eq . 7 . Exhaustive descriptions of the modules can be found in our code at : _CITE_ These TD modules represent each conditional p ( zi | zi − 1 , ..., z0 ) in Eq . 1 using p ( zi | hti ).__label__Supplement|Document|Produce
At the same time , it is also usually able to correctly estimate the depth of large and texture - less planar regions ( but , see column 6 for an example failure case ). Our overall inference method ( network predictions and globalization ) takes 24 seconds per - image when using an NVIDIA Titan X GPU . The source code for implementation , along with a pre - trained network model , are available at _CITE___label__Method|Code|Produce
While distribution is broadly similar , i2b2 had a higher percentage of duration expressions while THYME had many prepostexp expressions , which in i2b2 were annotated as the date category . We implemented a variety of systems in an attempt to empirically evaluate the best way to model the time span classification task . For all systems , the temporal expression extractor is implemented within Apache cTAKES _CITE_ ( clinical Text Analysis and Knowledge Extraction System ) ( Savova et al ., 2011 ), making use of its components for feature generation as well as its interface to the source general - domain NLP system ClearTK ( Bethard et al ., 2014 ) which in turn interfaces with different machine learning libraries , including LibSVM ( Chang and Lin , 2011 ) and CRFSuite ( Okazaki , 2007 ). We developed three sequence - based models for this task , each with different perceived strengths . The first system is perhaps the simplest , a standard BIO ( Begin - Inside - Outside ) tagger using an off the shelf support vector machine ( SVM ) classifier ( Cortes and Vapnik , 1995 ).__label__Method|Tool|Use
Finally , the design of a case study in strategy comparison is described . All analyses were performed using the R statistical programme , version 3 . 1 . 1 [ 19 ]. All computational tools for the comparison of modelling strategies can be found in the “ apricom ” package , available within the CRAN package repository ( _CITE_ ). A framework for strategy comparison It was proposed by Pestman et al . [ 17 ] that different strategies for linear regression model building could be compared prior to selecting a final strategy by means of a simple framework .__label__Method|Code|Use
Two final learning blocks , two 30 min memory blocks and one novel pairs block were missing from experiment 1 . All error bars in the figures are standard error of the mean ( SEM ). MATLAB ( RRID : SCR_001622 ) was used for data processing ( code available at _CITE_ Grogan , 2017 ), and SPSS ( RRID : SCR_002865 ) for statistical tests . A copy of the code is available at https :// github . com / elifesciences - publications / Effects - of - dopamine - on - RL - consolidation - in - PD .__label__Method|Code|Use
The overall system yields the precision , recall and F - measure values of 90 . 26 %, 71 . 91 % and 80 . 05 % respectively for the test dataset . Twitter has seen a phenomenal growth in the number of users during the last few years . Over 500 million user accounts have been registered with it with approx 302 million active users _CITE_ . Amount of user generated contents over the web would be unarguably enormous i . e . almost 500 million tweets per day .__label__Supplement|Website|Introduce
In this paper we exploit this novel connection to show an interesting application of the SVM setup for identfying large dense subgraphs . More specifically we make the following contributions . ∗ Relevant code and datasets can be found on _CITE___label__Material|Data|Produce
The Facility Profile is completed by each facility that provides dialysis to ESKD patients on December 31st of each year . CORR publishes an Annual Data Report that provides the latest data on dialysis , organ transplants , waiting list and donors . The latest report issued in 2014 , reports on data from 2003 – 2012 and is available on CIHI ’ s website ( _CITE_ ). The Center - Specific Reports on Clinical Measures reports are new series of reports , released in 2014 , designed for dialysis centres and derived primarily from the clinical measures captured in the annual follow - up survey . This includes patient demographics , comorbidities , and laboratory and clinic indicator comparisons for the center , the province and Canada .__label__Supplement|Website|Introduce
Then we evaluated each edge for a set of conditions . Edges were considered to be added to the autonomic dysfunction network ( absent for control but present for the autonomic dysfunction PLOS Computational Biology | _CITE_ July21 , 2017 12 / 39 Data - driven modeling of multi - organ networks driving physiological dysregulation phenotype ) if the following conditions were satisfied :__label__Supplement|Paper|Extent
Another important aspect of multi - class settings is that the use of more classes which is discriminated by the BCI device only at lower accuracy is likely to confuse the user . Current BCI research strives for enhanced information transfer rates . Several options are available : ( 1 ) training of the BCI users , which can be somewhat tedious if up to 300 hours of training would be necessary , ( 2 ) invasive BCI techniques , which we consider not applicable for healthy human test subjects , ( 3 ) improved machine learning and signal processing methods where , e . g ., new filtering , feature extraction and sophisticated classifiers are constantly tuned and improved _CITE_ , ( 4 ) faster trial speeds and finally ( 5 ) more classes among which the BCI user is choosing . This work analysed the theoretical and practical implications of using more than two classes , and also psychological issues were shortly discussed . In essence we found that higher a ITR is achieved with three classes , however , it seems unlikely that it can be increased by moving above four classes .__label__Method|Algorithm|Introduce
In this paper we detail our submission to SemEval Task 2 . We use an improved and revised version of the system presented in our SemEval 2014 submission ( Gupta et al ., 2014 ). As in Gupta et al ., 2014 , we employ a Machine Learning ( ML ) method which exploits available NLP technology , adding features inspired by deep semantics ( such as parsing and paraphrasing ) with distributional Similarity Measures , Conceptual Similarity Measures , Semantic Similarity Measures and Corpus Pattern Analysis _CITE_ ( CPA ). The remainder of the paper is structured as follows . Section 2 describes our approach , i . e .__label__Method|Algorithm|Extent
They lead to improved semi - supervised learning peformance and improved sample generation . We hope that some of them may form the basis for future work , providing formal guarantees of convergence . All code and hyperparameters may be found at _CITE___label__Method|Code|Produce
alization window showing also a graphical representation of the gene domain composition ( Figure 3 ). For NRPS / PKS cluster types , the predicted peptide monomer composition and its corresponding SMILES formula are specified . The predicted chemical structure is displayed as well using the SMILES Depictor web service ( _CITE_ ). Below the graphical representation of the predicted antiSMASH cluster , a summary of MIBiG Clusters similarities , BGC gene composition as well as tailoring cluster similarities are given . This last item relies on a knowledge database provided with antiSMASH about tailoring clusters already described in known BGCs and associated with publications .__label__Supplement|Website|Use
We created 4 , 000 identical concepts ( four for each leaf node ) using the protocol above , and recruited participants online through Amazon Mechanical Turk ( AMT , _CITE_ ) to obtain the human ground truth data . For each concept , an AMT HIT ( a single task presented to the human participants ) is formed with five example images and twenty query images , and the participants were asked whether each query belongs to the concept represented by the examples . Each HIT was completed by five unique participants , with a compensation of $ 0 . 05 USD per HIT .__label__Method|Tool|Use
In this study , the analyses were performed on a data set consisting of hyper - variable V6 sequences of the 16S rRNA gene , which were obtained from the application of 454 MPTS on temperate subtidal sandy samples at three sediment depth layers ( 0 – 15 cm depth , with a 5 - cm interval ) taken over 2 years ( 2005 – 2006 ). Detailed sample processing and DNA extraction has been described earlier ( 10 ) and the 454 MPTS of the extracted DNA was processed as described previously ( 5 ). The output from 454 MPTS was retrieved from the publicly available Visualization and Analysis of Microbial Populations Structure ( VAMPS ) web site ( _CITE_ ). An automatic annotation pipeline [ Global Alignment for Sequence Taxonomy ( GAST ) ( 5 )] using several known databases ( Entrez Genome , RDP and SILVA ) allowed the taxonomic assignment of the sequences . Despite the limitations of current databases , only 6 % of sequences from this data set were not taxonomically identified at all .__label__Supplement|Website|Use
Members are typically affiliated with either the Democratic Party or the Republican Party . Congressional election and Presidential election coincide every four years . The Corpus We use a corpus of public statements released by members of Congress in both the Senate and The House of Representatives , collected by Project Vote Smart _CITE_ . An example of a public statement is presented in Figure 1 . In this work we use all individual statements and press releases in a span of four years ( 20102013 ), a total of 134000 statements made by 641 representatives .__label__Material|Data|Use
; licensee BioMed Central Ltd . This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http :// creativecommons . org / licenses / by / 2 . 0 ), which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited . The Creative Commons Public Domain Dedication waiver ( _CITE_ ) applies to the data made available in this article , unless otherwise stated . Hutchins et al . Cell Regeneration 2014 , 3 : 1 Page 2 of 6 http :// www . cellregenerationjournal . com / content / 3 / 1 / 1 the major analytical frameworks for genomic analysis .__label__Supplement|License|Other
Operators : We implemented 1D calibrators and multilinear interpolation over a lattice as new C ++ operators in TensorFlow [ 15 ] and express each layer as a computational graph node using these new and existing TensorFlow operators . Our implementation is open sourced and can be found in _CITE_ We use the Adam optimizer [ 16 ] and batched stochastic gradients to update model parameters . After each batched gradient update , we project parameters to satisfy their monotonicity constraints .__label__Method|Code|Produce
All data used here have been released to the public and are available through the Planetary Data System ( _CITE_ ) and other sources . 2 . 2 . 1 . CheMin Diffraction patterns for the Windjana powder , as two - dimensional images from the CheMin CCD [ Blake et al ., 2012 ], were acquired for a total of 23 h over sols 623 – 632 .__label__Method|Tool|Use
All the model parameters have a physical interpretation and thus expert knowledge was used to set priors which produce realistic wind fields . We will also use ( 10 ) to help set ( hyper ) priors using real data in Z °. MCMC using the Metropolis algorithm ( Neal , 1993 ) is used to sample from ( 10 ) using the NETLAB _CITE_ library . Convergence of the Markov chain is currently assessed using visual inspection of the univariate sample paths since the generating parameters are known , although other diagnostics could be used ( Cowles and Carlin , 1996 ). We find that the procedure is insensitive to the initial value of the GP parameters , but that the parameters describing the location of the front ( Of , cif ) need to be initialised & apos ; close & apos ; to the correct values if the chain is to converge on a reasonable time - scale .__label__Method|Code|Use
The synset 00198451 - n is translated as 晋什 jìnshén , which should have been 晋升 jìnsheng ‘ promotion ’. 00198451 - n promotion “ act of raising in rank or position ” ( iii ) Need M de / A de to match the English POS The synset 01089369 - a is an adjectival , but the translation 兼职 jidnzhí ‘ part time ’ is a verb / noun , so we add 的 de to it ( 1 . 3 ). 01089369 - a part - time ; part time “ involving less than the standard or customary time for an activity ”: parttime employees ; a part - time job To improve the coverage and accuracy of COW , we make reference not only to many authoritative bilingual dictionaries , such as The American Heritage Dictionary for Learners of English ( Zhao , 2006 ), The 21st Century Unabridged EnglishChinese Dictionary ( Li , 2002 ), Collins COBUILD Advanced Learner ' s English - Chinese Dictionary ( Ke , 2011 ), Oxford Advanced Learner ' s EnglishChinese Dictionary ( 7th Edition ) ( Wang , Zhao , & Zou , 2009 ), Longman Dictionary of Contemporary English ( English - Chinese ) ( Zhu , 1998 ), etc ., but also online bilingual dictionaries , such as iciba _CITE_ , youdao , lingoes10 , dreye11 and bing12 . For example , the English synset 00203866 - v can be translated as 变坏 biàn huài ‘ decline ’ and 恶化 èhuà ‘ worsen ’, which are not available in the current wordnet , so we added them to COW . 00203866 - v worsen ; decline “ grow worse ”: Conditions in the slum worsened PWN groups nouns , verbs , adjectived and adverbs into synonyms ( synsets ), most of which are linked to other synsets through a number of semantic relations .__label__Material|Data|Use
Together , these data indicate that temperature may be a critical factor in the marked changes in brain size during evolution in different climate conditions . Specifically , Yu et al . BMC Evolutionary Biology 2014 , 14 : 178 Page 4 of 14 _CITE_ for a given body mass , warmer living conditions should result in larger brains . In addition to this temperature - driven brain enlargement , the difference in brain sizes of endotherms and ectotherms increases as body size increases ( Figure 1D ). For small body sizes ( below 1 gram ), the brains of endotherms are atleast 4 times larger than those of ectotherms ( black line in Figure 1D ).__label__Supplement|Paper|Introduce
The advantage of our method is that we can jointly learn the optimal feature representation and the optimal domain transformation parameter , which are aware of the subsequent transductive inference procedure . Following the standard evaluation protocol in the unsupervised domain adaptation community , we evaluate our method on the digit classification task using MNIST [ 19 ] and SVHN [ 21 ] as well as the object recognition task using the Office [ 25 ] dataset , and demonstrate state of the art performance in comparison to all existing unsupervised domain adaptation methods . Learned models and the source code can be reached from the project webpage _CITE___label__Method|Algorithm|Produce
CHDS : Data contributed for this submission are available on request from the CHDS ( john . horwood @ otago . ac . nz ). Colaus / PsyCoLaus : Data from the CoLaus / PsyCoLaus study can be requested according to the procedure described on the CoLaus website ( _CITE_ htm ). ELSA : ELSA data are made available through the ESDS website ( http :// www . elsa - project . ac . uk / availableData ).__label__Material|Data|Use
Simulation . To evaluate the lower bound in Theorem 3 . 1 and its approximation for B = 1 , we simulate the first - timestamp estimator on regular trees . _CITE_ Figure 2 illustrates the simulation results for B = 1 compared to the approximation above . Each data point is averaged over 5 , 000 trials . Empirically , the lower bound appears to be tight , especially as d grows .__label__Method|Code|Produce
Especially , SNF is demonstrated to outperform other integrative methods like iCluster [ 31 ] which is based on pre - selection of genes . Direct concatenation was implemented by the matrix concatenation operation in Matlab . The Matlab code of SNF was downloaded from _CITE_ Evaluating iBFE on the DNA methylation , mRNA expression and miRNA expression datasets of lung and kidney cancers produced by TCGA The DNA methylation , mRNA expression and miRNA expression datasets of lung squamous cell carcinoma__label__Method|Code|Produce
[ 20 ], profiling of N - glycosylated proteins in the serum of advanced breast cancer patients was performed in order to discover serum biomarkers for chemoresistance . Twenty three proteins were identified to be differentially expressed between patients defined as sensitive and patients defined as resistant to docetaxel and doxorubicin treatment . The expression pattern of several proteins was Abelson BMC Bioinformatics 2014 , 15 : 53 Page 7 of 10 _CITE_ later validated in independent samples . Interestingly , in our analysis we found transcripts encodes to 10 out of the 23 aforementioned proteins ( Additional file 1 ). Even though patients bearing different type of cancer and under different treatment regimen were investigated , it is very unlikely that these results are consequence of chance ( p = 9 . 29e - 20 by hypergeometric probability density test ); This is further supported by other studies , which demonstrate elevated expressions of different serum proteins found in our analysis and in patients with chemoresistant tumors [ 21 , 22 ].__label__Supplement|Paper|Introduce
The effect of filtering using the age - based threshold was evaluated by comparing HRV indices before and after filtering of the edited data set and the unedited data set , respectively . Karlsson et al . BioMedical Engineering OnLine 2012 , 11 : 2 Page 4 of 12 _CITE___label__Supplement|Paper|Introduce
By this definition , all 6 strings in TABLE 2 are often thought as a word , but they are not MSUs in our view . Their corresponding MSU forms are shown in TABLE 2 . We collect all the MSUs from the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff _CITE_ . We choose the Peking University ( PKU ) data because it is more fine - grained than all other corpora . Suppose we represent the segmented data as L ( In our case L is the PKU word segmentation data ), the MSU selecting algorithm is shown in TABLE 3 .__label__Material|Data|Use
3b and Fig . 3c ). We also implemented _CITE_ the distributed algorithms in Spark [ 9 ], an open - source cluster computing system . The DP - means and BP - means algorithms were initialized by pre - processing a small number of data points ( 1 / 16 of the first Pb points )— this reduces the number of data points sent to the master on the first epoch , while still preserving serializability of the algorithms . Our Spark implementations were tested on Amazon EC2 by processing a fixed data set on 1 , 2 , 4 , 8 m2 . 4xlarge instances .__label__Method|Code|Produce
The following information was supplied regarding data availability : All data and code are available on GitHub : https :// github . com / jbloomlab / phydms . Detailed documentation is at http :// jbloomlab . github . io / phydms . Hilton , Doud , and Bloom , bioRxiv , 2017 . _CITE___label__Supplement|Paper|Introduce
To allow full propagation of parametric uncertainty , we used an objective Bayesian approach , taking flat prior distributions in the absence of data and informative priors only when suitable external data were available . We used as many data as were available from these studies , including some which would not be available in other settings using only 1 source of data . Full details on the statistical methods used and the distributions of key parameters can be found in the Web Appendix ( _CITE_ ). Method 1 : paired serologic surveys . To estimate infection rates from paired serologic surveys , we defined overall seroconversion as a 4 - fold or greater rise in titer on hemagglutination inhibition ( HAI ) testing between baseline titers and subsequent samples for the same individual .__label__Supplement|Document|Produce
The pathway gene set database ( _CITE_ , file Human_GOBP_AllPathways_no_GO_iea_April_01_2017_symbol . gmt ) 13 from the Bader lab dated April 1 , 2017 was used in all analyses . This database contains pathways from Reactome61 , NCI Pathway Interaction Database62 , GO ( Gene Ontology ) biological process63 , HumanCyc64 , MSigdb65 , NetPath66 and Panther67 . For GO , terms inferred from electronic annotation were excluded from our analyses .__label__Material|Data|Use
The samples were filtered through 0 . 7 um Whatman GF / F glass fibre filters ( 25 mm ) immediately after sampling , and then the filters were stored in aluminium foil and kept at - 20 ° C . Filters with chl - a were extracted with 90 % acetone and sonicated for 10 minutes , and then extracted at 40 ° C in the dark for 24 hours . The fluorescence method was used to measure the chl - a concentration with a Turner Designs model 10 - AU fluorometer within 15 days from the sampling date [ 17 , 18 , PLOS ONE | _CITE_ January 12 , 2018 3 / 17 Chl - α in the Beibu Gulf 19 ]. These in - situ data are used to validate the MODIS - derived chl - a concentrations . For the comparison between the satellite and in - situ data in the Beibu Gulf , we selected MODIS L2 data ( 1 km ) taken from the same locations as in - situ data ( 9 sampling stations in Fig 1B ) on the same day .__label__Supplement|Paper|Introduce
At rates above 0 . 5 , saturation diminishes the signal about the true state frequencies at the root . The most accurate inference of both root and stationary state frequencies occurs at intermediate rates ( 0 . 05 to 0 . 2 ), but even in those cases , the difference between root and stationary frequencies is often underestimated . When the trees are not fixed to the true topology and very high rates are assumed ( two or more expected substitutions between root and tips ), we observe false positives in the recovery of the directional model , and the same is the case with entirely random data ( Supplementary File S4 , available on Dryad at _CITE_ ). Further analyses show that under such circumstances , that is without reliable information about topology and relative branch lengths , and in combination with an inadequate branch length prior ( we used an exponential distribution with rate 10 throughout ), the analyses recover a balanced tree with extreme directionality because this parameter combination increases the probability of observing a large number of substitutions on short branches .__label__Material|Data|Produce
For example , selection of negative examples based on cohyponyms was found useful for Cr classifiers in R10 , while random examples were used in the rest of the cases . We used SVMperf ( Joachims , 2006 ) with a linear kernel and binary feature weighting . For querying the corpus we used the Lucene search engine _CITE_ in its default setting . Up to 150 positive examples were retrieved for each classifier , with 5 examples set as the required minimum . This resulted in generating 100 % of the hypothesis classifiers for both datasets and 95 % and 70 % of the rule classifiers for R10 and 20NG , respectively .__label__Method|Tool|Use
Fig 10 shows the two most dominant bias types for the four evaluated sites on sample A . These bias types and their associated transcript length histograms are similar for all sites . One of the clusters consists of distributions concentrated around the middle of comparatively short transcripts while the other cluster contains slightly 3 ’ biased distributions PLOS Computational Biology | _CITE_ May 15 , 2017 18 / 25 Mixture models yield accurate transcript concentration estimates from RNA - Seq data for comparatively long transcripts . Also the remaining clusters are similar for all sites as can be seen from Fig J to Fig P in S2 Appendix . Biases are also similar across samples as can be seen for Fig M to Fig P in S2 Appendix which show clusters for samples A to D of BGI .__label__Supplement|Paper|Introduce
that all random variables are normalized to have expectation 0 and variance 1 . The objective R2z , X is monotone increasing , but not necessarily submodular [ 6 ]. We use two data sets from _CITE_ The budget k is set to { 10 , 12 , ... , 20 }. For estimating R2 in the optimization process , we use a random sample of 1000 instances .__label__Material|Data|Use
in data recording , storage , and coding methods in recent decades . Thanks to corpora and tools such as those developed in the context of the CHILDES project ( _CITE_ ), researchers in areas such as morphology and syntax have enjoyed a convenient and powerful method to analyze the morphosyntactic properties of adult languages and their acquisition by first and second language learners . In the area of phonetics , the Praat system ( http :// www . fon . hum . uva . nl / praat /) has expanded our abilities to conduct phonological modeling , computational simulations based on a variety of theoretical approaches , and articulatory synthesis . In this rapidly - expanding software universe , phonologists interested in the organization of sound systems ( e . g .__label__Method|Tool|Introduce
It is well known that most of the problems arising from learning on class - imbalanced data arise in the region where the two class - specific densities overlap . When the difference between the class - specific densities is large enough , the class - imbalance does not cause biased classification for the classifiers that we considered , even in the highdimensional setting [ 7 ]. The other reason is that when Blagus and Lusa BMC Bioinformatics 2013 , 14 : 106 Page 13 of 16 _CITE_ a very large number of variables is measured for each subject , in most situations the vast majority of variables do not differentiate the classes and the signal - to - noise ratio can be extreme . For example , Sotiriou et al . [ 36 ] identified 606 out of the 7 , 650 measured genes as discriminating ER + from ER - samples in their gene expression study ; at the same time ER status was the known clinico - pathological breast cancer phenotype for which the largest number of variables was identified ( 137 out of the 7 , 650 genes discriminated grade , 11 out of the 7 , 650 node positivity , 3 out of the 7 , 650 tumor size and 13 out of the 7 , 650 menopausal status ).__label__Supplement|Paper|Introduce
Unfortunately , the results for ME and TBL when training on one million words cannot be reported in this paper . The learning algorithms will still be occupied after the deadline for the final version of this manuscript . The results will be published on the author ' s web page _CITE_ as soon as the learners have finished their struggle . The total error rate , i . e . the percentage of erroneous tags , is shown in Figure 2 for each classifier .__label__Supplement|Document|Introduce
CTCF Chip - seq data used in the Shh region was adquired from Encode _CITE_ and painted in the representative model ( mm9 data ) with a black - to - white gradient , from high to low score .__label__Material|Data|Use
Third , the expressions in the ETL rules must be in the language of the local DBMS . For rules to be used and reused across different DBMSs , a rule conversion tool that automatically translates operators and functions from one SQL dialect into another is needed . Open source tools , such as SQL Renderer from the OHDSI community , _CITE_ could be a potential solution to this problem . Finally , even though rules are composed in plain text format , a graphical presentation of the structure of the rule will improve ETL rule maintenance and help ETL team members understand complex rules created by others . Data harmonization is an important step towards data interoperability which supports the progression of comparative effectiveness research .__label__Method|Tool|Use
To support such analysis and capture these patterns comprehensively , data with high temporal and spatial resolution and the low signal - to - noise ratio is needed . Recent advances in invasive monitoring technologies such as electrocorticography ( ECoG ) have risen to this challenge by recording high - resolution electrical signals © The Regents of the University of California . 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4 . 0 International License ( _CITE_ ), which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the Creative Commons license , and indicate if changes were made . The Creative Commons Public Domain Dedication waiver ( http :// creativecommons . org / publicdomain / zero / 1 . 0 /) applies to the data made available in this article , unless otherwise stated . The Regents of the University of California BMCBioinformatics 2017 , 18 ( Suppl 6 ): 236 Page 2 of 45 captured by electrodes placed directly on the cortical surface of the brain .__label__Supplement|License|Other
Since , for purposes of the contamination warning system , the syndromes monitored are the same for both acute and gradually developing events caused by biological , radiological , or chemical contaminants , the timeliness of anomaly detection coupled with subject matter expertise during subsequent alert investigations determines what type of contaminant ( s ) may be responsible . Haas et al . International Journal of Health Geographics 2011 , 10 : 22 Page 3 of 10 _CITE_ An example timeline for a contamination scenario is depicted in Figure 1 , demonstrating symptom onset , actions of an exposed individual , and the unique data outputs which can be analyzed by public health surveillance systems . The timeline illustrated in this figure is based on the expected symptom onset and potential health - seeking actions that would occur following exposure to a carbamate pesticide . Note that Poison Control Center ( PCC ) data entry is initiated by poison control specialists ( into the National Poison Data System [ 10 ]) immediately upon receipt of a call to the hotline .__label__Supplement|Paper|Introduce
It is , therefore , interesting to see how the minimum degree of rate fluctuation depends on the non - Poissonian feature of spike trains . In this study , we investigate the extent to which the non - Poissonian feature of spike trains affects the encoding efficiency of rate fluctuations . In addition , we address the question of how the de ∗ _CITE_ tectability of rate fluctuations depends on the encoding efficiency . For this purpose , we introduce the Kullback - Leibler ( KL ) divergence to measure the encoding efficiency , and assume that spike sequences are generated by time - rescaled renewal processes . With the aid of analytical and numerical studies , we suggest that the lower bound of detectable rate fluctuations , below which the empirical Bayes decoder cannot detect the rate fluctuations , is uniquely determined by the KL divergence .__label__Supplement|Document|Produce
To this aim , the author classifies the input into 3 - classes ( strongly - negative , ambivalent , and strongly - positive ), 4 classes ( strongly - negative , weakly - negative , weaklypositive and strongly - positive ) and 5 - classes ( strongly - negative , weakly - negative , ambivalent , weakly - positive and strongly - positive ). The results decrease considerably with the number of classes , from 62 % of accuracy for 3 - classes to 38 % of accuracy for 5 - classes . The evaluation of the system has been carried out using two corpora from two very distinct domains : the Sentence Polarity Movie Review Dataset _CITE_ and the one used in the SemEval 2007 Affective Text task . The first one consists of 10 . 662 sentences selected from different movie review websites . These sentences are labeled as positive or negative depending on whether they express a positive or negative opinion within the movie review .__label__Material|Data|Use
Due to unavailability of such a system for French , we adopted a hybrid syllabification method . For words included in Lexique ( New et al ., 2004 ), we used the gold syllabification included in the dictionary . For all other words , we generated API phonetic representations with espeak _CITE_ , and then applied the syllabification tool used for Lexique3 ( Pallier , 1999 ). The accuracy of this process exceeded 98 %. For the comparison with an AI model , we extracted the same 46 features ( see Table 2 for the complete list ) used in Franc ¸ ois ’ model and trained a SVM model .__label__Method|Tool|Use
All relevant data are available at the Gene Expression Omnibus ( GEO ) under accession numbers GSE2508 , GSE26637 , GSE27949 , GSE48964 , GSE62117 , GSE64567 , GSE33526 , GSE78958 , GSE65540 , GSE66306 , and GSE32575 ( see Supplementary Table 4 for details ). In addition , data for batch 11 was obtained from the Bgee Gene Expression Database35 and can be publicly accessed at _CITE_ Summary statistics from the Twins UK dataset used in Supplementary Figure 2 can be accessed at http :// expression . kcl . ac . uk / phenoexpress / 1 /.__label__Material|Data|Use
Water temperature is a major factor in determining the geographic distribution and preferred habitats of marine species , though the mechanism for these relationships is perhaps mediated by oxygen demand and availability [ 1 , 3 , 67 , 68 ]. We have shown that climate change in the 21st century will shift the location and available area of suitable thermal habitat for species inhabiting the North American shelf . These shifts in thermal habitat can interact in complex PLOS ONE | _CITE_ May 16 , 2018 15 / 28 Projecting thermal habitat shifts on the North American continental shelf Fig 6 . Projected change in thermal habitat availability . Mean percentage change in projected thermal habitat availability over the 21st century for low uncertainty species originating from seven regions of the North American shelf for ( A ) RCP 2 . 6 and ( B ) RCP 8 . 5 .__label__Supplement|Paper|Introduce
More than “ topics ”, these topics represent symbolic meanings . She concludes that , in order to understand them , a closed reading of the poems is necessary . We have run LDA Topic Modeling over our corpus of sonnets _CITE_ . Using different configurations ( 10 , 20 , 50 , 100 and 1000 topics ), we are developing several analysis . In the next sections I will present these analysis together with some preliminary results and comments .__label__Material|Data|Use
More details are provided in [ 1 ]. Transparency document . Supplementary material Transparency data associated with this article can be found in the online version at _CITE___label__Supplement|Document|Produce
No word sense disambiguation was performed and all synsets for a particular lemma were considered . We firstly recorded length information of given sentences pairs using following eight measure function where | A | stands for the number of non - repeated http :// nltk . org / words in sentence A , | A − B | means the number of unmatched words found in A but not in B , | A u B | stands for the set size of non - repeated words found in either A or B and | A n B | means the set size of shared words found in both A and B . Motivated by the hypothesis that two texts are considered to be more similar if they share more strings , we adopted the following five types of measurements : ( 1 ) longest common sequence similarity on the original and lemmatized sentences ; ( 2 ) Jaccard , Dice , Overlap coefficient on original word sequences ; ( 3 ) Jaccard similarity using n - grams , where n - grams were obtained at three different levels , i . e ., the original word level ( n = 1 , 2 , 3 ), the lemmatized word level ( n = 1 , 2 , 3 ) and the character level ( n = 2 , 3 , 4 ); ( 4 ) weighted word overlap feature ( ˇSari ´ c et al ., 2012 ) that takes the importance of words into consideration , where Web 1T 5 - gram Corpus _CITE_ was used to estimate the importance of words . ( 5 ) sentences were represented as vectors in tf * idf schema based on their lemmatized forms and then these vectors were used to calculate cosine , Manhattan , Euclidean distance and Pearson , Spearmanr , Kendalltau correlation coefficients based on different perspectives . Totally , we got thirty - one string based features .__label__Material|Data|Use
We also investigated the contribution of the feature nodes by running HTP without them . In addition , we ran HTP on a bipartite graph , i . e ., one created from English - foreign phrase alignments only without any phrase alignments between foreign languages . We used Callison - Burch ( 2008 )’ s implementation of SBP that is publicly available at _CITE_ SBP is based on BCB ( Bannard and Callison - Burch , 2005 ) which computes the probability that English phrase E ' is a paraphrase of E using the following formula :__label__Method|Code|Use
The two corpora we use are 1 .) a collection of 8447 articles from the New York Times from the years 1987 to 2007 with a vocabulary size of 8269 unique types and around one million tokens and 2 .) a sample of 10000 articles from Wikipedia ( _CITE_ ) with a vocabulary size of 15273 unique types and three million tokens .__label__Material|Data|Use
In releasing this data we hope to equip researchers with the data to support numerous research directions going forward . The JCLC is freely available to the research community and accessible via our website . _CITE_ It can be used via a web - based interface for querying the data . Alternatively , the original texts can be downloaded in text format for more advanced tasks . Interest in learning Chinese is rapidly growing , leading to increased research in Teaching Chinese as a Foreign Language ( TCFL ) and the development of related resources such as learner corpora ( Chen et al ., 2010 ).__label__Material|Data|Produce
This work is licensed under a Creative Commons Attribution 4 . 0 International Licence . Page numbers and proceedings footer are added by the organisers . Licence details : _CITE_ The purpose of this task is to enhance current research in Natural Language Processing ( NLP ) methods used in the clinical domain . The task is a continuation of the CLEF / eHealth ShARe 2013 Shared Task . In particular there were two specific tasks , viz .__label__Supplement|License|Other
Furthermore , they have been shown to model quite well the semantic composition of short phrases via simple vector addition ( Mikolov et al ., 2013b ). To build a vector for a sentence , we simply sum the distributed vectors of the individual words . _CITE_ For both representations , we remove the stopwords before building the vectors . To compute the similarity between two sentences , we compute the cosine similarity between their corresponding vectors . Semantic textual similarity ( STS ): Following on the work of Boltuˇzi ´ c and ˇSnajder ( 2014 ), we use an off - the - shelf STS system developed by ˇSari ´ c et al .__label__Method|Algorithm|Use
The corpus is a collection of 30 , 033 sentence pairs and consists of dialogs in travel situations ( 10 , 061 ) and parts of the BTEC corpus ( 19 , 972 ). Details about the provided corpus are described in ( Paul , 2009 ). We used the Stanford Parser _CITE_ to obtain word - level dependency structures of Chinese sentences , and GIZA ++ to obtain word alignments of the biligual corpus . We extracted the SCFG - MWU from the biligual corpus with word alignment . In order to investigate the coverage of the extracted rule , we counted the number of the recovered sentences , i . e .__label__Method|Tool|Use
To arrive at the point where information that is available in museum databases about paintings could be recorded using this model , we developed the painting ontology that integrates the CIDOC - CRM with more specific schemata . The Swedish Open Cultural Heritage ( SOCH ) is a web service used to search and fetch data from any organization that holds information related to the Swedish cultural heritage . _CITE_ The idea behind SOCH is to harvest any data format and structure that is used in the museum sector in Sweden and map it into SOCH ’ s categorization structure . The data model used by SOCH is an uniform data representation which is available in an RDF compatible form . The schema provided by SOCH helps to intermediate data between museums in Sweden and the Europeana portal .__label__Supplement|Website|Introduce
The post - processed proteomics data were also made freely available for queries through the PIQMIe server ( http :// piqmie . biotools . nl / results /< dataset >, where dataset refers to one of the six EMF exposures : ELF_human , ELF_mouse , UMTS_human , UMTS_mouse , WIFI_human or WIFI_mouse ). The TIFF images taken from the immunoblots and the intensities of the observed protein bands in these blots are provided in supplementary materials ( S1 File ). The source codes of the bioinformatics tools ( in R and Python languages ) developed to analyze semi - quantitative proteomics data , such as those from in vitro exposures to EMFs , can be found on GitHub and / or CERN ’ s Zenodo platform ( PIQMIe version 1 . 0 , _CITE_ EMF - DM version 1 . 0 . 1 , http :// dx . doi . org / 10 . 5281 / zenodo . 166705 ).__label__Method|Code|Use
For our submissions we applied the approach proposed by Espl ` a - Gomis et al . ( 2015b ), who use black - box bilingual resources from the Internet for word - level MTQE . In particular , we combined two on - line MT systems , Apertium and Google Translate , and the bilingual concordancer Reverso Context _CITE_ to spot sub - segment correspondences between a sentence S in the source language ( SL ) and a given translation hypothesis T in the target language ( TL ). To do so , both S and T are segmented into all possible overlapping sub segments up to a certain length and translated into the TL and the SL , respectively , by means of the sources of bilingual information mentioned above . These sub - segment correspondences are used to extract a collection of features that is then used by a binary classifier to determine the final word - level MTQE labels .__label__Method|Tool|Extent
permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited . Data Availability Statement : Data cannot be openly shared due to legal restrictions imposed by the Ministry of Health , Labour and Welfare ( URL : _CITE_ ). Applications to request data access may be sent as follows : Healthcare__label__Supplement|Website|Introduce
Here Nv , J is the number of times v occurs in the observation wJ . Computing the expectation over this Dirichlet posterior gives us the following Bayesian estimate for PV : To provide a meaningful comparison with previously - reported results , we use , without any modification , the dataset provided by Duygulu et al . [ 4 ] _CITE_ . This allows us to compare the age annotation . Our model ( CRM ) substantially outperforms all other models .__label__Material|Data|Use
Base features represent the basic properties of event causality like nouns , templates , and their excitation polarities ( See Section E in the supplementary notes ). For B3 and B4 , 500 semantic classes were obtained from our web corpus using the method of Kazama and Torisawa ( 2008 ). Using the above features , a classifier _CITE_ classifies each event causality candidate into causality and non - causality . An event causality candidate is given a causality score CScore , which is the SVM score ( distance from the hyperplane ) that is normalized to [ 0 , 1 ] by the sigmoid function 1 1 + e − x . Each event causality candidate may be given multiple original sentences , since a phrase pair can appear in multiple sentences , in which case it is given more than one SVM score .__label__Method|Tool|Use
We replaced these ‘ correct answers ’ with their explicit names . We also removed zeros in quoted sentences because they are quite different from other sentences . In addition , we decided to use the output of ChaSen 2 . 2 . 9 and CaboCha 0 . 34 _CITE_ instead of the morphological information and the dependency information provided by the Kyoto Corpus since classification of the joshi ( particles ) in the Corpus was not satisfactory for our purpose . Since CaboCha was trained by Kyoto Corpus 3 . 0 , CaboCha ’ s dependency output is very similar to that of the Corpus . In this paper , we combine heuristic ranking rules and machine learning .__label__Method|Algorithm|Use
In this section , we evaluate how well neurally - guided procedural models capture image - based constraints . We implemented our prototype system in the WebPPL probabilistic programming language [ 4 ] using the adnn neural network library . _CITE_ All timing data was collected on an Intel Core i7 - 3840QM machine with 16GB RAM running OSX 10 . 10 . 5 . In experiments which require target images , we use the following image collections : We augment the dataset with a horizontally - mirrored copy of each image , and we annotate each image with a starting point and direction from which to initialize the execution of a procedural model . Figure 3 shows some representative images from each collection .__label__Method|Code|Use
is a rule and transfer - based Machine Translation System . More information on the software is available at _CITE_ category are also kept track of . The Nepali Stemmer and Morphological Analyzer is a rule - based one and makes use of the following resources :__label__Supplement|Document|Produce
Each row corresponds to a motion sequence to which we would expect the second layer features to be roughly invariant . From this visualization , non - trivial invariances are observed such as non - linear warping , rotation , local non - affine changes and large scale translations . A video animation of this visualization is also available online _CITE_ . without ( top ) and with ( bottom ) temporal slowness . ( Right ) visualization of second layer features ( patch size 32x32 ), with each row corresponding to one pooling unit .__label__Supplement|Media|Produce
Mapping Brain Networks Using EEG toolbox . Accordingly , all EEG datasets later used for source localization had the same number of signals . Subsequently , we band - pass filtered the EEG data in the frequency range 1 – 80 Hz and we decomposed them into ICs by using the fast fixed - point ICA ( FastICA ) algorithm _CITE_ , to identify and remove artifacts of biological origin ( Mantini et al ., 2008 ). Artifactual ICs were automatically identified by using information from the signal kurtosis , the power spectrum and the correlation with horizontal and vertical electrooculogram ( hEOG and vEOG ) and electromyogram ( EMG ). Finally , we re - referenced the cleaned EEG signals using the average reference approach ( Liu et al ., 2015 ).__label__Method|Algorithm|Use
This also distinguishes DIS from methods such as adaptive importance sampling ( AIS ) [ 18 ]. We evaluate our approach ( DIS ) against AOBFS ( search , [ 16 ]) and WMB - IS ( sampling , [ 15 ]) on several benchmarks of real - world problem instances from recent UAI competitions . Our benchmarks include pedigree , 22 genetic linkage instances from the UAI ’ 08 inference challenge _CITE_ ; protein , 50 randomly selected instances made from the “ small ” protein side - chains of [ 22 ]; and BN , 50 randomly selected Bayesian networks from the UAI ’ 06 competition . These three sets are selected to illustrate different problem characteristics ; for example protein instances are relatively small ( M = 100 variables on average , and average induced width 11 . 2 ) but high cardinality ( average max | Xi |= 77 . 9 ), while pedigree and BN have more variables and higher induced width ( average M 917 . 1 and 838 . 6 , average width 25 . 5 and 32 . 8 ), but lower cardinality ( average max | Xi | 5 . 6 and 12 . 4 ). We alloted 1GB memory to all methods , first computing the largest ibound that fits the memory budget , and using the remaining memory for search .__label__Material|Data|Use
We have tested the performance of the proposed method on mixtures of different voice and music signals . The sample rate of the mixtures is 22 . 05kHz . Audio files for all the experiments are accessible at the website _CITE_ . Figure 2 shows experimental results . In experiments 1 and 2 , the mixed signals consist of one voice signal and one music signal .__label__Supplement|Website|Produce
We have presented a new top - down left - to - right parsing model . Its performance of 89 . 4 % is a 20 % error reduction over the previous singleparser performance , and indeed is a small improvement ( 0 . 6 %) over the best combinationparser result . The code is publically available . 1 l_CITE___label__Method|Code|Produce
The first level of interpretation of a Sanskrit text is its word - to - word segmentation , and our tagger will be able to assist a philology specialist to achieve complete morphological mark - up systematically . This will allow the development of concordance analysis tools recognizing morphological variants , a task which up to now has to be performed manually . At some point in the future , one may hope to develop for Sanskrit the same kind of informative repository that the Perseus web site provides for Latin and Classical Greek _CITE_ . Such resources are invaluable for the preservation of the cultural heritage of humanity . The considerable classical Sanskrit corpus , rich in philosophical texts but also in scientific , linguistic and medical knowledge , is an important challenge for computational linguistics .__label__Supplement|Website|Introduce
We detected emotions using four feature types : 1 ) interjections , 2 ) profanity , 3 ) emoticons and 4 ) overall sentiment of the tweet . Interjections , profanity , and emoticons are widely used by individuals to convey emotion , such as anger , surprise , happiness , etc . To identify these three feature types , we used a combination of POS tags in the English tagger ( which contains tags for interjections , emoticons , etc ), compiled lists of interjections and profanity from the web for both English and Spanish _CITE_ and regular expression patterns for emoticons . We also included sentiment features using the sentiment140 API ( Go et al ., 2009 ). This API provides a sentiment label ( positive , negative or neutral ) for a tweet corresponding to its overall sentiment .__label__Material|Data|Produce
We then run MAP inference on the factor graph using the LP formulation in [ 9 ] and compare the quality of the solutions obtained by Thetis with a Gibbs sampling - based approach [ 26 ]. We follow the LP - rounding algorithm in [ 16 ] to solve the MAP estimation problem . For entity linking , we use the TAC - KBP 2010 benchmark _CITE_ . The input graphical model has 12K boolean random variables and 17K factors . For text chunking , we use the CoNLL 2000 shared task .__label__Material|Data|Use
Final libraries were pooled to equal molarity in Buffer EB ( 10 mM Tris - HCl , pH 8 . 5 , Qiagen ) containing 0 . 1 % TWEEN 20 ( Sigma Aldrich ). Sequencing was performed on an Illumina NextSeq 500 instrument at TATAA Biocenter ( Gothenburg , Sweden ) using 150 bp single - end reads . Raw FastQ files were subsequently processed as described [ 28 ] using Debarcer Version 0 . 3 . 0 ( _CITE_ ). Sequence reads with the same barcode were grouped into families for each amplicon . Barcode families with at least 20 reads , where & gt ; 90 % of the reads were identical , were required to compute consensus reads .__label__Method|Tool|Use
We construct 1000 clusters employing the Brown method on 112 million words from the North American New York Times corpus . We keep the top 20 most frequent words for each cluster as paraphrases . To generate LSA paraphrases , we used the Infomap software _CITE_ on a 34 million word collection of articles from the American News Text corpus . We used the default parameter settings : a 20 , 000 word vocabulary , the 1000 most frequent words ( minus a stoplist ) for features , a 15 word context window on either side of a word , a 100 feature reduced representation , and the 20 most similar words as paraphrases . While we experimented with several parameter settings for LSA and Brown methods , we do not claim that the selected settings are necessarily optimal .__label__Method|Tool|Use
The code to fit the models and reproduce the figures is available online at : _CITE___label__Method|Code|Produce
The pre - publication history for this paper can be accessed here : _CITE_ pub Publish with OioMed Central and every scientist can read your work free of charge & quot ; BioMed Central will be the most significant development for disseminating the results of biomedical research in our lifetime .& quot ; Sir Paul Nurse , Cancer Research UK Your research papers will be : available free of charge to the entire biomedical community peer reviewed and published immediately upon acceptance cited in PubMed and archived on PubMed Central yours — you keep the copyright Submit your manuscript here : http :// www . biomedcentral . com / info / publishing_adv . asp BioMedcentral Page 5 of 5 ( page number not for citation purposes )__label__Supplement|Document|Produce
– Subclauses : The mean number of subclauses in each sentence , normalized by sentence length in words . The mean subclause length in words . Subclauses are labeled as “ SBAR ” in the parser tree generated by a commonly used NLP tool , Stanford Core NLP ( Klein and Manning , 2003 ), which is an integrated suite of natural language processing tools for English in Java _CITE_ , including part - of - speech tagging , parsing , co - reference , etc .. – Sentence level : The sum of the depth of all nodes in a parser tree generated by Stanford Core NLP . The height of the parser tree is also incorpo rated into the feature set . – Mode , preposition , comma : The number of modes , prepositions and commas in each sentence respectively , normalized by sentence length in words .__label__Method|Tool|Use
Banko and Brill ( 2001a , 2001b ) experiment with context - sensitive spelling correction , a task for which large amounts of data can be obtained straightforwardly , as no manual annotation is required . They demonstrate that the learning algorithms typically used for spelling correction benefit significantly from larger training sets , and that their performance shows no sign of reaching an asymptote as the size of the training set increases . Arguably , the largest data set that is available for NLP is the Web , _CITE_ which currently consists of at least 3 , 033 million pages . Data retrieved from the Web therefore provide enormous potential for training NLP algorithms , if Banko and Brill ’ s ( 2001a , 2001b ) findings for spelling corrections generalize ; potential applications include tasks that involve word n - grams and simple surface syntax . There is a small body of existing research that tries to harness the potential of the Web for NLP .__label__Material|Data|Introduce
Note that while the BNBP sampler in ( 12 ) is fully collapsed , the direct assignment sampler of the HDP - LDA in ( 14 ) is only partially collapsed as neither the globally shared Dirichlet process Ge nor the concentration parameter α are marginalized out . To derive a collapsed sampler for the HDP - LDA that marginalizes out Ge ( but still not α ), one has to use the Chinese restaurant franchise [ 6 ], which has cumbersome book - keeping as each word is indirectly linked to its topic via a latent table index . We consider the JACM _CITE_ , PsyReview , and NIPS12 corpora , restricting the vocabulary to terms that occur in five or more documents . The JACM corpus includes 536 documents , with V = 1539 unique terms and 68 , 055 total word counts . The PsyReview corpus includes 1281 documents , with V = 2566 and 71 , 279 total word counts .__label__Material|Data|Use
Metatranscriptome reads of each HMY and LMY sample were mapped to the two reference genomes as well as metagenome and metatranscriptome reads to all reassembled ldh genes and all genes in the custom lcdA genes database ( for information on database construction , see Additional file 5 : text S1 ) using BBmap ( http :// sourceforge . net / projects / bbmap /) with an ID cut - off of 98 % sequence similarity for ldh genes and genome sequences , and 60 % sequence similarity for lcdA genes and counting ambiguous reads for all matching genes . Read counts were normalised to RPM , and statistical analysis of normalised read counts was conducted in R via the WRS test and Benjamini - Hochberg correction ( for all genes in isolate genomes and ldh genes ) to select genes or transcripts with significantly different abundances between the HMY and LMY animals . Functional comparison to the Hungate 1000 genomes Functional identifiers of KEGG orthology genes from the metagenome dataset that showed significant correlation to methane yield in both the WRS test and sPLS analyses were uploaded into IMG / MER and used as screening IDs for all the bacterial genomes available from the Hungate 1000 project ( _CITE_ ) and all additionally available bacterial genomes derived from rumen habitats in June 2015 using the “ functions versus genomes ” tool in IMG / MER . Additional files Additional file 1 : Table S1 . Overview of samples analysed in this study and methods of analysis conducted .__label__Method|Tool|Use
PacBio sequencing . The PacBio sequences for H1 - ESC were obtained from the data used in the original IDP paper12 . It can be found at Gene Expression Omnibus ( GEO ) database , _CITE_ ( accession no . GSE51861 ). For the PacBio raw sequences , it will be provided upon contacting Kin Fai Au ( kinfai - au @ uiowa . edu ).__label__Material|Data|Use
We built several corpora using two different strategies . The first set was built using Wikipedia and the interlingual links available on articles ( that points to another version of the same article in another language ). We started from the list of all French articles _CITE_ and randomly selected articles that provide a link to Spanish and English versions . We downloaded those , and clean them by removing the wikipedia formatting tags to obtain raw UTF8 texts . Articles were not selected based on their sizes , the vocabulary used , nor a particular topic .__label__Material|Data|Use
To test our approach for PLSA initialization we developed an LSA implementation based on the SVDLIBC package ( _CITE_ dr / SVDLIBC /) for computing the singular values of sparse matrices . The PLSA implementation was based on an earlier implementation by Brants et al . ( 2002 ).__label__Method|Code|Use
© 2018 by the authors . Licensee MDPI , Basel , Switzerland . This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ( CC BY ) license ( _CITE_ ).__label__Supplement|License|Other
Various IRB - approved recruitment methods are used for the RVR , including health fairs , direct advertisement , and referral from other OHSU IRB - approved studies . The RVR includes an IRB - approved consent form , completed in person or online via a brief , secure REDCap survey . REDCap is a widely deployed secure web - based data collection and management application for building and managing online surveys and databases , especially for research activities ( _CITE_ ) [ 5 ]. Consented participants are provided with a link to a REDCap enrollment survey , or an option to complete the survey on paper or via phone . The survey includes demographic information , medical diagnoses , medications , and contact preferences .__label__Material|Data|Produce
Considering that most of its similar mentions are mapped to the American country singer “ Luke Bryan ”, our model tends to link “ LukeBryanOnline ” to the same entity . We evaluate our method on the public available data set shared by Meij et al . ( 2012 ) _CITE_ . Experimental results show that our method outperforms two baselines , i . e ., Wikify ! ( Mihalcea and Csomai , 2007 ) and system proposed by Meij et al .__label__Material|Data|Use
In addition , we integrated Cytoscape [ 23 ] Java Web Start technology so that the association network generated by eLSA can be immediately visualized . Based on these efforts , we anticipate that our novel eLSA methodology , as implemented by the newly developed pipeline software , will significantly assist researchers requiring systematic discovery of time - dependent associations . More information about the software and web services is available from the eLSA homepage at _CITE___label__Supplement|Document|Produce
The negative - sampling parameter is set to 15 in all the training processes . All embeddings are trained on a free Chinese news archive that contains about 170 millions sentences and 3 . 4 billions words . We segment and parse these sentences using the MVT implementation of ZPar 0 . 7 _CITE_ ( Zhang and Clark , 2011 ), which is trained on a large - scale annotated corpus and achieves state - of - the - art analyzing accuracy on contemporary Chinese ( Qiu et al ., 2014 ) . Targets and contexts for word and dependency embeddings were filtered with a minimum frequency of 100 and 10 , respectively , and all the four types of embeddings are trained with 200 dimensions . Three datasets are used for evaluating Chinese embeddings .__label__Method|Code|Use
For policy training , we train a linear SVM classifier using Liblinear ( Fan et al ., 2008 ). For all languages , we run DAgger for 20 iterations and se lect the best policy evaluated on the development set among the 20 policies obtained from each iteration . We use the publicly available implementation of MSTParser _CITE_ ( with modifications to the feature computation ) and its default settings , so the feature weights of the projective and non - projective parsers are trained by the MIRA algorithm ( Crammer and Singer , 2003 ; Crammer et al ., 2006 ). Our feature set contains most features proposed in the literature ( McDonald et al ., 2005a ; Koo and Collins , 2010 ). The basic feature components include lexical features ( token , prefix , suffix ), POS features ( coarse and fine ), edge length and direction .__label__Method|Code|Use
This work is licensed under a Creative Commons Attribution 4 . 0 International Licence . Page numbers and proceedings footer are added by the organisers . Licence details : _CITE_ This document is a description paper , therefore , we focus the rest of it on the features and models we used for carrying out the experiments . A complete description of the task and the dataset used are given in Marelli et al . ( 2014a ) and in Marelli et al .__label__Supplement|License|Other
Supplementary figures S1 – S4 and tables S1 and S2 are available at Genome Biology and Evolution online ( _CITE_ ).__label__Supplement|Website|Use
In addition , we employ additional online word lists to distinguish named entities and function words from potential informal words . As shown in Table 1 , alphabetic sequences in microblogs may refer to Chinese Pinyin or Pinyin abbreviations , rather than English ( e . g ., “ bs ” for bi shi ; “ to despise ”). Hence , we added dictionarybased features to indicate the presence of Pinyin initials , finals and standard Pinyin expansions , using a UK English word list _CITE_ . The final list of dictionary - based features employed are : Statistical Features . We use pointwise mutual information ( PMI ) variant ( Church and Hanks , 1990 ) to account for global , corpus - wide information .__label__Material|Data|Use
The detailed definition of this measure as applied for each formalism is provided in ( Clark and Curran , 2003 ; Miyao and Tsujii , 2008 ; Cahill et al ., 2004 ). For CCG , we use the evaluation script from the C & C tools . _CITE_ For HPSG , we evaluate all types of dependencies , including punctuations . For LFG , we consider the preds - only dependencies , which are the dependencies between pairs of words . Secondly , we also evaluate using unlabeled PARSEVAL , a standard measure for PCFG parsing ( Petrov and Klein , 2007 ; Charniak and Johnson , 2005 ; Charniak , 2000 ; Collins , 1997 ).__label__Method|Code|Use
Constructing a Markov chain that adapts to the target distribution while still drawing samples from the correct stationary distribution is challenging , although much research over the last 15 years has resulted in a variety of approaches and theoretical results . Adaptive MCMC for example , allows for global adaptation based on the partial or full history of a chain ; this breaks its Markov property , although it has been shown that subject to some technical conditions [ 2 , 3 ] the resulting chain will still converge to the desired stationary distribution . Most recently , advances in Riemannian Manifold MCMC allow locally changing , position specific proposals to be made based on the underlying ∗ _CITE_ geometry of the target distribution [ 1 ]. This directly takes into account the changing sensitivities of the model for different parameter values and enables very efficient inference over a number of popular statistical models . It is useful for inference over large numbers of strongly covarying parameters , however this methodology is still not suitable for all statistical models ; in its current form it is only applicable to models that admit an analytic expression for the metric tensor .__label__Supplement|Document|Extent
Each cluster had 3 CHWs , resulting in a total of 42 CHWs , and thus a corresponding number of reporting units , distributed across the study area . Parasitological assessments were conducted continuously from January 2011 to March 2013 in Luangwa and from April 2011 to March Hamainza et al . Malaria Journal 2014 , 13 : 489 Page 4 of 13 _CITE_ 2013 in Nyimba district in all the selected clusters . All consenting households received monthly active visits from CHWs , which included parasitological surveys using RDTs detecting histidine - rich protein 2 antigen ( Malaria Pf cassette test , ICT Diagnostics ), coupled with registers designed in a pre - defined questionnaire format [ 10 ]. Consent for household participation was given by the head of the household and consent was obtained from individual study participants , or parents / guardians in the case of minors , for the RDT test .__label__Supplement|Paper|Introduce
We consider first a simple classification problem where the goal is to classify whether a particular book is of liberal political inclination or not . The features of each book are given by the words in the Amazon . com front page for that particular book . The choice of books , labels , and relationships are given in the data collected by Valdis Krebs and available at _CITE_ . The data containing book features can be found at http :// www . statslab . cam . ac . uk /- silva . There are 105 books , 43 of which are labeled as liberal books .__label__Material|Data|Use
Publish with BioMed Central and every scientist can read your work free of charge & quot ; BioMed Central will be the most significant development for disseminating the results of biomedical research in our lifetime .& quot ; Sir Paul Nurse , Cancer Research UK Your research papers will be : available free of charge to the entire biomedical community peer reviewed and published immediately upon acceptance cited in PubMed and archived on PubMed Central yours — you keep the copyright Submit your manuscript here : _CITE_ BioMedcentral Page 17 of 17 ( page number not for citation purposes )__label__Supplement|Website|Other
The first type of baseline we considered is based on machine translation . We used a machine translation tool on the Japanese query , and then applied TFIDF or LSI . We considered three methods of machine translation : Google ’ s API _CITE_ or Fujitsu ’ s ATLAS was used to translate each query document , or we translated each word in the Japanese dictionary using ATLAS and then applied this word - based translation to a query . We also compared to CL - LSI [ 9 ] trained on all 90 , 000 Jap - Eng pairs from the training set . For PSI , we considered two cases : ( i ) apply the ATLAS machine translation tool first , and then use PSI trained on the task in Section 4 . 1 , e . g .__label__Method|Tool|Use
The PROP - diff , PROP - WL and the WL kernel were each run with 10 iterations . In the CSM kernel , the clique size parameter was set to k = 5 . Our kernel implementations and datasets ( with the exception of AIRWAYS ) can be found at _CITE_ Classification experiments were made on four datasets : ENZYMES , PROTEINS , AIRWAYS and SYNTHETIC . ENZYMES and PROTEINS are sets of proteins from the BRENDA database [ 22 ] and the dataset of Dobson and Doig [ 23 ], respectively .__label__Method|Code|Produce
The heuristic computes the error for a given validation image at level k in the pyramid as Lk ( Ik ) = min { z ;}|| Gk ( zj , u ( Ik + 1 )) − hk || 2 where { zj } is a large set of noise vectors , drawn from pnozse ( z ). In other words , the heuristic is asking , are any of the generated residual images close to the ground truth ? Torch training and evaluation code , along with model specification files can be found at _CITE_ For all models , the noise vector zk is drawn from a uniform [- 1 , 1 ] distribution .__label__Method|Code|Produce
Given the size of the datasets (~ 150 2 h imaging sessions ), we provide MATLAB data containing solely deconvolved images and labels for each stimulus ( concept or sentence ). The raw and processed NIFTI imaging datasets , as well as associated event files , will be shared via a repository ( http :// www . openfmri . org ), after re - processing . Any updates on the data and scripts will be posted on the paper website ( _CITE_ crwz7 ).__label__Method|Code|Produce
Libraries were constructed with 300 – 400 bp insert length , and 101 bp or 151 bp paired - end sequencing was performed on Illumina Hiseq instruments . The tumors were classified into four molecular subtypes as described previously by TCGA19 . We obtained WGS data of 40 GC tumors from TCGA ( _CITE_ ), 32 tumors from ICGC ( https :// ega - archive . org / datasets / EGAD00001003132 ), and 100 tumors from Wang et al . ( HK ) 20 . The molecular subtypes of tumors from the TCGA cohort were defined by TCGA .__label__Material|Data|Use
5 and 6 and we provide results on smaller corrupted datasets ( to show the performance of IWS - LS ) as well as non - corrupted data simulated according to [ 13 ] in § SI . 7 . Airline delay dataset The dataset consists of details of all commercial flights in the USA over 20 years . Dataset along with visualisations available from _CITE_ Selecting the first ntrain = 13 , 000 US Airways flights from January 2000 ( corresponding to approximately 1 . 5 weeks ) our goal is to predict the delay time of the next ntest = 5 , 000 US Airways flights . The features in this dataset consist of a binary vector representing origin - destination pairs and a real value representing distance ( p = 170 ).__label__Material|Data|Produce
All datasets described in this article , for the 87 countries listed in Tables 1 and 2 , are publicly and freely available through both the WorldPop Dataverse Repository ( Data Citation 1 , Data Citation 2 , Data Citation 3 , Data Citation 4 ) and the WorldPop website ( _CITE_ ). While the datasets available through the WorldPop Dataverse Repository will be preserved in their published form while the ones available through the WorldPop website will be integrated with additional countries ( Middle Eastern countries and Japan ). Furthermore , additional gridded subnational dependency ratio datasets and high resolution gridded 5 - year age / sex group count datasets for all countries located in Latin America and the Caribbean will be soon available through the WorldPop website .__label__Material|Data|Use
There are many samples generated by the models with MinVI objective that look clearly better than those generated by the model with ML objective . In this section , we evaluate our methods on MIR - Flickr database [ 11 ], which is composed of 1 million examples of image and their user tags collected from the social photo - sharing website Flickr . _CITE_ Among those , 25000 examples are annotated with 24 potential topics and 14 regular topics , which leads to 38 classes in total with distributed class membership . The topics include object categories such as dog , flower , and people , or scenic concepts such as sky , sea , and night . We used the same visual and text features as in [ 27 ].__label__Material|Data|Use
In our final set of experiments we employed KHA to denoise a human walking motion trajectory from the CMU motion capture database ( _CITE_ ), converted to Cartesian coordinates via Neil Lawrence ’ s Matlab Motion Capture Toolbox ( http :// www . dcs . shef . ac . uk /∼ neil / mocap /). The experimental setup was similar to that of Tangkuampien and Suter [ 15 ]: Gaussian noise was added to the frames of the original motion , then KHA with 25 PCs was used to denoise them . The results are shown in Figure 4 .__label__Material|Data|Use
However , the task is slightly more difficult than simply mining for a “ similar_to ” relation , which is addressed by our approach in section 5 . As the goal of this paper is to supply the tools for creating a large corpus of analogies from the Web , we require a reliable mechanism for automatically classifying if a text snippet contains an analogy or not . Such classification requires a Gold dataset which we construct in this section and which we make available to the community for download _CITE_ . As we expect the number of analogies in a completely random collection of web documents to be extremely low , we first start by collecting a set of web documents that are likely to contain an analogy by applying some easy - to - implement but rather coarse techniques as follows : In order to obtain a varied set of text snippets ( i . e . short excerpts from larger Web documents ), we first used a Web search engine ( Google Search API ) with simple Hearst - like patterns for crawling potentially relevant websites .__label__Material|Data|Produce
As part of this module , we have students work through the exercises in the draft chapter on command line tools in Chris Brew and Marc Moens ’ Data - Intensive Linguistics course notes or Ken Church ’ s Unix for Poets tutorial . Grammar engineering with OpenCCG . The grammar engineering component of Computational Syntax in Spring 2006 used OpenCCG , _CITE_ a categorial grammar parsing system that Baldridge created with Gann Bierner and Michael White . The problem with using OpenCCG is that its native grammar specification format is XML designed for machines , not people . Students in the course persevered and managed to complete the assignments ; nonetheless , it became glaringly apparent that the non - intuitive XML specification language was a major stumbling block that held students back from more interesting aspects of grammar engineering .__label__Method|Tool|Use
Since the classification accuracies deceased to be quite low for source spans with more than 10 words , only { C1 , ..., C10 } were integrated into the HPB translation system . For each translation task , the recent version of Moses HPB decoder ( Koehn et al ., 2007 ) with the training scripts was used as the baseline ( Base ). We used the default parameters for Moses , and a 5 - gram language model was trained on the target side of the training corpus by IRST LM Toolkit _CITE_ with improved Kneser - Ney smoothing . { C1 ,..., C10 } were integrated into the baseline with different weights , which were tuned by MERT ( Och , 2003 ) together with other feature weights ( language model , word penalty ,...) under the log - linear framework ( Och and Ney , 2002 ). represents a significant difference at the p & lt ; 0 . 01 level and - represents a significant difference at the p & lt ; 0 . 05 level against the BLM .__label__Method|Tool|Use
See _CITE_ for details . Data Documentation Initiative DDI DC is generic and its terms are broadly defined . For the domain of the social sciences , the DDI initiative has created a range of specific metadata standards for describing data produced by surveys and other methods in social and economic sciences , and that are used for the documentation , discovery and interpolation .__label__Supplement|Document|Produce
Given the size of the datasets (~ 150 2 h imaging sessions ), we provide MATLAB data containing solely deconvolved images and labels for each stimulus ( concept or sentence ). The raw and processed NIFTI imaging datasets , as well as associated event files , will be shared via a repository ( http :// www . openfmri . org ), after re - processing . Any updates on the data and scripts will be posted on the paper website ( _CITE_ crwz7 ).__label__Material|Data|Produce
This work is published under the standard license to publish agreement . After 12 months the work will become freely available and the license terms will switch to a Creative Commons AttributionNonCommercial - Share Alike 4 . 0 Unported License . Supplementary Information accompanies this paper on British Journal of Cancer website ( _CITE_ )__label__Supplement|Document|Produce
A change in the set of exporters of a product is thus necessary to cause the observed trajectories . The finding of Eq 3 suggests a scheme where most of the products are at their asymptotic market , with the logPRODY reflecting their Complexity . Their moving away from the asymptotic market is unpredictable and accompanied , on average , by a decrease of the competition PLOS ONE | _CITE_ May 17 , 2017 6 / 20 The complex dynamics of products and its asymptotic properties on the market for a given product . Their return to asymptotic market is instead much more regular and evident ( see Fig 1 panel a ) and corresponds to an increase in competition . This mechanism seems to be reliable enough that we can describe the motion on the plane with an equation that connects velocities to competition , and we can confidently say that vertical motion on the plane corresponds to shifts in the market composition .__label__Supplement|Paper|Introduce
Since this is generation from bag - of - words , the task is known to be at the high - complexity extreme of the run - time behavior of our algorithms . As such , we consider it a good test for the ability of our algorithms to scale up to increasingly complex inputs . We use a state - of - the - art , publicly available toolkit _CITE_ to train a trigram language model using Kneser - Ney smoothing , on 10 million sentences ( 170 million words ) from the Wall Street Journal ( WSJ ), lower case and no final punctuation . The test data is also lower case ( such that upper - case words cannot be hypothesized as first words ), with final punctuation removed ( such that periods cannot be hypothesized as final words ), and consists of 2000 unseen WSJ sentences of length 3 - 7 , and 2000 unseen WSJ sentences of length 10 - 25 . The algorithms we tested in this experiments were the ones presented in Section 3 . 2 , plus two baseline algorithms .__label__Method|Tool|Use
That is , we assigned 50 % of the patients with lower risk scores into the low risk group and vice versa . After that , we used log rank test to test if the risks of the two groups were significantly different ( p - value : 50 . 05 ). Kaplan Meier curves and the log rank test were performed by a tool ( _CITE_ ). Evaluation of discrimination performance across several data sets . In order to evaluate to what extent the signature can discriminate patients with different risks in various data sets , we defined the Discrimination score ( Dscore ) as follows :__label__Method|Tool|Use
Syntactic tag % Gloss Syntactic tag % Gloss acl 1 . 89 adjectival clause advcl 0 . 70 adverbial clause modifier advmod 2 . 12 adverbial modifier amod 8 . 34 adjectival modifier appos 1 . 69 appositional modifier aux 4 . 35 auxiliary auxpass 0 . 71 passive auxiliary case 9 . 80 case marking cc 3 . 09 coordinating conjunction ccomp 1 . 03 clausal complement compound 3 . 02 compound conj 3 . 80 conjunct cop 1 . 41 copula csubj 0 . 12 clausal subject csubjpass 0 . 03 clausal passive subject dep 0 . 01 unspecified dependency det 0 . 98 determiner discourse 0 . 71 discourse element dislocated 0 . 01 dislocated elements dobj 3 . 92 direct object expl 0 . 00 expletive foreign 0 . 01 foreign words goeswith 0 . 08 goes with iobj 0 . 22 indirect object list 0 . 00 list mark 3 . 59 marker mwe 0 . 32 multi - word expression name 1 . 56 name neg 0 . 30 negation modifier nmod 17 . 05 nominal modifier nsubj 5 . 97 nominal subject nsubjpass 0 . 65 passive nominal subject nummod 2 . 05 numeric modifier parataxis 1 . 47 parataxis punct 12 . 86 punctuation remnant 0 . 14 remnant in ellipsis root 4 . 51 root vocative 0 . 00 vocative xcomp 1 . 50 open clausal complement Table 1 : Syntactic tags in Croatian UD , sorted alphabetically , and listed together with their relative frequencies and short glosses . The frequencies are calculated for Croatian only , and for the entire collection ( train , dev , test ). The syntactic tags are further explained in the UD documentation : _CITE_ TIMES . HR to provide Croatian UD with a clean , unbiased start , contrasting the manual creation experience of McDonald et al . ( 2013 ) to the one of automatic conversions within the HamleDT project of Zeman et al .__label__Supplement|Document|Produce
The different classes ( types in Freebase ) have different properties . Although the Freebase types are not strict in inheriting properties , some types are still not mutually compatible ( intuitively ). For example , due to this misclassification , the instance of the United States of America ( _CITE_ ) is not only an instance of the types Country , Location but also of Food . We believe that such knowledge has to be represented in a different way . It is important to note that correcting such cases of instances classification to many disjoint types ( classes ) is outside the scope of the current version of FactForge .__label__Supplement|Document|Introduce
This context is possible through the use of multiple ontologies , which are specialized in different components , such as the annotation layer , the domain concepts , and the linguistic rules specification . Despite the higher computational cost that this approach can present when compared with some other options , the results , as described in the result analysis section , presents a good precision and are not dependent of a large volume of documents to generate basic and reference models . The computational phase of the methodology suggested is implemented in the SAURON system , developed in Java Language and the OWL Api _CITE_ support , integrating the Pellet reasoner10 . This system is inspired in the unifying logic layer of the standard technology stack for semantic web11 , since one of the objectives of this system is to unify the use of several semantic technologies applied . The system provides the necessary support to the tasks involving Natural Language Processing , such as the text preprocessing , the syntactic parser access and some format conversions tasks .__label__Supplement|Website|Use
Carreira et al . [ 35 ] compared a group provided with a thumb splint for 90 days to a group who only used the splint for evaluation . While the group who used the splint for 90 days showed superior results in terms of pain reduction ( significant large ES : - 1 . 1 ( 95 % CI - 1 . 90 to - 0 . 30 )), non - significant PLOS ONE | _CITE_ March 14 , 2018 11 / 42 Prosthetic and orthotic interventions : A systematic review small effect sizes were calculated for the remaining function and dexterity measures ( grip and pinch strength , upper limb dexterity and DASH ). The first of two studies which examined knee orthoses in individuals with osteoarthritis compared a knee brace group to a control group [ 58 ]. The knee brace group had superior pain reduction ( significant medium ES : - 0 . 75 ( 95 % CI - 1 . 16 to - 0 . 34 )) but no differences in KOOS and patellofemoral bone marrow lesion volume results .__label__Supplement|Paper|Introduce
To gain insight on the functions of both the putative positively selected and genes under accelerated evolution , a functional annotation was performed using the euKaryotic Orthologous Group ( KOG ) terminology , according to the eggNOG 4 . 0 [ 48 ] database and using its BLASTbased online search tool ( _CITE_ ). Additionally , a Gene Ontology enrichment analysis was performed to determine if any category was overrepresented for genes under positive selection and faster evolutionary rate . Statistically significant enrichment was tested against a reference of all genes analysed using the Fisher ' s exact test and a p - value for the independence__label__Method|Tool|Use
Such “ recursive ” training has previously been applied to neural networks for boundary detection [ 8 , 15 , 16 ], but not to ConvNets . ZNN for 3D deep learning Very deep ConvNets with 3D filters are computationally expensive , so an efficient software implementation is critical . We trained our networks with ZNN ( _CITE_ , [ 17 ]), which uses multicore CPU parallelism for speed . ZNN is one of the few deep learning implementations that is well - optimized for 3D .__label__Method|Tool|Use
More experiments and results on real data sets can be found on our web - page _CITE___label__Supplement|Document|Produce
Note that , while a naive implementation of the arg max in Algorithm 3 requires evaluating the objective for each item in U , here we can exploit the fact that DPPs are closed under conditioning to compute all necessary values with only two matrix inversions [ 5 ]. We report baseline runtimes using this optimized greedy algorithm , which is about 10 times faster than the naive version at N = 200 . The code and data for all experiments can be downloaded from _CITE___label__Method|Code|Produce
PSVM distributedly loads training data on parallel machines , reducing memory requirement through approximate factorization on the kernel matrix . PSVM solves IPM in parallel by cleverly arranging computation order . We have made PSVM open source at _CITE___label__Method|Code|Produce
Separate tables are available for node - by - node estimates of the diffusion properties along the length of the fiber groups , and for the subject metadata , and these tables can be merged in an unambiguous manner through a shared subject ID variable . These files can be read using the standard data science tool - box : Software libraries such as the Python pandas library52 , or using the R statistical language53 . Once data are read into tables , data processing and visualization with tools such as Seaborn ( _CITE_ ) or ggplot ( http :// ggplot2 . org /) are also straightforward . Furthermore , very few steps are required to apply machine learning techniques to the data , using tools such as the scikit - learn library54 , and results such as classifier weights can be easily interpreted with respect to known brain anatomy . An example of such an analysis is presented in Fig .__label__Method|Tool|Use
Examples of the use of JSrealB , and a webbased development environment are available at : _CITE_ jsrealb - bilingual - text - realiser The javascript code of the realizer , the lexicon and tables are made available to the NLG community at : https :// github . com / rali - udem / JSrealB__label__Method|Tool|Produce
One is in spoken language domain while the other is on newswire corpus . Both experiments are on Chinese - to - English translation . Experiments on spoken language domain were carried out on the Basic Traveling Expression Corpus ( BTEC ) ( Takezawa et al ., 2002 ) Chinese - to - English data augmented with HITcorpus _CITE_ . BTEC is a multilingual speech corpus which contains sentences spoken by tourists . 40K sentence - pairs are used in our experiment .__label__Material|Data|Use
Horev - Azaria et al . Particle and Fibre Toxicology 2013 , 10 : 32 Page 12 of 17 _CITE_ streptomycin ( Gibco , Invitrogen Corporation , Italy ). Cell preparations were maintained in standard cell culture conditions ( 37 ° C , 5 %, CO2 and 95 % humidity , HERAEUS incubator , Germany ) [ 25 ]. TK6 : TK6 cells are human lymphoblastoid cell line ( purchased from ATCC ).__label__Supplement|Paper|Introduce
We formu late our problem in terms of MALLET ’ s SimpleTagger class which is a command line interface to the MALLET CRF class . We modify the SimpleTagger class in order to include the provision for producing corresponding posterior probabilities of the predicted labels which are used later for ranking sentences . We build the MaxEnt system using Dr . Dekang Lin ’ s MaxEnt package _CITE_ . To define the exponential prior of the A values in MaxEnt models , an extra parameter α is used in the package during training . We keep the value of α as default .__label__Method|Code|Use
Details about these algorithms are in Section 6 . 2 of [ 14 ]. The high - level differences among these algorithms are best explained in the context of the disagreement region : OAC does importanceweighted querying of labels with an optimized query probability in the disagreement region , while using predicted labels outside ; IWAL0 and IWAL1 maintain a non - zero minimum query probability everywhere ; ORA - OAC , ORA - IWAL0 and ORA - IWAL1 query labels in their respective disagreement regions with probability 1 , using predicted labels otherwise . We implemented these algorithms in Vowpal Wabbit ( _CITE_ ), a fast learning system based on online convex optimization , using logistic regression as the ERM oracle . We performed experiments on 22 binary classification datasets with varying sizes ( 103 to 106 ) and diverse feature characteristics . Details about the datasets are in Appendix G . 1 of [ 14 ].__label__Method|Tool|Use
We thank Sonja Hopf from the Evolutionary and Functional Genomics group ( LMU Munich ) for valuable biological feedback . The work presented here was partially funded by the German Federal Ministry of Economy and Technology ( BMWi ) under the THESEUS project . Page 12 of 14 ( page number not for citation purposes ) BMC Bioinformatics 2008 , 9 : 207 _CITE___label__Supplement|Document|Introduce
We test its reliability on the WordNet sense inventory . Overall , the experimental results show high agreement , confirming our hypothesis that agreement at sense level might be higher than at the word level . The annotated sense inventory will be made publically available to other researchers at _CITE_ The remainder of this paper is organized as follows . Section 2 discusses previous related work .__label__Material|Data|Produce
The Reactome graph database is freely available at : https :// reactome . org / dev / graph - database . The API for the ContentService is available at _CITE_ with documentation and tutorials available at : https :// reactome . org / dev / content - service . The source code , in Java , is freely available at : https :// github . com / reactome ( See the graph - core , graphimporter and content - service repositories ). Future development will focus on updating the version of SDN and integrating interaction data from IntAct ( http :// www . ebi . ac . uk / intact /) directly to the Reactome graph database .__label__Supplement|Website|Use
